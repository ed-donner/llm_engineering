# Local RAG WebApp

A completely local Retrieval-Augmented Generation (RAG) web app for document Q&A, **specialized for AI/ML documentation and research**. Upload AI-related documents, research papers, tutorials, and more â€” ask questions and get accurate, properly formatted answers â€” no cloud required.

## âœ¨ Features

- **100% local**: data never leaves your machine
- **ğŸ¤– AI/ML specialization**: optimized for AI and machine learning content with 13 specialized categories (LLMs, Computer Vision, MLOps, AI Research, AI Agents & MCP, etc.)
- **Multi-format document support**: PDF, DOCX, PPTX, HTML, Images (with OCR*) â€” auto-converted to Markdown via Docling
- **AI-powered categorization**: automatic document categorization with confidence scores and subcategories
- **Category filtering**: browse and filter documents by categories in the UI
- **Smart RAG**: adaptive chunking + dynamic-k retrieval
- **Conversation memory**: LLM remembers last 5 turns (10 messages) for follow-up questions
- **Conversation history**: SQLite-based persistence with auto-generated AI titles
- **Streaming replies**: live token streaming over WebSocket
- **Sources panel**: see which documents informed each answer with clickable citations
- **Performance profiles**: Eco / Balanced / Performance
- **Modern UI**: Next.js 15 with dark/light theme support
- **Message persistence**: Keeps your prompts visible even when switching conversations during generation

_*OCR requires HuggingFace token - set `HF_TOKEN` environment variable to enable image processing_

## Simple Start (No Git Needed â€” for nonâ€‘experts)

No Git required: download the ZIP and run the oneâ€‘command script.
This is the easiest path for nonâ€‘experts on macOS.

For developers comfortable with Git and the command line. Please note that this section assumes familiarity with Git commands and the terminal.
- Docker Desktop: https://www.docker.com/products/docker-desktop/
- Ollama (Local LLM): https://ollama.ai/
- Node.js 18+ (LTS): https://nodejs.org/ or `brew install node`
- Python 3.9+ (macOS usually has `python3` preinstalled)

2) Download the app (ZIP)
- Open the project page in your browser: https://github.com/albertoclemente/local-rag-app
- Click the green â€œCodeâ€ button â†’ â€œDownload ZIPâ€
- Unzip it (doubleâ€‘click). Youâ€™ll get a folder like `local-rag-app-main`.

3) Start everything automatically (one command)
- Open Terminal and run these commands (adjust the path as needed):

```bash
cd ~/Downloads/local-rag-app-main
chmod +x scripts/quick_try.sh
./scripts/quick_try.sh
```

- When it finishes starting, open the UI: http://localhost:3000

4) If the oneâ€‘command script doesnâ€™t work, do it manually
- Start Qdrant (vector DB):
```bash
docker compose -f docker/docker-compose.yml up -d qdrant
```
- Start Ollama and pull a model:
```bash
ollama serve &
ollama pull qwen2.5:7b-instruct
```
- Start the backend (FastAPI):
```bash
cd backend
python3 -m venv .venv
source .venv/bin/activate
pip install -e .
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```
- Start the frontend (Next.js):
```bash
cd frontend
npm install
npm run dev
```
- Open the app: http://localhost:3000

5) Use it
- Click â€œUploadâ€ to add PDF/DOCX/TXT/MD/EPUB files
- Ask your question in the chat input
- Toggle the sources panel with the â€œiâ€ icon to see which docs were used

Tips
- First run may take a minute (model pull, first build). Refresh if the UI is blank.
- If ports are busy: stop other apps using 3000/8000/6333 or reboot Docker Desktop.
- To stop everything from the oneâ€‘command run, press Ctrl+C in the Terminal windows.

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Node.js 18+
- Docker (for Qdrant)
- Ollama (local LLM): https://ollama.ai/

### 1) Clone

```bash
git clone https://github.com/albertoclemente/local-rag-app.git
cd local-rag-app
```

### 2) Start vector database (Qdrant)

```bash
docker compose -f docker/docker-compose.yml up -d qdrant
```

### 3) Backend

```bash
cd backend
pip install -e .

# Start Ollama and pull a model (choose one)
ollama serve &
ollama pull qwen2.5:7b-instruct   # default here
# or: ollama pull llama3.1:8b

# Start FastAPI (reload for dev)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 4) Frontend

```bash
cd frontend
npm install
npm run dev
```

### 5) Open the app

- UI: http://localhost:3000
- API docs: http://localhost:8000/api/docs
- API health: http://localhost:8000/health

If the page doesn't load, give it a few seconds on first run and refresh.

## âš¡ Quick Try (one command)

Runs Qdrant (Docker), Backend (FastAPI), and Frontend (Next.js) for a quick local demo:

```bash
chmod +x scripts/quick_try.sh
./scripts/quick_try.sh
```

Notes:
- Requires Docker, Python, Node, and (ideally) Ollama installed.
- Press Ctrl+C in the terminal to stop both backend and frontend.

## ğŸ³ Docker (Frontend + Backend + Qdrant)

Run everything in Docker for a one-command start.

```bash
docker compose -f docker/docker-compose.yml --profile full up -d
```

Or use the helper that builds, starts, waits, and opens your browser automatically (macOS):

```bash
chmod +x scripts/docker_up_and_open.sh
./scripts/docker_up_and_open.sh
```

Services
- Frontend: http://localhost:3000
- Backend API docs: http://localhost:8000/api/docs
- Backend health: http://localhost:8000/health
- Qdrant: http://localhost:6333

Notes
- Ollama runs on your host at `http://localhost:11434`. Make sure itâ€™s serving and a model is pulled:
```bash
ollama serve &
ollama pull qwen2.5:7b-instruct
```
- On macOS/Windows, containers reach your host via `host.docker.internal` (preconfigured). On Linux, you may need to map the host gateway or expose Ollama differently.

Persistence
- Documents, parsed data, indices, and logs persist under `~/RAGApp` by default.
- In Docker, the backend binds `~/RAGApp` into the container (`/app/data`) and Qdrant binds `~/RAGApp/qdrant_data`.
- To change location, set `RAG_DATA_DIR` before `docker compose up`.

Warning
- Running `docker compose down -v` removes named volumes. With the new bind-mounts, data lives in your home folder and is not removed by `-v`. Do not delete `~/RAGApp` if you want to keep documents.

Migration
- If you previously used Docker with named volumes and want to migrate that data:
```bash
./scripts/migrate_docker_volumes.sh
```
- This copies old volume data into `~/RAGApp` so it persists with the new bind-mount setup.

## ğŸ§­ How To Use

### Upload documents
- Click the "Upload" button in the UI to add documents
- **Supported formats**: PDF, DOCX, PPTX, HTML, Images (PNG, JPG, TIFF, BMP), Markdown, AsciiDoc
- Documents are automatically converted to Markdown using Docling for better structure preservation
- **Image OCR**: To enable OCR for images and scanned PDFs, set the `HF_TOKEN` environment variable with your HuggingFace token
- **AI categorization**: Documents are automatically categorized by AI with confidence scores
- The status bar shows indexing progress; the Documents list updates to "indexed"

### Browse and filter documents
- **Category filtering**: Use the category filter in the Documents panel to browse by category
- **Search**: Use the search bar to find documents by name
- **Tag filtering**: Filter documents by tags
- **Category view**: Toggle "Group by Category" to organize documents by their categories
- **Category statistics**: View category distribution and document counts in the UI

### Ask questions
- Type queries in the chat input
- Responses render with **Markdown** and **KaTeX** (math supported: inline `$a^2+b^2=c^2$` or blocks with `$$...$$`)
- The LLM has **conversation memory** â€” it remembers your last 5 exchanges within the same session
- Ask follow-up questions naturally: "What about X?" or "Can you explain that differently?"

### Manage conversations
- **Conversation history**: All your conversations are saved in the left sidebar
- **Auto-generated titles**: Each conversation gets an AI-generated title based on the first exchange
- **Switch conversations**: Click any conversation to resume it â€” the LLM remembers the context
- **New chat**: Click the "+ New Chat" button to start a fresh conversation
- **Search**: Use the search bar in the sidebar to find past conversations
- **Delete**: Hover over a conversation and click the trash icon to delete it

### View sources
- Toggle the sources panel with the "i" icon in the header
- See which documents informed each answer with clickable citations
- Citations show the relevant text chunk and similarity score

### Theme
- The UI supports both **light and dark themes**
- All text, including chat messages and markdown content, is readable in both themes

### Model indicator
- The header shows the active LLM model name (default: Qwen 2.5 7B Instruct)

## ğŸ› ï¸ Configuration

Create `backend/.env` (values shown are sensible defaults):

```bash
# Performance profile
RAG_PROFILE=balanced   # eco|balanced|performance

# Data directory (default expands to ~/RAGApp)
RAG_DATA_DIR=~/RAGApp

# Services
QDRANT_URL=http://localhost:6333
OLLAMA_HOST=http://localhost:11434

# Models
RAG_LLM_MODEL=qwen2.5:7b-instruct  # or llama3.1:8b, etc.
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# OCR for images (optional - requires HuggingFace account)
HF_TOKEN=your_huggingface_token_here

# RAG parameters
RAG_CHUNK_SIZE=800
RAG_CHUNK_OVERLAP=200
RAG_MAX_CONTEXT_TOKENS=4000

# Debug logging
RAG_DEBUG=false
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Next.js UI    â”‚â—„â”€â”€â”‚  FastAPI + WS    â”‚â—„â”€â”€â”‚     Qdrant      â”‚
â”‚  (React/TypeScript)â”‚  â”‚   (Backend)      â”‚   â”‚  (Vector Store) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                      â”‚                       â”‚
                     â”‚                      â–¼                       â”‚
                     â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                     â”‚             â”‚     Ollama      â”‚              â”‚
                     â”‚             â”‚   (Local LLM)   â”‚              â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- Frontend: Next.js (React + TypeScript)
- Backend: FastAPI + WebSocket streaming
- Vector store: Qdrant (local)
- LLM: Ollama (qwen2.5, llama3.x, etc.)
- Embeddings: Sentence Transformers (local)

## ğŸ“ Project Structure

```
RAG_APP/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app entry
â”‚   â”‚   â”œâ”€â”€ api_complete.py         # REST endpoints (/api/status, /api/query, etc.)
â”‚   â”‚   â”œâ”€â”€ ws.py                   # WebSocket streaming
â”‚   â”‚   â”œâ”€â”€ models.py               # Pydantic models
â”‚   â”‚   â”œâ”€â”€ settings.py             # Configuration
â”‚   â”‚   â”œâ”€â”€ storage.py              # File operations (uploads, parsed)
â”‚   â”‚   â”œâ”€â”€ chunking.py             # Adaptive chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py           # Local embeddings
â”‚   â”‚   â”œâ”€â”€ qdrant_index.py         # Vector store operations
â”‚   â”‚   â”œâ”€â”€ retrieval.py            # Retrieval logic
â”‚   â”‚   â”œâ”€â”€ llm.py                  # LLM service
â”‚   â”‚   â”œâ”€â”€ markdown_converter.py   # Docling integration for document conversion
â”‚   â”‚   â”œâ”€â”€ categorization.py       # AI-powered document categorization
â”‚   â”‚   â”œâ”€â”€ conversation.py         # Conversation context management
â”‚   â”‚   â””â”€â”€ conversation_storage.py # SQLite-based conversation persistence
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ conversations.db        # SQLite database for chat history
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ app-header.tsx           # Header with theme toggle
â”‚       â”‚   â”œâ”€â”€ chat-view.tsx            # Main chat interface
â”‚       â”‚   â”œâ”€â”€ conversation-history.tsx # Conversation sidebar
â”‚       â”‚   â”œâ”€â”€ documents-panel.tsx      # Documents management with category filtering
â”‚       â”‚   â”œâ”€â”€ category-badge.tsx       # Category display and filtering components
â”‚       â”‚   â””â”€â”€ status-bar.tsx           # Status indicator
â”‚       â”œâ”€â”€ hooks/
â”‚       â”‚   â””â”€â”€ api.ts                   # React Query hooks
â”‚       â”œâ”€â”€ lib/
â”‚       â”‚   â”œâ”€â”€ api.ts                   # API client
â”‚       â”‚   â””â”€â”€ constants.ts             # Configuration
â”‚       â””â”€â”€ types/
â”‚           â””â”€â”€ index.ts                 # TypeScript types
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Qdrant and optional full stack
â””â”€â”€ README.md
```

## ğŸ“Š Performance Profiles

| Profile | CPU Usage | RAM Usage | Accuracy | Best For |
|--------:|-----------|-----------|----------|----------|
| Eco | Low | ~2GB | Good | Battery life, older laptops |
| Balanced | Medium | ~4GB | Better | Most users |
| Performance | High | ~8GB | Best | Powerful machines |

Set via env var:

```bash
export RAG_PROFILE=balanced
```

## ğŸ”§ Development

### Backend

```bash
cd backend
pip install -e ".[dev]"

# Run tests
pytest

# Lint/format
black app/ tests/
isort app/ tests/
mypy app/

# Start API with auto-reload
uvicorn app.main:app --reload --port 8000
```

### Frontend

```bash
cd frontend
npm install
npm run dev   # http://localhost:3000

# Build
npm run build
```

## ï¿½ Troubleshooting

**Qdrant connection failed**

```bash
curl http://localhost:6333/health
docker compose -f docker/docker-compose.yml restart qdrant
```

**Ollama model not found**

```bash
ollama list
ollama pull qwen2.5:7b-instruct
```

**Images not being processed / OCR not working**

- Set the `HF_TOKEN` environment variable with your HuggingFace token
- Create a token at https://huggingface.co/settings/tokens
- Add to `backend/.env`: `HF_TOKEN=your_token_here`
- Restart the backend server

**Frontend shows timeout (~30s) or slow status**

- Ensure both services are running (http://localhost:3000 and http://localhost:8000)
- Check `/api/status`: `curl http://localhost:8000/api/status`
- Make sure Ollama is serving and `RAG_LLM_MODEL` matches a pulled model
- Status health checks are capped to ~2s; if still slow, verify Qdrant and Ollama

**Out of memory errors**

```bash
export RAG_PROFILE=eco
export RAG_MAX_CONTEXT_TOKENS=2000
```

### Logs & Data

Application data (uploads, parsed, indices, logs) lives under `~/RAGApp/` by default.
Logs are written to `~/RAGApp/logs/app.jsonl`.

## ğŸ”’ Security & Privacy

- Local processing: All operations happen on your machine
- No external cloud calls: Works fully offline (Ollama + Qdrant local)
- Optional encryption at rest and secure deletion supported
- No telemetry or tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/my-change`)
3. Commit (`git commit -m "Describe your change"`)
4. Push (`git push origin feature/my-change`)
5. Open a Pull Request

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).

## ğŸ™ Acknowledgments

- Qdrant â€” Vector similarity search
- Ollama â€” Local LLM runtime
- FastAPI â€” Python web framework
- Next.js â€” React framework
- Sentence Transformers â€” Embeddings

---

Built with privacy in mind â€” your documents stay local.
