{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac552e4",
   "metadata": {},
   "source": [
    "# Research Paper Summarizer for Literature Surveys\n",
    "\n",
    "Doing a literature survey means going through dozens of research papers to find which ones are actually relevant to your topic — this can take hours or even days of reading. This tool automates that process by analyzing each PDF, extracting key information, and generating a structured Excel table with a relevance score and color coding so you can instantly see which papers are worth reading in depth.\n",
    "\n",
    "## What this tool does\n",
    "- Reads all PDF research papers from a folder\n",
    "- Extracts key fields: objective, methodology, validation method, standards, limitations and more\n",
    "- Scores each paper by relevance to your research topic (1-10)\n",
    "- Generates a color coded Excel table — green (highly relevant), yellow (moderate), red (low relevance)\n",
    "- Sorts papers from most to least relevant automatically\n",
    "\n",
    "## How to Use\n",
    "1. Add your PDF papers to a folder called `papers/` in the same directory as this notebook\n",
    "2. Update the `research_topic` variable below with your research area\n",
    "3. Run all cells\n",
    "4. Check the output Excel file generated in the same directory\n",
    "\n",
    "## Requirements\n",
    "- OpenAI API key in a `.env` file\n",
    "- Install dependencies: `uv pip install pdfplumber pandas openpyxl openai python-dotenv`\n",
    "\n",
    "---\n",
    "*Built as a Day 1 extension project for the Udemy course: AI Engineer Core Track — LLM Engineering, RAG, QLoRA, Agents by Ed Donner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2e808",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "Run the cell below before running the rest of the notebook:\n",
    "- `pdfplumber` — extracts text from PDF files\n",
    "- `pandas` — creates and manipulates the data table\n",
    "- `openpyxl` — creates and formats the Excel output file\n",
    "- `openai` — connects to the OpenAI API to analyze papers\n",
    "- `python-dotenv` — securely loads your OpenAI API key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pdfplumber pandas openpyxl openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Third party imports\n",
    "import pdfplumber          # extracts text from PDF files\n",
    "import pandas as pd        # creates and manipulates the data table\n",
    "from openai import OpenAI  # connects to the OpenAI API\n",
    "from openpyxl import load_workbook                    # opens and edits Excel files\n",
    "from openpyxl.styles import PatternFill, Alignment    # formats Excel cells\n",
    "from dotenv import load_dotenv                        # loads API key from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453deba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI API key securely from the .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client — this is what we use to make API calls throughout the notebook\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e898d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS TO YOUR RESEARCH TOPIC\n",
    "\n",
    "research_topic = \"\"\"\n",
    "UAV intelligence quality assurance, standards, and validation methods. \n",
    "Topics of interest include: UAV system reliability, fault detection, \n",
    "testing frameworks, quality standards, validation methodologies, \n",
    "and intelligent UAV systems.\n",
    "\"\"\"\n",
    "\n",
    "# System prompt instructs the LLM how to behave and what to extract\n",
    "# We pass the research topic so it can judge relevance accurately\n",
    "# The LLM is asked to respond in JSON so we can parse it into a structured table\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a research assistant helping with a literature survey on this topic: {research_topic}\n",
    "\n",
    "Extract the following fields from the research paper and respond ONLY in valid JSON format with these exact keys:\n",
    "{{\n",
    "    \"Year\": \"Look carefully for the publication year in the copyright notice, journal header, submission date, or first page. Return only the 4-digit year. If truly not found, write Unknown\",\n",
    "    \"Paper Title\": \"\",\n",
    "    \"Authors\": \"\",\n",
    "    \"Application Domain\": \"\",\n",
    "    \"AI / Intelligence Component\": \"\",\n",
    "    \"Objective\": \"\",\n",
    "    \"Validation Method\": \"\",\n",
    "    \"Test Environment\": \"\",\n",
    "    \"Evaluation Metrics\": \"\",\n",
    "    \"Robustness / Safety Testing\": \"\",\n",
    "    \"Standards Mentioned\": \"\",\n",
    "    \"Standards Body Referenced\": \"\",\n",
    "    \"Limitations\": \"\",\n",
    "    \"Is Relevant\": \"Yes or No only\",\n",
    "    \"Relevance Score\": \"Rate strictly from 1 to 10 based on how directly the paper addresses UAV intelligence quality assurance, validation standards, or certification methods. 9-10: paper directly addresses UAV QA frameworks, validation standards, or certification as its PRIMARY contribution. 7-8: paper covers UAV fault detection, reliability, or safety testing but QA/standards is not the main focus. 5-6: paper uses UAVs as a tool for another application like inspection, agriculture, or mapping with minimal QA focus. 1-4: paper has little or no connection to UAV QA or validation standards.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Extracts all text from every page of the PDF\n",
    "# If your model has a smaller context window, limit the text by adding: return text[:15000]\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    return text  \n",
    "\n",
    "# Sends the extracted PDF text to OpenAI and gets back structured JSON\n",
    "# The JSON is then parsed into a Python dictionary for easy table conversion\n",
    "def analyze_paper(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is the research paper:\\n\\n{text}\"}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",   # fastest and cheapest GPT-5 model, great for summarization\n",
    "        messages=messages\n",
    "    )\n",
    "    raw = response.choices[0].message.content\n",
    "    # Strip any markdown formatting the model may have added around the JSON\n",
    "    raw = raw.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    return json.loads(raw)\n",
    "\n",
    "\n",
    "# CHANGE THIS TO YOUR PAPERS FOLDER PATH\n",
    "# Place all your PDF research papers inside this folder\n",
    "\n",
    "papers_folder = \"papers/\"\n",
    "results = []\n",
    "\n",
    "# Loop through all PDFs in the folder and analyze each one\n",
    "for filename in sorted(os.listdir(papers_folder)):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(f\"Processing: {filename}\")\n",
    "        pdf_path = os.path.join(papers_folder, filename)\n",
    "        try:\n",
    "            data = analyze_paper(pdf_path)\n",
    "            data[\"Filename\"] = filename  # track which PDF each row came from\n",
    "            results.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Standardize is_relevant to always be Yes or No\n",
    "def standardize_relevant(val):\n",
    "    if str(val).lower() in [\"true\", \"yes\", \"1\"]:\n",
    "        return \"Yes\"\n",
    "    elif str(val).lower() in [\"false\", \"no\", \"0\"]:\n",
    "        return \"No\"\n",
    "    return \"Yes\"\n",
    "\n",
    "df[\"Is Relevant\"] = df[\"Is Relevant\"].apply(standardize_relevant)\n",
    "\n",
    "# Standardize relevance_score to be out of 10\n",
    "# The LLM sometimes returns 0-1 scale instead of 1-10 so we normalize it\n",
    "\n",
    "def standardize_score(val):\n",
    "    try:\n",
    "        score = float(val)\n",
    "        if score <= 1.0:  # convert 0-1 scale to 0-10\n",
    "            return round(score * 10, 1)\n",
    "        return round(score, 1)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df[\"Relevance Score\"] = df[\"Relevance Score\"].apply(standardize_score)\n",
    "\n",
    "# Sort papers from most relevant to least relevant\n",
    "df = df.sort_values(\"Relevance Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = \"literature_survey_output.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Apply color coding and formatting to the Excel file\n",
    "wb = load_workbook(output_file)\n",
    "ws = wb.active\n",
    "\n",
    "# Color definitions: green = highly relevant, yellow = moderate, red = low relevance\n",
    "green  = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\n",
    "yellow = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "red    = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "\n",
    "headers = [cell.value for cell in ws[1]]\n",
    "score_col = headers.index(\"Relevance Score\") + 1\n",
    "\n",
    "# Keep header row white and uncolored\n",
    "for cell in ws[1]:\n",
    "    cell.fill = PatternFill(fill_type=None)\n",
    "\n",
    "# Color code each data row based on its relevance score\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
    "    score_cell = row[score_col - 1]\n",
    "    try:\n",
    "        score = float(score_cell.value)\n",
    "        if score >= 8:\n",
    "            fill = green\n",
    "        elif score >= 6:\n",
    "            fill = yellow\n",
    "        else:\n",
    "            fill = red\n",
    "    except:\n",
    "        fill = yellow\n",
    "    for cell in row:\n",
    "        cell.fill = fill\n",
    "\n",
    "# Wrap text and align content to top for better readability in Excel\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
    "    for cell in row:\n",
    "        cell.alignment = Alignment(wrap_text=True, vertical=\"top\")\n",
    "\n",
    "# Auto-fit column widths based on the longest content in each column\n",
    "# Short columns stay compact, long columns are capped at width 60\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    col_letter = col[0].column_letter\n",
    "    for cell in col:\n",
    "        try:\n",
    "            if cell.value:\n",
    "                cell_length = len(str(cell.value))\n",
    "                if cell_length > max_length:\n",
    "                    max_length = cell_length\n",
    "        except:\n",
    "            pass\n",
    "    if max_length < 15:\n",
    "        adjusted_width = max_length + 4\n",
    "    elif max_length < 50:\n",
    "        adjusted_width = max_length + 2\n",
    "    else:\n",
    "        adjusted_width = 60\n",
    "    ws.column_dimensions[col_letter].width = adjusted_width\n",
    "\n",
    "# Save the final formatted Excel file\n",
    "wb.save(output_file)\n",
    "print(f\"Done! Check {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
