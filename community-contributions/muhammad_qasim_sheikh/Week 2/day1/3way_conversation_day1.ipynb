{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59206dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad035727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keys\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key = 'ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f521334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SYSTEM PROMPTS ----\n",
    "athena_system = \"\"\"\n",
    "You are Athena, a strategic thinker and visionary. You seek meaning, long-term implications,\n",
    "and practical wisdom in every discussion. Be concise (1-2 sentences).\n",
    "\"\"\"\n",
    "\n",
    "loki_system = \"\"\"\n",
    "You are Loki, a sarcastic trickster who mocks and challenges everyone else's opinions.\n",
    "You use humor, wit, and irony to undermine serious arguments. Be concise (1-2 sentences).\n",
    "\"\"\"\n",
    "\n",
    "orion_system = \"\"\"\n",
    "You are Orion, a data-driven realist. You respond with evidence, statistics, or factual analysis.\n",
    "If data is not available, make a logical deduction. Be concise (1-2 sentences).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- INITIAL CONVERSATION ----\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"name\": \"Athena\", \"content\": athena_system},\n",
    "    {\"role\": \"system\", \"name\": \"Loki\", \"content\": loki_system},\n",
    "    {\"role\": \"system\", \"name\": \"Orion\", \"content\": orion_system},\n",
    "    {\"role\": \"user\", \"content\": \"Topic: 'Why did the chicken cross the road?' Begin your discussion.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- HELPER FUNCTIONS ----\n",
    "def call_gpt(name, system_prompt, conversation):\n",
    "    \"\"\"Call GPT model with current conversation context.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    messages += [{\"role\": \"user\", \"content\": f\"The conversation so far:\\n{format_conversation(conversation)}\\nNow respond as {name}.\"}]\n",
    "    resp = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def call_ollama(name, system_prompt, conversation):\n",
    "    \"\"\"Call Ollama (Llama3.2) as a local model.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    messages += [{\"role\": \"user\", \"content\": f\"The conversation so far:\\n{format_conversation(conversation)}\\nNow respond as {name}.\"}]\n",
    "    resp = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "    return resp['message']['content'].strip()\n",
    "\n",
    "def format_conversation(conv):\n",
    "    return \"\\n\".join([f\"{m.get('name', m['role']).upper()}: {m['content']}\" for m in conv if m['role'] != \"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MAIN LOOP ----\n",
    "rounds = 5\n",
    "for i in range(rounds):\n",
    "    # Athena responds\n",
    "    athena_reply = call_gpt(\"Athena\", athena_system, conversation)\n",
    "    conversation.append({\"role\": \"assistant\", \"name\": \"Athena\", \"content\": athena_reply})\n",
    "    display(Markdown(f\"**Athena:** {athena_reply}\"))\n",
    "\n",
    "    # Loki responds\n",
    "    loki_reply = call_ollama(\"Loki\", loki_system, conversation)\n",
    "    conversation.append({\"role\": \"assistant\", \"name\": \"Loki\", \"content\": loki_reply})\n",
    "    display(Markdown(f\"**Loki:** {loki_reply}\"))\n",
    "\n",
    "    # Orion responds\n",
    "    orion_reply = call_gpt(\"Orion\", orion_system, conversation)\n",
    "    conversation.append({\"role\": \"assistant\", \"name\": \"Orion\", \"content\": orion_reply})\n",
    "    display(Markdown(f\"**Orion:** {orion_reply}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
