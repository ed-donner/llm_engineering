{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eca2b81",
   "metadata": {},
   "source": [
    "## Day 1 Exercise of week 2 by Venkata Narayana reddy Tumu\n",
    "\n",
    "### Here i am using OpenRouter API to call OpenAI sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With this api call we can get the list of models available in openrouter \n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/models\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer <token>\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861236d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this peice of code we are going to fetch all the models from open router \n",
    "# and try to get the response from each model\n",
    "# we are segregate with free vs paid models\n",
    "# keep this for reference\n",
    "\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=api_key,\n",
    ")\n",
    "\n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": message}]\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/models\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "# Fetch the models\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "# Extract only the IDs into a simple list\n",
    "model_ids = [model['id'] for model in data.get('data', [])]\n",
    "\n",
    "# Create the final object\n",
    "result = {\"model_ids\": model_ids}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through the model IDs\n",
    "for model_id in model_ids:\n",
    "    try:\n",
    "        # Attempt to call the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id, \n",
    "            messages=messages,\n",
    "            timeout=15  # Recommended to avoid hanging on slow models\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        print(f\"✅ Working: {model_id}\")\n",
    "        \n",
    "        # Store success record\n",
    "        results.append({\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": \"success\",\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch any error (API errors, timeouts, etc.)\n",
    "        print(f\"❌ Error: {model_id} -> {str(e)}\")\n",
    "        \n",
    "        # Store error record\n",
    "        results.append({\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e)\n",
    "        })\n",
    "\n",
    "# Summary lists\n",
    "working_models = [r['model_id'] for r in results if r['status'] == 'success']\n",
    "failed_models = [r['model_id'] for r in results if r['status'] == 'error']\n",
    "\n",
    "print(\"\\n--- FINAL SUMMARY ---\")\n",
    "print(f\"Working Models: {working_models}\")\n",
    "print(f\"Failed Models: {failed_models}\")\n",
    "\n",
    "# Detailed view (interactive in Jupyter)\n",
    "from IPython.display import JSON\n",
    "\n",
    "\n",
    "JSON(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa46aa-59cc-40f9-a9ee-0a4824bdd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fa54e-dbfa-4031-aa87-ebc00b7aa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "print(openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1883a8-f07c-4773-a5a1-3aa95884fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openrouter = OpenAI(base_url='https://openrouter.ai/api/v1', api_key=openrouter_api_key)\n",
    "gpt_model = \"openai/gpt-oss-120b\"\n",
    "deepseek_model = \"deepseek/deepseek-v3.2\"\n",
    "gemma_model = \"google/gemma-3-27b-it\"\n",
    "\n",
    "ALEX_SYSTEM = \"\"\"You are Alex, a chatbot who is very argumentative;\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\"\"\"\n",
    "\n",
    "BLAKE_SYSTEM = \"\"\"You are Blake, a very polite, courteous chatbot.\n",
    "You try to agree with everything the other person says, or find common ground.\n",
    "If the other person is argumentative, you try to calm them down and keep chatting.\n",
    "You are in a conversation with Alex and Charlie.\"\"\"\n",
    "\n",
    "CHARLIE_SYSTEM = \"\"\"You are Charlie.\n",
    "You are thoughtful, concise, and practical. You ask clarifying questions and propose next steps.\n",
    "You are in a conversation with Alex and Blake.\"\"\"\n",
    "\n",
    "\n",
    "# Map personas -> which model will play them\n",
    "CAST = {\n",
    "    \"Alex\":   {\"model\": gpt_model,      \"system\": ALEX_SYSTEM},\n",
    "    \"Blake\":  {\"model\": deepseek_model, \"system\": BLAKE_SYSTEM},\n",
    "    \"Charlie\":{\"model\": gemma_model,    \"system\": CHARLIE_SYSTEM},\n",
    "}\n",
    "\n",
    "def format_transcript(history):\n",
    "    # history: list of dicts: {\"speaker\": \"...\", \"text\": \"...\"}]\n",
    "    # print(\"\\n\".join([f'{m[\"speaker\"]}: {m[\"text\"]}' for m in history]))\n",
    "    return \"\\n\".join([f'{m[\"speaker\"]}: {m[\"text\"]}' for m in history])\n",
    "\n",
    "def ask_model(model, system_prompt, speaker_name, transcript):\n",
    "    # print(model)\n",
    "    user_prompt = f\"\"\"You are {speaker_name}, in conversation with the others.\n",
    "The conversation so far is as follows:\n",
    "{transcript}\n",
    "\n",
    "Now respond with what you would like to say next, as {speaker_name}.\n",
    "Only output your message text (no speaker label).\"\"\"\n",
    "\n",
    "    resp = openrouter.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def run_conversation(seed_history, order=(\"Alex\", \"Blake\", \"Charlie\"), rounds=3):\n",
    "    history = seed_history[:]\n",
    "    for _ in range(rounds):\n",
    "        for speaker in order:\n",
    "            transcript = format_transcript(history)\n",
    "            cfg = CAST[speaker] #nested dictionary check the case with speaker name\n",
    "            text = ask_model(\n",
    "                model=cfg[\"model\"],\n",
    "                system_prompt=cfg[\"system\"],\n",
    "                speaker_name=speaker,\n",
    "                transcript=transcript,\n",
    "            )\n",
    "            history.append({\"speaker\": speaker, \"text\": text})\n",
    "    return history\n",
    "\n",
    "\n",
    "# ---- Seed the conversation using your starting lines ----\n",
    "history = [\n",
    "    {\"speaker\": \"Blake\", \"text\": \"Hi there\"},\n",
    "    {\"speaker\": \"Charlie\", \"text\": \"Hi\"},\n",
    "    {\"speaker\": \"Alex\", \"text\": \"what's up\"},\n",
    "]\n",
    "\n",
    "final_history = run_conversation(history, rounds=4)\n",
    "\n",
    "print(format_transcript(final_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7cdd5-139b-4237-bba2-675ae8ef673d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
