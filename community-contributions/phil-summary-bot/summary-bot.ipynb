{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd30a1b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20b9438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "from scrap import fetch_article\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd4c04",
   "metadata": {},
   "source": [
    "# Constants & Switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aecc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = \"gpt-5-nano\"       # Cloud model\n",
    "MODEL_LLAMA = \"llama3.2\"       # Local Ollama\n",
    "USE_LOCAL_MODEL = False         # True = Ollama, False = Cloud GPT/OpenRouter\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212ad2c",
   "metadata": {},
   "source": [
    "#### detect which api-keys detects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6665a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenRouter API key detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    if api_key.startswith(\"sk-or-\") and len(api_key) > 10:\n",
    "        print(\"OpenRouter API key detected\")\n",
    "        BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "    elif api_key.startswith(\"sk-proj-\") and len(api_key) > 10:\n",
    "        print(\"OpenAI Project API key detected\")\n",
    "        BASE_URL = None\n",
    "    else:\n",
    "        print(\"API key format not recognized\")\n",
    "        BASE_URL = None\n",
    "else:\n",
    "    print(\"No API key found\")\n",
    "    BASE_URL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e71fa470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cloud client (OpenRouter/OpenAI)\n",
    "cloud_client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=BASE_URL\n",
    ") if not USE_LOCAL_MODEL else None\n",
    "\n",
    "# Ollama client (Local)\n",
    "ollama_client = OpenAI(\n",
    "    api_key=\"ollama\",\n",
    "    base_url=OLLAMA_BASE_URL\n",
    ") if USE_LOCAL_MODEL else None\n",
    "\n",
    "def get_client():\n",
    "    if USE_LOCAL_MODEL:\n",
    "        return ollama_client\n",
    "    else:\n",
    "        return cloud_client\n",
    "\n",
    "def get_model():\n",
    "    return MODEL_LLAMA if USE_LOCAL_MODEL else MODEL_GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590babd",
   "metadata": {},
   "source": [
    "## Client switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ff96174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask a technical question and get a clear explanation from the selected LLM.\n",
    "    \"\"\"\n",
    "    client = get_client()\n",
    "    model = get_model()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "You are an expert AI tutor.\n",
    "\n",
    "Explain technical concepts clearly for a developer.\n",
    "- Step-by-step explanation\n",
    "- Use simple language\n",
    "- Include examples if needed\n",
    "- Avoid jargon unless explained\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    explanation = response.choices[0].message.content\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "630e280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question: str):\n",
    "    answer = explain_question(question)\n",
    "    display(Markdown(answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8c2f3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Retrieval Augmented Generation (RAG) is a way to build a question-answering system that combines two ideas: look up documents (retrieval) and then write an answer using those documents (generation).\n",
       "\n",
       "Step-by-step, at a high level:\n",
       "\n",
       "- Step 1: Build a knowledge store\n",
       "  - Collect a set of documents you want the system to know about (manuals, articles, PDFs, web pages).\n",
       "  - Break them into chunks (e.g., 200–500 words per chunk) so they’re easy to search.\n",
       "\n",
       "- Step 2: Create a retriever\n",
       "  - Turn each doc chunk into a numerical vector (an “embedding”) that captures its meaning.\n",
       "  - Put all chunks into a vector store or index (examples: FAISS, Pinecone, or a simple database with embeddings).\n",
       "  - The retriever’s job is to take a question and find the most relevant chunks.\n",
       "\n",
       "- Step 3: When a user asks a question\n",
       "  - Convert the question into a query vector (same method used for docs).\n",
       "  - Retrieve the top-K most relevant doc chunks from the vector store.\n",
       "\n",
       "- Step 4: Generate an answer\n",
       "  - Feed the question and the retrieved chunks to a language model (the “generator”).\n",
       "  - The model writes an answer that borrows information from the retrieved chunks, rather than relying only on what it was trained on.\n",
       "\n",
       "- Step 5: (Optional) refine or re-rank\n",
       "  - You can re-rank the candidate answers or use a separate step to merge info from multiple chunks more cleanly.\n",
       "\n",
       "- Step 6: Deliver the result\n",
       "  - Return the answer to the user, and optionally show which docs were used as sources.\n",
       "\n",
       "Two common flavors you’ll hear about:\n",
       "\n",
       "- RAG-Token\n",
       "  - The model can switch between retrieved chunks while generating each token.\n",
       "  - It may look up different documents as it writes the answer.\n",
       "\n",
       "- RAG-Sequence\n",
       "  - The model generates a whole answer conditioned on the set of retrieved chunks.\n",
       "  - The retrieval is done first, then the generator produces the final text.\n",
       "\n",
       "A simple example\n",
       "\n",
       "- You have a knowledge store with product manuals and FAQs.\n",
       "- A user asks: “How do I reset my device?”\n",
       "- The retriever pulls the most relevant chunks from the manuals (e.g., “to reset, hold the power button for 10 seconds,” etc.).\n",
       "- The generator writes an answer like: “To reset your device, press and hold the power button for 10 seconds until the screen goes off, then release and press it again to power on. If it still doesn’t start, try…”\n",
       "- The answer is grounded in the retrieved docs, not just invented.\n",
       "\n",
       "Why use RAG?\n",
       "\n",
       "- Keeps answers grounded in real documents rather than just what the model “thinks.”\n",
       "- Can use up-to-date or domain-specific knowledge by updating the doc store.\n",
       "- Can handle long or technical information by pulling only the relevant chunks as context.\n",
       "\n",
       "What to watch out for\n",
       "\n",
       "- Quality of the document store matters: wrong or outdated docs lead to wrong answers.\n",
       "- Retrieval can fail if the vectors aren’t good or the index isn’t well-tuned.\n",
       "- There’s still potential for errors or misinterpretation if the generator over-weights noisy chunks.\n",
       "- Costs: embedding, indexing, and running the generator can be heavier than plain generation.\n",
       "\n",
       "Common components you’ll use\n",
       "\n",
       "- Document store: your collection of docs chunked for search.\n",
       "- Retriever: dense (neural embeddings) or sparse (BM25); can be hybrid.\n",
       "- Generator: a language model (e.g., a seq-to-seq model or a large LM) that takes the question and retrieved docs as input.\n",
       "- Optional: a re-ranker, metadata filters, or a librarian-like module to verify sources.\n",
       "\n",
       "A tiny mental model\n",
       "\n",
       "- Think of the system as two parts: a smart librarian (the retriever) and a careful writer (the generator). The librarian fetches relevant passages, and the writer composes an answer that cites those passages.\n",
       "\n",
       "If you want, I can tailor this to a particular stack (e.g., Haystack/LangChain, FAISS, or Pinecone) and sketch a minimal implementation plan."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    question = \"Explain Retrieval Augmented Generation (RAG) in simple terms\"\n",
    "    ask_llm(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
