{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb2a9be",
   "metadata": {},
   "source": [
    "### End of week 1 exercise\n",
    "\n",
    "This notebook demonstrates how one can use OpenAI API with openrouter to help the user understnad a code snipet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25b816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75d7c0",
   "metadata": {},
   "source": [
    "##### Authorize and Initialize the openai client use the access token stored in .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2920db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"openai/gpt-4o-mini\"\n",
    "\n",
    "openai = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "304d1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message =r\"\"\"I am trying to understand this code snippet. Please briefly exaplain what is doing and only highlight the parts that are important to understand the overall code\n",
    "```\n",
    " def create_model(self, model_args: dict[str, Any]):\n",
    "        payload = {\n",
    "            \"config\": {\n",
    "                \"batchSize\": model_args.get(\"batch\", -1),\n",
    "                \"epochs\": model_args.get(\"epochs\", 300),\n",
    "                \"imageSize\": model_args.get(\"imgsz\", 640),\n",
    "                \"patience\": model_args.get(\"patience\", 100),\n",
    "                \"device\": str(model_args.get(\"device\", \"\")),  # convert None to string\n",
    "                \"cache\": str(model_args.get(\"cache\", \"ram\")),  # convert True, False, None to string\n",
    "            },\n",
    "            \"dataset\": {\"name\": model_args.get(\"data\")},\n",
    "            \"lineage\": {\n",
    "                \"architecture\": {\"name\": self.filename.replace(\".pt\", \"\").replace(\".yaml\", \"\")},\n",
    "                \"parent\": {},\n",
    "            },\n",
    "            \"meta\": {\"name\": self.filename},\n",
    "        }\n",
    "\n",
    "        if self.filename.endswith(\".pt\"):\n",
    "            payload[\"lineage\"][\"parent\"][\"name\"] = self.filename\n",
    "\n",
    "        self.model.create_model(payload)\n",
    "\n",
    "        if not self.model.id:\n",
    "            raise HUBModelError(f\"‚ùå Failed to create model '{self.filename}' on Ultralytics HUB. Please try again.\")\n",
    "\n",
    "        self.model_url = f\"{HUB_WEB_ROOT}/models/{self.model.id}\"\n",
    "\n",
    "        # Start heartbeats for HUB to monitor agent\n",
    "        self.model.start_heartbeat(self.rate_limits[\"heartbeat\"])\n",
    "\n",
    "        LOGGER.info(f\"{PREFIX}View model at {self.model_url} üöÄ\")\n",
    "```\n",
    "\"\"\"\n",
    "llm_response = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ca8a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can explain code snippets without rewriting code. User parts of it like function names and varibales for explanation\"},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97b83f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(conversation=conversation, model=MODEL):\n",
    "    \"\"\"\n",
    "    This function takes a conversation and a model and returns the response from the LLM. It creates a context for the conversation\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=conversation,\n",
    "    )\n",
    "    conversation.append(\n",
    "        {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f56ad3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_llm(conversation=conversation, model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62736731",
   "metadata": {},
   "source": [
    "#### Pretty prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0829573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet defines a method named **create_model** that is responsible for creating a model based on configuration and data provided in the **model_args** dictionary. Here‚Äôs a breakdown of the key components:\n",
       "\n",
       "1. **payload Construction**: \n",
       "   - A **payload** dictionary is created which organizes the configuration settings necessary for model creation. The settings include:\n",
       "     - **batchSize**: Obtained from **model_args** with a default value of -1.\n",
       "     - **epochs**: The number of training epochs, defaulting to 300.\n",
       "     - **imageSize**: The size of the images for input, defaulting to 640.\n",
       "     - **patience**: A term often used in training that might refer to early stopping thresholds, defaulting to 100.\n",
       "     - **device**: Specifies the computational device (like GPU), converted to a string.\n",
       "     - **cache**: The caching method, also converted to a string.\n",
       "\n",
       "2. **dataset**: \n",
       "   - It extracts the dataset name from **model_args**.\n",
       "\n",
       "3. **lineage**:\n",
       "   - Includes architecture information based on the **filename**, which is modified to remove extensions (.pt or .yaml).\n",
       "   - Sets a **parent** dictionary initialized as empty.\n",
       "\n",
       "4. **Model ID Check**:\n",
       "   - After calling **self.model.create_model(payload)** to initiate the model creation with the constructed **payload**, there is a check to see if **self.model.id** was successfully set. If not, it raises a **HUBModelError** indicating a failure in creating the model.\n",
       "\n",
       "5. **Model URL**:\n",
       "   - If the model is successfully created, a URL is constructed to access the model using **self.model.id** and the global **HUB_WEB_ROOT** variable.\n",
       "\n",
       "6. **Heartbeats**:\n",
       "   - The method starts a heartbeat process to keep the model monitored on a platform called \"HUB\". This is done through **self.model.start_heartbeat**, utilizing a rate limit defined by **self.rate_limits[\"heartbeat\"]**.\n",
       "\n",
       "7. **Logging**:\n",
       "   - Finally, a log entry is created to inform about the model's viewable URL.\n",
       "\n",
       "Overall, this method organizes various parameters for model creation, invokes the model creation function, handles the result, and sets up monitoring and logging for the created model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
