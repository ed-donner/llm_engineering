{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db38793",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Load tokens/keys once so the UI fails fast with a clear error instead of hanging mid-request.\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# The judge should be stable and reasonably strong; this is the one that decides the winner.\n",
    "OPENAI_JUDGE_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Nano is a third contender generator; it competes against local Llama and HF free.\n",
    "OPENAI_NANO_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "# Local generator model served by Ollama; keep the tag explicit to avoid accidental model drift.\n",
    "LLAMA_MODEL = \"llama3.1:8b\"\n",
    "\n",
    "# Free remote generator via HF router; this is server-side inference with quota/rate limits.\n",
    "HF_FREE_CHAT_MODEL = \"HuggingFaceTB/SmolLM3-3B:hf-inference\"\n",
    "HF_ROUTER_BASE_URL = \"https://router.huggingface.co/v1\"\n",
    "\n",
    "# Two clients: OpenAI for judge + nano, HF router client for the free contender.\n",
    "openai = OpenAI()\n",
    "hf_client = OpenAI(base_url=HF_ROUTER_BASE_URL, api_key=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d19581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_code_extractor(path: str) -> str:\n",
    "    # Read the .ipynb as JSON and extract only code cells to avoid markdown noise in the prompt.\n",
    "    nb = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Preserve cell order to keep context consistent with how the notebook is actually executed.\n",
    "    parts = []\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "\n",
    "        # Join the source array into a single string per cell; .ipynb stores it line-by-line.\n",
    "        parts.append(\"\".join(cell.get(\"source\", [])))\n",
    "\n",
    "    # Separate cells with blank lines so function boundaries remain readable to the model.\n",
    "    return \"\\n\\n\".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep prompts stable; otherwise the judge is comparing outputs produced under different instructions.\n",
    "system_message_comments = (\n",
    "    \"You are a senior developer. Improve the code documentation by adding docstrings and short, useful comments. \"\n",
    "    \"Keep it natural and practical. Do not over-comment obvious lines. \"\n",
    "    \"Reply with code only.\"\n",
    ")\n",
    "\n",
    "system_message_summary = (\n",
    "    \"You are a senior developer. Summarize the code clearly: what it does, overall flow, inputs/outputs, and key points. \"\n",
    "    \"Do not show the code. Do not use Markdown. Reply with plain text only.\"\n",
    ")\n",
    "\n",
    "def user_prompt_for(code: str) -> str:\n",
    "    # A single, explicit instruction reduces variance across different backends.\n",
    "    return \"Add docstrings and helpful comments. Reply with code only.\\n\\n\" + code\n",
    "\n",
    "def user_prompt_for_summary(code: str) -> str:\n",
    "    # Summary prompt is separated so models don't “leak” code back into the summary.\n",
    "    return \"Summarize this code.\\n\\n\" + code\n",
    "\n",
    "def messages_for(code: str):\n",
    "    # System+user is supported by OpenAI and by HF router chat completions.\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_comments},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(code)},\n",
    "    ]\n",
    "\n",
    "def messages_for_summary(code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_summary},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for_summary(code)},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_local(code: str):\n",
    "    # Ollama runs locally; this path is “free” after the model is pulled.\n",
    "    r1 = ollama.chat(model=LLAMA_MODEL, messages=messages_for(code))\n",
    "    r2 = ollama.chat(model=LLAMA_MODEL, messages=messages_for_summary(code))\n",
    "\n",
    "    # Normalize return shape to plain strings so the benchmark pipeline is backend-agnostic.\n",
    "    return r1[\"message\"][\"content\"], r2[\"message\"][\"content\"]\n",
    "\n",
    "def call_hf_free(code: str):\n",
    "    # HF router calls are remote; this requires HF_TOKEN and is subject to quota/latency.\n",
    "    if not HF_TOKEN:\n",
    "        raise RuntimeError(\"HF_TOKEN is not set in your environment.\")\n",
    "\n",
    "    c1 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for(code),\n",
    "        max_tokens=1000,  # Cap output so latency/cost don’t explode on large notebooks.\n",
    "    )\n",
    "    c2 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for_summary(code),\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n",
    "\n",
    "def call_gpt_nano(code: str):\n",
    "    # Nano competes as a generator; the stronger judge is kept separate to reduce bias.\n",
    "    c1 = openai.chat.completions.create(model=OPENAI_NANO_MODEL, messages=messages_for(code))\n",
    "    c2 = openai.chat.completions.create(model=OPENAI_NANO_MODEL, messages=messages_for_summary(code))\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_once(model_name: str, code: str):\n",
    "    # Use wall-clock latency (perf_counter) to include network time for remote models.\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    m = (model_name or \"\").strip().lower()\n",
    "    if m.startswith(\"llama\"):\n",
    "        commented, summary = call_llama_local(code)\n",
    "    elif m.startswith(\"hf\"):\n",
    "        commented, summary = call_hf_free(code)\n",
    "    else:\n",
    "        commented, summary = call_gpt_nano(code)\n",
    "\n",
    "    # This time is the single number used later for value scoring (score per second).\n",
    "    return commented, summary, (time.perf_counter() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50adda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_score(text: str) -> float:\n",
    "    # Parsing is intentionally forgiving; we only need the first \"score: X\" pattern.\n",
    "    m = re.search(r\"\\bscore\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?)\", text, flags=re.IGNORECASE)\n",
    "    return float(m.group(1)) if m else 0.0\n",
    "\n",
    "def judge_quality_llm(code: str, commented: str, summary: str) -> str:\n",
    "    # Constrain the judge output tightly so the benchmark remains machine-parsable.\n",
    "    rubric = (\n",
    "        \"You are a strict code reviewer. Evaluate the assistant output for the given original code.\\n\"\n",
    "        \"Return a short verdict with a single numeric score from 0 to 10.\\n\"\n",
    "        \"Criteria (equal weight):\\n\"\n",
    "        \"1) Correctness: comments/docstrings match what code does (no hallucinations).\\n\"\n",
    "        \"2) Usefulness: captures intent, assumptions, edge cases, and non-obvious behavior.\\n\"\n",
    "        \"3) Clarity: readable, consistent, avoids redundant commentary.\\n\"\n",
    "        \"4) Naturalness: reads like a human developer wrote it.\\n\"\n",
    "        \"Output format (exact):\\n\"\n",
    "        \"score: <number>\\n\"\n",
    "        \"notes: <one paragraph>\\n\"\n",
    "    )\n",
    "\n",
    "    # Feed the judge the original and both outputs; this is the minimum context to score quality.\n",
    "    payload = (\n",
    "        \"ORIGINAL CODE:\\n\"\n",
    "        f\"{code}\\n\\n\"\n",
    "        \"COMMENTED CODE:\\n\"\n",
    "        f\"{commented}\\n\\n\"\n",
    "        \"SUMMARY:\\n\"\n",
    "        f\"{summary}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rubric},\n",
    "        {\"role\": \"user\", \"content\": payload},\n",
    "    ]\n",
    "\n",
    "    # Judge is fixed to gpt-4o-mini to avoid a moving target; only contenders vary.\n",
    "    c = openai.chat.completions.create(model=OPENAI_JUDGE_MODEL, messages=messages, max_tokens=400)\n",
    "    return c.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc18223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_and_pick_winner(file_obj):\n",
    "    # This function is called by Gradio; it returns (report, extracted_code, winner_commented, winner_summary).\n",
    "    try:\n",
    "        if file_obj is None:\n",
    "            return \"ERROR: Please upload a .ipynb file.\", \"\", \"\", \"\"\n",
    "\n",
    "        # Gradio provides a temporary file path on disk for the upload.\n",
    "        path = file_obj.name\n",
    "        if not path.lower().endswith(\".ipynb\"):\n",
    "            return \"ERROR: The uploaded file is not a .ipynb notebook.\", \"\", \"\", \"\"\n",
    "\n",
    "        # Extract notebook code once so all contenders see identical input.\n",
    "        code = notebook_code_extractor(path)\n",
    "        if not code.strip():\n",
    "            return \"ERROR: No code cells found in the notebook.\", \"\", \"\", \"\"\n",
    "\n",
    "        # Three contenders: free remote (HF), local (Llama), paid/cheap (Nano).\n",
    "        candidates = [\"HF (free)\", \"Llama (local)\", \"GPT (nano)\"]\n",
    "        results = {}\n",
    "\n",
    "        for name in candidates:\n",
    "            # Generate outputs and measure latency for value scoring.\n",
    "            commented, summary, secs = run_model_once(name, code)\n",
    "\n",
    "            # Judge quality independently so we can compare across different generators.\n",
    "            verdict = judge_quality_llm(code, commented, summary)\n",
    "            score = _extract_score(verdict)\n",
    "\n",
    "            # Score per second favors models that are both good and fast.\n",
    "            value = (score / secs) if secs > 0 else 0.0\n",
    "\n",
    "            results[name] = {\n",
    "                \"commented\": commented,\n",
    "                \"summary\": summary,\n",
    "                \"secs\": secs,\n",
    "                \"verdict\": verdict,\n",
    "                \"score\": score,\n",
    "                \"value\": value,\n",
    "            }\n",
    "\n",
    "        # Pick winner by value first; break ties using raw score so quality wins if times are similar.\n",
    "        winner = max(results.items(), key=lambda kv: (kv[1][\"value\"], kv[1][\"score\"]))[0]\n",
    "        w = results[winner]\n",
    "\n",
    "        # Build a compact, readable report to justify the winner choice.\n",
    "        lines = []\n",
    "        for name in candidates:\n",
    "            r = results[name]\n",
    "            lines.append(f\"{name}: score={r['score']:.2f}, time={r['secs']:.3f}s, score/time={r['value']:.3f}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"WINNER: {winner}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Judge verdict (winner):\")\n",
    "        lines.append(w[\"verdict\"].strip())\n",
    "\n",
    "        return \"\\n\".join(lines), code, w[\"commented\"], w[\"summary\"]\n",
    "\n",
    "    except Exception:\n",
    "        # Returning the traceback into the UI makes failures debuggable without crashing the Gradio queue.\n",
    "        return \"ERROR:\\n\" + traceback.format_exc(), \"\", \"\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a389ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".comments {background-color: #00599C;}\n",
    ".summary {background-color: #008B8B;}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\n",
    "        \"### Notebook Documentation Tool\\n\"\n",
    "        \"Upload a notebook, generate docs with three models, and rank them with a GPT-4o-mini judge.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        nb_file = gr.File(label=\"Upload .ipynb\", file_types=[\".ipynb\"])\n",
    "\n",
    "    with gr.Row():\n",
    "        run_bench = gr.Button(\"Generate and pick winner\")\n",
    "\n",
    "    with gr.Row():\n",
    "        report = gr.Textbox(label=\"Benchmark report\", lines=10)\n",
    "\n",
    "    with gr.Row():\n",
    "        source_code = gr.Textbox(label=\"Extracted notebook code (read-only)\", lines=14, interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        commented_code = gr.Textbox(label=\"Winner: documented code\", lines=14, elem_classes=[\"comments\"])\n",
    "        code_summary = gr.Textbox(label=\"Winner: summary\", lines=14, elem_classes=[\"summary\"])\n",
    "\n",
    "    run_bench.click(\n",
    "        benchmark_and_pick_winner,\n",
    "        inputs=[nb_file],\n",
    "        outputs=[report, source_code, commented_code, code_summary],\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
