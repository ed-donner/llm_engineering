{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a408e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "LLAMA_MODEL = \"llama3.1:8b\"\n",
    "\n",
    "HF_FREE_CHAT_MODEL = \"HuggingFaceTB/SmolLM3-3B:hf-inference\"\n",
    "#HF_FREE_CHAT_MODEL=\"katanemo/Arch-Router-1.5B\"\n",
    "HF_ROUTER_BASE_URL = \"https://router.huggingface.co/v1\"\n",
    "\n",
    "openai = OpenAI()\n",
    "hf_client = OpenAI(base_url=HF_ROUTER_BASE_URL, api_key=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_code_extractor(path: str) -> str:\n",
    "    nb = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    parts = []\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") == \"code\":\n",
    "            parts.append(\"\".join(cell.get(\"source\", [])))\n",
    "    return \"\\n\\n\".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_comments = (\n",
    "    \"You are a senior developer. Improve the code documentation by adding docstrings and short, useful comments. \"\n",
    "    \"Keep it natural and practical. Do not over-comment obvious lines. \"\n",
    "    \"Reply with code only.\"\n",
    ")\n",
    "\n",
    "system_message_summary = (\n",
    "    \"You are a senior developer. Summarize the code clearly: what it does, overall flow, inputs/outputs, and key points. \"\n",
    "    \"Do not show the code. Do not use Markdown. Reply with plain text only.\"\n",
    ")\n",
    "\n",
    "def user_prompt_for(code: str) -> str:\n",
    "    return \"Add docstrings and helpful comments. Reply with code only.\\n\\n\" + code\n",
    "\n",
    "def user_prompt_for_summary(code: str) -> str:\n",
    "    return \"Summarize this code.\\n\\n\" + code\n",
    "\n",
    "def messages_for(code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_comments},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(code)},\n",
    "    ]\n",
    "\n",
    "def messages_for_summary(code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_summary},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for_summary(code)},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_local(code: str):\n",
    "    r1 = ollama.chat(model=LLAMA_MODEL, messages=messages_for(code))\n",
    "    r2 = ollama.chat(model=LLAMA_MODEL, messages=messages_for_summary(code))\n",
    "    return r1[\"message\"][\"content\"], r2[\"message\"][\"content\"]\n",
    "\n",
    "def call_gpt(code: str):\n",
    "    c1 = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(code))\n",
    "    c2 = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for_summary(code))\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n",
    "\n",
    "def call_hf_free(code: str):\n",
    "    if not HF_TOKEN:\n",
    "        raise RuntimeError(\"HF_TOKEN is not set in your environment.\")\n",
    "    c1 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for(code),\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    c2 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for_summary(code),\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def document_uploaded_notebook(file_obj, model: str):\n",
    "    try:\n",
    "        if file_obj is None:\n",
    "            return \"ERROR: Please upload a .ipynb file.\", \"\", \"\"\n",
    "\n",
    "        path = file_obj.name\n",
    "        if not path.lower().endswith(\".ipynb\"):\n",
    "            return \"ERROR: The uploaded file is not a .ipynb notebook.\", \"\", \"\"\n",
    "\n",
    "        code = notebook_code_extractor(path)\n",
    "        if not code.strip():\n",
    "            return \"ERROR: No code cells found in the notebook.\", \"\", \"\"\n",
    "\n",
    "        m = (model or \"\").strip().lower()\n",
    "        if m.startswith(\"llama\"):\n",
    "            commented, summary = call_llama_local(code)\n",
    "        elif m.startswith(\"gpt\"):\n",
    "            commented, summary = call_gpt(code)\n",
    "        elif m.startswith(\"hf\"):\n",
    "            commented, summary = call_hf_free(code)\n",
    "        else:\n",
    "            return f\"ERROR: Unsupported model: {model!r}\", \"\", \"\"\n",
    "\n",
    "        return code, commented, summary\n",
    "\n",
    "    except Exception:\n",
    "        return \"ERROR:\\n\" + traceback.format_exc(), \"\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37601540",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".comments {background-color: #00599C;}\n",
    ".summary {background-color: #008B8B;}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"### Notebook Documentation Tool\\nUpload a notebook and generate docstrings/comments + a summary.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        nb_file = gr.File(label=\"Upload .ipynb\", file_types=[\".ipynb\"])\n",
    "\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown(\n",
    "            [\"HF (free)\", \"Llama (local)\", \"GPT (API)\"],\n",
    "            label=\"Model\",\n",
    "            value=\"HF (free)\",\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        run = gr.Button(\"Generate documentation\")\n",
    "\n",
    "    with gr.Row():\n",
    "        source_code = gr.Textbox(label=\"Extracted notebook code (read-only)\", lines=14, interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        commented_code = gr.Textbox(label=\"Documented code\", lines=14, elem_classes=[\"comments\"])\n",
    "        code_summary = gr.Textbox(label=\"Summary\", lines=14, elem_classes=[\"summary\"])\n",
    "\n",
    "    run.click(\n",
    "        document_uploaded_notebook,\n",
    "        inputs=[nb_file, model],\n",
    "        outputs=[source_code, commented_code, code_summary],\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
