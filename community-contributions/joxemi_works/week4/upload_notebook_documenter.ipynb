{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a408e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "LLAMA_MODEL = \"llama3.1:8b\"\n",
    "\n",
    "HF_FREE_CHAT_MODEL = \"HuggingFaceTB/SmolLM3-3B:hf-inference\"\n",
    "#HF_FREE_CHAT_MODEL=\"katanemo/Arch-Router-1.5B\"\n",
    "HF_ROUTER_BASE_URL = \"https://router.huggingface.co/v1\"\n",
    "\n",
    "openai = OpenAI()\n",
    "hf_client = OpenAI(base_url=HF_ROUTER_BASE_URL, api_key=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e592182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_code_extractor(path: str) -> str:\n",
    "    nb = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    parts = []\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") == \"code\":\n",
    "            parts.append(\"\".join(cell.get(\"source\", [])))\n",
    "    return \"\\n\\n\".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92ef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_comments = (\n",
    "    \"You are a senior developer. Improve the code documentation by adding docstrings and short, useful comments. \"\n",
    "    \"Keep it natural and practical. Do not over-comment obvious lines. \"\n",
    "    \"Reply with code only.\"\n",
    ")\n",
    "\n",
    "system_message_summary = (\n",
    "    \"You are a senior developer. Summarize the code clearly: what it does, overall flow, inputs/outputs, and key points. \"\n",
    "    \"Do not show the code. Do not use Markdown. Reply with plain text only.\"\n",
    ")\n",
    "\n",
    "def user_prompt_for(code: str) -> str:\n",
    "    return \"Add docstrings and helpful comments. Reply with code only.\\n\\n\" + code\n",
    "\n",
    "def user_prompt_for_summary(code: str) -> str:\n",
    "    return \"Summarize this code.\\n\\n\" + code\n",
    "\n",
    "def messages_for(code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_comments},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(code)},\n",
    "    ]\n",
    "\n",
    "def messages_for_summary(code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message_summary},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for_summary(code)},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea40c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_local(code: str):\n",
    "    r1 = ollama.chat(model=LLAMA_MODEL, messages=messages_for(code))\n",
    "    r2 = ollama.chat(model=LLAMA_MODEL, messages=messages_for_summary(code))\n",
    "    return r1[\"message\"][\"content\"], r2[\"message\"][\"content\"]\n",
    "\n",
    "def call_gpt(code: str):\n",
    "    c1 = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(code))\n",
    "    c2 = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for_summary(code))\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n",
    "\n",
    "def call_hf_free(code: str):\n",
    "    if not HF_TOKEN:\n",
    "        raise RuntimeError(\"HF_TOKEN is not set in your environment.\")\n",
    "    c1 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for(code),\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    c2 = hf_client.chat.completions.create(\n",
    "        model=HF_FREE_CHAT_MODEL,\n",
    "        messages=messages_for_summary(code),\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    return c1.choices[0].message.content, c2.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3d84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def document_uploaded_notebook(file_obj, model: str):\n",
    "    try:\n",
    "        if file_obj is None:\n",
    "            return \"ERROR: Please upload a .ipynb file.\", \"\", \"\"\n",
    "\n",
    "        path = file_obj.name\n",
    "        if not path.lower().endswith(\".ipynb\"):\n",
    "            return \"ERROR: The uploaded file is not a .ipynb notebook.\", \"\", \"\"\n",
    "\n",
    "        code = notebook_code_extractor(path)\n",
    "        if not code.strip():\n",
    "            return \"ERROR: No code cells found in the notebook.\", \"\", \"\"\n",
    "\n",
    "        m = (model or \"\").strip().lower()\n",
    "        if m.startswith(\"llama\"):\n",
    "            commented, summary = call_llama_local(code)\n",
    "        elif m.startswith(\"gpt\"):\n",
    "            commented, summary = call_gpt(code)\n",
    "        elif m.startswith(\"hf\"):\n",
    "            commented, summary = call_hf_free(code)\n",
    "        else:\n",
    "            return f\"ERROR: Unsupported model: {model!r}\", \"\", \"\"\n",
    "\n",
    "        return code, commented, summary\n",
    "\n",
    "    except Exception:\n",
    "        return \"ERROR:\\n\" + traceback.format_exc(), \"\", \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37601540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 111, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 391, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 290, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"C:\\Users\\jmgimenez\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"g:\\joxemi_gimenez\\000_enerlogix\\udemy\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jmgimenez\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\locks.py\", line 209, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jmgimenez\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x00000217D3082D20 [unset]> is bound to a different event loop\n"
     ]
    }
   ],
   "source": [
    "css = \"\"\"\n",
    ".comments {background-color: #00599C;}\n",
    ".summary {background-color: #008B8B;}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"### Notebook Documentation Tool\\nUpload a notebook and generate docstrings/comments + a summary.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        nb_file = gr.File(label=\"Upload .ipynb\", file_types=[\".ipynb\"])\n",
    "\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown(\n",
    "            [\"HF (free)\", \"Llama (local)\", \"GPT (API)\"],\n",
    "            label=\"Model\",\n",
    "            value=\"HF (free)\",\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        run = gr.Button(\"Generate documentation\")\n",
    "\n",
    "    with gr.Row():\n",
    "        source_code = gr.Textbox(label=\"Extracted notebook code (read-only)\", lines=14, interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        commented_code = gr.Textbox(label=\"Documented code\", lines=14, elem_classes=[\"comments\"])\n",
    "        code_summary = gr.Textbox(label=\"Summary\", lines=14, elem_classes=[\"summary\"])\n",
    "\n",
    "    run.click(\n",
    "        document_uploaded_notebook,\n",
    "        inputs=[nb_file, model],\n",
    "        outputs=[source_code, commented_code, code_summary],\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
