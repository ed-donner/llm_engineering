{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmFuoj3y-t1P"
   },
   "source": [
    "# Security Awareness Synthetic Q/A Dataset Generator (CSV)\n",
    "\n",
    "This notebook generates **synthetic, public-safe training/evaluation data** for future projects such as **RAG (Retrieval-Augmented Generation)** and internal assistants.\n",
    "\n",
    "Each click produces one structured example:\n",
    "- **CONTEXT**: a short didactic mini-guide (120–180 words)\n",
    "- **QUESTION**: a question answerable **only** from the context\n",
    "- **ANSWER**: a grounded answer using **only** the context\n",
    "\n",
    "Saved output is a CSV dataset with:\n",
    "- `chunk_id` (incremental ID, e.g., `SA-000001`)\n",
    "- `topic`, `difficulty`, `tags`\n",
    "- `context`, `question`, `answer`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30170,
     "status": "ok",
     "timestamp": 1770399003897,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "y1ojvIYPQmlA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# - accelerate: device_map=\"auto\" support\n",
    "# - bitsandbytes: 4-bit quantization\n",
    "# - gradio: UI\n",
    "!pip install -q --upgrade  accelerate bitsandbytes gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1770399003934,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "X3967StoR0Ax"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os                      # File paths, environment variables\n",
    "import json                    # JSON serialization\n",
    "import uuid                    # Unique IDs for dataset entries\n",
    "from datetime import datetime  # Timestamps for filenames/logging\n",
    "\n",
    "# PyTorch imports\n",
    "import torch                 # Tensors, GPU check\n",
    "\n",
    "# Hugging Face / Transformers imports\n",
    "from huggingface_hub import login                 # HF login to download gated models\n",
    "from transformers import (\n",
    "    AutoTokenizer,                                 # Tokenizer loader\n",
    "    AutoModelForCausalLM,                           # Causal LM loader\n",
    "    BitsAndBytesConfig                              # 4-bit quant config\n",
    ")\n",
    "\n",
    "# Colab-specific imports\n",
    "from google.colab import userdata                  # Colab Secrets store\n",
    "\n",
    "# UI imports\n",
    "import gradio as gr                                # Gradio UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1770399004643,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "qRTvFHSESFJ5",
    "outputId": "c6a71427-8227-4ed7-df40-1298933b61bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Read Hugging Face token from Colab \"Secrets\"\n",
    "hf_token = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face so you can download Llama weights\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "\n",
    "# Check if we have a GPU (recommended for speed)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "9d454b99b2734f389fb14741d5b94f0e",
      "2b08951153934e939d8768c48a7a3495",
      "faa3a2fe37c14f1a93d66f046cdd4826",
      "8054817ca5eb485e85c3b903eaa5c69c",
      "31219595f2d044b58ae795315105b68d",
      "e1e7bb7d4645489f8f7233f906dc369c",
      "2a8d12725fbd427181595de335cfbae3",
      "f8e2e2df780743f38d4930e07b42c0ca",
      "8468cce582e7439d9aca6b546b5a8251",
      "62aa07368e0c4bf7ad1f8f5dea471900",
      "8234234c66a24f688d0e2cd2352d4a99"
     ]
    },
    "executionInfo": {
     "elapsed": 43560,
     "status": "ok",
     "timestamp": 1770399048220,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "G7MYDFzgSWSZ",
    "outputId": "81f9c7ac-4d2d-432d-f3a1-ccf3c58c4c91"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define 4-bit quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit weight loading\n",
    "    bnb_4bit_use_double_quant=True,         # Double quantization for improved compression\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute dtype for matmuls\n",
    "    bnb_4bit_quant_type=\"nf4\"               # NF4 quantization\n",
    ")\n",
    "\n",
    "# Load tokenizer for the chat model\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "\n",
    "# Some Llama tokenizers do not define a pad token; set it to EOS for safe padding behavior\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with device_map=\"auto\" so it can place layers on GPU/CPU as needed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA,\n",
    "    device_map=\"auto\",                      # Automatically place layers to available devices\n",
    "    quantization_config=quant_config        # Apply the 4-bit quantization config\n",
    ")\n",
    "\n",
    "# Put the model into evaluation mode\n",
    "\n",
    "print(\"Model loaded:\", LLAMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1770399048296,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "_D4xmMWBS3MC",
    "outputId": "54392802-a88d-4afb-eec2-febfe1d03947"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TOPICS = {\n",
    "    \"Phishing Emails — Red Flags\": (\n",
    "        \"Explain common phishing indicators (sender spoofing, urgency, suspicious links/attachments, \"\n",
    "        \"credential requests). Describe what a user should do if they suspect phishing.\"\n",
    "    ),\n",
    "    \"Strong Passwords and Passphrases\": (\n",
    "        \"Explain why unique passwords matter, what makes a password strong, and how passphrases work. \"\n",
    "        \"Include practical guidance and common mistakes to avoid.\"\n",
    "    ),\n",
    "    \"Multi-Factor Authentication (MFA) Basics\": (\n",
    "        \"Explain what MFA is, why it helps, and common MFA methods. Mention 'push fatigue' and the \"\n",
    "        \"importance of verifying login prompts.\"\n",
    "    ),\n",
    "    \"Software Updates and Patching\": (\n",
    "        \"Explain why updates matter, how vulnerabilities are exploited, and safe habits for updating \"\n",
    "        \"operating systems and apps.\"\n",
    "    ),\n",
    "    \"Safe Use of Public Wi-Fi\": (\n",
    "        \"Explain risks of public Wi-Fi, what information should not be entered, and safer alternatives \"\n",
    "        \"like mobile hotspots or VPN usage (conceptually, no vendor names).\"\n",
    "    ),\n",
    "    \"Handling Sensitive Data\": (\n",
    "        \"Explain basic data sensitivity levels (public/internal/confidential), secure sharing habits, \"\n",
    "        \"and how to reduce accidental exposure.\"\n",
    "    ),\n",
    "    \"Social Engineering by Phone\": (\n",
    "        \"Explain pretexting, why verification is necessary, and safe steps to validate identity before \"\n",
    "        \"sharing information or granting access.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Topics:\")\n",
    "for k in TOPICS.keys():\n",
    "    print(\"-\", k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51032,
     "status": "ok",
     "timestamp": 1770399099331,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "Y-XlhcxC9KsG",
    "outputId": "00b19e90-debb-4a34-a7f6-55745867dd11"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _topic_to_tags(topic_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Derive short tags from the topic name.\n",
    "    Stored in CSV as a single string separated by '|'.\n",
    "    \"\"\"\n",
    "    base = topic_name.lower()\n",
    "    tags = [\"security_awareness\"]\n",
    "\n",
    "    if \"phishing\" in base:\n",
    "        tags += [\"phishing\", \"email\", \"links\"]\n",
    "    if \"password\" in base or \"passphrase\" in base:\n",
    "        tags += [\"passwords\", \"passphrases\", \"credentials\"]\n",
    "    if \"mfa\" in base or \"multi-factor\" in base:\n",
    "        tags += [\"mfa\", \"authentication\", \"push_fatigue\"]\n",
    "    if \"update\" in base or \"patch\" in base:\n",
    "        tags += [\"updates\", \"patching\", \"vulnerabilities\"]\n",
    "    if \"wi\" in base or \"wifi\" in base:\n",
    "        tags += [\"public_wifi\", \"network\", \"privacy\"]\n",
    "    if \"sensitive data\" in base or \"handling sensitive\" in base:\n",
    "        tags += [\"sensitive_data\", \"sharing\", \"classification\"]\n",
    "    if \"social engineering\" in base or \"phone\" in base:\n",
    "        tags += [\"social_engineering\", \"verification\", \"pretexting\"]\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    dedup = []\n",
    "    for t in tags:\n",
    "        if t not in dedup:\n",
    "            dedup.append(t)\n",
    "\n",
    "    return \"|\".join(dedup[:6])\n",
    "\n",
    "\n",
    "def generate_one_text(topic_name: str, difficulty: str, temperature: float = 0.2, max_new_tokens: int = 450) -> dict:\n",
    "    \"\"\"\n",
    "    Generate ONE example in English using fixed separators.\n",
    "    Returns: topic, difficulty, tags, context, question, answer\n",
    "\n",
    "    Key fix:\n",
    "    - decode ONLY newly generated tokens, not the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    topic_desc = TOPICS[topic_name]\n",
    "    tags = _topic_to_tags(topic_name)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a technical instructor for security awareness (non-expert audience). \"\n",
    "        \"You must write a didactic mini-manual chunk and a grounded Q/A pair. \"\n",
    "        \"Use EXACTLY the separators requested and fill EVERY section. \"\n",
    "        \"Do not mention vendor or brand names.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Topic: {topic_name}\n",
    "Topic requirements: {topic_desc}\n",
    "Difficulty: {difficulty}\n",
    "\n",
    "Return output in ENGLISH using EXACTLY this structure:\n",
    "\n",
    "===CONTEXT===\n",
    "Write the context here.\n",
    "\n",
    "===QUESTION===\n",
    "Write the question here.\n",
    "\n",
    "===ANSWER===\n",
    "Write the answer here.\n",
    "\n",
    "Rules:\n",
    "- CONTEXT must be 120 to 180 words.\n",
    "- QUESTION must be answerable ONLY using the CONTEXT.\n",
    "- ANSWER must use ONLY the CONTEXT (no outside knowledge).\n",
    "- Do not include any extra sections or commentary.\n",
    "- Do not leave any section empty.\n",
    "\"\"\".strip()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    tokenized = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    # Handle Tensor vs BatchEncoding\n",
    "    if isinstance(tokenized, torch.Tensor):\n",
    "        input_ids = tokenized\n",
    "        attention_mask = None\n",
    "    else:\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized.get(\"attention_mask\", None)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(\"cuda\")\n",
    "\n",
    "    prompt_len = input_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=float(temperature),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode ONLY newly generated tokens\n",
    "    new_tokens = output_ids[0, prompt_len:]\n",
    "    decoded_new = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Extract sections\n",
    "    def _between(text, a, b):\n",
    "        i = text.find(a)\n",
    "        if i == -1:\n",
    "            return \"\"\n",
    "        i += len(a)\n",
    "        j = text.find(b, i)\n",
    "        if j == -1:\n",
    "            return text[i:].strip()\n",
    "        return text[i:j].strip()\n",
    "\n",
    "    context = _between(decoded_new, \"===CONTEXT===\", \"===QUESTION===\")\n",
    "    question = _between(decoded_new, \"===QUESTION===\", \"===ANSWER===\")\n",
    "    answer = decoded_new.split(\"===ANSWER===\")[-1].strip() if \"===ANSWER===\" in decoded_new else \"\"\n",
    "\n",
    "    # Fallback: if separators failed, show raw output so you always see something\n",
    "    if not context and not question and not answer:\n",
    "        return {\n",
    "            \"topic\": topic_name,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"tags\": tags,\n",
    "            \"context\": \"\",\n",
    "            \"question\": \"\",\n",
    "            \"answer\": decoded_new.strip(),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"topic\": topic_name,\n",
    "        \"difficulty\": difficulty,\n",
    "        \"tags\": tags,\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "ex = generate_one_text(\"Phishing Emails — Red Flags\", \"medium\", temperature=0.2)\n",
    "print(ex[\"tags\"])\n",
    "print(ex[\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1770399099374,
     "user": {
      "displayName": "Jose Miguel Gimenez",
      "userId": "06543827004545850321"
     },
     "user_tz": -60
    },
    "id": "9Yb61ndF9XKW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "CSV_PATH = \"/content/security_awareness_qa.csv\"\n",
    "\n",
    "CSV_COLUMNS = [\"chunk_id\", \"topic\", \"difficulty\", \"tags\", \"context\", \"question\", \"answer\"]\n",
    "\n",
    "def _next_chunk_id(path: str = CSV_PATH) -> str:\n",
    "    \"\"\"\n",
    "    Generate incremental IDs like SA-000001, SA-000002, ...\n",
    "    Based on number of existing data lines in the CSV (excluding header).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return \"SA-000001\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        n_lines = sum(1 for _ in f)\n",
    "\n",
    "    n_data = max(0, n_lines - 1)  # subtract header\n",
    "    next_id = n_data + 1\n",
    "    return f\"SA-{next_id:06d}\"\n",
    "\n",
    "def append_example_to_csv(example: dict, path: str = CSV_PATH) -> str:\n",
    "    \"\"\"\n",
    "    Append one example to CSV, auto-adding chunk_id.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(path)\n",
    "    chunk_id = _next_chunk_id(path)\n",
    "\n",
    "    row = {\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"topic\": example.get(\"topic\", \"\"),\n",
    "        \"difficulty\": example.get(\"difficulty\", \"\"),\n",
    "        \"tags\": example.get(\"tags\", \"\"),\n",
    "        \"context\": example.get(\"context\", \"\"),\n",
    "        \"question\": example.get(\"question\", \"\"),\n",
    "        \"answer\": example.get(\"answer\", \"\"),\n",
    "    }\n",
    "\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "    return path\n",
    "\n",
    "def preview_csv(path: str = CSV_PATH, n_lines: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Preview first lines of the CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return \"(CSV not created yet.)\"\n",
    "\n",
    "    lines = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in range(n_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line.rstrip(\"\\n\"))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Optional reset (uncomment if you want to start fresh)\n",
    "# if os.path.exists(CSV_PATH): os.remove(CSV_PATH)\n",
    "# print(\"Reset:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "4ZJp4ffS9fLf",
    "outputId": "27c1cfd7-da0a-4003-9258-cebef9f78bc2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gradio as gr\n",
    "import traceback\n",
    "\n",
    "_last_example = None\n",
    "\n",
    "def ui_generate(topic_name, difficulty, temperature):\n",
    "    global _last_example\n",
    "    try:\n",
    "        ex = generate_one_text(topic_name, difficulty, temperature=float(temperature))\n",
    "        _last_example = ex\n",
    "\n",
    "        preview = (\n",
    "            f\"TOPIC: {ex['topic']}\\n\"\n",
    "            f\"DIFFICULTY: {ex['difficulty']}\\n\"\n",
    "            f\"TAGS: {ex['tags']}\\n\\n\"\n",
    "            f\"===CONTEXT===\\n{ex['context']}\\n\\n\"\n",
    "            f\"===QUESTION===\\n{ex['question']}\\n\\n\"\n",
    "            f\"===ANSWER===\\n{ex['answer']}\\n\"\n",
    "        )\n",
    "        return preview, \"Generated OK (not saved yet).\"\n",
    "    except Exception:\n",
    "        _last_example = None\n",
    "        return traceback.format_exc(), \"Generation failed (see error above).\"\n",
    "\n",
    "def ui_save():\n",
    "    global _last_example\n",
    "    try:\n",
    "        if _last_example is None:\n",
    "            return \"Nothing to save. Generate first.\"\n",
    "\n",
    "        append_example_to_csv(_last_example, CSV_PATH)\n",
    "        return f\"Saved to: {CSV_PATH}\\n\\nPreview:\\n{preview_csv(CSV_PATH, n_lines=5)}\"\n",
    "    except Exception:\n",
    "        return traceback.format_exc()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Security Awareness Q/A — CSV Dataset Generator\")\n",
    "    gr.Markdown(\"Generate 1 example (context + question + answer) and append it to a CSV with chunk_id and tags.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        topic = gr.Dropdown(list(TOPICS.keys()), value=\"Phishing Emails — Red Flags\", label=\"Topic\")\n",
    "        difficulty = gr.Dropdown([\"easy\", \"medium\", \"hard\"], value=\"medium\", label=\"Difficulty\")\n",
    "        temperature = gr.Slider(0.1, 1.0, step=0.1, value=0.2, label=\"Temperature (lower = more stable)\")\n",
    "\n",
    "    gen_btn = gr.Button(\"Generate 1 example\")\n",
    "    out_text = gr.Textbox(label=\"Output / Errors\", lines=18)\n",
    "    status = gr.Textbox(label=\"Status\", lines=1)\n",
    "\n",
    "    save_btn = gr.Button(\"Save last row to CSV\")\n",
    "    save_status = gr.Textbox(label=\"Save status\", lines=8)\n",
    "\n",
    "    gen_btn.click(ui_generate, inputs=[topic, difficulty, temperature], outputs=[out_text, status])\n",
    "    save_btn.click(ui_save, inputs=None, outputs=[save_status])\n",
    "\n",
    "# In Colab, share=True is typically the most reliable way to view Gradio.\n",
    "demo.launch(share=True, debug=True, prevent_thread_lock=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPhfMAi3LW+JkzyMmufJmsf",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2a8d12725fbd427181595de335cfbae3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b08951153934e939d8768c48a7a3495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1e7bb7d4645489f8f7233f906dc369c",
      "placeholder": "​",
      "style": "IPY_MODEL_2a8d12725fbd427181595de335cfbae3",
      "value": "Loading weights: 100%"
     }
    },
    "31219595f2d044b58ae795315105b68d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62aa07368e0c4bf7ad1f8f5dea471900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8054817ca5eb485e85c3b903eaa5c69c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62aa07368e0c4bf7ad1f8f5dea471900",
      "placeholder": "​",
      "style": "IPY_MODEL_8234234c66a24f688d0e2cd2352d4a99",
      "value": " 254/254 [00:33&lt;00:00, 100.73it/s, Materializing param=model.norm.weight]"
     }
    },
    "8234234c66a24f688d0e2cd2352d4a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8468cce582e7439d9aca6b546b5a8251": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d454b99b2734f389fb14741d5b94f0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b08951153934e939d8768c48a7a3495",
       "IPY_MODEL_faa3a2fe37c14f1a93d66f046cdd4826",
       "IPY_MODEL_8054817ca5eb485e85c3b903eaa5c69c"
      ],
      "layout": "IPY_MODEL_31219595f2d044b58ae795315105b68d"
     }
    },
    "e1e7bb7d4645489f8f7233f906dc369c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8e2e2df780743f38d4930e07b42c0ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faa3a2fe37c14f1a93d66f046cdd4826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8e2e2df780743f38d4930e07b42c0ca",
      "max": 254,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8468cce582e7439d9aca6b546b5a8251",
      "value": 254
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
