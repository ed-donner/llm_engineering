{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc16f15",
   "metadata": {},
   "source": [
    "# Looking behind the curtain of GPT!\n",
    "\n",
    "You can simply scroll down to the diagram below to see the results.\n",
    "\n",
    "If you'd like to try this yourself - this runs nicely on a free Colab CPU box.\n",
    "\n",
    "## Setup instructions\n",
    "\n",
    "### First get an OpenAI key if you don't already have one\n",
    "\n",
    "1. Visit [OpenAI](https://platform.openai.com/) and create an API account as needed\n",
    "2. On OpenAI's [billing](https://platform.openai.com/settings/organization/billing/overview) page, top up with the minimum amount to get started, typically $5 is the minimum\n",
    "3. On OpenAI's [API key](https://platform.openai.com/settings/organization/api-keys) page, create an API key to use and keep it somewhere safe\n",
    "4. Check that everything is working by visiting the [playground](https://platform.openai.com/playground/chat?models=gpt-4o-mini)\n",
    "\n",
    "### Then set the key as a 'Secret' in this colab, which will be private for you\n",
    "\n",
    "1. Press the key symbol in the left sidebar\n",
    "2. Enter a Name of `OPENAI_API_KEY` and paste your actual key in\n",
    "3. Ensure the switch for \"Notebook access\" is switched on.\n",
    "\n",
    "## Then you're ready for showtime.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install the libraries for OpenAI and for the visualizer\n",
    "\n",
    "#!pip install -q openai networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the api key from the secrets\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OpenAI connectivity with a not-so taxing question\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI(api_key=api_key)\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import math\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to call OpenAI and return a list of dicts of predictions to display\n",
    "\n",
    "class TokenPredictor:\n",
    "    def __init__(self, client, model_name: str, temperature: int):\n",
    "        self.client = client\n",
    "        self.messages = []\n",
    "        self.predictions = []\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def predict_tokens(self, prompt: str, max_tokens: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate text token by token and track prediction probabilities.\n",
    "        Returns list of predictions with top token and alternatives.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=self.temperature,\n",
    "            logprobs=True,\n",
    "            seed=42,\n",
    "            top_logprobs=7,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        predictions = []\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                token = chunk.choices[0].delta.content\n",
    "                logprobs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
    "                logprob_dict = {item.token: item.logprob for item in logprobs}\n",
    "\n",
    "                # Get top predicted token and probability\n",
    "                top_token = token\n",
    "                top_prob = logprob_dict[token]\n",
    "\n",
    "                # Get alternative predictions\n",
    "                alternatives = []\n",
    "                for alt_token, alt_prob in logprob_dict.items():\n",
    "                    if alt_token != token:\n",
    "                        alternatives.append((alt_token, math.exp(alt_prob)))\n",
    "                alternatives.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                prediction = {'token': top_token, 'probability': math.exp(top_prob),'alternatives': alternatives[:2]}\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ca422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create a directed graph based on the predictions\n",
    "\n",
    "def create_token_graph(model_name:str, predictions: List[Dict]) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a directed graph showing token predictions and alternatives.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    G.add_node(\"START\", token=model_name, prob=\"START\", color='lightgreen', size=4000)\n",
    "\n",
    "    # First, create all main token nodes in sequence\n",
    "    for i, pred in enumerate(predictions):\n",
    "        token_id = f\"t{i}\"\n",
    "        G.add_node(token_id, token=pred['token'], prob=f\"{pred['probability']*100:.1f}%\", color='lightblue', size=6000)\n",
    "        G.add_edge(f\"t{i-1}\" if i else \"START\", token_id)\n",
    "\n",
    "    # Then add alternative nodes with a different y-position\n",
    "    last_id = None\n",
    "    for i, pred in enumerate(predictions):\n",
    "        parent_token = \"START\" if i == 0 else f\"t{i-1}\"\n",
    "\n",
    "        # Add alternative token nodes slightly below main sequence\n",
    "        for j, (alt_token, alt_prob) in enumerate(pred['alternatives']):\n",
    "            alt_id = f\"t{i}_alt{j}\"\n",
    "            G.add_node(alt_id, token=alt_token, prob=f\"{alt_prob*100:.1f}%\", color='lightgray', size=6000)\n",
    "            G.add_edge(parent_token, alt_id)\n",
    "\n",
    "    G.add_node(\"END\", token=\"END\", prob=\"100%\", color='red', size=6000)\n",
    "    G.add_edge(parent_token, \"END\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph using Matplotlib\n",
    "\n",
    "def visualize_predictions(G: nx.DiGraph, figsize=(14, 150)):\n",
    "    \"\"\"\n",
    "    Visualize the token prediction graph with vertical layout and alternating alternatives.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    pos = {}\n",
    "    spacing_y = 10\n",
    "    spacing_x = 5\n",
    "\n",
    "    # Position main token nodes in a vertical line\n",
    "    main_nodes = [n for n in G.nodes() if '_alt' not in n]\n",
    "    for i, node in enumerate(main_nodes):\n",
    "        pos[node] = (0, -i * spacing_y)  # Center main tokens vertically\n",
    "\n",
    "    # Position alternative nodes to left and right of main tokens\n",
    "    for node in G.nodes():\n",
    "        if '_alt' in node:\n",
    "            main_token = node.split('_')[0]\n",
    "            alt_num = int(node.split('_alt')[1])\n",
    "            if main_token in pos:\n",
    "                # Place first alternative to left, second to right\n",
    "                x_offset = spacing_x if alt_num else -spacing_x\n",
    "                pos[node] = (x_offset, pos[main_token][1] + 0.05)\n",
    "\n",
    "    # Draw nodes\n",
    "    node_colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)\n",
    "\n",
    "    # Draw all edges as straight lines\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=20, alpha=0.7)\n",
    "\n",
    "    # Add labels with token and probability\n",
    "    labels = {node: f\"{G.nodes[node]['token']}\\n{G.nodes[node]['prob']}\" for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=14)\n",
    "\n",
    "    plt.title(\"About the warning message..\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Adjust plot limits to ensure all nodes are visible, then show!\n",
    "    margin = 8\n",
    "    x_values = [x for x, y in pos.values()]\n",
    "    y_values = [y for x, y in pos.values()]\n",
    "    plt.xlim(min(x_values) - margin, max(x_values) + margin)\n",
    "    plt.ylim(min(y_values) - margin, max(y_values) + margin)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec7ed9",
   "metadata": {},
   "source": [
    "## Just before showtime - some essential resources!\n",
    "\n",
    "- [Connect with me](https://www.linkedin.com/in/eddonner/) on LinkedIn - I love connecting\n",
    "- [Follow me](https://x.com/edwarddonner) on X\n",
    "- [See my other videos](https://www.youtube.com/@Edward.Donner) on YouTube\n",
    "- And most of all: consider my intensive 8 week training course to [Master LLM engineering](https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models/?referralCode=35EB41EBB11DD247CF54)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now, pick a model and run!\n",
    "\n",
    "# For advanced experimenters: try changing temperature to a higher number like 0.4\n",
    "# A higher temperature means that occasionally a token other than the one with the highest probability will be picked\n",
    "# Resulting in a more diverse output\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "temperature = 0.0\n",
    "\n",
    "predictor = TokenPredictor(openai, model_name, temperature)\n",
    "#prompt = \"How would you describe the color blue to someone who has never been able to see, in no more than 3 sentences.\"\n",
    "prompt=r\"\"\"g:\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 9 (\t) missing from font(s) DejaVu Sans.\n",
    "  fig.canvas.print_figure(bytes_io, **kw)\"\"\"\n",
    "predictions = predictor.predict_tokens(prompt)\n",
    "#G = create_token_graph(model_name, predictions)\n",
    "#visualize_predictions(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages,temperature=temperature)\n",
    "salida=response.choices[0].message.content\n",
    "display(Markdown(salida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = create_token_graph(model_name, predictions)\n",
    "visualize_predictions(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
