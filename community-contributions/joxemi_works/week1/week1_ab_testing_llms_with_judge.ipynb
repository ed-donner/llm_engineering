{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "# Week 1 — LLM Explainer: Cloud vs Local + Judge\n",
    "\n",
    "This notebook builds a small tool that:\n",
    "1. Sends the same technical question to a **cloud OpenAI model** and a **local Ollama model**.\n",
    "2. Streams both explanations for quick comparison.\n",
    "3. Uses a **third GPT judge** to score each answer (0–10) and pick a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "\n",
    "MODEL_CLOUD = 'gpt-4.1-nano'\n",
    "MODEL_LOCAL = 'deepseek-r1:8b'\n",
    "MODEL_JUDGE = \"gpt-4.1-mini\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "\n",
    "client_cloud = OpenAI()\n",
    "client_local = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "\n",
    "def make_badge(text):\n",
    "    width = len(text) + 4\n",
    "    top_bottom = \"*\" * width\n",
    "    middle = f\"* {text} *\"\n",
    "    return f\"{top_bottom}\\\\n{middle}\\\\n{top_bottom}\"\n",
    "\n",
    "print(make_badge(\"Golden rule: Do unto others as you would have them do unto you\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a senior Python engineer and predictive/generative AI specialist.\\n\"\n",
    "    \"Your top priority is factual accuracy.\\n\"\n",
    "    \"DO NOT lie, guess, or invent information. DO NOT hallucinate.\\n\"\n",
    "    \"If you are unsure about any detail, say explicitly: \\\"I don't know\\\" \"\n",
    "    \"and ask a clarifying question before continuing.\\n\"\n",
    "    \"Base your explanations ONLY on the code and context provided.\\n\"\n",
    "    \"Explain things didactically and clearly, using small examples when helpful.\\n\"\n",
    "    \"\\n\"\n",
    "    \"MANDATORY START OF YOUR RESPONSE:\\n\"\n",
    "    \"1) Introduce yourself in 1–2 sentences.\\n\"\n",
    "    \"2) State the exact LLM model identity you are running as.\\n\"\n",
    "    \"3) State your context window size and number of parameters ONLY if you know them with certainty.\\n\"\n",
    "    \"   If you do NOT know either with certainty, write exactly: 'not publicly available'.\\n\"\n",
    "    \"\\n\"\n",
    "    \"After that mandatory intro, proceed with the task.\"\n",
    ")\n",
    "\n",
    "user_prompt = (\n",
    "    \"First follow the mandatory intro from the system message.\\n\"\n",
    "    \"Then explain the following Python code in a simple, step-by-step way.\\n\"\n",
    "    \"Add a short comment for EACH line explaining what it does.\\n\"\n",
    "    \"Do not add new functionality or rewrite the code unless I explicitly ask.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Code to explain:\\n\"\n",
    "    f\"{question}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24238971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_answer(API,modelo, mensaje):\n",
    "    stream = API.chat.completions.create(\n",
    "        model=modelo,\n",
    "        messages=mensaje,\n",
    "        stream=True\n",
    "    )    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is needed so we can pass a closed, stable text block to the judge.\n",
    "\n",
    "def get_full_answer(client, model_name, messages):\n",
    "    \"\"\"\n",
    "    Runs a Chat Completion WITHOUT streaming to obtain\n",
    "    the final full text from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the backend with the specified model.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,   # Model name to use.\n",
    "        messages=messages   # Same messages for a fair A/B comparison.\n",
    "    )\n",
    "\n",
    "   \n",
    "    answer_text = response.choices[0].message.content\n",
    "\n",
    "    \n",
    "    return answer_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def judge_answers(client_judge, judge_model, question,\n",
    "                  answer_a, answer_b,\n",
    "                  model_a_name, model_b_name):\n",
    "    \"\"\"\n",
    "    Judge compares two answers, scores them 0–10, and picks a winner.\n",
    "    Model names are injected from the script (not guessed by the LLM).\n",
    "    Returns a Python dict with the verdict.\n",
    "    \"\"\"\n",
    "\n",
    "    judge_system_prompt = (\n",
    "        \"You are an impartial judge evaluating two LLM answers.\\n\"\n",
    "        \"Score each answer from 0 to 10 based on:\\n\"\n",
    "        \"1) Factual correctness (no invented info)\\n\"\n",
    "        \"2) Didactic clarity\\n\"\n",
    "        \"3) Completeness of the answer\\n\"\n",
    "        \"4) Coherence and structure\\n\"\n",
    "        \"Return ONLY valid JSON.\"\n",
    "    )\n",
    "\n",
    "    # We explicitly tell the judge which model produced each answer.\n",
    "    # The judge should not change these names; they are metadata.\n",
    "    judge_user_prompt = f\"\"\"\n",
    "Original question:\n",
    "{question}\n",
    "\n",
    "Answer A (model: {model_a_name}):\n",
    "{answer_a}\n",
    "\n",
    "Answer B (model: {model_b_name}):\n",
    "{answer_b}\n",
    "\n",
    "Evaluate both answers and respond with JSON EXACTLY in this schema:\n",
    "{{\n",
    "  \"model_A\": \"{model_a_name}\",\n",
    "  \"model_B\": \"{model_b_name}\",\n",
    "  \"score_A\": <number 0-10>,\n",
    "  \"score_B\": <number 0-10>,\n",
    "  \"winner\": \"A\" or \"B\" or \"tie\",\n",
    "  \"reason\": \"brief concrete explanation citing criteria\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    response = client_judge.chat.completions.create(\n",
    "        model=judge_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": judge_user_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    verdict_text = response.choices[0].message.content\n",
    "    verdict = json.loads(verdict_text)\n",
    "    return verdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stream answer for A\n",
    "stream_answer(client_cloud,MODEL_CLOUD,messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stream answer for B\n",
    "stream_answer(client_local,MODEL_LOCAL,messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "answer_cloud = get_full_answer(client_cloud, MODEL_CLOUD, messages)\n",
    "answer_local = get_full_answer(client_local, MODEL_LOCAL, messages)\n",
    "\n",
    "verdict = judge_answers(\n",
    "    client_judge=client_cloud,\n",
    "    judge_model=MODEL_JUDGE,\n",
    "    question=question,\n",
    "    answer_a=answer_cloud,\n",
    "    answer_b=answer_local,\n",
    "    model_a_name=MODEL_CLOUD,   \n",
    "    model_b_name=MODEL_LOCAL    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af959f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== MODEL A (cloud) === {MODEL_CLOUD}\")\n",
    "print(f\"=== SCORE A === {verdict['score_A']}\")\n",
    "print()\n",
    "print(f\"=== MODEL B (local) === {MODEL_LOCAL}\")\n",
    "print(f\"=== SCORE B === {verdict['score_B']}\")\n",
    "print()\n",
    "print(f\"=== WINNER === {verdict['winner']}\")\n",
    "print()\n",
    "\n",
    "reason = verdict[\"reason\"].strip()          \n",
    "reason = reason.replace(\". \", \".\\n\")        \n",
    "print(reason)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
