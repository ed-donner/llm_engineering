{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Support Reply Copilot (Compare + Judge + DB + Audio)\n",
        "# \n",
        "# This notebook builds a practical prototype:\n",
        "# - Model 1 (Cloud) vs Model 2 (Local/Ollama)\n",
        "# - A Judge model scores and picks a winner\n",
        "# - A SQLite DB provides approved support macros (so the models don't invent policy-like text)\n",
        "# - Optional TTS reads the winning answer\n",
        "# - A Gradio UI ties it together\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Imports\n",
        "# =========================\n",
        "\n",
        "import os  # For environment variables and paths\n",
        "import json  # For parsing / validating judge output\n",
        "import sqlite3  # For SQLite DB access\n",
        "from typing import Dict, Any, List  # For type hints\n",
        "\n",
        "import gradio as gr  # For UI\n",
        "from dotenv import load_dotenv  # For loading .env\n",
        "from openai import OpenAI  # OpenAI / Ollama-compatible client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=========================\n",
        "# 2) Environment + Clients + Model defaults\n",
        "# =========================\n",
        "\n",
        "# Load environment variables from .env (e.g., OPENAI_API_KEY)\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Create the cloud client (uses OPENAI_API_KEY from env)\n",
        "client_cloud = OpenAI()\n",
        "\n",
        "# Create the local client (Ollama OpenAI-compatible endpoint)\n",
        "# - Ollama must be running: `ollama serve`\n",
        "client_local = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "# Model defaults (as requested)\n",
        "MODEL_1_DEFAULT = \"gpt-4.1-nano\"   # Cloud / Model 1\n",
        "MODEL_2_DEFAULT = \"llama3.1:8b\"    # Local / Model 2 (Ollama)\n",
        "JUDGE_DEFAULT   = \"gpt-4.1-mini\"   # Judge (cloud)\n",
        "\n",
        "# Fixed decoding settings (no UI control)\n",
        "TEMPERATURE_FIXED = 0.2  # Keep small randomness but stable\n",
        "\n",
        "# Check whether Ollama is reachable\n",
        "try:\n",
        "    _ = client_local.models.list()  # Simple ping to local server\n",
        "    ollama_ok = True\n",
        "except Exception:\n",
        "    ollama_ok = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) SQLite DB: Support macros (init + seed)\n",
        "# =========================\n",
        "\n",
        "# SQLite DB file name (created locally next to the notebook)\n",
        "DB_PATH = \"support_macros.db\"\n",
        "\n",
        "def init_macros_db(db_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create the SQLite database schema and seed it with a small set of approved support macros.\n",
        "    This DB is meant to be a controlled source of templates (macros) the assistant can reuse.\n",
        "    \"\"\"\n",
        "    # Connect to the SQLite DB (creates the file if it doesn't exist)\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        cur = conn.cursor()  # Cursor executes SQL statements\n",
        "\n",
        "        # Create the macros table if it does not already exist\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS macros (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            intent TEXT NOT NULL,\n",
        "            title TEXT NOT NULL,\n",
        "            content TEXT NOT NULL,\n",
        "            tags TEXT NOT NULL\n",
        "        );\n",
        "        \"\"\")\n",
        "\n",
        "        # Check how many rows exist (to avoid inserting duplicates on re-run)\n",
        "        cur.execute(\"SELECT COUNT(*) FROM macros;\")\n",
        "        count = cur.fetchone()[0]\n",
        "\n",
        "        # Seed only if the table is empty\n",
        "        if count == 0:\n",
        "            seed_rows = [\n",
        "                # -------------------------\n",
        "                # Billing / refunds\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"refund\",\n",
        "                    \"Double charge / duplicate payment\",\n",
        "                    \"Thanks for reporting this. I can see how frustrating that is. \"\n",
        "                    \"Please share the invoice IDs (or the last 4 digits of the card + the charge dates), and we’ll verify the duplicate charge and process a refund if confirmed. \"\n",
        "                    \"Once validated, refunds typically appear within 5–10 business days depending on your bank.\",\n",
        "                    \"billing,refund,double charge,invoice,card\"\n",
        "                ),\n",
        "                (\n",
        "                    \"billing_issue\",\n",
        "                    \"Invoice / billing discrepancy\",\n",
        "                    \"Thanks for reaching out. Please share your account email and the invoice number(s) affected, and tell us what looks incorrect (amount, plan, dates, taxes). \"\n",
        "                    \"We’ll review and get back with a correction or explanation.\",\n",
        "                    \"billing,invoice,pricing,taxes\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Login / authentication\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"login_help\",\n",
        "                    \"Login issue after password reset\",\n",
        "                    \"Sorry you’re having trouble logging in. Please confirm the email on the account and whether you see an error message. \"\n",
        "                    \"If you recently reset your password, try clearing cache/cookies or using an incognito window; also confirm your device time is correct. \"\n",
        "                    \"If it still fails, we can help verify the account and restore access.\",\n",
        "                    \"login,password reset,auth,cache,cookies\"\n",
        "                ),\n",
        "                (\n",
        "                    \"2fa_issue\",\n",
        "                    \"2FA codes not arriving\",\n",
        "                    \"Thanks for the details. If 2FA codes aren’t arriving, please check your spam folder and confirm the mailbox isn’t blocking automated emails. \"\n",
        "                    \"If you use an authenticator app, confirm the app is synced to the correct time. \"\n",
        "                    \"If you’re locked out, we can initiate a secure recovery—please share your account email and any recent successful login date you remember.\",\n",
        "                    \"2FA,authentication,codes,email,authenticator\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Incident / outage\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"technical_outage\",\n",
        "                    \"Service outage acknowledgement\",\n",
        "                    \"Thanks for flagging this. We’re currently investigating the disruption and we’ll share updates as soon as we have confirmed details. \"\n",
        "                    \"If you can, please send the approximate time it started, your region, and any error message or screenshot—this helps us correlate logs faster.\",\n",
        "                    \"outage,incident,errors,region,screenshot\"\n",
        "                ),\n",
        "                (\n",
        "                    \"eta_request\",\n",
        "                    \"ETA request during incident\",\n",
        "                    \"I understand you need an ETA. We’re actively working on the issue and will provide the next update by <TIME WINDOW>. \"\n",
        "                    \"If you share your region and any error code you’re seeing, I can also confirm whether it matches the incident scope.\",\n",
        "                    \"ETA,incident,update,scope\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Shipping / delivery (generic e-commerce)\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"shipping_delay\",\n",
        "                    \"Shipping delay / late delivery\",\n",
        "                    \"Sorry about the delay. Please share your order number and the delivery address postcode/ZIP, and I’ll check the latest carrier scan and expected delivery date. \"\n",
        "                    \"If the package is stalled, we can start a carrier investigation.\",\n",
        "                    \"shipping,delay,delivery,carrier,order\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Escalation / handoff\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"escalation\",\n",
        "                    \"Escalate to specialist\",\n",
        "                    \"Thanks—this looks like it needs a specialist. I’m escalating it now. \"\n",
        "                    \"To speed things up, please include: your account email, exact steps to reproduce, time of occurrence, and any screenshots/logs. \"\n",
        "                    \"We’ll follow up as soon as we have an update.\",\n",
        "                    \"escalation,specialist,logs,screenshots\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Polite closing\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"closing\",\n",
        "                    \"Polite closing\",\n",
        "                    \"If you reply with the requested details, we’ll take it from there. Thanks for your patience.\",\n",
        "                    \"closing,thanks,patience\"\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "            # Insert all seed rows in one call\n",
        "            cur.executemany(\n",
        "                \"INSERT INTO macros (intent, title, content, tags) VALUES (?, ?, ?, ?);\",\n",
        "                seed_rows\n",
        "            )\n",
        "\n",
        "        # Commit changes to persist DB to disk\n",
        "        conn.commit()\n",
        "\n",
        "# Initialize (create/seed) DB\n",
        "init_macros_db(DB_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) DB Search: search_macros(query, top_k)\n",
        "# =========================\n",
        "\n",
        "def search_macros(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Keyword-based search: extracts simple tokens from the query and matches ANY token\n",
        "    across intent/title/content/tags using LIKE.\n",
        "    \"\"\"\n",
        "    q = (query or \"\").strip().lower()\n",
        "    if not q:\n",
        "        return {\"query\": query, \"hits\": []}\n",
        "\n",
        "    # 1) Very simple tokenization: keep alphanumerics, split on whitespace\n",
        "    tokens = []\n",
        "    current = []\n",
        "    for ch in q:\n",
        "        if ch.isalnum():\n",
        "            current.append(ch)\n",
        "        else:\n",
        "            if current:\n",
        "                tokens.append(\"\".join(current))\n",
        "                current = []\n",
        "    if current:\n",
        "        tokens.append(\"\".join(current))\n",
        "\n",
        "    # 2) Remove very short tokens (noise) and cap how many we use\n",
        "    tokens = [t for t in tokens if len(t) >= 3]\n",
        "    tokens = tokens[:10]  # limit to keep query small and fast\n",
        "\n",
        "    if not tokens:\n",
        "        return {\"query\": query, \"hits\": []}\n",
        "\n",
        "    # 3) Build OR conditions: match any token in any field\n",
        "    #    (intent/title/content/tags)\n",
        "    where_clauses = []\n",
        "    params = []\n",
        "    for t in tokens:\n",
        "        like = f\"%{t}%\"\n",
        "        where_clauses.append(\"(lower(intent) LIKE ? OR lower(title) LIKE ? OR lower(content) LIKE ? OR lower(tags) LIKE ?)\")\n",
        "        params.extend([like, like, like, like])\n",
        "\n",
        "    where_sql = \" OR \".join(where_clauses)\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT id, intent, title, content\n",
        "        FROM macros\n",
        "        WHERE {where_sql}\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "\n",
        "    with sqlite3.connect(DB_PATH) as conn:\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(sql, (*params, top_k))\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "    hits: List[Dict[str, Any]] = []\n",
        "    for mid, intent, title, content in rows:\n",
        "        excerpt = content.strip()\n",
        "        if len(excerpt) > 280:\n",
        "            excerpt = excerpt[:280] + \"...\"\n",
        "        hits.append({\n",
        "            \"id\": mid,\n",
        "            \"intent\": intent,\n",
        "            \"title\": title,\n",
        "            \"excerpt\": excerpt,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "    return {\"query\": query, \"hits\": hits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5) DB Trace formatting (for UI)\n",
        "# =========================\n",
        "\n",
        "def format_db_trace(db_result: Dict[str, Any]) -> str:\n",
        "    \"\"\"Format DB hits into a small Markdown block for transparency.\"\"\"\n",
        "    # Handle missing result\n",
        "    if not db_result:\n",
        "        return \"No DB lookup.\"\n",
        "\n",
        "    # Extract hits\n",
        "    hits = db_result.get(\"hits\", [])\n",
        "\n",
        "    # If no hits, show that explicitly\n",
        "    if not hits:\n",
        "        return (\n",
        "            \"### DB macros\\n\"\n",
        "            f\"- Query: `{db_result.get('query','')}`\\n\"\n",
        "            \"- Result: **No hits**\"\n",
        "        )\n",
        "\n",
        "    # Build a compact list of hits\n",
        "    lines = [\n",
        "        \"### DB macros\",\n",
        "        f\"- Query: `{db_result.get('query','')}`\",\n",
        "        f\"- Hits: **{len(hits)}**\"\n",
        "    ]\n",
        "    for h in hits:\n",
        "        lines.append(f\"  - (id={h['id']}) **{h['intent']}** — {h['title']}\")\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6) System prompt (Support Agent)\n",
        "# =========================\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a professional customer support agent.\n",
        "Your priority is factual accuracy and clarity.\n",
        "You must not invent policies, SLAs, refunds, ETAs, pricing, or account details.\n",
        "If information is missing, ask for the minimum necessary details.\n",
        "Write a single email-style reply, concise and courteous, with clear next steps.\n",
        "If an internal \"APPROVED MACROS\" reference is provided, reuse it and stay consistent with it.\n",
        "Respond in English.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7) Helper: Build \"APPROVED MACROS\" block for LLM context\n",
        "# =========================\n",
        "\n",
        "def build_approved_macros_block(db_result: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Convert DB hits into a high-priority reference block for the LLM.\n",
        "    This block is injected as a SYSTEM message so it is treated as authoritative guidance.\n",
        "    \"\"\"\n",
        "    # Pull hits\n",
        "    hits = db_result.get(\"hits\", []) if db_result else []\n",
        "\n",
        "    # If no hits, still provide guidance (so model doesn't hallucinate)\n",
        "    if not hits:\n",
        "        return (\n",
        "            \"APPROVED MACROS:\\n\"\n",
        "            \"No matching macros found.\\n\"\n",
        "            \"Instruction: Ask for missing details and respond professionally without inventing policies.\"\n",
        "        )\n",
        "\n",
        "    # Build the block with multiple macros\n",
        "    lines = [\"APPROVED MACROS (use and adapt as appropriate):\"]\n",
        "    for h in hits:\n",
        "        lines.append(\n",
        "            f\"\\n[Macro id={h['id']} | intent={h['intent']} | title={h['title']}]\\n\"\n",
        "            f\"{h['content']}\"\n",
        "        )\n",
        "    lines.append(\"\\nInstruction: Prefer using these macros; do not invent policy details not present above.\")\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 8) LLM Streaming helper (no tool-calling; DB is injected deterministically)\n",
        "# =========================\n",
        "\n",
        "def stream_answer(client: OpenAI, model: str, messages: List[Dict[str, str]]):\n",
        "    \"\"\"\n",
        "    Stream assistant output and yield incremental text for UI updates.\n",
        "    \"\"\"\n",
        "    # Create streaming chat completion\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,                 # Model name\n",
        "        messages=messages,           # Chat history\n",
        "        stream=True,                 # Enable streaming\n",
        "        temperature=TEMPERATURE_FIXED\n",
        "    )\n",
        "\n",
        "    # Accumulate text as tokens arrive\n",
        "    text = \"\"\n",
        "\n",
        "    # Iterate over streaming chunks\n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta  # Incremental delta\n",
        "        if delta and delta.content:     # If text content exists\n",
        "            text += delta.content       # Append to full text\n",
        "            yield text                  # Yield partial output for UI\n",
        "\n",
        "    # Yield final text once more (convenient for callers)\n",
        "    yield text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 9) Judge function (strict JSON verdict)\n",
        "# =========================\n",
        "\n",
        "def judge_two_answers(\n",
        "    client: OpenAI,\n",
        "    judge_model: str,\n",
        "    customer_message: str,\n",
        "    answer_a: str,\n",
        "    answer_b: str,\n",
        "    model_a_name: str,\n",
        "    model_b_name: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Judge compares two answers and returns a strict JSON verdict.\n",
        "\n",
        "    The judge must output:\n",
        "      { model_A, model_B, score_A, score_B, winner, reason }\n",
        "    \"\"\"\n",
        "    # Define judge system prompt\n",
        "    judge_system_prompt = (\n",
        "        \"You are an impartial judge evaluating two customer-support answers.\\n\"\n",
        "        \"Score each answer from 0 to 10 based on:\\n\"\n",
        "        \"1) Factual correctness (no invented policies, SLAs, ETAs)\\n\"\n",
        "        \"2) Professional tone\\n\"\n",
        "        \"3) Clarity and actionable next steps\\n\"\n",
        "        \"4) Completeness given the customer message\\n\"\n",
        "        \"Return ONLY valid JSON.\"\n",
        "    )\n",
        "\n",
        "    # Define judge user prompt (includes both answers)\n",
        "    judge_user_prompt = f\"\"\"\n",
        "Customer message:\n",
        "{customer_message}\n",
        "\n",
        "Answer A (model: {model_a_name}):\n",
        "{answer_a}\n",
        "\n",
        "Answer B (model: {model_b_name}):\n",
        "{answer_b}\n",
        "\n",
        "Respond with JSON EXACTLY in this schema:\n",
        "{{\n",
        "  \"model_A\": \"{model_a_name}\",\n",
        "  \"model_B\": \"{model_b_name}\",\n",
        "  \"score_A\": <number 0-10>,\n",
        "  \"score_B\": <number 0-10>,\n",
        "  \"winner\": \"A\" or \"B\" or \"tie\",\n",
        "  \"reason\": \"brief concrete explanation citing criteria\"\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Call judge model using JSON response format\n",
        "    resp = client.chat.completions.create(\n",
        "        model=judge_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}  # Request JSON object\n",
        "    )\n",
        "\n",
        "    # Extract the JSON text\n",
        "    verdict_text = resp.choices[0].message.content\n",
        "\n",
        "    # Parse JSON\n",
        "    verdict = json.loads(verdict_text)\n",
        "\n",
        "    # Minimal validation\n",
        "    required = [\"model_A\", \"model_B\", \"score_A\", \"score_B\", \"winner\", \"reason\"]\n",
        "    for k in required:\n",
        "        if k not in verdict:\n",
        "            raise ValueError(f\"Judge verdict missing field: {k}\")\n",
        "\n",
        "    if verdict[\"winner\"] not in [\"A\", \"B\", \"tie\"]:\n",
        "        raise ValueError(\"Judge winner must be 'A', 'B', or 'tie'\")\n",
        "\n",
        "    return verdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 10) Optional TTS helper (winner audio)\n",
        "# =========================\n",
        "\n",
        "def tts_to_file(client: OpenAI, text: str, filename: str = \"winner_tts.mp3\") -> str:\n",
        "    \"\"\"\n",
        "    Generate TTS audio for the provided text and write it to an MP3 file.\n",
        "    Returns the file path.\n",
        "    \"\"\"\n",
        "    # Create speech audio bytes from the TTS endpoint\n",
        "    speech = client.audio.speech.create(\n",
        "        model=\"gpt-4o-mini-tts\",  # TTS model\n",
        "        voice=\"onyx\",             # Voice name\n",
        "        input=text                # Text to synthesize\n",
        "    )\n",
        "\n",
        "    # Save the audio bytes to disk\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(speech.content)\n",
        "\n",
        "    # Return filename so Gradio can load it\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 11) Compare runner (DB injected to BOTH models, then Judge, then optional TTS)\n",
        "# =========================\n",
        "\n",
        "def compare_mode_run(\n",
        "    customer_message: str,\n",
        "    system_prompt: str,\n",
        "    cloud_model: str,\n",
        "    local_model: str,\n",
        "    judge_model: str,\n",
        "    enable_db: bool,\n",
        "    enable_tts: bool\n",
        "):\n",
        "    \"\"\"\n",
        "    Generator for Gradio streaming updates:\n",
        "      1) (Optional) Search DB macros deterministically\n",
        "      2) Inject approved macros into BOTH model contexts (system message)\n",
        "      3) Stream Model 1 (cloud) answer\n",
        "      4) Stream Model 2 (local) answer\n",
        "      5) Judge compares and picks a winner\n",
        "      6) (Optional) Generate TTS for winner\n",
        "    \"\"\"\n",
        "    # If local backend is not available, we cannot run Model 2 or judge\n",
        "    if not ollama_ok:\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n⚠️ Local backend unavailable (Ollama not reachable).\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n⚠️ Start Ollama with: `ollama serve`.\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n⚠️ Cannot judge without Model 2.\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, \"No DB lookup.\"\n",
        "        return\n",
        "\n",
        "    # 1) Deterministic DB lookup (if enabled)\n",
        "    db_result = search_macros(customer_message, top_k=3) if enable_db else {\"query\": \"\", \"hits\": []}\n",
        "\n",
        "    # 2) Build DB trace for UI\n",
        "    db_trace_md = format_db_trace(db_result) if enable_db else \"DB disabled.\"\n",
        "\n",
        "    # 3) Build approved macros reference for LLM context\n",
        "    approved_macros = build_approved_macros_block(db_result) if enable_db else \"\"\n",
        "\n",
        "    # 4) Build shared messages for both models\n",
        "    messages: List[Dict[str, str]] = []\n",
        "    messages.append({\"role\": \"system\", \"content\": system_prompt.strip()})  # Core behavior\n",
        "\n",
        "    # Inject macros as an authoritative reference (system-level)\n",
        "    if enable_db:\n",
        "        messages.append({\"role\": \"system\", \"content\": approved_macros})\n",
        "\n",
        "    # Add the customer message as the user message\n",
        "    messages.append({\"role\": \"user\", \"content\": customer_message.strip()})\n",
        "\n",
        "    # 5) Stream Model 1 (Cloud)\n",
        "    cloud_text = \"\"\n",
        "    for partial in stream_answer(client_cloud, cloud_model, messages):\n",
        "        cloud_text = partial\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n*(waiting...)*\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n*(waiting...)*\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, db_trace_md\n",
        "\n",
        "    # 6) Stream Model 2 (Local/Ollama)\n",
        "    local_text = \"\"\n",
        "    for partial in stream_answer(client_local, local_model, messages):\n",
        "        local_text = partial\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n{local_text}\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n*(waiting...)*\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, db_trace_md\n",
        "\n",
        "    # 7) Judge compares both answers\n",
        "    verdict = judge_two_answers(\n",
        "        client=client_cloud,\n",
        "        judge_model=judge_model,\n",
        "        customer_message=customer_message.strip(),\n",
        "        answer_a=cloud_text,\n",
        "        answer_b=local_text,\n",
        "        model_a_name=cloud_model,\n",
        "        model_b_name=local_model\n",
        "    )\n",
        "\n",
        "    # 8) Build judge panel Markdown\n",
        "    j_panel = (\n",
        "        f\"## Judge (REMOTE) — `{judge_model}`\\n\\n\"\n",
        "        f\"- Model A (Model 1): **{verdict['model_A']}** — score **{verdict['score_A']}/10**\\n\"\n",
        "        f\"- Model B (Model 2): **{verdict['model_B']}** — score **{verdict['score_B']}/10**\\n\"\n",
        "        f\"- Winner: **{verdict['winner']}**\\n\\n\"\n",
        "        f\"**Reason:** {verdict['reason']}\"\n",
        "    )\n",
        "\n",
        "    # 9) Select the winning answer for TTS (tie -> Model 1)\n",
        "    winner_text = cloud_text if verdict[\"winner\"] in [\"A\", \"tie\"] else local_text\n",
        "\n",
        "    # 10) Generate TTS if enabled\n",
        "    audio_path = tts_to_file(client_cloud, winner_text) if enable_tts else None\n",
        "\n",
        "    # 11) Final yield with stable panels\n",
        "    m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "    m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n{local_text}\"\n",
        "\n",
        "    yield m1_panel, m2_panel, j_panel, audio_path, db_trace_md\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 12) Gradio UI (Compare + Judge + DB + Audio)\n",
        "# =========================\n",
        "\n",
        "with gr.Blocks(title=\"Support Reply Copilot (Compare + Judge + DB + Audio)\") as demo:\n",
        "    # Header\n",
        "    gr.Markdown(\"# Support Reply Copilot\")\n",
        "    gr.Markdown(\"Compare two models, judge their replies, optionally use DB macros, and optionally generate TTS for the winner.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Left column: inputs\n",
        "        with gr.Column(scale=1):\n",
        "            # Customer input\n",
        "            customer_in = gr.Textbox(\n",
        "                label=\"Customer message\",\n",
        "                lines=6,\n",
        "                placeholder=\"\"  # No background text\n",
        "            )\n",
        "\n",
        "            # Clickable examples (fill the textbox)\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    \"Hi, I was charged twice for my subscription this month. Can you fix this and confirm when I’ll get the refund?\",\n",
        "                    \"I can’t log in after resetting my password—2FA codes never arrive. Please help ASAP.\",\n",
        "                    \"Your service has been down for 30 minutes. What’s the ETA and will we receive a credit?\"\n",
        "                ],\n",
        "                inputs=[customer_in],\n",
        "                label=\"Examples (click to fill)\"\n",
        "            )\n",
        "\n",
        "            # Model selectors\n",
        "            model1 = gr.Dropdown(\n",
        "                choices=[\"gpt-4.1-nano\", \"gpt-4.1-mini\"],\n",
        "                value=MODEL_1_DEFAULT,\n",
        "                label=\"Model 1 (REMOTE / Cloud)\"\n",
        "            )\n",
        "\n",
        "            model2 = gr.Dropdown(\n",
        "                choices=[\"llama3.1:8b\"],\n",
        "                value=MODEL_2_DEFAULT,\n",
        "                label=\"Model 2 (LOCAL / Ollama)\"\n",
        "            )\n",
        "\n",
        "            judge_model = gr.Dropdown(\n",
        "                choices=[\"gpt-4.1-mini\", \"gpt-4.1-nano\"],\n",
        "                value=JUDGE_DEFAULT,\n",
        "                label=\"Judge (REMOTE)\"\n",
        "            )\n",
        "\n",
        "            # Settings section\n",
        "            gr.Markdown(\"### Settings\")\n",
        "\n",
        "            system_prompt_in = gr.Textbox(\n",
        "                label=\"System prompt\",\n",
        "                value=DEFAULT_SYSTEM_PROMPT,\n",
        "                lines=8\n",
        "            )\n",
        "\n",
        "            enable_db = gr.Checkbox(value=True, label=\"Enable DB macros (SQLite)\")\n",
        "            enable_tts = gr.Checkbox(value=False, label=\"Enable TTS (winner audio)\")\n",
        "\n",
        "            # Buttons\n",
        "            run_btn = gr.Button(\"Run\")\n",
        "            clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "        # Right column: outputs\n",
        "        with gr.Column(scale=2):\n",
        "            out_m1 = gr.Markdown(label=\"Model 1 output\")\n",
        "            out_m2 = gr.Markdown(label=\"Model 2 output\")\n",
        "            out_judge = gr.Markdown(label=\"Judge output\")\n",
        "            out_audio = gr.Audio(label=\"Winner audio (TTS)\", autoplay=False)\n",
        "            out_db = gr.Markdown(\"No DB lookup.\")\n",
        "\n",
        "    # Clear callback\n",
        "    def clear_all():\n",
        "        \"\"\"Reset UI outputs.\"\"\"\n",
        "        return \"\", \"\", \"\", None, \"No DB lookup.\"\n",
        "\n",
        "    # Connect clear button\n",
        "    clear_btn.click(\n",
        "        fn=clear_all,\n",
        "        inputs=[],\n",
        "        outputs=[out_m1, out_m2, out_judge, out_audio, out_db]\n",
        "    )\n",
        "\n",
        "    # Run callback (streaming generator bridge)\n",
        "    def run_app(customer_msg, m1, m2, j, sys_prompt, use_db, use_tts):\n",
        "        \"\"\"Run the compare pipeline and stream intermediate outputs to Gradio.\"\"\"\n",
        "        # Validate input\n",
        "        if not customer_msg or not customer_msg.strip():\n",
        "            yield (\n",
        "                \"## Model 1\\n\\n⚠️ Please paste a customer message.\",\n",
        "                \"## Model 2\\n\\n*(waiting...)*\",\n",
        "                \"## Judge\\n\\n*(waiting...)*\",\n",
        "                None,\n",
        "                \"No DB lookup.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # Create generator\n",
        "        gen = compare_mode_run(\n",
        "            customer_message=customer_msg,\n",
        "            system_prompt=sys_prompt,\n",
        "            cloud_model=m1,\n",
        "            local_model=m2,\n",
        "            judge_model=j,\n",
        "            enable_db=use_db,\n",
        "            enable_tts=use_tts\n",
        "        )\n",
        "\n",
        "        # Stream each intermediate result to the UI\n",
        "        for a, b, c, audio_path, db_md in gen:\n",
        "            yield a, b, c, audio_path, db_md\n",
        "\n",
        "    # Connect run button to callback\n",
        "    run_btn.click(\n",
        "        fn=run_app,\n",
        "        inputs=[customer_in, model1, model2, judge_model, system_prompt_in, enable_db, enable_tts],\n",
        "        outputs=[out_m1, out_m2, out_judge, out_audio, out_db]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
