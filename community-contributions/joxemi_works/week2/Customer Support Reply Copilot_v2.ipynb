{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Support Reply Copilot (Compare + Judge + DB Mode + Audio)\n",
        "# \n",
        "# This notebook builds a practical prototype:\n",
        "# - Model 1 (Cloud) vs Model 2 (Local/Ollama), with streaming output\n",
        "# - A Judge model scores both answers and picks a winner\n",
        "# - A SQLite DB provides approved support macros (to reduce invented policy-like text)\n",
        "# - A DB mode selector controls when the DB is used:\n",
        "#     - Always: the app queries the DB before calling any LLM\n",
        "#     - Auto: the REMOTE model decides whether to query the DB via tool-calling; if it queries and gets no hits,\n",
        "#             the app still injects a \"no macros found\" guardrail to prevent hallucinations\n",
        "#     - Off: the DB is never queried\n",
        "# - Optional TTS reads the winning answer\n",
        "# - A Gradio UI ties it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Imports\n",
        "# =========================\n",
        "\n",
        "import os  # For environment variables and paths\n",
        "import json  # For parsing / validating judge output\n",
        "import sqlite3  # For SQLite DB access\n",
        "from typing import Dict, Any, List  # For type hints\n",
        "\n",
        "import gradio as gr  # For UI\n",
        "from dotenv import load_dotenv  # For loading .env\n",
        "from openai import OpenAI  # OpenAI / Ollama-compatible client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=========================\n",
        "# 2) Environment + Clients + Model defaults\n",
        "# =========================\n",
        "\n",
        "# Load environment variables from .env (e.g., OPENAI_API_KEY)\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Create the cloud client (uses OPENAI_API_KEY from env)\n",
        "client_cloud = OpenAI()\n",
        "\n",
        "# Create the local client (Ollama OpenAI-compatible endpoint)\n",
        "# - Ollama must be running: `ollama serve`\n",
        "client_local = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "# Model defaults (as requested)\n",
        "MODEL_1_DEFAULT = \"gpt-4.1-nano\"   # Cloud / Model 1\n",
        "MODEL_2_DEFAULT = \"llama3.1:8b\"    # Local / Model 2 (Ollama)\n",
        "JUDGE_DEFAULT   = \"gpt-4.1-mini\"   # Judge (cloud)\n",
        "\n",
        "# Fixed decoding settings (no UI control)\n",
        "TEMPERATURE_FIXED = 0.2  # Keep small randomness but stable\n",
        "\n",
        "# Check whether Ollama is reachable\n",
        "try:\n",
        "    _ = client_local.models.list()  # Simple ping to local server\n",
        "    ollama_ok = True\n",
        "except Exception:\n",
        "    ollama_ok = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) SQLite DB: Support macros (init + seed)\n",
        "# =========================\n",
        "\n",
        "# SQLite DB file name (created locally next to the notebook)\n",
        "DB_PATH = \"support_macros.db\"\n",
        "\n",
        "def init_macros_db(db_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Create the SQLite database schema and seed it with a small set of approved support macros.\n",
        "    This DB is meant to be a controlled source of templates (macros) the assistant can reuse.\n",
        "    \"\"\"\n",
        "    # Connect to the SQLite DB (creates the file if it doesn't exist)\n",
        "    with sqlite3.connect(db_path) as conn:\n",
        "        cur = conn.cursor()  # Cursor executes SQL statements\n",
        "\n",
        "        # Create the macros table if it does not already exist\n",
        "        cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS macros (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            intent TEXT NOT NULL,\n",
        "            title TEXT NOT NULL,\n",
        "            content TEXT NOT NULL,\n",
        "            tags TEXT NOT NULL\n",
        "        );\n",
        "        \"\"\")\n",
        "\n",
        "        # Check how many rows exist (to avoid inserting duplicates on re-run)\n",
        "        cur.execute(\"SELECT COUNT(*) FROM macros;\")\n",
        "        count = cur.fetchone()[0]\n",
        "\n",
        "        # Seed only if the table is empty\n",
        "        if count == 0:\n",
        "            seed_rows = [\n",
        "                # -------------------------\n",
        "                # Billing / refunds\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"refund\",\n",
        "                    \"Double charge / duplicate payment\",\n",
        "                    \"Thanks for reporting this. I can see how frustrating that is. \"\n",
        "                    \"Please share the invoice IDs (or the last 4 digits of the card + the charge dates), and we’ll verify the duplicate charge and process a refund if confirmed. \"\n",
        "                    \"Once validated, refunds typically appear within 5–10 business days depending on your bank.\",\n",
        "                    \"billing,refund,double charge,invoice,card\"\n",
        "                ),\n",
        "                (\n",
        "                    \"billing_issue\",\n",
        "                    \"Invoice / billing discrepancy\",\n",
        "                    \"Thanks for reaching out. Please share your account email and the invoice number(s) affected, and tell us what looks incorrect (amount, plan, dates, taxes). \"\n",
        "                    \"We’ll review and get back with a correction or explanation.\",\n",
        "                    \"billing,invoice,pricing,taxes\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Login / authentication\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"login_help\",\n",
        "                    \"Login issue after password reset\",\n",
        "                    \"Sorry you’re having trouble logging in. Please confirm the email on the account and whether you see an error message. \"\n",
        "                    \"If you recently reset your password, try clearing cache/cookies or using an incognito window; also confirm your device time is correct. \"\n",
        "                    \"If it still fails, we can help verify the account and restore access.\",\n",
        "                    \"login,password reset,auth,cache,cookies\"\n",
        "                ),\n",
        "                (\n",
        "                    \"2fa_issue\",\n",
        "                    \"2FA codes not arriving\",\n",
        "                    \"Thanks for the details. If 2FA codes aren’t arriving, please check your spam folder and confirm the mailbox isn’t blocking automated emails. \"\n",
        "                    \"If you use an authenticator app, confirm the app is synced to the correct time. \"\n",
        "                    \"If you’re locked out, we can initiate a secure recovery—please share your account email and any recent successful login date you remember.\",\n",
        "                    \"2FA,authentication,codes,email,authenticator\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Incident / outage\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"technical_outage\",\n",
        "                    \"Service outage acknowledgement\",\n",
        "                    \"Thanks for flagging this. We’re currently investigating the disruption and we’ll share updates as soon as we have confirmed details. \"\n",
        "                    \"If you can, please send the approximate time it started, your region, and any error message or screenshot—this helps us correlate logs faster.\",\n",
        "                    \"outage,incident,errors,region,screenshot\"\n",
        "                ),\n",
        "                (\n",
        "                    \"eta_request\",\n",
        "                    \"ETA request during incident\",\n",
        "                    \"I understand you need an ETA. We’re actively working on the issue and will provide the next update by <TIME WINDOW>. \"\n",
        "                    \"If you share your region and any error code you’re seeing, I can also confirm whether it matches the incident scope.\",\n",
        "                    \"ETA,incident,update,scope\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Shipping / delivery (generic e-commerce)\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"shipping_delay\",\n",
        "                    \"Shipping delay / late delivery\",\n",
        "                    \"Sorry about the delay. Please share your order number and the delivery address postcode/ZIP, and I’ll check the latest carrier scan and expected delivery date. \"\n",
        "                    \"If the package is stalled, we can start a carrier investigation.\",\n",
        "                    \"shipping,delay,delivery,carrier,order\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Escalation / handoff\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"escalation\",\n",
        "                    \"Escalate to specialist\",\n",
        "                    \"Thanks—this looks like it needs a specialist. I’m escalating it now. \"\n",
        "                    \"To speed things up, please include: your account email, exact steps to reproduce, time of occurrence, and any screenshots/logs. \"\n",
        "                    \"We’ll follow up as soon as we have an update.\",\n",
        "                    \"escalation,specialist,logs,screenshots\"\n",
        "                ),\n",
        "\n",
        "                # -------------------------\n",
        "                # Polite closing\n",
        "                # -------------------------\n",
        "                (\n",
        "                    \"closing\",\n",
        "                    \"Polite closing\",\n",
        "                    \"If you reply with the requested details, we’ll take it from there. Thanks for your patience.\",\n",
        "                    \"closing,thanks,patience\"\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "            # Insert all seed rows in one call\n",
        "            cur.executemany(\n",
        "                \"INSERT INTO macros (intent, title, content, tags) VALUES (?, ?, ?, ?);\",\n",
        "                seed_rows\n",
        "            )\n",
        "\n",
        "        # Commit changes to persist DB to disk\n",
        "        conn.commit()\n",
        "\n",
        "# Initialize (create/seed) DB\n",
        "init_macros_db(DB_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) DB Search: search_macros(query, top_k)\n",
        "# =========================\n",
        "\n",
        "def search_macros(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Keyword-based search: extracts simple tokens from the query and matches ANY token\n",
        "    across intent/title/content/tags using LIKE.\n",
        "    \"\"\"\n",
        "    q = (query or \"\").strip().lower()\n",
        "    if not q:\n",
        "        return {\"query\": query, \"hits\": []}\n",
        "\n",
        "    # 1) Very simple tokenization: keep alphanumerics, split on whitespace\n",
        "    tokens = []\n",
        "    current = []\n",
        "    for ch in q:\n",
        "        if ch.isalnum():\n",
        "            current.append(ch)\n",
        "        else:\n",
        "            if current:\n",
        "                tokens.append(\"\".join(current))\n",
        "                current = []\n",
        "    if current:\n",
        "        tokens.append(\"\".join(current))\n",
        "\n",
        "    # 2) Remove very short tokens (noise) and cap how many we use\n",
        "    tokens = [t for t in tokens if len(t) >= 3]\n",
        "    tokens = tokens[:10]  # limit to keep query small and fast\n",
        "\n",
        "    if not tokens:\n",
        "        return {\"query\": query, \"hits\": []}\n",
        "\n",
        "    # 3) Build OR conditions: match any token in any field\n",
        "    #    (intent/title/content/tags)\n",
        "    where_clauses = []\n",
        "    params = []\n",
        "    for t in tokens:\n",
        "        like = f\"%{t}%\"\n",
        "        where_clauses.append(\"(lower(intent) LIKE ? OR lower(title) LIKE ? OR lower(content) LIKE ? OR lower(tags) LIKE ?)\")\n",
        "        params.extend([like, like, like, like])\n",
        "\n",
        "    where_sql = \" OR \".join(where_clauses)\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT id, intent, title, content\n",
        "        FROM macros\n",
        "        WHERE {where_sql}\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "\n",
        "    with sqlite3.connect(DB_PATH) as conn:\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(sql, (*params, top_k))\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "    hits: List[Dict[str, Any]] = []\n",
        "    for mid, intent, title, content in rows:\n",
        "        excerpt = content.strip()\n",
        "        if len(excerpt) > 280:\n",
        "            excerpt = excerpt[:280] + \"...\"\n",
        "        hits.append({\n",
        "            \"id\": mid,\n",
        "            \"intent\": intent,\n",
        "            \"title\": title,\n",
        "            \"excerpt\": excerpt,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "    return {\"query\": query, \"hits\": hits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5) DB Trace formatting (for UI)\n",
        "# =========================\n",
        "\n",
        "def format_db_trace(db_result: Dict[str, Any]) -> str:\n",
        "    \"\"\"Format DB hits into a small Markdown block for transparency.\"\"\"\n",
        "    # Handle missing result\n",
        "    if not db_result:\n",
        "        return \"No DB lookup.\"\n",
        "\n",
        "    # Extract hits\n",
        "    hits = db_result.get(\"hits\", [])\n",
        "\n",
        "    # If no hits, show that explicitly\n",
        "    if not hits:\n",
        "        return (\n",
        "            \"### DB macros\\n\"\n",
        "            f\"- Query: `{db_result.get('query','')}`\\n\"\n",
        "            \"- Result: **No hits**\"\n",
        "        )\n",
        "\n",
        "    # Build a compact list of hits\n",
        "    lines = [\n",
        "        \"### DB macros\",\n",
        "        f\"- Query: `{db_result.get('query','')}`\",\n",
        "        f\"- Hits: **{len(hits)}**\"\n",
        "    ]\n",
        "    for h in hits:\n",
        "        lines.append(f\"  - (id={h['id']}) **{h['intent']}** — {h['title']}\")\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6) System prompt (Support Agent)\n",
        "# =========================\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "You are a professional customer support agent.\n",
        "Your priority is factual accuracy and clarity.\n",
        "You must not invent policies, SLAs, refunds, ETAs, pricing, or account details.\n",
        "If information is missing, ask for the minimum necessary details.\n",
        "Write a single email-style reply, concise and courteous, with clear next steps.\n",
        "If an internal \"APPROVED MACROS\" reference is provided, reuse it and stay consistent with it.\n",
        "Respond in English.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7) Helper: Build \"APPROVED MACROS\" block for LLM context\n",
        "# =========================\n",
        "\n",
        "def build_approved_macros_block(db_result: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Convert DB hits into a high-priority reference block for the LLM.\n",
        "    This block is injected as a SYSTEM message so it is treated as authoritative guidance.\n",
        "    \"\"\"\n",
        "    # Pull hits\n",
        "    hits = db_result.get(\"hits\", []) if db_result else []\n",
        "\n",
        "    # If no hits, still provide guidance (so model doesn't hallucinate)\n",
        "    if not hits:\n",
        "        return (\n",
        "            \"APPROVED MACROS:\\n\"\n",
        "            \"No matching macros found.\\n\"\n",
        "            \"Instruction: Ask for missing details and respond professionally without inventing policies.\"\n",
        "        )\n",
        "\n",
        "    # Build the block with multiple macros\n",
        "    lines = [\"APPROVED MACROS (use and adapt as appropriate):\"]\n",
        "    for h in hits:\n",
        "        lines.append(\n",
        "            f\"\\n[Macro id={h['id']} | intent={h['intent']} | title={h['title']}]\\n\"\n",
        "            f\"{h['content']}\"\n",
        "        )\n",
        "    lines.append(\"\\nInstruction: Prefer using these macros; do not invent policy details not present above.\")\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 8) LLM Streaming helper (no tool-calling; DB is injected deterministically)\n",
        "# =========================\n",
        "\n",
        "def stream_answer(client: OpenAI, model: str, messages: List[Dict[str, str]]):\n",
        "    \"\"\"\n",
        "    Stream assistant output and yield incremental text for UI updates.\n",
        "    \"\"\"\n",
        "    # Create streaming chat completion\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,                 # Model name\n",
        "        messages=messages,           # Chat history\n",
        "        stream=True,                 # Enable streaming\n",
        "        temperature=TEMPERATURE_FIXED\n",
        "    )\n",
        "\n",
        "    # Accumulate text as tokens arrive\n",
        "    text = \"\"\n",
        "\n",
        "    # Iterate over streaming chunks\n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta  # Incremental delta\n",
        "        if delta and delta.content:     # If text content exists\n",
        "            text += delta.content       # Append to full text\n",
        "            yield text                  # Yield partial output for UI\n",
        "\n",
        "    # Yield final text once more (convenient for callers)\n",
        "    yield text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 9) Judge function (strict JSON verdict)\n",
        "# =========================\n",
        "\n",
        "def judge_two_answers(\n",
        "    client: OpenAI,\n",
        "    judge_model: str,\n",
        "    customer_message: str,\n",
        "    answer_a: str,\n",
        "    answer_b: str,\n",
        "    model_a_name: str,\n",
        "    model_b_name: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Judge compares two answers and returns a strict JSON verdict.\n",
        "\n",
        "    The judge must output:\n",
        "      { model_A, model_B, score_A, score_B, winner, reason }\n",
        "    \"\"\"\n",
        "    # Define judge system prompt\n",
        "    judge_system_prompt = (\n",
        "        \"You are an impartial judge evaluating two customer-support answers.\\n\"\n",
        "        \"Score each answer from 0 to 10 based on:\\n\"\n",
        "        \"1) Factual correctness (no invented policies, SLAs, ETAs)\\n\"\n",
        "        \"2) Professional tone\\n\"\n",
        "        \"3) Clarity and actionable next steps\\n\"\n",
        "        \"4) Completeness given the customer message\\n\"\n",
        "        \"Return ONLY valid JSON.\"\n",
        "    )\n",
        "\n",
        "    # Define judge user prompt (includes both answers)\n",
        "    judge_user_prompt = f\"\"\"\n",
        "Customer message:\n",
        "{customer_message}\n",
        "\n",
        "Answer A (model: {model_a_name}):\n",
        "{answer_a}\n",
        "\n",
        "Answer B (model: {model_b_name}):\n",
        "{answer_b}\n",
        "\n",
        "Respond with JSON EXACTLY in this schema:\n",
        "{{\n",
        "  \"model_A\": \"{model_a_name}\",\n",
        "  \"model_B\": \"{model_b_name}\",\n",
        "  \"score_A\": <number 0-10>,\n",
        "  \"score_B\": <number 0-10>,\n",
        "  \"winner\": \"A\" or \"B\" or \"tie\",\n",
        "  \"reason\": \"brief concrete explanation citing criteria\"\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Call judge model using JSON response format\n",
        "    resp = client.chat.completions.create(\n",
        "        model=judge_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}  # Request JSON object\n",
        "    )\n",
        "\n",
        "    # Extract the JSON text\n",
        "    verdict_text = resp.choices[0].message.content\n",
        "\n",
        "    # Parse JSON\n",
        "    verdict = json.loads(verdict_text)\n",
        "\n",
        "    # Minimal validation\n",
        "    required = [\"model_A\", \"model_B\", \"score_A\", \"score_B\", \"winner\", \"reason\"]\n",
        "    for k in required:\n",
        "        if k not in verdict:\n",
        "            raise ValueError(f\"Judge verdict missing field: {k}\")\n",
        "\n",
        "    if verdict[\"winner\"] not in [\"A\", \"B\", \"tie\"]:\n",
        "        raise ValueError(\"Judge winner must be 'A', 'B', or 'tie'\")\n",
        "\n",
        "    return verdict\n",
        "\n",
        "\n",
        "# Tools + Auto-mode helper (remote decides) + Tool-call handler\n",
        "\n",
        "def build_search_macros_tool_schema() -> Dict[str, Any]:\n",
        "    \"\"\"OpenAI tool schema for letting the REMOTE model request a DB macro search.\"\"\"\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_macros\",\n",
        "            \"description\": \"Search approved customer support macros from the internal SQLite database.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The customer message or a short query to find relevant macros.\"\n",
        "                    },\n",
        "                    \"top_k\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"How many macro hits to return.\",\n",
        "                        \"default\": 3\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "                \"additionalProperties\": False\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "SEARCH_MACROS_TOOL = build_search_macros_tool_schema()\n",
        "TOOLS = [SEARCH_MACROS_TOOL]\n",
        "\n",
        "\n",
        "def handle_tool_calls_for_macros(message) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Handle tool calls coming from the remote model.\n",
        "    Returns:\n",
        "      {\n",
        "        \"tool_messages\": [ {role:'tool', content:'...', tool_call_id:'...'}, ... ],\n",
        "        \"db_result\": {query, hits}\n",
        "      }\n",
        "    \"\"\"\n",
        "    tool_messages = []\n",
        "    last_db_result: Dict[str, Any] = {\"query\": \"\", \"hits\": []}\n",
        "\n",
        "    for tool_call in (message.tool_calls or []):\n",
        "        if tool_call.function.name == \"search_macros\":\n",
        "            # Parse tool arguments safely\n",
        "            args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "            q = args.get(\"query\", \"\")\n",
        "            top_k = int(args.get(\"top_k\", 3))\n",
        "\n",
        "            # Query SQLite DB\n",
        "            last_db_result = search_macros(q, top_k=top_k)\n",
        "\n",
        "            # IMPORTANT: Return JSON string as tool content (easy for LLM to parse)\n",
        "            tool_messages.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": json.dumps(last_db_result, ensure_ascii=False),\n",
        "                \"tool_call_id\": tool_call.id\n",
        "            })\n",
        "\n",
        "    return {\"tool_messages\": tool_messages, \"db_result\": last_db_result}\n",
        "\n",
        "\n",
        "def auto_decide_db_by_remote(\n",
        "    customer_message: str,\n",
        "    system_prompt: str,\n",
        "    cloud_model: str,\n",
        "    top_k: int = 3,\n",
        "    max_tool_rounds: int = 3\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Auto-mode: only the REMOTE model can decide whether to query the DB.\n",
        "    If it calls the tool, we execute it and feed results back until the model stops calling tools.\n",
        "\n",
        "    Returns:\n",
        "      {\n",
        "        \"used_db\": bool,\n",
        "        \"db_result\": {query, hits},\n",
        "        \"approved_macros_block\": str\n",
        "      }\n",
        "    \"\"\"\n",
        "    used_db = False\n",
        "    db_result: Dict[str, Any] = {\"query\": \"\", \"hits\": []}\n",
        "\n",
        "    # Decision messages: prompt + user message\n",
        "    # (We do not inject macros here; we want the remote model to decide to call the tool.)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": customer_message.strip()}\n",
        "    ]\n",
        "\n",
        "    # Ask remote model with tools enabled (so it can call search_macros)\n",
        "    resp = client_cloud.chat.completions.create(\n",
        "        model=cloud_model,\n",
        "        messages=messages,\n",
        "        tools=TOOLS,\n",
        "        temperature=TEMPERATURE_FIXED\n",
        "    )\n",
        "\n",
        "    rounds = 0\n",
        "    while resp.choices[0].finish_reason == \"tool_calls\" and rounds < max_tool_rounds:\n",
        "        rounds += 1\n",
        "        used_db = True\n",
        "\n",
        "        assistant_msg = resp.choices[0].message\n",
        "        messages.append(assistant_msg)\n",
        "\n",
        "        handled = handle_tool_calls_for_macros(assistant_msg)\n",
        "        tool_messages = handled[\"tool_messages\"]\n",
        "        db_result = handled[\"db_result\"]\n",
        "\n",
        "        # Append tool messages to the conversation\n",
        "        messages.extend(tool_messages)\n",
        "\n",
        "        # Call remote again (it may call tools again or finalize)\n",
        "        resp = client_cloud.chat.completions.create(\n",
        "            model=cloud_model,\n",
        "            messages=messages,\n",
        "            tools=TOOLS,\n",
        "            temperature=TEMPERATURE_FIXED\n",
        "        )\n",
        "\n",
        "    # Recommended behavior:\n",
        "    # - If DB was used but no hits, still inject a \"No matching macros found\" block.\n",
        "    # - If DB not used, return empty approved block.\n",
        "    if used_db:\n",
        "        # If model asked the tool but db_result is empty, we still provide explicit guidance\n",
        "        # to avoid hallucinating policies.\n",
        "        if not db_result.get(\"hits\"):\n",
        "            approved_block = (\n",
        "                \"APPROVED MACROS:\\n\"\n",
        "                f\"Query: {db_result.get('query','')}\\n\"\n",
        "                \"No matching macros found.\\n\"\n",
        "                \"Instruction: Ask for the minimum necessary details and do not invent policies, SLAs, ETAs, or refunds.\"\n",
        "            )\n",
        "        else:\n",
        "            approved_block = build_approved_macros_block(db_result)\n",
        "    else:\n",
        "        approved_block = \"\"\n",
        "\n",
        "    return {\"used_db\": used_db, \"db_result\": db_result, \"approved_macros_block\": approved_block}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 10) Optional TTS helper (winner audio)\n",
        "# =========================\n",
        "\n",
        "def tts_to_file(client: OpenAI, text: str, filename: str = \"winner_tts.mp3\") -> str:\n",
        "    \"\"\"\n",
        "    Generate TTS audio for the provided text and write it to an MP3 file.\n",
        "    Returns the file path.\n",
        "    \"\"\"\n",
        "    # Create speech audio bytes from the TTS endpoint\n",
        "    speech = client.audio.speech.create(\n",
        "        model=\"gpt-4o-mini-tts\",  # TTS model\n",
        "        voice=\"onyx\",             # Voice name\n",
        "        input=text                # Text to synthesize\n",
        "    )\n",
        "\n",
        "    # Save the audio bytes to disk\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(speech.content)\n",
        "\n",
        "    # Return filename so Gradio can load it\n",
        "    return filename\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# BLOCK 11 \n",
        "# compare_mode_run now supports DB mode: Always / Auto / Off\n",
        "# Auto decision is made ONLY by the REMOTE model (recommended)\n",
        "# =========================\n",
        "\n",
        "def compare_mode_run(\n",
        "    customer_message: str,\n",
        "    system_prompt: str,\n",
        "    cloud_model: str,\n",
        "    local_model: str,\n",
        "    judge_model: str,\n",
        "    db_mode: str,        # \"Always\" | \"Auto\" | \"Off\"\n",
        "    enable_tts: bool\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare runner with DB mode selector:\n",
        "    - Off: never consult DB\n",
        "    - Always: consult DB deterministically before LLMs\n",
        "    - Auto: ONLY the remote model decides whether to consult DB (tool-calling)\n",
        "            If remote uses DB and hits are empty, we still inject a \"No macros\" block (recommended)\n",
        "    \"\"\"\n",
        "\n",
        "    # Local backend check\n",
        "    if not ollama_ok:\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n⚠️ Local backend unavailable (Ollama not reachable).\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n⚠️ Start Ollama with: `ollama serve`.\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n⚠️ Cannot judge without Model 2.\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, \"No DB lookup.\"\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # 1) Determine DB behavior\n",
        "    # -------------------------\n",
        "    db_trace_md = \"No DB lookup.\"\n",
        "    approved_macros_block = \"\"\n",
        "\n",
        "    mode = (db_mode or \"Always\").strip()\n",
        "\n",
        "    if mode == \"Off\":\n",
        "        # No DB lookup, no macros\n",
        "        db_trace_md = \"DB mode: **Off** (no lookup).\"\n",
        "        approved_macros_block = \"\"\n",
        "\n",
        "    elif mode == \"Always\":\n",
        "        # Deterministic DB lookup BEFORE LLMs\n",
        "        db_result = search_macros(customer_message, top_k=3)\n",
        "        db_trace_md = \"DB mode: **Always**\\n\\n\" + format_db_trace(db_result)\n",
        "\n",
        "        # Always inject macros block; even if no hits, build_approved_macros_block provides guidance\n",
        "        approved_macros_block = build_approved_macros_block(db_result)\n",
        "\n",
        "    elif mode == \"Auto\":\n",
        "        # Auto: REMOTE model decides whether to consult DB via tool-calling\n",
        "        auto = auto_decide_db_by_remote(\n",
        "            customer_message=customer_message,\n",
        "            system_prompt=system_prompt,\n",
        "            cloud_model=cloud_model,\n",
        "            top_k=3\n",
        "        )\n",
        "\n",
        "        used_db = auto[\"used_db\"]\n",
        "        db_result = auto[\"db_result\"]\n",
        "        approved_macros_block = auto[\"approved_macros_block\"]\n",
        "\n",
        "        if used_db:\n",
        "            db_trace_md = (\n",
        "                \"DB mode: **Auto**\\n\"\n",
        "                \"- Remote requested DB: **YES**\\n\\n\"\n",
        "                + format_db_trace(db_result)\n",
        "            )\n",
        "        else:\n",
        "            db_trace_md = (\n",
        "                \"DB mode: **Auto**\\n\"\n",
        "                \"- Remote requested DB: **NO**\"\n",
        "            )\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Fallback: treat unknown as Always\n",
        "        db_result = search_macros(customer_message, top_k=3)\n",
        "        db_trace_md = \"DB mode: **Always** (fallback)\\n\\n\" + format_db_trace(db_result)\n",
        "        approved_macros_block = build_approved_macros_block(db_result)\n",
        "\n",
        "    # -------------------------\n",
        "    # 2) Build shared messages\n",
        "    # -------------------------\n",
        "    messages: List[Dict[str, str]] = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "    ]\n",
        "\n",
        "    # Inject macros block only if present\n",
        "    if approved_macros_block:\n",
        "        messages.append({\"role\": \"system\", \"content\": approved_macros_block})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": customer_message.strip()})\n",
        "\n",
        "    # -------------------------\n",
        "    # 3) Stream Model 1 (Cloud)\n",
        "    # -------------------------\n",
        "    cloud_text = \"\"\n",
        "    for partial in stream_answer(client_cloud, cloud_model, messages):\n",
        "        cloud_text = partial\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n*(waiting...)*\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n*(waiting...)*\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, db_trace_md\n",
        "\n",
        "    # -------------------------\n",
        "    # 4) Stream Model 2 (Local)\n",
        "    # -------------------------\n",
        "    local_text = \"\"\n",
        "    for partial in stream_answer(client_local, local_model, messages):\n",
        "        local_text = partial\n",
        "        m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "        m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n{local_text}\"\n",
        "        j_panel  = f\"## Judge (REMOTE) — `{judge_model}`\\n\\n*(waiting...)*\"\n",
        "        yield m1_panel, m2_panel, j_panel, None, db_trace_md\n",
        "\n",
        "    # -------------------------\n",
        "    # 5) Judge\n",
        "    # -------------------------\n",
        "    verdict = judge_two_answers(\n",
        "        client=client_cloud,\n",
        "        judge_model=judge_model,\n",
        "        customer_message=customer_message.strip(),\n",
        "        answer_a=cloud_text,\n",
        "        answer_b=local_text,\n",
        "        model_a_name=cloud_model,\n",
        "        model_b_name=local_model\n",
        "    )\n",
        "\n",
        "    j_panel = (\n",
        "        f\"## Judge (REMOTE) — `{judge_model}`\\n\\n\"\n",
        "        f\"- Model A (Model 1): **{verdict['model_A']}** — score **{verdict['score_A']}/10**\\n\"\n",
        "        f\"- Model B (Model 2): **{verdict['model_B']}** — score **{verdict['score_B']}/10**\\n\"\n",
        "        f\"- Winner: **{verdict['winner']}**\\n\\n\"\n",
        "        f\"**Reason:** {verdict['reason']}\"\n",
        "    )\n",
        "\n",
        "    # -------------------------\n",
        "    # 6) Optional TTS (winner)\n",
        "    # -------------------------\n",
        "    winner_text = cloud_text if verdict[\"winner\"] in [\"A\", \"tie\"] else local_text\n",
        "    audio_path = tts_to_file(client_cloud, winner_text) if enable_tts else None\n",
        "\n",
        "    m1_panel = f\"## Model 1 (REMOTE / Cloud) — `{cloud_model}`\\n\\n{cloud_text}\"\n",
        "    m2_panel = f\"## Model 2 (LOCAL / Ollama) — `{local_model}`\\n\\n{local_text}\"\n",
        "\n",
        "    yield m1_panel, m2_panel, j_panel, audio_path, db_trace_md\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# BLOCK 12 (REPLACE/UPDATE)\n",
        "# Gradio UI: add DB mode selector Always / Auto / Off\n",
        "# =========================\n",
        "\n",
        "with gr.Blocks(title=\"Support Reply Copilot (Compare + Judge + DB + Audio)\") as demo:\n",
        "    gr.Markdown(\"# Support Reply Copilot\")\n",
        "    gr.Markdown(\"Compare two models, judge their replies, optionally use DB macros, and optionally generate TTS for the winner.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            customer_in = gr.Textbox(\n",
        "                label=\"Customer message\",\n",
        "                lines=6,\n",
        "                placeholder=\"\"\n",
        "            )\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    \"Hi, I was charged twice for my subscription this month. Can you fix this and confirm when I’ll get the refund?\",\n",
        "                    \"I can’t log in after resetting my password—2FA codes never arrive. Please help ASAP.\",\n",
        "                    \"Your service has been down for 30 minutes. What’s the ETA and will we receive a credit?\"\n",
        "                ],\n",
        "                inputs=[customer_in],\n",
        "                label=\"Examples (click to fill)\"\n",
        "            )\n",
        "\n",
        "            model1 = gr.Dropdown(\n",
        "                choices=[\"gpt-4.1-nano\", \"gpt-4.1-mini\"],\n",
        "                value=MODEL_1_DEFAULT,\n",
        "                label=\"Model 1 (REMOTE / Cloud)\"\n",
        "            )\n",
        "\n",
        "            model2 = gr.Dropdown(\n",
        "                choices=[\"llama3.1:8b\"],\n",
        "                value=MODEL_2_DEFAULT,\n",
        "                label=\"Model 2 (LOCAL / Ollama)\"\n",
        "            )\n",
        "\n",
        "            judge_model = gr.Dropdown(\n",
        "                choices=[\"gpt-4.1-mini\", \"gpt-4.1-nano\"],\n",
        "                value=JUDGE_DEFAULT,\n",
        "                label=\"Judge (REMOTE)\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Settings\")\n",
        "\n",
        "            system_prompt_in = gr.Textbox(\n",
        "                label=\"System prompt\",\n",
        "                value=DEFAULT_SYSTEM_PROMPT,\n",
        "                lines=8\n",
        "            )\n",
        "\n",
        "            db_mode = gr.Radio(\n",
        "                choices=[\"Always\", \"Auto\", \"Off\"],\n",
        "                value=\"Always\",\n",
        "                label=\"Macro DB usage\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\n",
        "                \"**Always**: The app queries the SQLite macro DB *before* calling any model.\\n\"\n",
        "                \"**Auto**: The *remote (cloud) model* decides whether to query the DB (tool-calling).\\n\"\n",
        "                \"**Off**: The DB is never queried.\"\n",
        "            )\n",
        "\n",
        "\n",
        "            enable_tts = gr.Checkbox(value=False, label=\"Enable TTS (winner audio)\")\n",
        "\n",
        "            run_btn = gr.Button(\"Run\")\n",
        "            clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            out_m1 = gr.Markdown()\n",
        "            out_m2 = gr.Markdown()\n",
        "            out_judge = gr.Markdown()\n",
        "            out_audio = gr.Audio(label=\"Winner audio (TTS)\", autoplay=False)\n",
        "            out_db = gr.Markdown(\"No DB lookup.\")\n",
        "\n",
        "    def clear_all():\n",
        "        return \"\", \"\", \"\", None, \"No DB lookup.\"\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_all,\n",
        "        inputs=[],\n",
        "        outputs=[out_m1, out_m2, out_judge, out_audio, out_db]\n",
        "    )\n",
        "\n",
        "    def run_app(customer_msg, m1, m2, j, sys_prompt, mode, use_tts):\n",
        "        if not customer_msg or not customer_msg.strip():\n",
        "            yield (\n",
        "                \"## Model 1\\n\\n⚠️ Please paste a customer message.\",\n",
        "                \"## Model 2\\n\\n*(waiting...)*\",\n",
        "                \"## Judge\\n\\n*(waiting...)*\",\n",
        "                None,\n",
        "                \"No DB lookup.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        gen = compare_mode_run(\n",
        "            customer_message=customer_msg,\n",
        "            system_prompt=sys_prompt,\n",
        "            cloud_model=m1,\n",
        "            local_model=m2,\n",
        "            judge_model=j,\n",
        "            db_mode=mode,\n",
        "            enable_tts=use_tts\n",
        "        )\n",
        "\n",
        "        for a, b, c, audio_path, db_md in gen:\n",
        "            yield a, b, c, audio_path, db_md\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=run_app,\n",
        "        inputs=[customer_in, model1, model2, judge_model, system_prompt_in, db_mode, enable_tts],\n",
        "        outputs=[out_m1, out_m2, out_judge, out_audio, out_db]\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
