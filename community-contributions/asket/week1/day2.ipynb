{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to the Day 2 Lab!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get started --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffe36f",
   "metadata": {},
   "source": [
    "## First - let's talk about the Chat Completions API\n",
    "\n",
    "1. The simplest way to call an LLM\n",
    "2. It's called Chat Completions because it's saying: \"here is a conversation, please predict what should come next\"\n",
    "3. The Chat Completions API was invented by OpenAI, but it's so popular that everybody uses it!\n",
    "\n",
    "### We will start by calling OpenAI again - but don't worry non-OpenAI people, your time is coming!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38f17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-or-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-or- (OpenRouter format); please check you're using OPENROUTER_API_KEY - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846274",
   "metadata": {},
   "source": [
    "## Do you know what an Endpoint is?\n",
    "\n",
    "If not, please review the Technical Foundations guide in the guides folder\n",
    "\n",
    "And, here is an endpoint that might interest you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af5c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-5-nano',\n",
       " 'messages': [{'role': 'user', 'content': 'Tell me a fun fact'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact\"}]\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0ab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'gen-1771996100-sXVKzaR8UwIOADPMoaLR',\n",
       " 'provider': 'OpenAI',\n",
       " 'model': 'openai/gpt-5-nano',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1771996100,\n",
       " 'choices': [{'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'native_finish_reason': 'completed',\n",
       "   'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Fun fact: Honey never spoils. Archaeologists have found pots of honey in ancient Egyptian tombs that are thousands of years old and still edible. Want another?',\n",
       "    'refusal': None,\n",
       "    'reasoning': '**Providing a fun fact**\\n\\nThe user wants a fun fact, so I need to pick something succinct and interesting. I’m considering options like \"Honey never spoils,\" which is a classic. It\\'s true that archaeologists have found 3,000-year-old honey still edible due to its low water activity and acidity. I\\'m also thinking about others, like \"A group of flamingos is a flamboyance,\" or \"Wombats produce cube-shaped poo.\" I\\'ll stick to one and share more if the user is curious!**Crafting a fun fact**\\n\\nI’m settling on a fun fact to share. \"Honey never spoils\" stands out as a classic — it\\'s true that archaeologists found edible honey in ancient Egyptian tombs thousands of years old! I think that will grab the user’s attention. Since one fact should suffice, I’ll present it clearly and concisely. I\\'ll also invite the user to ask for more if they\\'re interested. Maybe I’ll add, “Want another?” but I’ll keep it minimal. Time to prepare the final response!',\n",
       "    'reasoning_details': [{'format': 'openai-responses-v1',\n",
       "      'index': 0,\n",
       "      'type': 'reasoning.summary',\n",
       "      'summary': '**Providing a fun fact**\\n\\nThe user wants a fun fact, so I need to pick something succinct and interesting. I’m considering options like \"Honey never spoils,\" which is a classic. It\\'s true that archaeologists have found 3,000-year-old honey still edible due to its low water activity and acidity. I\\'m also thinking about others, like \"A group of flamingos is a flamboyance,\" or \"Wombats produce cube-shaped poo.\" I\\'ll stick to one and share more if the user is curious!**Crafting a fun fact**\\n\\nI’m settling on a fun fact to share. \"Honey never spoils\" stands out as a classic — it\\'s true that archaeologists found edible honey in ancient Egyptian tombs thousands of years old! I think that will grab the user’s attention. Since one fact should suffice, I’ll present it clearly and concisely. I\\'ll also invite the user to ask for more if they\\'re interested. Maybe I’ll add, “Want another?” but I’ll keep it minimal. Time to prepare the final response!'},\n",
       "     {'id': 'rs_071ecce9fbc3d4be01699e83c4df788197a0a5a8bb4e3e4016',\n",
       "      'format': 'openai-responses-v1',\n",
       "      'index': 0,\n",
       "      'type': 'reasoning.encrypted',\n",
       "      'data': 'gAAAAABpnoPMc48ZqOKOSJn-IMK5W-sMCvxozXZ1xufB7qDIh7z7qFFQFCGVHYoZPlRJc_HTSjcdtSZGWkULEAPJ4lG9TguDaQy2RsChCUBLLzaGs7kPf5yigtG4QfR4-4akzkOV1_xs1AI7P43z-ys-q7UGi0HcwHxlWwDZxsaS_BElGKXSxMkHLbvXL8MoJkbsWdp3ps9ae9_QN4cJhCXH73moMSLKMMCbJ3rxV5tarO_qG_-lJF1MWgcRxaKb3EY6RCU8vutEFC1g3yKmLZvW5hio2BZPz3Wel8c2iq5QpSYNhYp7YbIB19RoQM_0r4wuKk7s7LTjtzMvtRV9TzUZsqw9aHfSwtqsw3BqhkDJ6lghif4Yf2jQEDvfwNoceaEJwoPFQ1k5nTpULXuLejLN71oJEnaPdsqiuprcFLVCJ3dqEEzytl7ZB0pEyHpMg7Eb9URSWHw0MRYyp1CEWVVSeASTyep7K-w6JDScl1_mGUEjDz9ycY0mad1ODxHqcVXB2maxmtqOgt6_Z7PZ9Du6LIfxfkhOUBl647KRb5ZCTSeB--TSkH_GFsLUopCT9gOM-tI7LRfhWgQ1EVNyrVAyfZb1WM84Zm3YFsQ--9l7kw_4eKaHtdczKaNvxPMYFq4IrP-HwQspFHwgXs3cDDEVXPDDiCDJmFXLEFAvvoF8AINtk-rURjoqXIi2-4r4gE3v_F5ZmaKeXzWeYAAYxQemLDeqdwVjacyh8TydS1b0YDRFaI88nHFdVFAYw8iETAZWcRlxGw3OPmXGCp4g3DZyLh9pShJBcARezgym7QN9KkWf85rR-fVAEGgpiaf-haGq7or3SkaY377yKZGZ5AWy08gSAwiPlx0pd6IM1BaatMnVJaBzmCh5I2kFwQWMS09fPthgQeTaV1hy77-uqBZf5AxQr7bUMRVg4CV441Nin8ZmcDeykVccR9TSBofjJeIu1-72lIRHVizC61nEv6ElD-LKclWJ4G4UPPK5ckURoJoMFg5AN3CIosDo8IZODN6rY_Ll3hFmemj6Qk5XAMLXSbFKQN_tZgtIsApmqgOFOdJyDXxZFCM95Cs9fLIt6CQcuZHx7-epWaT5G91Efay1JH3IS2i383Q9rw6BpMTrceNG2BuQZo7RStedDgaUqp-lkGWaxATQD4l3CRYATTKCS9KhiYapuX9QzqGxGomftyS6ao6h0Hlj_W7JX_2wxhKGqEUX0u-m5SG8-2l0_sMUqKXFvAAh3ovlgpO4fs2Er2yZ7eCbjmTAs1HgGB3fN1tkt75rpasgdEVOmtsvUui-evHj_Lpytq77xBkjxuefRkiZDx9nv0JAWLlDRLdEUb5xhqfXh-9se1Pd5NNj_MZfEB8ghvM6el7Ys0gHbY_DcaNG65iDeADq10ZsCAItpL7pG4nLlM8_4o2GxnI9WuDx4Ef8xT4nT7Rw6SYA76khOyLZimRyFsALQ3a15bNReT6Oc92ZjKDnwNQHtHSHWieV2B5yAOY5x6NVhHhkgFPehr7K_0487J7bFXgTOKL_T_4KnWhhhqi_JiRJ2EPRI8hHX-avhjgXcQMtTuPkH1re9WF8c434XtEHVObuCGFaOPnZQgBem-b27cBdqmxeGDx6llZ8OngJEwjuaA9DL_zzLqFvVSS8xnUtABRGoUNE2FU2i1PW1GRgppFDLHjGqpI8Y31DK1YtlWed1oT5Zbi2EYW31hkS2ui2texEdYjJQnSvRL6FMCYF1_f0a-oHk5awa79AM-MONYmkiQLQhNQeRM96_5irULhKOkK2zriBjBkzev_kB8rqyBQj1fFXZzkQkJX2uRPfrML4wMntHfHe1CMDTtx9FvIypoNrVIbCYR-1kYVSwNshsq3KQBMQXud7WHuCt0jEzS1SjiWyXYfOkQY74nc_GmtmQBkn6Op4Wm0cyP342fL7fmcPFNPmVlTNy_Q101ytP-EFpJW23RO9hhm-vm0KatCBtuK5hOiqUHomGsQiATGbb-cy6BaHdi0BRWrTXPS1w3Y0I-4gzo__DBPHFpfYLtASQNQdITm9zbbK6ZsJmLrSmCGBvwblnKjSqLTIL7u3qnderKor0Siu7Km4h9k3AnoMIb0EecojS90kJWvNtmgzwgO0DHU2Ks84uHPzMUmbGC-vsjt2CPhB6Kjy_UMGSvfRnSEHHActH7sZrCXLwR6W3CiMFmOuZIf59-tEwHA_YN6r8MSqBENPtHfZhvvNf71qnZ8cZ_pYCsjZtatn4ymkJx-kuEbdiZKszBOlz47RSi9qrrEy_YH1_0GjjpVc8MSHXqhT7Iunxdxt-sNixL3la5yi-j8VKKtlADvDGZyK0ArQRdjER4ep6wQySAxYQ93VYxQNbSaXW5n2hvskDflwBU9S4zq5yDDrIXiETIHirI93HO9jt69cLSUS2-UJnXsk7JhRrMTlDIzyxy-GEInZKnpvyv9DYDN-gS9GgW8lWPxbXp4tlIhu5Sv8D9YtceTg0SwxpcY90RdkDzK5apIglDnQxEISaifsD-P2Q4_wlGFoqjIEsK7D8ByToq6w-Bfe26mIV0AmZGGuJagpZDY0VrA1gcqaprDi2yrV_zhwexBZtE5pt3msTtwIh0Jfe2F75_H8aG7_SPnQ0CbI21V_-ghsVWNdaob5JBqAIvf7Z8T-iuaPV3zXhJefKAe_aFzc3Vb-EAfQnm7EtNh-aTLz7vbXh8A_B88nlBsD5FxZGWOev5PuyJDRyTaYlRbTsGTDzmKnsZq4nDRy7_d4mjO7eQ1Inui4U_N8Ssoas1cCeZfZPEHHHb2xEi5AwkvJuORrpMEpBM-P7SEFn93L1hQD3mXzO23QquyoDD85DWzi7qer-4Yonm6tFyR2H9g_yyxppcenw_HWSNLb0F5LiICNFHxEeKj4dX98pV2o0IPCu7ML3Xg---DHXQtnuKCL3RzW6Jr315IrZnIOGmVjRra-CwA_pm5DnVwssk4aM5KGxfKZyQNK0RGV0DA1ll8R1UAQ93biq48RqgYOuyj0j0DD5vwAngIzQljGUNZyHW6RrDxxDXh0E5HshZb-bR_wuWG5Vlrfqx7SDEUp9VfSsiK1sXlF01Fa7A-BLp3TuKQbiQDsGHwSlKUPq394ZDinVAM='}]}}],\n",
       " 'usage': {'prompt_tokens': 11,\n",
       "  'completion_tokens': 417,\n",
       "  'total_tokens': 428,\n",
       "  'cost': 0.00016735,\n",
       "  'is_byok': False,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0},\n",
       "  'cost_details': {'upstream_inference_cost': 0.00016735,\n",
       "   'upstream_inference_prompt_cost': 5.5e-07,\n",
       "   'upstream_inference_completions_cost': 0.0001668},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 320, 'image_tokens': 0}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use OpenRouter endpoint (your OPENROUTER_API_KEY only works here, not at api.openai.com)\n",
    "response = requests.post(\n",
    "    \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb11a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: Honey never spoils. Archaeologists have found pots of honey in ancient Egyptian tombs that are thousands of years old and still edible. Want another?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3026a",
   "metadata": {},
   "source": [
    "# What is the openai package?\n",
    "\n",
    "It's known as a Python Client Library.\n",
    "\n",
    "It's nothing more than a wrapper around making this exact call to the http endpoint.\n",
    "\n",
    "It just allows you to work with nice Python code instead of messing around with janky json objects.\n",
    "\n",
    "But that's it. It's open-source and lightweight. Some people think it contains OpenAI model code - it doesn't!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490fdf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: Honey never spoils. Archaeologists have found edible honey in ancient Egyptian tombs that’s thousands of years old. Want another one in a specific category?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create OpenAI-compatible client pointing at OpenRouter (uses OPENROUTER_API_KEY)\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7739cda",
   "metadata": {},
   "source": [
    "## And then this great thing happened:\n",
    "\n",
    "OpenAI's Chat Completions API was so popular, that the other model providers created endpoints that are identical.\n",
    "\n",
    "They are known as the \"OpenAI Compatible Endpoints\".\n",
    "\n",
    "For example, google made one here: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "\n",
    "And OpenAI decided to be kind: they said, hey, you can just use the same client library that we made for GPT. We'll allow you to specify a different endpoint URL and a different key, to use another provider.\n",
    "\n",
    "So you can use:\n",
    "\n",
    "```python\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=\"AIz....\")\n",
    "gemini.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "And to be clear - even though OpenAI is in the code, we're only using this lightweight python client library to call the endpoint - there's no OpenAI model involved here.\n",
    "\n",
    "If you're confused, please review Guide 9 in the Guides folder!\n",
    "\n",
    "And now let's try it!\n",
    "\n",
    "## THIS IS OPTIONAL - but if you wish to try out Google Gemini, please visit:\n",
    "\n",
    "https://aistudio.google.com/\n",
    "\n",
    "And set up your API key at\n",
    "\n",
    "https://aistudio.google.com/api-keys\n",
    "\n",
    "And then add your key to the `.env` file, being sure to Save the .env file after you change it:\n",
    "\n",
    "`GOOGLE_API_KEY=AIz...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74293bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272432",
   "metadata": {},
   "source": [
    "## And Ollama also gives an OpenAI compatible endpoint\n",
    "\n",
    "...and it's on your local machine!\n",
    "\n",
    "If the next cell doesn't print \"Ollama is running\" then please open a terminal and run `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06280ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3807",
   "metadata": {},
   "source": [
    "### Download llama3.2 from meta\n",
    "\n",
    "Change this to llama3.2:1b if your computer is smaller.\n",
    "\n",
    "Don't use llama3.3 or llama4! They are too big for your computer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9419762",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2456cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fun fact\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try deepseek-r1:1.5b - this is DeepSeek \"distilled\" into Qwen from Alibaba Cloud\n",
    "\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25002f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48f55c",
   "metadata": {},
   "source": [
    "## Solution: Webpage summarizer with Ollama\n",
    "\n",
    "Below we use the **week1 scraper** to fetch a page and **Ollama** (the `ollama` client from above) to summarize it. Run from repo root or from this folder; ensure `ollama serve` is running and you have run `ollama pull llama3.2` (or use `llama3.2:3b-instruct-q4_0` / `llama3.2:1b` — see `ollama list`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe894cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three clear bullet points summarizing the content:\n",
      "\n",
      "* GitHub offers a range of tools and features to help developers create, manage, and secure code, including AI-powered assistances like Copilot, automated workflows, and issue tracking.\n",
      "* The platform caters to different users and use cases, including enterprises, small and medium teams, startups, nonprofits, healthcare, financial services, manufacturing, and government.\n",
      "* GitHub also provides resources for learning and support, such as documentation, customer support, community forums, and training programs like the Security Lab and Accelerator.\n"
     ]
    }
   ],
   "source": [
    "# Path so we can import week1 scraper (run from repo root or community-contributions/asket)\n",
    "import os, sys\n",
    "for _path in (\"week1\", os.path.join(os.getcwd(), \"..\", \"..\", \"week1\")):\n",
    "    _p = os.path.abspath(_path)\n",
    "    if _p not in sys.path and os.path.isdir(_p):\n",
    "        sys.path.insert(0, _p)\n",
    "        break\n",
    "from scraper import fetch_website_contents\n",
    "from openai import OpenAI\n",
    "\n",
    "# Ollama client (so this cell runs even if you didn't run the earlier Ollama cells)\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# Try these model tags in order; first one that Ollama has will be used\n",
    "OLLAMA_MODELS_TO_TRY = [\"llama3.2:3b-instruct-q4_0\", \"llama3.2:1b-instruct-q4_0\", \"llama3.2:1b\", \"llama3.2\"]\n",
    "\n",
    "def summarize_url(url):\n",
    "    from openai import NotFoundError\n",
    "    text = fetch_website_contents(url)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Summarize the following webpage content in a few clear bullet points. No code blocks.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    for model in OLLAMA_MODELS_TO_TRY:\n",
    "        try:\n",
    "            r = ollama.chat.completions.create(model=model, messages=messages)\n",
    "            return r.choices[0].message.content\n",
    "        except NotFoundError:\n",
    "            continue\n",
    "    raise RuntimeError(\"No Ollama model found. Run: ollama pull llama3.2\")\n",
    "\n",
    "# Example: summarize a page (change URL as you like)\n",
    "url = \"https://github.com/frank-asket\"\n",
    "print(summarize_url(url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
