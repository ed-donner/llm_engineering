{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1358df7f",
      "metadata": {},
      "source": [
        "## Sensor Dataset Generator\n",
        "\n",
        "- Google Colab tool to generate synthetic sensor datasets (temperature, humidity, tachometer)\n",
        "- Choose from several Colab-friendly language models to create tabular data\n",
        "- Supports 4-bit quantization (better efficiency, lower resource use)\n",
        "- Gradio interface: configure, generate, and download datasets easily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd55368",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate \"transformers==4.53.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45712f5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "from threading import Thread\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive, userdata\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TextIteratorStreamer,\n",
        "    TextStreamer,\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e943e850",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS = {\n",
        "    \"Llama 3.2 3B\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"Phi-4 mini\": \"microsoft/Phi-4-mini-instruct\",\n",
        "    \"Gemma 3 270M\": \"google/gemma-3-270m-it\",\n",
        "    \"Qwen3 4B\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    \"DeepSeek R1 Distill 1.5B\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "}\n",
        "\n",
        "SENSOR_TYPES = [\"temperature\", \"humidity\", \"tachometer\"]\n",
        "\n",
        "# One prompt per sensor ({n} is for number of rows)\n",
        "PROMPTS = {\n",
        "    \"temperature\": \"Generate a CSV dataset for a temperature sensor. Columns: timestamp, sensor_id, temperature_celsius, location, unit. Include {n} rows of realistic readings (e.g. room, outdoor, machine). Return only the table, no explanation.\",\n",
        "    \"humidity\": \"Generate a CSV dataset for a humidity sensor. Columns: timestamp, sensor_id, humidity_percent, location, unit. Include {n} rows. Return only the table.\",\n",
        "    \"tachometer\": \"Generate a CSV dataset for a tachometer (RPM). Columns: timestamp, sensor_id, rpm, machine_id, unit. Include {n} rows of realistic rotation readings. Return only the table.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ec6619",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_quantization_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "\n",
        "def load_model(model_id: str, use_quantization: bool):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    if use_quantization:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=get_quantization_config(),\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5e1bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "_current_model_id = None\n",
        "_use_quantization = None\n",
        "_tokenizer = None\n",
        "_model = None\n",
        "\n",
        "\n",
        "def get_or_load_model(model_id: str, use_quantization: bool):\n",
        "    \"\"\"Load model only when selection changes; reuse otherwise.\"\"\"\n",
        "    global _current_model_id, _use_quantization, _tokenizer, _model\n",
        "    if _tokenizer is not None and _model is not None and _current_model_id == model_id and _use_quantization == use_quantization:\n",
        "        return _tokenizer, _model\n",
        "    if _model is not None:\n",
        "        del _model\n",
        "        if _tokenizer is not None:\n",
        "            del _tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "    _current_model_id = model_id\n",
        "    _use_quantization = use_quantization\n",
        "    _tokenizer, _model = load_model(model_id, use_quantization)\n",
        "    return _tokenizer, _model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa4bd6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_messages(sensor_type: str, n_rows: int):\n",
        "    system = \"You are a dataset engineer. Generate only the requested sensor data table. Output ONLY the table, no extra text, code blocks, or explanations.\"\n",
        "    user_text = PROMPTS.get(sensor_type, PROMPTS[\"temperature\"]).format(n=n_rows)\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "\n",
        "def generate_dataset(model_name: str, use_quantization: bool, sensor_type: str, n_rows: int):\n",
        "    model_id = MODELS.get(model_name, list(MODELS.values())[0])\n",
        "    tokenizer, model = get_or_load_model(model_id, use_quantization)\n",
        "    messages = build_messages(sensor_type, n_rows)\n",
        "\n",
        "    device = next(model.parameters()).device if hasattr(model, \"parameters\") else \"cuda\"\n",
        "    try:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages, add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    except Exception:\n",
        "        prompt = \"\\n\".join(m.get(\"content\", \"\") for m in messages if m.get(\"role\") == \"user\")\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "    thread = Thread(\n",
        "        target=lambda: model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=4096,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "            streamer=streamer,\n",
        "        )\n",
        "    )\n",
        "    thread.start()\n",
        "    out = \"\"\n",
        "    for chunk in streamer:\n",
        "        out += chunk\n",
        "\n",
        "    out = out.strip().removeprefix(\"```\").removesuffix(\"```\").strip()\n",
        "    return out.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e609cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ui(model_name: str, use_quantization: bool, sensor_type: str, n_rows: int):\n",
        "    try:\n",
        "        n = max(5, min(int(n_rows), 500))\n",
        "    except (TypeError, ValueError):\n",
        "        n = 50\n",
        "    return generate_dataset(model_name, use_quantization, sensor_type, n)\n",
        "\n",
        "with gr.Blocks(title=\"Sensor Dataset Generator\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## Sensor Dataset Generator\")\n",
        "    gr.Markdown(\"Generate synthetic **temperature**, **humidity**, or **tachometer** datasets. Choose model and quantization to compare outputs.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dd = gr.Dropdown(\n",
        "            choices=list(MODELS.keys()),\n",
        "            value=list(MODELS.keys())[0],\n",
        "            label=\"Model\",\n",
        "        )\n",
        "        quant_cb = gr.Checkbox(value=True, label=\"Use 4-bit quantization (saves VRAM)\")\n",
        "    with gr.Row():\n",
        "        sensor_dd = gr.Dropdown(\n",
        "            choices=SENSOR_TYPES,\n",
        "            value=SENSOR_TYPES[0],\n",
        "            label=\"Sensor type\",\n",
        "        )\n",
        "        n_rows_num = gr.Number(value=50, label=\"Number of rows\", minimum=5, maximum=500, step=5)\n",
        "\n",
        "    gen_btn = gr.Button(\"Generate dataset\")\n",
        "    out_text = gr.Textbox(label=\"Generated dataset\", lines=20, max_lines=30)\n",
        "\n",
        "    gen_btn.click(fn=run_ui, inputs=[model_dd, quant_cb, sensor_dd, n_rows_num], outputs=out_text)\n",
        "\n",
        "    in_colab = \"google.colab\" in sys.modules\n",
        "    demo.launch(share=in_colab, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
