{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic Data Generator - Google Colab Version\n",
        "\n",
        "This notebook provides a synthetic data generator using Hugging Face models with a Gradio interface. It can generate structured data based on templates or custom prompts.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. **Get a Hugging Face API Key**:\n",
        "   - Go to [Hugging Face](https://huggingface.co/settings/tokens)\n",
        "   - Create a new token with read access\n",
        "   - Copy the token\n",
        "\n",
        "2. **Run the cells below in order**\n",
        "\n",
        "3. **Enter your API key when prompted**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gradio>=4.0.0 httpx>=0.24.0 python-dotenv>=1.0.0 jsonschema>=4.19.0 transformers>=4.30.0 torch>=2.0.0 huggingface-hub>=0.16.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from jsonschema import validate as jsonschema_validate, ValidationError\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. API Key Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter your Hugging Face API key\n",
        "HF_TOKEN = userdata.get('HF_TOKEN') # use the name of the variable in the userdata\n",
        "\n",
        "print(\"API key configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_json_load(text: str) -> Optional[Any]:\n",
        "    \"\"\"\n",
        "    Attempt to parse JSON. Heuristics:\n",
        "      1) direct json.loads\n",
        "      2) extract first [...] block\n",
        "      3) extract first {...} block\n",
        "      4) try to find JSON after common prefixes\n",
        "    Returns parsed object or None.\n",
        "    \"\"\"\n",
        "    # Clean up the text first\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Debug: Print what we're trying to parse\n",
        "    print(f\"DEBUG: Attempting to parse JSON from text (length: {len(text)})\")\n",
        "    print(f\"DEBUG: First 200 chars: {text[:200]}\")\n",
        "    \n",
        "    try:\n",
        "        result = json.loads(text)\n",
        "        print(f\"DEBUG: Direct JSON parse successful\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Direct JSON parse failed: {e}\")\n",
        "\n",
        "    # Try to find JSON array first (most common for our use case)\n",
        "    arr_match = re.search(r'(\\[.*?\\])', text, flags=re.S)\n",
        "    if arr_match:\n",
        "        try:\n",
        "            result = json.loads(arr_match.group(1))\n",
        "            print(f\"DEBUG: Array regex match successful\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Array regex match failed: {e}\")\n",
        "\n",
        "    # Try to find JSON object\n",
        "    obj_match = re.search(r'(\\{.*?\\})', text, flags=re.S)\n",
        "    if obj_match:\n",
        "        try:\n",
        "            result = json.loads(obj_match.group(1))\n",
        "            print(f\"DEBUG: Object regex match successful\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Object regex match failed: {e}\")\n",
        "\n",
        "    # Try to find JSON after common prefixes like \"Here is the JSON:\" or \"```json\"\n",
        "    json_patterns = [\n",
        "        r'(?:Here is the JSON:|```json|JSON:|Output:)\\s*(\\[.*?\\])',\n",
        "        r'(?:Here is the JSON:|```json|JSON:|Output:)\\s*(\\{.*?\\})',\n",
        "    ]\n",
        "    \n",
        "    for pattern in json_patterns:\n",
        "        match = re.search(pattern, text, flags=re.S | re.I)\n",
        "        if match:\n",
        "            try:\n",
        "                result = json.loads(match.group(1))\n",
        "                print(f\"DEBUG: Pattern match successful\")\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Pattern match failed: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"DEBUG: All parsing attempts failed\")\n",
        "    return None\n",
        "\n",
        "print(\"Utility functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Template Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template data\n",
        "TEMPLATES = {\n",
        "    \"user_profile\": {\n",
        "        \"id\": \"user_profile\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"first_name\": { \"type\": \"string\" },\n",
        "                \"last_name\": { \"type\": \"string\" },\n",
        "                \"email\": { \"type\": \"string\", \"format\": \"email\" },\n",
        "                \"age\": { \"type\": \"integer\", \"minimum\": 18, \"maximum\": 80 },\n",
        "                \"country\": { \"type\": \"string\" }\n",
        "            },\n",
        "            \"required\": [\"first_name\", \"last_name\", \"email\", \"age\"]\n",
        "        },\n",
        "        \"prompt_template\": \"Generate {{count}} user profile(s) as a JSON array matching this schema: {{schema}}. Tone: {{tone}}. Output ONLY the JSON array, no other text.\"\n",
        "    },\n",
        "    \"job_description\": {\n",
        "        \"id\": \"job_description\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"title\": { \"type\": \"string\" },\n",
        "                \"company\": { \"type\": \"string\" },\n",
        "                \"location\": { \"type\": \"string\" },\n",
        "                \"salary_range\": { \"type\": \"string\" },\n",
        "                \"requirements\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n",
        "                \"benefits\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } }\n",
        "            },\n",
        "            \"required\": [\"title\", \"company\", \"location\"]\n",
        "        },\n",
        "        \"prompt_template\": \"Generate {{count}} job description(s) as a JSON array matching this schema: {{schema}}. Tone: {{tone}}. Output ONLY the JSON array, no other text.\"\n",
        "    },\n",
        "    \"product_spec\": {\n",
        "        \"id\": \"product_spec\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": { \"type\": \"string\" },\n",
        "                \"category\": { \"type\": \"string\" },\n",
        "                \"price\": { \"type\": \"number\" },\n",
        "                \"description\": { \"type\": \"string\" },\n",
        "                \"features\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } },\n",
        "                \"in_stock\": { \"type\": \"boolean\" }\n",
        "            },\n",
        "            \"required\": [\"name\", \"category\", \"price\"]\n",
        "        },\n",
        "        \"prompt_template\": \"Generate {{count}} product specification(s) as a JSON array matching this schema: {{schema}}. Tone: {{tone}}. Output ONLY the JSON array, no other text.\"\n",
        "    },\n",
        "    \"address\": {\n",
        "        \"id\": \"address\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"street\": { \"type\": \"string\" },\n",
        "                \"city\": { \"type\": \"string\" },\n",
        "                \"state\": { \"type\": \"string\" },\n",
        "                \"zip_code\": { \"type\": \"string\" },\n",
        "                \"country\": { \"type\": \"string\" }\n",
        "            },\n",
        "            \"required\": [\"street\", \"city\", \"state\", \"zip_code\"]\n",
        "        },\n",
        "        \"prompt_template\": \"Generate {{count}} address(es) as a JSON array matching this schema: {{schema}}. Tone: {{tone}}. Output ONLY the JSON array, no other text.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Template data loaded successfully!\")\n",
        "print(f\"Available templates: {list(TEMPLATES.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Core Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemplateRegistry:\n",
        "    def __init__(self, templates: Dict[str, Any]):\n",
        "        self.templates = templates\n",
        "\n",
        "    def get_ids(self) -> List[str]:\n",
        "        return list(self.templates.keys())\n",
        "\n",
        "    def get_template(self, template_id: str) -> Optional[Dict[str, Any]]:\n",
        "        return self.templates.get(template_id)\n",
        "\n",
        "class PromptEngine:\n",
        "    def build_prompt(self, template: Dict[str, Any], params: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Build a single prompt string.\n",
        "        Template must include 'prompt_template' and 'schema'.\n",
        "        params may include count, tone, etc.\n",
        "        \"\"\"\n",
        "        tpl = template.get(\"prompt_template\", \"\")\n",
        "        schema = template.get(\"schema\", {})\n",
        "        # Render minimal placeholders: {{count}}, {{tone}}, {{schema}}\n",
        "        prompt = tpl.replace(\"{{count}}\", str(params.get(\"count\", 1)))\n",
        "        prompt = prompt.replace(\"{{tone}}\", str(params.get(\"tone\", \"concise\")))\n",
        "        prompt = prompt.replace(\"{{schema}}\", json.dumps(schema))\n",
        "        return prompt\n",
        "\n",
        "print(\"TemplateRegistry and PromptEngine classes defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HFClient:\n",
        "    def __init__(self, model_id: Optional[str] = None, api_key: Optional[str] = None):\n",
        "        self.model_id = model_id or \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "        self.api_key = api_key or os.getenv(\"HF_TOKEN\")\n",
        "        \n",
        "        if not self.api_key:\n",
        "            raise RuntimeError(\"HF_TOKEN not provided in environment.\")\n",
        "        \n",
        "        print(f\"DEBUG: Initializing HFClient with model: {self.model_id}\")\n",
        "        \n",
        "        # Login to Hugging Face\n",
        "        try:\n",
        "            login(token=self.api_key, add_to_git_credential=True)\n",
        "            print(\"DEBUG: Successfully logged in to Hugging Face\")\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Login failed: {e}\")\n",
        "            raise RuntimeError(f\"Failed to login to Hugging Face: {e}\")\n",
        "        \n",
        "        # Load tokenizer and model\n",
        "        try:\n",
        "            print(\"DEBUG: Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "            print(\"DEBUG: Loading model...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_id,\n",
        "                dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "            )\n",
        "            print(\"DEBUG: Model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Model loading failed: {e}\")\n",
        "            raise RuntimeError(f\"Failed to load model {self.model_id}: {e}\")\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.2, max_tokens: int = 512) -> Tuple[str, Dict[str, Any]]:\n",
        "        print(f\"DEBUG: Generating text with prompt length: {len(prompt)}\")\n",
        "        \n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "            \n",
        "            # Move to same device as model\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "            \n",
        "            # Generate with the model\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decode the generated text\n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Remove the original prompt from the generated text\n",
        "            if generated_text.startswith(prompt):\n",
        "                generated_text = generated_text[len(prompt):].strip()\n",
        "            \n",
        "            print(f\"DEBUG: Generated text length: {len(generated_text)}\")\n",
        "            print(f\"DEBUG: Generated text preview: {generated_text[:200]}\")\n",
        "            \n",
        "            return generated_text, {\"status\": \"success\", \"model\": self.model_id}\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Generation failed: {e}\")\n",
        "            return f\"Generation error: {str(e)}\", {\"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "print(\"HFClient class defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Validator:\n",
        "    def validate(self, schema: Dict[str, Any], data: Any) -> Dict[str, Any]:\n",
        "        if not schema:\n",
        "            return {\"valid\": True, \"errors\": []}\n",
        "        try:\n",
        "            # If top-level is array and schema expects object, validate each item\n",
        "            if isinstance(data, list):\n",
        "                for item in data:\n",
        "                    jsonschema_validate(instance=item, schema=schema)\n",
        "            else:\n",
        "                jsonschema_validate(instance=data, schema=schema)\n",
        "            return {\"valid\": True, \"errors\": []}\n",
        "        except ValidationError as e:\n",
        "            return {\"valid\": False, \"errors\": [str(e)]}\n",
        "        except Exception as e:\n",
        "            return {\"valid\": False, \"errors\": [str(e)]}\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, hf_client: HFClient, registry: TemplateRegistry, prompt_engine: PromptEngine, validator: Validator):\n",
        "        self.hf = hf_client\n",
        "        self.registry = registry\n",
        "        self.engine = prompt_engine\n",
        "        self.validator = validator\n",
        "\n",
        "    def generate(self,\n",
        "                 template_id: Optional[str],\n",
        "                 params: Dict[str, Any],\n",
        "                 custom_prompt: Optional[str] = None,\n",
        "                 temperature: float = 0.2,\n",
        "                 max_tokens: int = 512) -> Dict[str, Any]:\n",
        "        # Load template or build ephemeral one\n",
        "        if custom_prompt:\n",
        "            template = {\"id\": \"custom\", \"prompt_template\": custom_prompt, \"schema\": {}}\n",
        "        else:\n",
        "            template = self.registry.get_template(template_id) or {\"id\": template_id, \"prompt_template\": \"\", \"schema\": {}}\n",
        "\n",
        "        prompt = self.engine.build_prompt(template, params)\n",
        "        try:\n",
        "            raw_text, meta = self.hf.generate(prompt, temperature=temperature, max_tokens=max_tokens)\n",
        "        except Exception as exc:\n",
        "            return {\"status\": \"error\", \"output\": None, \"raw_model_text\": \"\", \"validation\": {\"valid\": False, \"errors\": [str(exc)]}}\n",
        "\n",
        "        parsed = safe_json_load(raw_text)\n",
        "        if parsed is None:\n",
        "            return {\"status\": \"error\", \"output\": None, \"raw_model_text\": raw_text, \"validation\": {\"valid\": False, \"errors\": [\"parse_error\"]}}\n",
        "\n",
        "        validation = self.validator.validate(template.get(\"schema\", {}), parsed)\n",
        "        return {\"status\": \"ok\", \"output\": parsed, \"raw_model_text\": raw_text, \"validation\": validation}\n",
        "\n",
        "print(\"Validator and Generator classes defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize components\n",
        "registry = TemplateRegistry(TEMPLATES)\n",
        "prompt_engine = PromptEngine()\n",
        "hf_client = HFClient(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", api_key=HF_TOKEN)\n",
        "validator = Validator()\n",
        "generator = Generator(hf_client, registry, prompt_engine, validator)\n",
        "\n",
        "print(\"All components initialized successfully!\")\n",
        "print(f\"Available templates: {registry.get_ids()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Gradio Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_action(template_id: str, count: int, tone: str, temperature: float, max_tokens: int, custom_prompt: str, show_raw: bool):\n",
        "    params = {\"count\": count, \"tone\": tone}\n",
        "    if template_id == \"custom\":\n",
        "        result = generator.generate(None, params, custom_prompt=custom_prompt, temperature=temperature, max_tokens=max_tokens)\n",
        "    else:\n",
        "        result = generator.generate(template_id, params, custom_prompt=None, temperature=temperature, max_tokens=max_tokens)\n",
        "    output = result.get(\"output\")\n",
        "    raw = result.get(\"raw_model_text\", \"\")\n",
        "    validation = result.get(\"validation\", {\"valid\": False, \"errors\": []})\n",
        "    json_text = json.dumps(output, indent=2) if output is not None else \"\"\n",
        "    \n",
        "    # Debug: Always show raw output for now to help debug\n",
        "    debug_info = f\"Status: {result.get('status', 'unknown')}\\nRaw length: {len(raw)} chars\\nFirst 200 chars: {raw[:200]}\"\n",
        "    \n",
        "    if show_raw:\n",
        "        return json_text, raw, json.dumps(validation, indent=2)\n",
        "    return json_text, debug_info, json.dumps(validation, indent=2)\n",
        "\n",
        "print(\"Generate function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build UI\n",
        "templates = registry.get_ids()\n",
        "templates_dropdown = templates + [\"custom\"]\n",
        "\n",
        "with gr.Blocks(title=\"Synthetic Data Generator - Colab\") as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            template = gr.Dropdown(label=\"Template\", choices=templates_dropdown, value=templates_dropdown[0])\n",
        "            count = gr.Slider(label=\"Count\", minimum=1, maximum=10, value=1, step=1)\n",
        "            tone = gr.Textbox(label=\"Tone (optional)\", value=\"concise\")\n",
        "            temperature = gr.Slider(label=\"Temperature\", minimum=0.0, maximum=1.0, value=0.2, step=0.05)\n",
        "            max_tokens = gr.Number(label=\"Max tokens\", value=512)\n",
        "            custom_prompt = gr.Textbox(label=\"Custom prompt (used if Template=custom)\", lines=6, visible=False)\n",
        "            show_raw = gr.Checkbox(label=\"Show raw model output\", value=False)\n",
        "            gen_btn = gr.Button(\"Generate\")\n",
        "        with gr.Column(scale=1):\n",
        "            output = gr.Code(label=\"Output (JSON)\", language=\"json\")\n",
        "            raw_out = gr.Textbox(label=\"Raw model output\", lines=8)\n",
        "            validation = gr.Code(label=\"Validation\", language=\"json\")\n",
        "\n",
        "    def on_template_change(t):\n",
        "        return gr.update(visible=(t == \"custom\"))\n",
        "\n",
        "    template.change(on_template_change, inputs=[template], outputs=[custom_prompt])\n",
        "\n",
        "    gen_btn.click(fn=generate_action,\n",
        "                  inputs=[template, count, tone, temperature, max_tokens, custom_prompt, show_raw],\n",
        "                  outputs=[output, raw_out, validation])\n",
        "\n",
        "print(\"Gradio interface created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Launch the Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the interface\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Instructions\n",
        "\n",
        "1. **Select a Template**: Choose from predefined templates (user_profile, job_description, product_spec, address) or select \"custom\" for your own prompt\n",
        "\n",
        "2. **Configure Parameters**:\n",
        "   - **Count**: Number of items to generate (1-10)\n",
        "   - **Tone**: Style of generation (e.g., \"concise\", \"detailed\", \"professional\")\n",
        "   - **Temperature**: Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
        "   - **Max tokens**: Maximum length of generated text\n",
        "\n",
        "3. **Custom Prompts**: If you select \"custom\" template, you can enter your own prompt in the text area\n",
        "\n",
        "4. **Generate**: Click the \"Generate\" button to create synthetic data\n",
        "\n",
        "5. **View Results**:\n",
        "   - **Output (JSON)**: The structured JSON data\n",
        "   - **Raw model output**: The raw text from the model\n",
        "   - **Validation**: Whether the output matches the expected schema\n",
        "\n",
        "## Example Custom Prompts\n",
        "\n",
        "Here are some example custom prompts you can try:\n",
        "\n",
        "**Generate product reviews:**\n",
        "```\n",
        "Generate 3 product reviews as a JSON array. Each review should have: rating (1-5), title, content, reviewer_name, date. Tone: honest and detailed.\n",
        "```\n",
        "\n",
        "**Generate social media posts:**\n",
        "```\n",
        "Generate 2 social media posts as a JSON array. Each post should have: platform, content, hashtags (array), engagement_metrics (likes, shares, comments). Tone: engaging and modern.\n",
        "```\n",
        "\n",
        "**Generate customer support tickets:**\n",
        "```\n",
        "Generate 3 customer support tickets as a JSON array. Each ticket should have: ticket_id, customer_email, subject, description, priority (low/medium/high), status. Tone: realistic and varied.\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
