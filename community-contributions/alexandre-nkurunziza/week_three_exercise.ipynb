{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Exercise: Generating Synthetic Data\n",
    "\n",
    "**Objectives:**\n",
    "1. Build models that can generate datasets (JSON, CSV, or Raw Text).\n",
    "2. Use a variety of Hugging Face models and prompts for diverse outputs.\n",
    "3. Create a Gradio UI so users can choose the model and generate synthetic data.\n",
    "\n",
    "**Run this notebook in Google Colab**. Add Hugging Face token in Colab Secrets: `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade bitsandbytes accelerate \"transformers==4.57.6\" sentencepiece gradio torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import lru_cache\n",
    "\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"Llama-3.2-3B-Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"SmolLM2-1.7B-Instruct\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "    \"Qwen2-1.5B-Instruct\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "}\n",
    "\n",
    "FORMAT_RULES = {\n",
    "    \"JSON\": \"Return a JSON array containing exactly the requested number of objects with consistent fields tailored to the context. No explanations.\",\n",
    "    \"CSV\": \"Return a CSV document with a header row and the requested number of data rows aligned to the context. No explanations.\",\n",
    "    \"Raw Text\": \"Return the requested number of short prose entries separated by blank lines that reflect the context. No explanations.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization\n",
    "def get_quantization_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=6)\n",
    "def load_text_components(model_id: str, use_quant: bool):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if use_quant and torch.cuda.is_available():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=get_quantization_config(),\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_TEMPLATES = {\n",
    "    \"Concise\": \"Keep each record brief and to the point.\",\n",
    "    \"Detailed\": \"Include rich, realistic detail in each record.\",\n",
    "    \"Diverse\": \"Maximize variety across records (names, values, categories).\",\n",
    "    \"Technical\": \"Use precise, technical language where appropriate.\",\n",
    "    \"Balanced\": \"Mix clarity and variety without being verbose.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_messages(\n",
    "    style: str,\n",
    "    context: str,\n",
    "    return_format: str,\n",
    "    record_count: int,\n",
    ") -> list:\n",
    "    context_value = (context or \"general purpose scenario\").strip()\n",
    "    style_value = (style or \"Balanced\").strip()\n",
    "    style_instruction = STYLE_TEMPLATES.get(style_value, STYLE_TEMPLATES[\"Balanced\"])\n",
    "    directive = FORMAT_RULES.get(return_format, FORMAT_RULES[\"JSON\"])\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You generate synthetic datasets that are high quality, diverse, and free of personally identifiable information. \"\n",
    "        + directive\n",
    "        + \" Ensure outputs are consistent in structure and avoid any explanation or commentary.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Context: {context_value}\\n\"\n",
    "        f\"Style: {style_instruction}\\n\"\n",
    "        f\"Generate exactly {record_count} records. Output format: {return_format}.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_data(\n",
    "    model_choice: str,\n",
    "    style: str,\n",
    "    context: str,\n",
    "    return_format: str,\n",
    "    quantize: bool,\n",
    "    record_count: int,\n",
    ") -> str:\n",
    "    model_id = MODELS.get(model_choice)\n",
    "    if not model_id:\n",
    "        return \"Error: Unknown model selected.\"\n",
    "\n",
    "    tokenizer, model = load_text_components(model_id, bool(quantize))\n",
    "    messages = build_text_messages(style, context, return_format, int(record_count))\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    else:\n",
    "        prompt = messages[-1][\"content\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    if isinstance(inputs, dict):\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "    else:\n",
    "        input_ids = inputs.to(device) if hasattr(inputs, \"to\") else torch.tensor([inputs], device=device)\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.05,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    new_tokens = generated[:, input_ids.shape[-1] :]\n",
    "    text = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Generating Synthetic Data\") as demo:\n",
    "    gr.Markdown(\"## Generating Synthetic Data\")\n",
    "    gr.Markdown(\"Choose a Hugging Face model and generate datasets in JSON, CSV, or Raw Text.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_choice = gr.Dropdown(\n",
    "            choices=list(MODELS.keys()),\n",
    "            value=\"SmolLM2-1.7B-Instruct\",\n",
    "            label=\"Model\",\n",
    "        )\n",
    "        return_format = gr.Dropdown(\n",
    "            choices=[\"JSON\", \"CSV\", \"Raw Text\"],\n",
    "            value=\"JSON\",\n",
    "            label=\"Output format\",\n",
    "        )\n",
    "        style = gr.Dropdown(\n",
    "            choices=list(STYLE_TEMPLATES.keys()),\n",
    "            value=\"Balanced\",\n",
    "            label=\"Style\",\n",
    "        )\n",
    "\n",
    "    context_input = gr.Textbox(\n",
    "        label=\"Context\",\n",
    "        lines=4,\n",
    "        placeholder=\"e.g. Product catalog for an online electronics store: name, category, price, sku, in_stock\",\n",
    "    )\n",
    "    record_count = gr.Slider(1, 20, value=5, step=1, label=\"Number of records\")\n",
    "\n",
    "    generate_btn = gr.Button(\"Generate\")\n",
    "    text_output = gr.Textbox(label=\"Generated data\", lines=16)\n",
    "\n",
    "    def run_generate(model_choice, style, context, return_format, record_count):\n",
    "        return generate_text_data(model_choice, style, context, return_format, True, record_count)\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=run_generate,\n",
    "        inputs=[model_choice, style, context_input, return_format, record_count],\n",
    "        outputs=text_output,\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_to_file(text: str, return_format: str):\n",
    "    \"\"\"Save generated text to a file and return its path for download. Returns None if error or empty.\"\"\"\n",
    "    if not (text and text.strip()) or text.startswith(\"Error:\"):\n",
    "        return None\n",
    "    ext = {\"JSON\": \".json\", \"CSV\": \".csv\", \"Raw Text\": \".txt\"}.get(return_format, \".txt\")\n",
    "    fd, path = tempfile.mkstemp(suffix=ext)\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        return path\n",
    "    except Exception:\n",
    "        try:\n",
    "            os.close(fd)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Generating Synthetic Data\") as demo:\n",
    "    gr.Markdown(\"## Generating Synthetic Data\")\n",
    "    gr.Markdown(\"Choose a Hugging Face model and generate datasets in JSON, CSV, or Raw Text.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_choice = gr.Dropdown(\n",
    "            choices=list(MODELS.keys()),\n",
    "            value=\"SmolLM2-1.7B-Instruct\",\n",
    "            label=\"Model\",\n",
    "        )\n",
    "        return_format = gr.Dropdown(\n",
    "            choices=[\"JSON\", \"CSV\", \"Raw Text\"],\n",
    "            value=\"JSON\",\n",
    "            label=\"Output format\",\n",
    "        )\n",
    "        style = gr.Dropdown(\n",
    "            choices=list(STYLE_TEMPLATES.keys()),\n",
    "            value=\"Balanced\",\n",
    "            label=\"Style\",\n",
    "        )\n",
    "\n",
    "    context_input = gr.Textbox(\n",
    "        label=\"Context\",\n",
    "        lines=4,\n",
    "        placeholder=\"e.g. Product catalog for an online electronics store: name, category, price, sku, in_stock\",\n",
    "    )\n",
    "    record_count = gr.Slider(1, 20, value=5, step=1, label=\"Number of records\")\n",
    "\n",
    "    generate_btn = gr.Button(\"Generate\")\n",
    "    text_output = gr.Textbox(label=\"Generated data (preview)\", lines=16)\n",
    "    file_output = gr.File(label=\"Download file\")\n",
    "\n",
    "    def run_generate(model_choice, style, context, return_format, record_count):\n",
    "        text = generate_text_data(model_choice, style, context, return_format, True, record_count)\n",
    "        file_path = save_generated_to_file(text, return_format)\n",
    "        return text, file_path\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=run_generate,\n",
    "        inputs=[model_choice, style, context_input, return_format, record_count],\n",
    "        outputs=[text_output, file_output],\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
