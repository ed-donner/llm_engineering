{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd179be",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Error: OpenAI API key not found. Please set it in the .env file.\")\n",
    "if anthropic_api_key:\n",
    "    print(\"Anthropic API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Error: Anthropic API key not found. Please set it in the .env file.\")\n",
    "if google_api_key:\n",
    "    print(\"Gemini API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Error: Gemini API key not found. Please set it in the .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3be1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "openai = OpenAI()\n",
    "anthropic = OpenAI(base_url=anthropic_url, api_key=anthropic_api_key)\n",
    "gemini = OpenAI(base_url=gemini_url, api_key=google_api_key)\n",
    "ollama = OpenAI(base_url=ollama_url, api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4.1-mini\"\n",
    "ANTHROPIC_MODEL = \"claude-haiku-4-5\"\n",
    "GEMINI_MODEL = \"gemini-2.5-flash-lite\"\n",
    "OLLAMA_MODEL = \"llama3.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69225820",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful technical tutor who answers questions regarding software development, programming, and computer science.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e18352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_datetime():\n",
    "    return f\"Current date and time is: {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_function = {\n",
    "    \"name\": \"get_current_datetime\",\n",
    "    \"description\": \"Get the current date and time.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": [],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43545b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": datetime_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(message):\n",
    "    responses = []\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"get_current_datetime\":\n",
    "            print(f\"Handling tool call: {tool_call.function.name}\")\n",
    "            response = get_current_datetime()\n",
    "            responses.append({\"role\": \"tool\", \"content\": response, \"tool_call_id\": tool_call.id})\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(messages, tools, stream):\n",
    "    return openai.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    stream=stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(messages, tools, stream):\n",
    "    return anthropic.chat.completions.create(\n",
    "        model=ANTHROPIC_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini(messages, tools, stream):\n",
    "    return gemini.chat.completions.create(\n",
    "        model=GEMINI_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0583d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_ollama(messages, stream):\n",
    "    return ollama.chat.completions.create(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=messages,\n",
    "        stream=stream,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f1046",
   "metadata": {},
   "source": [
    "No tools for llama3.2, I removed it because I was getting blank replies sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_call(model, messages, tools, stream=False):\n",
    "    if model == OPENAI_MODEL:\n",
    "        return stream_gpt(messages, tools, stream)\n",
    "    elif model == ANTHROPIC_MODEL:\n",
    "        return stream_claude(messages, tools, stream)\n",
    "    elif model == GEMINI_MODEL:\n",
    "        return stream_gemini(messages, tools, stream)\n",
    "    elif model == OLLAMA_MODEL:\n",
    "        return stream_ollama(messages, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d97680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo(message, history, model):\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        + history\n",
    "        + [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "\n",
    "    response = handle_call(model, messages, tools, stream=False)\n",
    "    while response.choices[0].finish_reason == \"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        responses = handle_tool_calls(message)\n",
    "        messages.append(message)\n",
    "        messages.extend(responses)\n",
    "        response = handle_call(model, messages, tools, stream=False)\n",
    "\n",
    "    stream = handle_call(model, messages, tools, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4789bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(\n",
    "    fn=echo,\n",
    "    type=\"messages\",\n",
    "    title=\"Technical Tutor\",\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown(choices=[OPENAI_MODEL, ANTHROPIC_MODEL, GEMINI_MODEL, OLLAMA_MODEL])\n",
    "    ],\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
