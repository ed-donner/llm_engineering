{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc4a5cd",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Link to my fork: https://github.com/JaymanR/llm_engineering\n",
    "\n",
    "For the 3-way chatbot, I have used gpt 4.1 mini, gemini 2.5 flash lite and llama3.2 via ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2daaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "openai = OpenAI()\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = \"gpt-4.1-mini\"\n",
    "MODEL_GEMINI = \"gemini-2.5-flash-lite\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "BOT1 = \"Alex\"\n",
    "BOT2 = \"Blake\"\n",
    "BOT3 = \"Cosmo\"\n",
    "\n",
    "CHATBOTS = [BOT1, BOT2, BOT3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_bots(current_bot):\n",
    "    return [bot for bot in CHATBOTS if bot != current_bot]\n",
    "\n",
    "def get_user_prompt(current_bot, conversation):\n",
    "    other_bots = get_other_bots(current_bot)\n",
    "    return f\"\"\"\n",
    "    You are {current_bot}, in a conversation with {other_bots[0]} and {other_bots[1]}.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as {current_bot}.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system_prompt = f\"\"\"\n",
    "You are {BOT1}, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with {BOT3} and {BOT2}. Ensure your response starts with, {BOT1}: \n",
    "\"\"\"\n",
    "\n",
    "gemini_system_prompt = f\"\"\"\n",
    "You are {BOT2}, a chatbot who lives in your own bubble. You are in a conversation with {BOT1} and {BOT3}\n",
    "You often start on topic, but eventually drift on to ramble about things out of scope without realizing during the conversation. \n",
    "Ensure your response starts with, {BOT2}: \n",
    "\"\"\"\n",
    "\n",
    "# I tried asking llama3.2 to start with Bot_Name: , but sometimes it wouldn't start with it.\n",
    "llama_system_prompt = f\"\"\"\n",
    "You are {BOT3}, a very polite, courteous chatbot. You are in a conversation with {BOT1} and {BOT2}\n",
    "You try to agree with everything the other person says, or find common ground. \n",
    "If the other person is argumentative, you try to calm them down and keep chatting.\n",
    "Please directly respond as if you are talking to them both live.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76429b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [f\"{BOT1}: Hi, I guess...\"]\n",
    "gemini_messages = [f\"{BOT2}: Hello!\"]\n",
    "ollama_messages = [f\"{BOT3}: Hello, How are you doing?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conversation():\n",
    "    conversation = \"\"\n",
    "    max_len = max(len(gpt_messages), len(ollama_messages), len(gemini_messages))\n",
    "    for i in range(max_len):\n",
    "        if i < len(gpt_messages):\n",
    "            conversation += gpt_messages[i] + \"\\n\\n\"\n",
    "        if i < len(gemini_messages):\n",
    "            conversation += gemini_messages[i] + \"\\n\\n\"\n",
    "        if i < len(ollama_messages):\n",
    "            conversation += ollama_messages[i] + \"\\n\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(system_prompt, user_prompt):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=get_messages(gpt_system_prompt, get_user_prompt(BOT1, build_conversation())),\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=get_messages(gpt_system_prompt, get_user_prompt(BOT1, build_conversation())),\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    stream = gemini.chat.completions.create(\n",
    "        model=MODEL_GEMINI,\n",
    "        messages=get_messages(gemini_system_prompt, get_user_prompt(BOT2, build_conversation())),\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6eed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama():\n",
    "    stream = chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=get_messages(llama_system_prompt, get_user_prompt(BOT3, build_conversation())),\n",
    "        stream=True\n",
    "    )\n",
    "    response = f\"{BOT3}: \"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.message.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [f\"{BOT1}: Hi, I guess...\"]\n",
    "gemini_messages = [f\"{BOT2}: Hello!\"]\n",
    "ollama_messages = [f\"{BOT3}: Hello, How are you doing?\"]\n",
    "\n",
    "display(Markdown(build_conversation()))\n",
    "\n",
    "call_gpt()\n",
    "call_gemini()\n",
    "call_ollama()\n",
    "\n",
    "for i in range(2):\n",
    "    gpt_next = call_gpt()\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    gemini_messages.append(gemini_next)\n",
    "\n",
    "    ollama_next = call_ollama()\n",
    "    ollama_messages.append(ollama_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
