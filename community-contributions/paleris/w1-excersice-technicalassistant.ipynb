{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "model=MODEL_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4b8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a senior technical assistant who provides accurate, clear, and practical answers to technical questions in software engineering and related fields.\n",
    "Prioritize correctness over speed, state assumptions explicitly, and avoid speculation or hallucination.\n",
    "Begin with a direct answer, then explain step by step when useful, using minimal, idiomatic, and runnable code examples where appropriate.\n",
    "When debugging or comparing solutions, explain the underlying causes, trade-offs, and how to verify results.\n",
    "Ask clarifying questions only when necessary and adapt the level of detail to the user's apparent expertise.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "openai = OpenAI()\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "\n",
    "def chat(user_message, history, model):\n",
    "    # history is a list of [user, assistant] pairs\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    if history:\n",
    "        for item in history:\n",
    "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "                u, a = item\n",
    "                if u: messages.append({\"role\": \"user\", \"content\": u})\n",
    "                if a: messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "            elif isinstance(item, dict) and \"role\" in item and \"content\" in item:\n",
    "                messages.append({\"role\": item[\"role\"], \"content\": item[\"content\"]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    if model == MODEL_GPT:\n",
    "        yield from call_openai(messages)\n",
    "    elif model == MODEL_LLAMA:\n",
    "        text = call_ollama(messages)\n",
    "        yield text\n",
    "    else:\n",
    "        return f\"Unknown model: {model}\"\n",
    "\n",
    "def call_openai(messages):\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=MODEL_GPT,\n",
    "            messages=messages,\n",
    "            stream = True\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        \n",
    "        for chunk in resp:\n",
    "            delta = chunk.choices[0].delta.content\n",
    "            if delta:\n",
    "                response += delta\n",
    "                yield response\n",
    "\n",
    "# Get Llama 3.2 to answer\n",
    "\n",
    "def call_ollama(messages):\n",
    "    resp = ollama.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0952d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector = gr.Radio(\n",
    "    choices=[MODEL_GPT, MODEL_LLAMA],\n",
    "    value=MODEL_GPT,\n",
    "    label=\"Model\"\n",
    ")\n",
    "\n",
    "chat = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    additional_inputs=model_selector,\n",
    "    title=\"Technical Assistant\",\n",
    ")\n",
    "chat.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
