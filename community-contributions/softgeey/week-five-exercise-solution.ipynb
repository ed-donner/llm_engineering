{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a86277e",
   "metadata": {},
   "source": [
    "# AI Personal Knowledge worker\n",
    "\n",
    "A local, private chatbot that ingests your documents, vectorizes them, and lets you have a conversation with your own knowledge base. No data leaves your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "!pip install gradio chromadb sentence-transformers anthropic pypdf python-docx pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c079f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from anthropic import Anthropic\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "import pickle\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec841cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "DOCS_FOLDER = \"docs\"\n",
    "CHROMA_FOLDER = \"chroma_db\"\n",
    "MODEL = \"claude-opus-4-6\"\n",
    "\n",
    "GOOGLE_CLIENT_ID = os.environ.get(\"GOOGLE_CLIENT_ID\")\n",
    "GOOGLE_CLIENT_SECRET = os.environ.get(\"GOOGLE_CLIENT_SECRET\")\n",
    "GOOGLE_REDIRECT_URI = os.environ.get(\"GOOGLE_REDIRECT_URI\")\n",
    "TOKEN_FILE = \"google_token.pickle\"\n",
    "\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/drive.readonly\",\n",
    "    \"https://www.googleapis.com/auth/documents.readonly\"\n",
    "]\n",
    "\n",
    "CLIENT_CONFIG = {\n",
    "    \"installed\": {\n",
    "        \"client_id\": GOOGLE_CLIENT_ID,\n",
    "        \"client_secret\": GOOGLE_CLIENT_SECRET,\n",
    "        \"redirect_uris\": [\"http://localhost\"],\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\"\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs(DOCS_FOLDER, exist_ok=True)\n",
    "os.makedirs(CHROMA_FOLDER, exist_ok=True)\n",
    "\n",
    "client = Anthropic(api_key=ANTHROPIC_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database Setup\n",
    "embed_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_FOLDER)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"knowledge_base\",\n",
    "    embedding_function=embed_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f00d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  File Parsers\n",
    "def parse_pdf(filepath):\n",
    "    reader = PdfReader(filepath)\n",
    "    return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "def parse_docx(filepath):\n",
    "    doc = Document(filepath)\n",
    "    return \"\\n\".join(para.text for para in doc.paragraphs if para.text.strip())\n",
    "\n",
    "def parse_txt(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df.to_string(index=False)\n",
    "\n",
    "def parse_file(filepath):\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    parsers = {\n",
    "        \".pdf\": parse_pdf,\n",
    "        \".docx\": parse_docx,\n",
    "        \".txt\": parse_txt,\n",
    "        \".md\": parse_txt,\n",
    "        \".csv\": parse_csv\n",
    "    }\n",
    "    parser = parsers.get(ext)\n",
    "    if parser:\n",
    "        return parser(filepath)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c50904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Document Indexing\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def index_documents():\n",
    "    files = os.listdir(DOCS_FOLDER)\n",
    "    if not files:\n",
    "        return \"No files found in /docs folder.\"\n",
    "\n",
    "    indexed = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(DOCS_FOLDER, filename)\n",
    "        text = parse_file(filepath)\n",
    "\n",
    "        if not text:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        existing = collection.get(where={\"source\": filename})\n",
    "\n",
    "        if existing[\"ids\"]:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        ids = [f\"{filename}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"source\": filename} for _ in chunks]\n",
    "\n",
    "        collection.add(documents=chunks, ids=ids, metadatas=metadatas)\n",
    "        indexed += 1\n",
    "\n",
    "    return f\"Indexing complete ✓ | Indexed: {indexed} files | Skipped: {skipped} files | Total chunks: {collection.count()}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a43ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Retrieval & Q&A\n",
    "def retrieve_context(query, n_results=5):\n",
    "    results = collection.query(query_texts=[query], n_results=n_results)\n",
    "    docs = results[\"documents\"][0]\n",
    "    sources = [m[\"source\"] for m in results[\"metadatas\"][0]]\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    unique_sources = list(set(sources))\n",
    "    return context, unique_sources\n",
    "\n",
    "def ask_claude(query, chat_history):\n",
    "    context, sources = retrieve_context(query)\n",
    "\n",
    "    system_prompt = f\"\"\"You are a helpful personal knowledge assistant. \n",
    "Answer the user's question using only the context provided from their personal documents.\n",
    "If the answer is not in the context, say so clearly.\n",
    "\n",
    "Context from personal documents:\n",
    "{context}\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in chat_history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    answer = response.content[0].text\n",
    "    if sources:\n",
    "        answer += f\"\\n\\n*Sources: {', '.join(sources)}*\"\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Knowledge Base Management\n",
    "\n",
    "\n",
    "def list_indexed_documents():\n",
    "    results = collection.get()\n",
    "    if not results[\"ids\"]:\n",
    "        return \"No documents indexed yet.\"\n",
    "    sources = sorted(set(m[\"source\"] for m in results[\"metadatas\"]))\n",
    "    doc_list = \"\\n\".join(f\"• {s}\" for s in sources)\n",
    "    return f\"Indexed documents ({len(sources)}):\\n\\n{doc_list}\"\n",
    "\n",
    "def clear_all():\n",
    "    global collection\n",
    "\n",
    "    chroma_client.delete_collection(\"knowledge_base\")\n",
    "\n",
    "    shutil.rmtree(CHROMA_FOLDER, ignore_errors=True)\n",
    "    os.makedirs(CHROMA_FOLDER, exist_ok=True)\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"knowledge_base\",\n",
    "        embedding_function=embed_fn\n",
    "    )\n",
    "\n",
    "    for f in os.listdir(DOCS_FOLDER):\n",
    "        os.remove(os.path.join(DOCS_FOLDER, f))\n",
    "\n",
    "    return \"All documents cleared.\", list_indexed_documents()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Google Auth Setup\n",
    "\n",
    "\n",
    "\n",
    "def get_google_services():\n",
    "    creds = None\n",
    "    if os.path.exists(TOKEN_FILE):\n",
    "        with open(TOKEN_FILE, \"rb\") as f:\n",
    "            creds = pickle.load(f)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_config(CLIENT_CONFIG, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open(TOKEN_FILE, \"wb\") as f:\n",
    "            pickle.dump(creds, f)\n",
    "    drive_service = build(\"drive\", \"v3\", credentials=creds)\n",
    "    docs_service = build(\"docs\", \"v1\", credentials=creds)\n",
    "    return drive_service, docs_service\n",
    "\n",
    "def disconnect_google():\n",
    "    if os.path.exists(TOKEN_FILE):\n",
    "        os.remove(TOKEN_FILE)\n",
    "        return \"Disconnected from Google Workspace ✓ Re-sync to reconnect.\"\n",
    "    return \"No active Google session found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddde621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fetch & Index Google Workspace Files\n",
    "GOOGLE_MIME_EXPORT = {\n",
    "    \"application/vnd.google-apps.document\": \"text/plain\",\n",
    "    \"application/vnd.google-apps.spreadsheet\": \"text/csv\",\n",
    "    \"application/vnd.google-apps.presentation\": \"text/plain\"\n",
    "}\n",
    "\n",
    "google_page_token = None\n",
    "total_files_indexed = 0\n",
    "\n",
    "def fetch_google_doc_text(docs_service, file_id):\n",
    "    doc = docs_service.documents().get(documentId=file_id).execute()\n",
    "    text = \"\"\n",
    "    for element in doc.get(\"body\", {}).get(\"content\", []):\n",
    "        paragraph = element.get(\"paragraph\")\n",
    "        if paragraph:\n",
    "            for part in paragraph.get(\"elements\", []):\n",
    "                text_run = part.get(\"textRun\")\n",
    "                if text_run:\n",
    "                    text += text_run.get(\"content\", \"\")\n",
    "    return text.strip()\n",
    "\n",
    "def fetch_exported_text(drive_service, file_id, mime_type):\n",
    "    export_mime = GOOGLE_MIME_EXPORT[mime_type]\n",
    "    content = drive_service.files().export(fileId=file_id, mimeType=export_mime).execute()\n",
    "    return content.decode(\"utf-8\").strip() if isinstance(content, bytes) else content.strip()\n",
    "\n",
    "def fetch_and_index_google_workspace(reset=False):\n",
    "    global google_page_token, total_files_indexed\n",
    "\n",
    "    if reset:\n",
    "        google_page_token = None\n",
    "        total_files_indexed = 0\n",
    "\n",
    "    try:\n",
    "        drive_service, docs_service = get_google_services()\n",
    "    except Exception as e:\n",
    "        return f\"Auth failed: {str(e)}\", list_indexed_documents()\n",
    "\n",
    "    query = \" or \".join([f\"mimeType='{m}'\" for m in GOOGLE_MIME_EXPORT.keys()])\n",
    "    query += \" and trashed=false\"\n",
    "\n",
    "    try:\n",
    "        response = drive_service.files().list(\n",
    "            q=query,\n",
    "            corpora=\"allDrives\",\n",
    "            includeItemsFromAllDrives=True,\n",
    "            supportsAllDrives=True,\n",
    "            spaces=\"drive\",\n",
    "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
    "            pageSize=20,\n",
    "            pageToken=google_page_token\n",
    "        ).execute()\n",
    "    except Exception as e:\n",
    "        return f\"Drive API error: {str(e)}\", list_indexed_documents()\n",
    "\n",
    "    files = response.get(\"files\", [])\n",
    "    google_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "    if not files:\n",
    "        return \"No Google Workspace files found.\", list_indexed_documents()\n",
    "\n",
    "    indexed, skipped = 0, 0\n",
    "\n",
    "    for file in files:\n",
    "        file_id = file[\"id\"]\n",
    "        mime_type = file[\"mimeType\"]\n",
    "        file_type = mime_type.split(\".\")[-1]\n",
    "        doc_name = f\"gdrive_{file_type}_{file['name']}\"\n",
    "\n",
    "        existing = collection.get(where={\"source\": doc_name})\n",
    "        if existing[\"ids\"]:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if mime_type == \"application/vnd.google-apps.document\":\n",
    "                text = fetch_google_doc_text(docs_service, file_id)\n",
    "            else:\n",
    "                text = fetch_exported_text(drive_service, file_id, mime_type)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {file['name']}: {str(e)}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if not text:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        ids = [f\"{doc_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"source\": doc_name} for _ in chunks]\n",
    "\n",
    "        collection.add(documents=chunks, ids=ids, metadatas=metadatas)\n",
    "        indexed += 1\n",
    "\n",
    "    total_files_indexed += indexed\n",
    "    more = \"More files available — click Load More.\" if google_page_token else \"All files synced ✓\"\n",
    "    status = f\"Batch complete | This batch: {indexed} indexed, {skipped} skipped | Total indexed: {total_files_indexed} | {more}\"\n",
    "\n",
    "    return status, list_indexed_documents()\n",
    "\n",
    "def sync_google_fresh():\n",
    "    return fetch_and_index_google_workspace(reset=True)\n",
    "\n",
    "def load_more_google():\n",
    "    return fetch_and_index_google_workspace(reset=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e681c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gradio UI\n",
    "def upload_and_index(files):\n",
    "    if not files:\n",
    "        return \"No files uploaded.\", list_indexed_documents()\n",
    "    for file in files:\n",
    "        filename = os.path.basename(file.name)\n",
    "        dest = os.path.join(DOCS_FOLDER, filename)\n",
    "        with open(file.name, \"rb\") as src, open(dest, \"wb\") as dst:\n",
    "            dst.write(src.read())\n",
    "    status = index_documents()\n",
    "    return status, list_indexed_documents()\n",
    "\n",
    "def chat(user_message, history):\n",
    "    if not user_message.strip():\n",
    "        return \"\", history\n",
    "\n",
    "    context, sources = retrieve_context(user_message)\n",
    "\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    system_prompt = f\"\"\"You are a helpful personal knowledge assistant.\n",
    "Answer the user's question using only the context provided from their personal documents.\n",
    "If the answer is not in the context, say so clearly.\n",
    "\n",
    "Context from personal documents:\n",
    "{context}\"\"\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    answer = response.content[0].text\n",
    "    if sources:\n",
    "        answer += f\"\\n\\n*Sources: {', '.join(sources)}*\"\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    return \"\", history\n",
    "\n",
    "with gr.Blocks(title=\"Personal Knowledge Worker\") as app:\n",
    "    gr.Markdown(\"# Personal Knowledge Worker\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Local Documents\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Upload Documents\")\n",
    "                    file_input = gr.File(file_count=\"multiple\", label=\"Select Files\")\n",
    "                    index_btn = gr.Button(\"Index Documents\", variant=\"primary\")\n",
    "                    index_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                    gr.Markdown(\"### Knowledge Base\")\n",
    "                    doc_list = gr.Textbox(label=\"Indexed Documents\", lines=6, interactive=False,\n",
    "                                          value=list_indexed_documents())\n",
    "                    clear_btn = gr.Button(\"Clear All Documents\", variant=\"stop\")\n",
    "\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"### Chat with your Knowledge Base\")\n",
    "                    chatbot = gr.Chatbot(height=450)\n",
    "                    msg_input = gr.Textbox(placeholder=\"Ask a question about your documents...\", label=\"Your Question\")\n",
    "                    clear_chat_btn = gr.ClearButton([msg_input, chatbot])\n",
    "\n",
    "        with gr.Tab(\"Google Workspace\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Google Workspace Sync\")\n",
    "                    gr.Markdown(\"Fetches 20 files at a time from your Google Drive.\")\n",
    "                    with gr.Row():\n",
    "                        google_sync_btn = gr.Button(\"Sync First 20\", variant=\"primary\")\n",
    "                        google_more_btn = gr.Button(\"Load More\", variant=\"secondary\")\n",
    "                    google_disconnect_btn = gr.Button(\"Disconnect Google\", variant=\"secondary\")\n",
    "                    google_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                    gr.Markdown(\"### Knowledge Base\")\n",
    "                    google_doc_list = gr.Textbox(label=\"Indexed Documents\", lines=6, interactive=False,\n",
    "                                                  value=list_indexed_documents())\n",
    "                    google_clear_btn = gr.Button(\"Clear All Documents\", variant=\"stop\")\n",
    "\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"### Chat with your Knowledge Base\")\n",
    "                    chatbot2 = gr.Chatbot(height=450)\n",
    "                    msg_input2 = gr.Textbox(placeholder=\"Ask a question about your Google Workspace...\", label=\"Your Question\")\n",
    "                    clear_chat_btn2 = gr.ClearButton([msg_input2, chatbot2])\n",
    "\n",
    "    # Local tab events\n",
    "    index_btn.click(upload_and_index, inputs=file_input, outputs=[index_status, doc_list])\n",
    "    clear_btn.click(clear_all, outputs=[index_status, doc_list])\n",
    "    msg_input.submit(chat, inputs=[msg_input, chatbot], outputs=[msg_input, chatbot])\n",
    "\n",
    "    # Google tab events\n",
    "    google_sync_btn.click(sync_google_fresh, outputs=[google_status, google_doc_list])\n",
    "    google_more_btn.click(load_more_google, outputs=[google_status, google_doc_list])\n",
    "    google_disconnect_btn.click(disconnect_google, outputs=[google_status])\n",
    "    google_clear_btn.click(clear_all, outputs=[google_status, google_doc_list])\n",
    "    msg_input2.submit(chat, inputs=[msg_input2, chatbot2], outputs=[msg_input2, chatbot2])\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
