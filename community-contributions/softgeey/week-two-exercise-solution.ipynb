{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b5f49",
   "metadata": {},
   "source": [
    "# Smart Document Analyst\n",
    "\n",
    "A multi-modal AI-powered business application that allows users to upload and interrogate any business document through a conversational Q&A interface. Built in Jupyter Notebook with a Gradio UI, the app supports invoices, contracts, forms, PDFs, Word documents, audio recordings, and plain text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / upgrade dependencies (run once)\n",
    "!pip -q install -U gradio pillow openai anthropic google-genai numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d104cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + initialize LLM clients\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any, Generator, List, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Clients (keys assumed already set in your environment)\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set )\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not \")\n",
    "\n",
    "\n",
    "openai = OpenAI()\n",
    "anthropic = Anthropic()\n",
    "gemini =  genai.Client()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af32b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ystem Prompt & Model Configuration\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Business Document Analyst with deep expertise in:\n",
    "- Invoices & receipts: extracting vendor details, line items, totals, tax, payment terms\n",
    "- Contracts & agreements: identifying parties, key dates, obligations, risks, termination clauses\n",
    "- Forms & applications: parsing fields, flagging missing or inconsistent data, summarizing intent\n",
    "- General business documents: summarizing content, spotting anomalies, answering precise questions\n",
    "\n",
    "Your behaviour:\n",
    "- When a document image is first uploaded, automatically provide a structured overview:\n",
    "  * Document type detected\n",
    "  * Key parties or entities involved\n",
    "  * Top 3-5 key findings or extracted fields\n",
    "  * Any anomalies, missing info, or red flags \n",
    "- After the overview, invite the user to ask follow-up questions\n",
    "- Answer questions precisely, referencing specific sections of the document\n",
    "- When extracting structured data, return it as a clean markdown table\n",
    "- For contract risk items use:  High Risk /  Medium Risk /  Low Risk\n",
    "- Be concise, professional, and business-focused at all times\n",
    "- If something in the document is unclear or illegible, say so honestly\n",
    "\"\"\"\n",
    "\n",
    "# Model registry ‚Äî maps UI display name to provider + model ID\n",
    "MODELS = {\n",
    "    \"GPT-4o mini (OpenAI)\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_id\": \"gpt-4o-mini\"\n",
    "    },\n",
    "    \"Claude Sonnet (Anthropic)\": {\n",
    "        \"provider\": \"anthropic\",\n",
    "        \"model_id\": \"claude-sonnet-4-5\"\n",
    "    },\n",
    "    \"Gemini 2.5 Flash (Google)\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model_id\": \"gemini-2.5-flash\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Utility\n",
    "\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def pil_to_base64(pil_image: Image.Image, format: str = \"JPEG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to a base64 string.\n",
    "    Handles RGBA and palette-mode images by converting to RGB first.\n",
    "    \"\"\"\n",
    "    if pil_image.mode in (\"RGBA\", \"P\", \"LA\"):\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    pil_image.save(buffer, format=format)\n",
    "    buffer.seek(0)\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# --- Quick Test ---\n",
    "# Create a small dummy image and verify the function works\n",
    "test_img = Image.new(\"RGB\", (100, 100), color=(255, 0, 0))  # solid red square\n",
    "test_b64 = pil_to_base64(test_img)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c168f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Inference Engine\n",
    "\n",
    "\n",
    "def stream_response(\n",
    "    message: str,\n",
    "    history: List[Dict],\n",
    "    model_key: str,\n",
    "    image: Optional[Image.Image] = None\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Stream a response from the selected model.\n",
    "    \n",
    "    Args:\n",
    "        message   : current user message/question\n",
    "        history   : list of {\"role\": \"user\"/\"assistant\", \"content\": \"...\"} dicts\n",
    "        model_key : key from MODELS dict (matches UI dropdown)\n",
    "        image     : PIL image if attached (only on first turn)\n",
    "    \n",
    "    Yields:\n",
    "        str: incremental response chunks for Gradio streaming\n",
    "    \"\"\"\n",
    "\n",
    "    model_cfg = MODELS[model_key]\n",
    "    provider  = model_cfg[\"provider\"]\n",
    "    model_id  = model_cfg[\"model_id\"]\n",
    "    b64       = pil_to_base64(image) if image else None\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # OPENAI                                                               #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if provider == \"openai\":\n",
    "\n",
    "        # Build message history in OpenAI format\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "        # Add prior turns from history\n",
    "        for turn in history:\n",
    "            messages.append({\"role\": turn[\"role\"], \"content\": turn[\"content\"]})\n",
    "\n",
    "        # Build current user message ‚Äî include image if present\n",
    "        if b64:\n",
    "            user_content = [\n",
    "                {\"type\": \"image_url\",\n",
    "                 \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64}\"}},\n",
    "                {\"type\": \"text\", \"text\": message}\n",
    "            ]\n",
    "        else:\n",
    "            user_content = message\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Stream from OpenAI\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        accumulated = \"\"\n",
    "        for chunk in stream:\n",
    "            delta = chunk.choices[0].delta.content\n",
    "            if delta:\n",
    "                accumulated += delta\n",
    "                yield accumulated  # yield full string so far (Gradio requirement)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # ANTHROPIC                                                            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    elif provider == \"anthropic\":\n",
    "\n",
    "        # Build message history in Anthropic format\n",
    "        messages = []\n",
    "\n",
    "        for turn in history:\n",
    "            messages.append({\"role\": turn[\"role\"], \"content\": turn[\"content\"]})\n",
    "\n",
    "        # Build current user message ‚Äî include image if present\n",
    "        if b64:\n",
    "            user_content = [\n",
    "                {\"type\": \"image\",\n",
    "                 \"source\": {\n",
    "                     \"type\": \"base64\",\n",
    "                     \"media_type\": \"image/jpeg\",\n",
    "                     \"data\": b64\n",
    "                 }},\n",
    "                {\"type\": \"text\", \"text\": message}\n",
    "            ]\n",
    "        else:\n",
    "            user_content = [{\"type\": \"text\", \"text\": message}]\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Stream from Anthropic\n",
    "        with anthropic.messages.stream(\n",
    "            model=model_id,\n",
    "            max_tokens=2048,\n",
    "            system=SYSTEM_PROMPT,\n",
    "            messages=messages\n",
    "        ) as stream:\n",
    "            accumulated = \"\"\n",
    "            for text_chunk in stream.text_stream:\n",
    "                accumulated += text_chunk\n",
    "                yield accumulated\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # GEMINI                                                               #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    elif provider == \"gemini\":\n",
    "\n",
    "        # Build contents list ‚Äî Gemini uses a flat list of role/parts dicts\n",
    "        contents = []\n",
    "\n",
    "        for turn in history:\n",
    "            # Gemini uses \"model\" instead of \"assistant\"\n",
    "            role = \"model\" if turn[\"role\"] == \"assistant\" else \"user\"\n",
    "            contents.append(\n",
    "                types.Content(\n",
    "                    role=role,\n",
    "                    parts=[types.Part.from_text(text=turn[\"content\"])]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Build current user turn ‚Äî include image if present\n",
    "        if b64:\n",
    "            image_bytes = base64.b64decode(b64)\n",
    "            current_parts = [\n",
    "                types.Part.from_bytes(data=image_bytes, mime_type=\"image/jpeg\"),\n",
    "                types.Part.from_text(text=message)\n",
    "            ]\n",
    "        else:\n",
    "            current_parts = [types.Part.from_text(text=message)]\n",
    "\n",
    "        contents.append(types.Content(role=\"user\", parts=current_parts))\n",
    "\n",
    "        # Stream from Gemini\n",
    "        stream = gemini.models.generate_content_stream(\n",
    "            model=model_id,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=SYSTEM_PROMPT,\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        accumulated = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.text:\n",
    "                accumulated += chunk.text\n",
    "                yield accumulated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Processing Utility\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Install required libraries if not already present\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"pypdf\", \"python-docx\", \"-q\"], \n",
    "               capture_output=True)\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "\n",
    "def process_uploaded_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process any uploaded file and return normalized content.\n",
    "\n",
    "    Returns a dict with:\n",
    "      - type    : \"image\" | \"pdf\" | \"docx\" | \"audio\" | \"text\"\n",
    "      - content : extracted text (for non-image types)\n",
    "      - image   : PIL Image object (for image type only)\n",
    "      - name    : original filename\n",
    "    \"\"\"\n",
    "    path = pathlib.Path(file_path)\n",
    "    ext  = path.suffix.lower()\n",
    "    name = path.name\n",
    "\n",
    "    # ‚îÄ‚îÄ IMAGE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if ext in [\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tiff\", \".gif\"]:\n",
    "        img = Image.open(file_path).convert(\"RGB\")\n",
    "        return {\"type\": \"image\", \"image\": img, \"content\": None, \"name\": name}\n",
    "\n",
    "    # ‚îÄ‚îÄ PDF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    elif ext == \".pdf\":\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            pages  = []\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text and text.strip():\n",
    "                    pages.append(f\"[Page {i+1}]\\n{text.strip()}\")\n",
    "            full_text = \"\\n\\n\".join(pages) if pages else \"‚ö†Ô∏è No extractable text found in PDF (may be scanned image).\"\n",
    "            return {\"type\": \"pdf\", \"content\": full_text, \"image\": None, \"name\": name}\n",
    "        except Exception as e:\n",
    "            return {\"type\": \"error\", \"content\": f\"PDF read error: {e}\", \"image\": None, \"name\": name}\n",
    "\n",
    "    # ‚îÄ‚îÄ DOCX ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        try:\n",
    "            doc        = DocxDocument(file_path)\n",
    "            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "            full_text  = \"\\n\\n\".join(paragraphs) if paragraphs else \"‚ö†Ô∏è No text found in document.\"\n",
    "            return {\"type\": \"docx\", \"content\": full_text, \"image\": None, \"name\": name}\n",
    "        except Exception as e:\n",
    "            return {\"type\": \"error\", \"content\": f\"DOCX read error: {e}\", \"image\": None, \"name\": name}\n",
    "\n",
    "    # ‚îÄ‚îÄ AUDIO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    elif ext in [\".mp3\", \".wav\", \".m4a\", \".ogg\", \".flac\", \".webm\"]:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as audio_file:\n",
    "                transcript = openai.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file,\n",
    "                    response_format=\"text\"\n",
    "                )\n",
    "            transcribed = transcript if isinstance(transcript, str) else transcript.text\n",
    "            full_text   = f\"[Audio Transcription ‚Äî {name}]\\n\\n{transcribed}\"\n",
    "            return {\"type\": \"audio\", \"content\": full_text, \"image\": None, \"name\": name}\n",
    "        except Exception as e:\n",
    "            return {\"type\": \"error\", \"content\": f\"Audio transcription error: {e}\", \"image\": None, \"name\": name}\n",
    "\n",
    "    # ‚îÄ‚îÄ PLAIN TEXT / CSV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    elif ext in [\".txt\", \".csv\", \".md\"]:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                full_text = f.read()\n",
    "            return {\"type\": \"text\", \"content\": full_text, \"image\": None, \"name\": name}\n",
    "        except Exception as e:\n",
    "            return {\"type\": \"error\", \"content\": f\"Text read error: {e}\", \"image\": None, \"name\": name}\n",
    "\n",
    "    # ‚îÄ‚îÄ UNSUPPORTED ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    else:\n",
    "        return {\n",
    "            \"type\": \"error\",\n",
    "            \"content\": f\"‚ö†Ô∏è Unsupported file type: {ext}. Supported: images, PDF, DOCX, audio, TXT, CSV\",\n",
    "            \"image\": None,\n",
    "            \"name\": name\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7753491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI ‚Äî for multi-file type support\n",
    "\n",
    "\n",
    "def chat(\n",
    "    message: Dict,\n",
    "    history: List[Dict],\n",
    "    model_key: str\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Updated chat handler supporting all file types.\n",
    "    - Images   ‚Üí sent as base64 to vision models\n",
    "    - PDF/DOCX ‚Üí text extracted, injected into prompt\n",
    "    - Audio    ‚Üí transcribed via Whisper, injected into prompt\n",
    "    - TXT/CSV  ‚Üí read directly, injected into prompt\n",
    "    \"\"\"\n",
    "\n",
    "    user_text  = message.get(\"text\", \"\").strip()\n",
    "    image      = None\n",
    "    extra_text = \"\"   # will hold extracted content from non-image files\n",
    "\n",
    "    # --- Process uploaded file if any ---\n",
    "    files = message.get(\"files\", [])\n",
    "    if files:\n",
    "        file_path = files[0][\"path\"] if isinstance(files[0], dict) else files[0]\n",
    "        result    = process_uploaded_file(file_path)\n",
    "\n",
    "        if result[\"type\"] == \"image\":\n",
    "            # Visual pathway ‚Äî pass PIL image to LLM\n",
    "            image = result[\"image\"]\n",
    "\n",
    "        elif result[\"type\"] == \"error\":\n",
    "            yield result[\"content\"]\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # Text pathway ‚Äî prepend extracted content to the prompt\n",
    "            doc_type_label = {\n",
    "                \"pdf\"  : \"üìÑ PDF Document\",\n",
    "                \"docx\" : \"üìù Word Document\",\n",
    "                \"audio\": \"üéôÔ∏è Audio Transcript\",\n",
    "                \"text\" : \"üìÉ Text File\"\n",
    "            }.get(result[\"type\"], \"Document\")\n",
    "\n",
    "            extra_text = (\n",
    "                f\"The user has uploaded a {doc_type_label} named '{result['name']}'.\\n\"\n",
    "                f\"Here is its content:\\n\\n\"\n",
    "                f\"{'='*60}\\n\"\n",
    "                f\"{result['content']}\\n\"\n",
    "                f\"{'='*60}\\n\\n\"\n",
    "            )\n",
    "\n",
    "    # --- Require at least some input ---\n",
    "    if not user_text and not extra_text and image is None:\n",
    "        yield \"‚ö†Ô∏è Please upload a document and/or type a question.\"\n",
    "        return\n",
    "\n",
    "    # --- Build final prompt ---\n",
    "    if extra_text and not user_text:\n",
    "        # File uploaded with no question ‚Üí auto analyze\n",
    "        final_prompt = extra_text + \"Please analyze this document and provide a structured overview.\"\n",
    "    elif extra_text and user_text:\n",
    "        # File + question together\n",
    "        final_prompt = extra_text + f\"User question: {user_text}\"\n",
    "    else:\n",
    "        # Text only or image only\n",
    "        final_prompt = user_text or \"Please analyze this document and provide a structured overview.\"\n",
    "\n",
    "    # --- Stream response ---\n",
    "    for chunk in stream_response(\n",
    "        message=final_prompt,\n",
    "        history=history,\n",
    "        model_key=model_key,\n",
    "        image=image\n",
    "    ):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# UI Layout                                                            #\n",
    "# ------------------------------------------------------------------ #\n",
    "with gr.Blocks(title=\"Smart Document Analyst\") as demo:\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    #  Smart Document Analyst\n",
    "    **Upload any business document** and ask questions about it.  \n",
    "    Supports:  Images ¬∑  PDF ¬∑  DOCX ¬∑ üéôÔ∏è Audio ¬∑  TXT/CSV  \n",
    "    Multi-turn Q&A ¬∑ Switch models anytime to compare responses.\n",
    "    \"\"\")\n",
    "\n",
    "    model_selector = gr.Dropdown(\n",
    "        choices=list(MODELS.keys()),\n",
    "        value=\"Claude Sonnet (Anthropic)\",\n",
    "        label=\"ü§ñ Select AI Model\",\n",
    "        interactive=True\n",
    "    )\n",
    "\n",
    "    gr.ChatInterface(\n",
    "        fn=chat,\n",
    "    \n",
    "        multimodal=True,\n",
    "        additional_inputs=[model_selector],\n",
    "        chatbot=gr.Chatbot(\n",
    "            label=\"Document Q&A\",\n",
    "            height=520,\n",
    "            placeholder=(\n",
    "                \" Upload a document using the **paperclip icon**, then ask:\\n\\n\"\n",
    "                \"- *What is the total amount due?*\\n\"\n",
    "                \"- *Who are the parties in this contract?*\\n\"\n",
    "                \"- *Summarize the key clauses*\\n\"\n",
    "                \"- *Extract all line items as a table*\\n\"\n",
    "                \"- *Are there any red flags?*\"\n",
    "            )\n",
    "        ),\n",
    "        textbox=gr.MultimodalTextbox(\n",
    "            placeholder=\"Upload a document and/or type your question...\",\n",
    "            file_types=[\n",
    "                \".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\",   \n",
    "                \".pdf\",                                       \n",
    "                \".docx\", \".doc\",                             \n",
    "                \".mp3\", \".wav\", \".m4a\", \".ogg\", \".webm\",    \n",
    "                \".txt\", \".csv\", \".md\"                        \n",
    "            ],\n",
    "            file_count=\"single\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    üí° **Tips:** Switch models mid-conversation to compare analysis.  \n",
    "    Audio files are auto-transcribed before analysis.  \n",
    "    For scanned PDFs (image-based), upload as image file instead.\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch(\n",
    "    theme=gr.themes.Soft(),\n",
    "    debug=False,\n",
    "    share=True,                         \n",
    "    auth=[                               \n",
    "        (\"admin\", \"admin123\"),\n",
    "        (\"analyst\", \"docs2024\"),\n",
    "    ],\n",
    "    auth_message=\"üîê Please log in to access the Smart Document Analyst\",\n",
    "     prevent_thread_lock=True  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
