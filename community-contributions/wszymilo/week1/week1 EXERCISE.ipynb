{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "## Original\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!\n",
    "\n",
    "## My modification\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, implement a system where two models are interacting \n",
    "with each other. One model ask a Python-related question, while the other tries to answer that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, update_display\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants & env. setup\n",
    "\n",
    "MODEL_GPT = 'gpt-4.1-mini'\n",
    "MODEL_LLAMA = 'codellama:latest'\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up models\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "openai = OpenAI()\n",
    "\n",
    "def model_response(model_obj: OpenAI, model_name: str, prompt: dict, streaming: bool = False) -> ChatCompletion:\n",
    "    \"\"\"Generate a response from a model.\"\"\"\n",
    "    response = model_obj.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt['system']},\n",
    "            {\"role\": \"user\", \"content\": prompt['user']}\n",
    "            ],\n",
    "        stream=streaming\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def display_streaming(response) -> None:\n",
    "    \"\"\"Display a streaming response.\"\"\" \n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    data = \"\"\n",
    "    for chunk in response:\n",
    "        data += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(data), display_id=display_handle.display_id)\n",
    "\n",
    "def interact(model_objects: list[OpenAI], model_names: list[str], prompts: dict) -> None:\n",
    "    \"\"\"Model-to-model interaction.\"\"\"\n",
    "    generator, answerer = model_objects\n",
    "    generator_name, answerer_name = model_names\n",
    "    question = model_response(generator, generator_name, prompts['question_generator']).choices[0].message.content\n",
    "    print('Question:')\n",
    "    display(Markdown(question))\n",
    "\n",
    "    print('Answer:')\n",
    "    prompt = prompts['question_solver'].copy()\n",
    "    prompt['user'] = prompt['user'].format(question=question)\n",
    "    answer = model_response(answerer, answerer_name, prompt, streaming=True)\n",
    "    display_streaming(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "questiongen_system_prompt = \"\"\"\n",
    "You are a query generating assistant who create simple question about Python3 language.\n",
    "The intermediate level Python developer should be able to answer your question.\n",
    "\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\n",
    "Example questions:\n",
    "Q: What does the code do: \n",
    "```\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "```\n",
    "\n",
    "Q: What is the purpose of the following code:\n",
    "```\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "questiongen_user_prompt = \"Generate an intermediate level question about Python3 language.\"\n",
    "\n",
    "questionsolver_system_prompt = \"\"\"You are a question solver assistant. Your task is to answer a question about Python3 language given to you by the user.\n",
    "Respond in a concise manner, in a few sentences. Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\n",
    "Example:\n",
    "Q: What is the purpose of the following code:\n",
    "```\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "A: This function greets the user with a message containing the user's name.\"\"\"\n",
    "\n",
    "questionsolver_user_prompt = \"\"\"Provide answer to the following question:\n",
    "\n",
    "# {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompts = {\n",
    "    \"question_generator\": {\n",
    "        \"system\": questiongen_system_prompt,\n",
    "        \"user\": questiongen_user_prompt\n",
    "    },\n",
    "    \"question_solver\": {\n",
    "        \"system\": questionsolver_system_prompt,\n",
    "        \"user\": questionsolver_user_prompt\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "interact([openai, openai], [MODEL_GPT, MODEL_GPT], prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get codellama to answer\n",
    "interact([ollama, ollama], [MODEL_LLAMA, MODEL_LLAMA], prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One model asking another model\n",
    "interact([openai, ollama], [MODEL_GPT, MODEL_LLAMA], prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaec665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the other way around\n",
    "interact([ollama, openai], [MODEL_LLAMA, MODEL_GPT], prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
