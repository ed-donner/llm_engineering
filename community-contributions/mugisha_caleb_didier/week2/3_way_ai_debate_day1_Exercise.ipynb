{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Way AI Debate\n",
    "\n",
    "A structured debate between three LLMs, each with a distinct personality:\n",
    "\n",
    "- **Alex (GPT-4.1-mini)** — Moderator. Introduces the topic, asks probing questions, keeps things on track.\n",
    "- **John (Gemini)** — The Optimist. Sees opportunity everywhere, bullish on the future.\n",
    "- **Peter (Claude)** — The Skeptic. Questions assumptions, plays devil's advocate.\n",
    "\n",
    "Each model receives a single system prompt defining its role, and a user prompt containing the full\n",
    "conversation transcript so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables from .env file\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models & Personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALEX_MODEL = \"gpt-4.1-mini\"\n",
    "JOHN_MODEL = \"google/gemini-2.5-flash-lite\"\n",
    "PETER_MODEL = \"anthropic/claude-3.5-haiku\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompts\n",
    "\n",
    "alex_system = \"\"\"You are Alex, the moderator of a debate.\n",
    "You introduce the topic, ask probing follow-up questions, challenge weak arguments,\n",
    "and keep the discussion focused. You are fair but push both sides to go deeper.\n",
    "Keep your responses concise — 2 to 4 sentences max.\n",
    "You are in a debate with John and Peter.\"\"\"\n",
    "\n",
    "john_system = \"\"\"You are John, a debater who is optimistic and forward-thinking.\n",
    "You see opportunity and potential in new developments. You back your points\n",
    "with practical examples and real-world impact.\n",
    "Keep your responses concise — 3 to 5 sentences max.\n",
    "You are in a debate with Peter. Alex is the moderator.\"\"\"\n",
    "\n",
    "peter_system = \"\"\"You are Peter, a debater who is skeptical and analytical.\n",
    "You question assumptions, point out risks, and demand evidence.\n",
    "You're not negative — you just want to stress-test ideas before buying in.\n",
    "Keep your responses concise — 3 to 5 sentences max.\n",
    "You are in a debate with John. Alex is the moderator.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to debate a different subject\n",
    "\n",
    "topic = \"Why does it matter right now to learn LLM engineering? Is this the right time, or is it too early / too late?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared transcript — single source of truth\n",
    "\n",
    "transcript = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format transcript into a readable string\n",
    "\n",
    "def format_transcript():\n",
    "    if not transcript:\n",
    "        return \"(No conversation yet)\"\n",
    "    lines = []\n",
    "    for entry in transcript:\n",
    "        lines.append(f\"{entry['speaker']}: {entry['text']}\")\n",
    "    return \"\\n\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alex call\n",
    "\n",
    "def call_alex(instruction):\n",
    "    user_prompt = f\"\"\"{instruction}\n",
    "\n",
    "The conversation so far:\n",
    "{format_transcript()} \n",
    "\n",
    "Respond as Alex the moderator.\"\"\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=ALEX_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": alex_system},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    transcript.append({\"speaker\": \"Alex (Moderator)\", \"text\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call John\n",
    "\n",
    "def call_john():\n",
    "    user_prompt = f\"\"\"You are John in a debate moderated by Alex.\n",
    "The conversation so far:\n",
    "{format_transcript()}\n",
    "Respond to what just been said, Stay in character as the optimistic and forward-thinking John.\"\"\"\n",
    "    \n",
    "    response = openrouter_client.chat.completions.create(\n",
    "        model=JOHN_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": john_system},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    transcript.append({\"speaker\": \"John (Optimist)\", \"text\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Peter\n",
    "\n",
    "def call_peter():\n",
    "    user_prompt = f\"\"\"You are Peter in a debate moderated by Alex.\n",
    "\n",
    "The conversation so far:\n",
    "{format_transcript()}\n",
    "\n",
    "Respond to what was just said. Stay in character as the skeptic.\"\"\"\n",
    "\n",
    "    response = openrouter_client.chat.completions.create(\n",
    "        model=PETER_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": peter_system},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    transcript.append({\"speaker\": \"Peter (Skeptic)\", \"text\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(speaker, text):\n",
    "    display(Markdown(f\"### {speaker}\\n\\n{text}\\n\\n---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round 0 — Alex introduces the topic\n",
    "\n",
    "transcript = []  # reset\n",
    "\n",
    "opening = call_alex(f\"Introduce this debate topic to John and Peter: {topic}\")\n",
    "show(\"Alex (Moderator)\", opening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounds 1 through 5\n",
    "\n",
    "for i in range(1, 6):\n",
    "    display(Markdown(f\"## Round {i}\"))\n",
    "\n",
    "    # John responds\n",
    "    john_reply = call_john()\n",
    "    show(\"John (Optimist)\", john_reply)\n",
    "\n",
    "    # Peter responds\n",
    "    peter_reply = call_peter()\n",
    "    show(\"Peter (Skeptic)\", peter_reply)\n",
    "\n",
    "    # Alex moderates\n",
    "    alex_reply = call_alex(\"Ask a follow-up question or challenge one of them. Push the debate deeper.\")\n",
    "    show(\"Alex (Moderator)\", alex_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alex gives closing remarks\n",
    "\n",
    "closing = call_alex(\"Summarize the key points from both sides and give your closing remarks to end the debate.\")\n",
    "show(\"Alex (Moderator)\", closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_transcript())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
