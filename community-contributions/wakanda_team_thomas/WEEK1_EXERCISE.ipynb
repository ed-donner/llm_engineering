{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fkdv3q9lll",
   "metadata": {},
   "source": [
    "# Week 1 Exercise —By Mougang Thomas Gasmyr\n",
    "\n",
    "## Approach\n",
    "A technical Q&A tool that takes a question and gets explanations from two LLMs:\n",
    "- **GPT-4o-mini** (via OpenRouter) — streamed response with live display updates\n",
    "- **Llama 3.2** (via local Ollama) — streamed response with live display updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file\")\n",
    "\n",
    "openrouter_base_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# OpenAI client\n",
    "openai_client = OpenAI(\n",
    "    base_url=openrouter_base_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Ollama client (OpenAI-compatible endpoint)\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# System prompt for technical explanations\n",
    "system_prompt = \"You are a helpful technical tutor. Explain concepts clearly and concisely, using examples where appropriate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eu7nrsf3xis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable functions for querying LLMs with streaming display\n",
    "\n",
    "def ask_gpt(question, client, model, system_prompt):\n",
    "    \"\"\"Stream a response from GPT and display live.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response\n",
    "\n",
    "def ask_ollama(question, client, model, system_prompt):\n",
    "    \"\"\"Stream a response from Ollama and display live.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask your technical question (defaults to the example if left blank)\n",
    "\n",
    "question = \"How to evaluate the performance of production AI agents? What are the best practices and tools for monitoring and improving their performance over time?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## GPT-4o-mini Response\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Evaluating the performance of production AI agents involves a systematic approach that encompasses various metrics, best practices, and tools. Here’s a concise guide:\n",
       "\n",
       "### Key Concepts in Performance Evaluation\n",
       "\n",
       "1. **Performance Metrics**:\n",
       "   - **Accuracy**: The percentage of correct predictions made by the AI agent.\n",
       "   - **Precision**: The ratio of true positives to the sum of true positives and false positives (useful in classification tasks).\n",
       "   - **Recall (Sensitivity)**: The ratio of true positives to the sum of true positives and false negatives (important in imbalanced classes).\n",
       "   - **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
       "   - **AUC-ROC**: The area under the receiver operating characteristic curve, useful for binary classification.\n",
       "   - **Response Time**: For real-time AI applications, how quickly the agent responds to queries.\n",
       "   - **Throughput**: The number of queries processed per unit of time.\n",
       "\n",
       "2. **Error Analysis**: Understanding the types of errors the AI agent is making (false positives vs. false negatives) can help identify areas for improvement.\n",
       "\n",
       "3. **User Feedback**: Collect feedback from end-users to assess the practical effectiveness of the AI agent in real-world scenarios.\n",
       "\n",
       "### Best Practices for Performance Evaluation\n",
       "\n",
       "1. **Continuous Monitoring**: Implement monitoring systems that track key performance metrics in real-time to quickly identify any performance degradation.\n",
       "\n",
       "2. **Regular Retraining**: Periodically retrain the AI model with new data to ensure it stays relevant and adapts to changes over time.\n",
       "\n",
       "3. **A/B Testing**: Deploy different versions of models or changes to model configurations concurrently to compare performance in a controlled manner.\n",
       "\n",
       "4. **Version Control**: Maintain clear versioning for models to keep track of changes, allowing for easier rollback if a new version underperforms.\n",
       "\n",
       "5. **Data Quality Assessment**: Ensure that the training and evaluation datasets are of high quality and representative of the real-world context in which the AI agent operates.\n",
       "\n",
       "6. **Bias and Fairness Checks**: Regularly assess the model for bias to ensure outputs are fair and equitable across all user demographics.\n",
       "\n",
       "### Tools for Monitoring and Improving Performance\n",
       "\n",
       "1. **Monitoring Platforms**:\n",
       "   - **Prometheus** and **Grafana**: For real-time performance monitoring and visualization of metrics.\n",
       "   - **Elastic Stack (ELK)**: For logging and monitoring AI application performance.\n",
       "   - **Neptune.ai**, **Weights & Biases**, or **Comet.ml**: Tools specifically designed for tracking experiments and model performance over time.\n",
       "\n",
       "2. **Model Evaluation Frameworks**:\n",
       "   - **MLflow**: For tracking experiments, managing models, and monitoring performance.\n",
       "   - **TensorBoard**: Useful for visualizing the training process and metrics of TensorFlow models.\n",
       "\n",
       "3. **Data Drift Detection**: Tools like **Evidently** or **NannyML** can be employed to detect changes in data distributions over time, guiding when a model may need retraining.\n",
       "\n",
       "4. **Feedback Systems**: Incorporate user satisfaction surveys or feedback collection forms to gather actionable insights.\n",
       "\n",
       "By utilizing these concepts, practices, and tools, you can establish a robust system for evaluating and continuously improving the performance of AI agents in production environments. This ensures these systems remain effective, reliable, and relevant as conditions change."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "print(\"## GPT-4o-mini Response\\n\")\n",
    "try:\n",
    "    gpt_response = ask_gpt(question, openai_client, MODEL_GPT, system_prompt)\n",
    "except Exception as e:\n",
    "    print(f\"GPT error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Llama 3.2 Response\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Evaluating the performance of production AI agents is crucial to ensure they are meeting their expected objectives and delivering consistent results. Here's a comprehensive overview of best practices, tools, and techniques for monitoring and improving performance:\n",
       "\n",
       "**Evaluation Metrics**\n",
       "\n",
       "1. **Accuracy**: Measure the proportion of correct predictions or classifications.\n",
       "2. **Precision**: Evaluate the ratio of true positives to total predicted positive instances.\n",
       "3. **Recall**: Calculate the proportion of actual positive instances that were correctly identified.\n",
       "4. **F1-score**: Compute the harmonic mean of precision and recall.\n",
       "5. **Time-to-Resolution**: Measure response time for critical tasks or decisions.\n",
       "\n",
       "**Real-Time Monitoring**\n",
       "\n",
       "1. **Instrumentation**: Integrate monitoring tools with your AI system to collect performance data as it runs.\n",
       "2. **Logging**: Establish a logging framework to capture relevant metrics, errors, and warnings.\n",
       "3. **Monitoring Dashboard**: Create visualizations to help developers quickly grasp the current health of their AI systems.\n",
       "\n",
       "**Tools for Monitoring**\n",
       "\n",
       "1. **TensorBoard**: A popular tool for monitoring TensorFlow models in real-time.\n",
       "2. **HyperLance**: An all-in-one platform for logging, visualization, and hyperparameter tuning.\n",
       "3. **Prometheus**: A scalable time-series database for storing performance metrics.\n",
       "4. **Grafana**: A powerful dashboarding tool for creating visualizations and alerts.\n",
       "\n",
       "**Improvement Techniques**\n",
       "\n",
       "1. **Exploratory Data Analysis (EDA)**: Investigate data distribution and patterns to identify trends or biases.\n",
       "2. **Model Evaluation**: Regularly assess model performance on a separate test dataset using metrics such as accuracy, precision, recall, and F1-score.\n",
       "3. **Hyperparameter Tuning**: Use techniques like grid search, random search, or Bayesian optimization to optimize hyperparameters for production AI agents.\n",
       "4. **Data Augmentation**: Enhance data quality by creating augmented versions of existing data (e.g., noisy instances) to improve model robustness.\n",
       "\n",
       "**Automated Testing and Validation**\n",
       "\n",
       "1. **Randomized Controlled Trials (RCTs)**: Conduct experiments to test the efficacy of new models or techniques in production environments.\n",
       "2. **Automated Testing Frameworks**: Use frameworks like Pytest, Behave, or Nose to write automated tests for AI systems.\n",
       "3. **Validation Frameworks**: Establish a suite of pre-built verification routines to check against expected behaviors.\n",
       "\n",
       "**Model Serving and Model Updates**\n",
       "\n",
       "1. **Model Serving Platforms**: Utilize platforms like TensorFlow Serving, AWS SageMaker, or Azure Machine Learning to host and deploy AI models in production environments.\n",
       "2. **Automated Model Updating**: Design pipelines that automate feature updates, data refreshes, or new model deployment for maintaining accurate and updated performance.\n",
       "\n",
       "By implementing these best practices, tools, and techniques, you can ensure the reliable and high-performing operation of your production AI agents. Regular monitoring, exploration, and improvement will keep your models adaptable to changing environments and continuous learning from user feedback and operational context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer, with streaming\n",
    "\n",
    "print(\"## Llama 3.2 Response\\n\")\n",
    "try:\n",
    "    llama_response = ask_ollama(question, ollama_client, MODEL_LLAMA, system_prompt)\n",
    "except Exception as e:\n",
    "    print(f\"Ollama error: {e}. Is Ollama running?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
