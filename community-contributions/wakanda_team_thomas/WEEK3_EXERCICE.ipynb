{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Week 3 Exercise: Multi-Language Text Analyzer\n",
    "## By Mougang Thomas Gasmyr from the Wakanda Team\n",
    "\n",
    "A multi-language text analysis tool that combines **HuggingFace pipelines**, a **locally quantized LLM**,\n",
    "**streaming generation**, and **tokenizer exploration** into a unified Gradio application.\n",
    "\n",
    "### Features\n",
    "- **Language Detection** -- automatically identifies the input language using a HuggingFace pipeline\n",
    "- **Translation** -- translates text to a target language using a local quantized Llama 3.2 3B\n",
    "- **Summarization** -- generates a concise summary with streaming output via TextIteratorStreamer\n",
    "- **Sentiment Analysis** -- extracts sentiment using a multilingual BERT pipeline\n",
    "- **Tokenizer Explorer** -- compare how different tokenizers handle multilingual text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook is designed to run on **Google Colab with a T4 GPU** (free tier).\n",
    "\n",
    "### Required Colab Secrets\n",
    "- `HF_TOKEN` -- Your HuggingFace access token (needed for gated Llama model)\n",
    "\n",
    "### GPU Check\n",
    "Make sure you have selected **Runtime > Change runtime type > T4 GPU** before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import threading\n",
    "import gradio as gr\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextIteratorStreamer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "LANG_DETECT_MODEL = \"papluca/xlm-roberta-base-language-detection\"\n",
    "SENTIMENT_MODEL = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "TARGET_LANGUAGES = [\"English\", \"French\", \"Spanish\", \"German\"]\n",
    "\n",
    "TOKENIZER_MODELS = {\n",
    "    \"Llama 3.2\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"BERT Multilingual\": \"bert-base-multilingual-cased\",\n",
    "    \"GPT-2\": \"gpt2\",\n",
    "}\n",
    "\n",
    "SAMPLE_TEXTS = {\n",
    "    \"English\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"French\": \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "    \"Spanish\": \"El r\\u00e1pido zorro marr\\u00f3n salta sobre el perro perezoso.\",\n",
    "    \"German\": \"Der schnelle braune Fuchs springt \\u00fcber den faulen Hund.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU available: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Model loading and inference will fail.\")\n",
    "    print(\"On Colab: Runtime > Change runtime type > T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lightweight HuggingFace pipelines for language detection and sentiment\n",
    "\n",
    "print(\"Loading language detection pipeline...\")\n",
    "lang_detector = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=LANG_DETECT_MODEL,\n",
    "    device=0,\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(\"Loading sentiment analysis pipeline...\")\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=SENTIMENT_MODEL,\n",
    "    device=0,\n",
    ")\n",
    "\n",
    "print(\"Pipelines loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 3.2 3B Instruct with 4-bit quantization\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading quantized Llama 3.2 3B model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "print(\"Llama model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core analysis functions\n",
    "\n",
    "LANG_NAME_TO_CODE = {\n",
    "    \"english\": \"en\", \"french\": \"fr\", \"spanish\": \"es\", \"german\": \"de\",\n",
    "}\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of the input text using HF pipeline.\"\"\"\n",
    "    results = lang_detector(text[:512])\n",
    "    if isinstance(results[0], list):\n",
    "        results = results[0]\n",
    "    top_result = results[0]\n",
    "    lang_code = top_result[\"label\"]\n",
    "    confidence = top_result[\"score\"]\n",
    "\n",
    "    predictions_text = \"\\n\".join(\n",
    "        f\"  - **{r['label']}**: {r['score']:.1%}\" for r in results\n",
    "    )\n",
    "    return lang_code, confidence, predictions_text\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment using multilingual BERT pipeline.\"\"\"\n",
    "    result = sentiment_analyzer(text[:512])[0]\n",
    "    label = result[\"label\"]\n",
    "    score = result[\"score\"]\n",
    "\n",
    "    star_count = int(label.split()[0])\n",
    "    sentiment_map = {\n",
    "        1: \"Very Negative\",\n",
    "        2: \"Negative\",\n",
    "        3: \"Neutral\",\n",
    "        4: \"Positive\",\n",
    "        5: \"Very Positive\",\n",
    "    }\n",
    "    sentiment_label = sentiment_map.get(star_count, label)\n",
    "    return sentiment_label, star_count, score\n",
    "\n",
    "\n",
    "def build_llama_messages(system_content, user_content):\n",
    "    \"\"\"Build chat messages and tokenize using apply_chat_template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "    )\n",
    "    if hasattr(input_ids, \"input_ids\"):\n",
    "        input_ids = input_ids[\"input_ids\"]\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def translate_text(text, source_lang, target_lang):\n",
    "    \"\"\"Translate text using the local Llama model (non-streaming).\"\"\"\n",
    "    system_message = (\n",
    "        f\"You are a professional translator. Translate the following text from \"\n",
    "        f\"{source_lang} to {target_lang}. Output ONLY the translation, nothing else.\"\n",
    "    )\n",
    "    input_ids, attention_mask = build_llama_messages(system_message, text)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=1024,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][input_ids.shape[-1]:], skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def summarize_text_streaming(text, source_lang):\n",
    "    \"\"\"Summarize text using Llama with streaming via TextIteratorStreamer.\"\"\"\n",
    "    system_message = (\n",
    "        \"You are a skilled summarizer. Provide a concise summary of the following text. \"\n",
    "        \"The summary should capture the key points in 2-4 sentences. \"\n",
    "        \"Write the summary in English.\"\n",
    "    )\n",
    "    user_content = (\n",
    "        f\"The following text is in {source_lang}. Please summarize it in English:\\n\\n{text}\"\n",
    "    )\n",
    "    input_ids, attention_mask = build_llama_messages(system_message, user_content)\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    thread = threading.Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"streamer\": streamer,\n",
    "        },\n",
    "    )\n",
    "    thread.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        yield new_text\n",
    "\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis orchestrator -- connects to Gradio UI\n",
    "\n",
    "def analyze_text(text, target_language):\n",
    "    \"\"\"Full analysis pipeline with progressive Gradio output.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        yield \"\", \"\", \"\", \"\", \"\"\n",
    "        return\n",
    "\n",
    "    # Step 1: Language Detection (HF pipeline)\n",
    "    lang_code, confidence, predictions_text = detect_language(text)\n",
    "    lang_display = (\n",
    "        f\"**Detected:** {lang_code} ({confidence:.1%})\\n\\n\"\n",
    "        f\"**Top predictions:**\\n{predictions_text}\"\n",
    "    )\n",
    "    yield lang_display, \"Translating...\", \"\", \"\", \"\"\n",
    "\n",
    "    # Step 2: Translation (Llama)\n",
    "    target_code = LANG_NAME_TO_CODE.get(target_language.lower(), \"en\")\n",
    "    needs_translation = lang_code.lower() != target_code\n",
    "    if needs_translation:\n",
    "        translated = translate_text(text, lang_code, target_language)\n",
    "    else:\n",
    "        translated = text\n",
    "    yield lang_display, translated, \"Generating summary...\", \"\", \"\"\n",
    "\n",
    "    # Step 3: Summarization with streaming (Llama + TextIteratorStreamer)\n",
    "    summary_so_far = \"\"\n",
    "    for chunk in summarize_text_streaming(text, lang_code):\n",
    "        summary_so_far += chunk\n",
    "        yield lang_display, translated, summary_so_far, \"\", \"\"\n",
    "\n",
    "    # Step 4: Sentiment Analysis (HF pipeline)\n",
    "    sentiment_label, star_count, sentiment_score = analyze_sentiment(text)\n",
    "    stars = \"\\u2605\" * star_count + \"\\u2606\" * (5 - star_count)\n",
    "    sentiment_display = (\n",
    "        f\"**Sentiment:** {sentiment_label}\\n\\n\"\n",
    "        f\"**Rating:** {stars} ({star_count}/5)\\n\\n\"\n",
    "        f\"**Confidence:** {sentiment_score:.1%}\"\n",
    "    )\n",
    "    yield lang_display, translated, summary_so_far, sentiment_display, \"Analysis complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Explorer -- compare how different tokenizers handle multilingual text\n",
    "\n",
    "def explore_tokenizers(text):\n",
    "    \"\"\"Compare how different tokenizers process the same text.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"Please enter some text to analyze.\"\n",
    "\n",
    "    results = [f\"## Tokenizer Comparison\\n> {text}\\n\"]\n",
    "\n",
    "    for name, model_id in TOKENIZER_MODELS.items():\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(model_id)\n",
    "            token_ids = tok.encode(text)\n",
    "            tokens = tok.convert_ids_to_tokens(token_ids)\n",
    "            decoded_tokens = [tok.decode([tid]) for tid in token_ids]\n",
    "\n",
    "            results.append(f\"### {name} (`{model_id}`)\")\n",
    "            results.append(f\"- **Vocabulary size:** {tok.vocab_size:,}\")\n",
    "            results.append(f\"- **Token count:** {len(token_ids)}\")\n",
    "            results.append(f\"- **Tokens:** `{tokens}`\")\n",
    "            results.append(f\"- **Decoded:** {' | '.join(decoded_tokens)}\")\n",
    "            results.append(\"\")\n",
    "        except Exception as e:\n",
    "            results.append(f\"### {name}\\n- Error loading tokenizer: {e}\\n\")\n",
    "\n",
    "    # Efficiency comparison table\n",
    "    results.append(\"### Efficiency Comparison\")\n",
    "    results.append(\"| Tokenizer | Token Count | Vocab Size |\")\n",
    "    results.append(\"|-----------|------------|------------|\")\n",
    "    for name, model_id in TOKENIZER_MODELS.items():\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(model_id)\n",
    "            count = len(tok.encode(text))\n",
    "            results.append(f\"| {name} | {count} | {tok.vocab_size:,} |\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Blocks UI\n",
    "\n",
    "with gr.Blocks(title=\"Multi-Language Text Analyzer\") as analyzer_app:\n",
    "    gr.Markdown(\n",
    "        \"# Multi-Language Text Analyzer\\n\"\n",
    "        \"Analyze text in any language: detect the language, translate it, \"\n",
    "        \"summarize it with streaming, and extract sentiment.\\n\\n\"\n",
    "        \"*Powered by HuggingFace pipelines + locally quantized Llama 3.2 3B*\"\n",
    "    )\n",
    "\n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Text Analyzer\n",
    "        with gr.TabItem(\"Text Analyzer\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Input Text (any language)\",\n",
    "                        placeholder=\"Paste or type text in any language...\",\n",
    "                        lines=8,\n",
    "                    )\n",
    "                    target_lang = gr.Dropdown(\n",
    "                        choices=TARGET_LANGUAGES,\n",
    "                        value=\"English\",\n",
    "                        label=\"Translate to\",\n",
    "                    )\n",
    "                    analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
    "\n",
    "                with gr.Column(scale=3):\n",
    "                    lang_output = gr.Markdown(label=\"Language Detection\")\n",
    "                    translation_output = gr.Markdown(label=\"Translation\")\n",
    "                    summary_output = gr.Markdown(label=\"Summary (Streaming)\")\n",
    "                    sentiment_output = gr.Markdown(label=\"Sentiment Analysis\")\n",
    "                    status_output = gr.Markdown(label=\"Status\")\n",
    "\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"Bonjour le monde! Le Cameroon c'est un pays africain avec une population de 24 millions d'habitants.\", \"English\"],\n",
    "                    [\"Die K\\u00fcnstliche Intelligenz ver\\u00e4ndert unsere Welt in einer Geschwindigkeit, die wir uns kaum vorstellen k\\u00f6nnen. Sie beeinflusst Medizin, Bildung und Wirtschaft.\", \"English\"],\n",
    "                    [\"La inteligencia artificial est\\u00e1 transformando todos los aspectos de nuestra vida cotidiana, desde la salud hasta la educaci\\u00f3n.\", \"French\"],\n",
    "                ],\n",
    "                inputs=[text_input, target_lang],\n",
    "            )\n",
    "\n",
    "            analyze_btn.click(\n",
    "                fn=analyze_text,\n",
    "                inputs=[text_input, target_lang],\n",
    "                outputs=[lang_output, translation_output, summary_output, sentiment_output, status_output],\n",
    "            )\n",
    "\n",
    "        # Tab 2: Tokenizer Explorer\n",
    "        with gr.TabItem(\"Tokenizer Explorer\"):\n",
    "            gr.Markdown(\n",
    "                \"## Compare Tokenizers Across Languages\\n\\n\"\n",
    "                \"Notice how token counts vary across languages!\"\n",
    "            )\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    tok_input = gr.Textbox(\n",
    "                        label=\"Text to tokenize\",\n",
    "                        placeholder=\"Enter text in any language...\",\n",
    "                        lines=3,\n",
    "                    )\n",
    "                    tok_btn = gr.Button(\"Compare Tokenizers\", variant=\"primary\")\n",
    "\n",
    "                    gr.Examples(\n",
    "                        examples=[[v] for v in SAMPLE_TEXTS.values()],\n",
    "                        inputs=[tok_input],\n",
    "                    )\n",
    "\n",
    "                with gr.Column():\n",
    "                    tok_output = gr.Markdown(label=\"Tokenizer Comparison\")\n",
    "\n",
    "            tok_btn.click(\n",
    "                fn=explore_tokenizers,\n",
    "                inputs=[tok_input],\n",
    "                outputs=[tok_output],\n",
    "            )\n",
    "\n",
    "print(\"UI built. Ready to launch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_app.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}