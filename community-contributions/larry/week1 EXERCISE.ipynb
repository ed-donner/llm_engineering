{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up client\n",
    "\n",
    "openai=OpenAI(\n",
    "  api_key=\"llama3.2\",\n",
    "  base_url=\"http://localhost:11434/v1/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd4567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(sys_prompt, usr_prompt):\n",
    "  model_url =  'http://localhost:11434/v1/'\n",
    "  msg = [{'role':'system', 'content':sys_prompt},{'role':'user', 'content':usr_prompt}]\n",
    "  response = openai.chat.completions.create(model=MODEL_LLAMA, messages=msg)\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "336e0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a helpful assistant who helps me understand software engineering concepts.\\n'\n",
    "usr_prompt = 'Using a simple anology please explain the concept of Transformer Architecture?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you're trying to communicate with someone who speaks your native language, but they're standing on the other side of the room, and you can only see each other's profiles. To have a conversation, you need to align the perspective from which each person is seeing the world.\n",
       "\n",
       "The Transformer architecture is like this alignment process. It helps the model (our \"someone speaking our native language\") pay attention to all the information about both inputs simultaneously, instead of doing it sequentially. Here's how:\n",
       "\n",
       "**Sequence Alignment**\n",
       "\n",
       "In traditional sequence-to-sequence models, we usually do two passes: one from input to output and another from output to input. The first pass is called \"encode\" or \"forward pass,\" where the model processes each word in order. Then, the second pass is called \"decode\" or \"backward pass,\" where the model generates words according to the previous results.\n",
       "\n",
       "However, this sequential approach can lead to several issues:\n",
       "\n",
       "1.  **Inattention Penalties**: When one part of the input doesn't get enough attention from another part.\n",
       "2.  **Context Overlap**: Words that are close together in time but not in meaning overlap each other's context.\n",
       "\n",
       "**Transformer Architecture**\n",
       "\n",
       "The Transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al., addresses these issues using self-attention mechanisms.\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "1.  **Self-Attention Mechanism**: Instead of processing words sequentially, we calculate attention weights for each word based on all other words.\n",
       "2.  **Parallel Processing**: These attention weights can be computed in parallel, so instead of a sequential forward pass, these attention matrices and query vectors are calculated simultaneously.\n",
       "3.  **Context Representation**: It is created that represents the \"alignment\" perspective by attending to every part of each input.\n",
       "\n",
       "The Transformer architecture consists of an encoding layer and decoding layer:\n",
       "\n",
       "*   Encording layer: takes in two input streams (query and key) and produces an output (attention matrix).\n",
       "*   Decoding layer: generates an output based on the attention matrix, query vector, and previous output.\n",
       "\n",
       "The self-attention mechanism replaces traditional recurrent neural networks and convolutional layers with linear layers to calculate the context representation of all input. \n",
       "\n",
       "This leads to improvements over traditional sequence models:\n",
       "\n",
       "1.  **Parallelization**: much more parallel computing\n",
       "2.  More accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resp = ask_model(sys_prompt, usr_prompt)\n",
    "display(Markdown(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a573174-779b-4d50-8792-fa0889b37211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
