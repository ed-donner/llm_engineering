{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up client\n",
    "\n",
    "openai=OpenAI(\n",
    "  api_key=\"llama3.2\",\n",
    "  base_url=\"http://localhost:11434/v1/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd4567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(sys_prompt, usr_prompt):\n",
    "  model_url =  'http://localhost:11434/v1/'\n",
    "  msg = [{'role':'system', 'content':sys_prompt},{'role':'user', 'content':usr_prompt}]\n",
    "  response = openai.chat.completions.create(model=MODEL_LLAMA, messages=msg)\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336e0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"You are a helpful assistant who helps me understand software engineering concepts.\\n\"\n",
    "usr_prompt = \"Using a simple analogy, please explain the concept of Transformer architecture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'd be happy to help you understand the Transformer architecture using an analogy.\n",
       "\n",
       "Imagine you're trying to translate a message from English to Spanish. You need to find the correct words in both languages that match each other, while also understanding the context and nuances of the conversation.\n",
       "\n",
       "**Traditional Architecture: RNNs (Recurrent Neural Networks)**\n",
       "In traditional NLP tasks like machine translation, we used to use RNNs. These are essentially \"memory-based\" models that rely on previous inputs to generate the next output. It's like using a notebook where you write down each word as you translate it, and then try to find the correct equivalent word in Spanish based on what you've written already.\n",
       "\n",
       "For example, if we're translating the sentence \"Hello, how are you?\", the RNN model would look something like this:\n",
       "\n",
       "... (write down \"Hello\" in English notebook)\n",
       "... (write down \"hello\" in Spanish notebook) -> find similarity with previous output\n",
       "... (write down \"how to say that\" in English notebook, maybe write down a word or phrase)\n",
       "... (write down the translation of the phrase in the Spanish notebook)\n",
       "\n",
       "**Transformer Architecture**\n",
       "Now, imagine using a completely new approach. Instead of relying on previous inputs, we focus on the entire sequence of words at once and use self-attention mechanisms to find relationships between them.\n",
       "\n",
       "In the Transformer architecture, we focus on three key aspects for each word in the input sentence:\n",
       "\n",
       "1. **Self-attention**: We look at all other words simultaneously to see how similar they are to our current word in terms of meaning.\n",
       "2. **Query**: Each word acts as a \"query\" pointing to its relevant context.\n",
       "3. **Score**: A weighted sum that captures strengths and weaknesses between words.\n",
       "\n",
       "We multiply these together (think of it like a matrix product) to generate an output representation that combines all the contextual information from each other word. In our English-to-Spanish translation example, this would look something like:\n",
       "\n",
       "... \"Hello\" -> self-attention relationships with surrounding words\n",
       "... calculate query vectors combining individual context tokens\n",
       "... multiply and aggregate results (weighted scores)\n",
       "\n",
       "The key insights from Transformer are:\n",
       "\n",
       "* **Parallel processing**: We process the entire input sequence simultaneously, which leads to significant speedup in training times.\n",
       "* **Self-attention mechanism**: This innovative attention layer efficiently captures long-range relationships between words by reducing the need for recurrent neural networks' sequential dependencies.\n",
       "\n",
       "This analogy is certainly oversimplified, but it should give you an idea of how Transformers differ from traditional RNN-based architectures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resp = ask_model(sys_prompt, usr_prompt)\n",
    "display(Markdown(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a573174-779b-4d50-8792-fa0889b37211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
