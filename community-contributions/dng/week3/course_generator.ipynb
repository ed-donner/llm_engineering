{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "COURSE VIDEO GENERATOR - Google Colab Application\n",
        "================================================================================\n",
        "\n",
        "This application converts books/text into educational videos with AI narration\n",
        "and provides an interactive chat interface for Q&A.\n",
        "\n",
        "Models Used (Optimized for T4 GPU - ~16GB VRAM):\n",
        "- LLM: Qwen/Qwen2.5-1.5B-Instruct (4-bit)\n",
        "- Image Gen: stabilityai/sdxl-turbo\n",
        "- TTS: parler-tts/parler_tts_mini_v0.1\n",
        "- ASR: openai/whisper-tiny\n",
        "\n",
        "\n",
        "Author: Denis Ngugi Gathondu"
      ],
      "metadata": {
        "id": "pnvw09zyV25_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#          INSTALL DEPENDENCIES       \n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "c9mltvLjWx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell first to install all required packages\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q accelerate bitsandbytes\n",
        "!pip install -q diffusers accelerate\n",
        "!pip install -q parler-tts\n",
        "!pip install -q gradio>=4.0.0\n",
        "!pip install -q moviepy pillow\n",
        "!pip install -q PyPDF2 python-docx\n",
        "!pip install -q sentencepiece protobuf\n",
        "!pip install -q scipy soundfile"
      ],
      "metadata": {
        "id": "5pEBMK6fWAC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#        IMPORTS AND CONFIGURATION\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "3wp9vZOIXj3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum, auto\n",
        "from typing import Optional, List, Dict, Any, Tuple, Generator\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from google.colab import userdata\n",
        "\n",
        "# Hugging face\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Transformers and ML\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSpeechSeq2Seq,\n",
        "    AutoProcessor,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "# Diffusion for image generation\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "\n",
        "# TTS\n",
        "from parler_tts import ParlerTTSForConditionalGeneration\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "\n",
        "# Gradio for UI\n",
        "import gradio as gr\n",
        "\n",
        "# Video generation\n",
        "from moviepy.editor import (\n",
        "    ImageClip,\n",
        "    AudioFileClip,\n",
        "    concatenate_videoclips,\n",
        ")\n",
        "\n",
        "# Document processing\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# login to hugging face\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "JPwHnYaoXvTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CONFIGURATION\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "C7NFcHDOYByw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AppConfig:\n",
        "    \"\"\"Application configuration with model names and settings.\"\"\"\n",
        "\n",
        "    # Model names (all non-gated, small models for T4 GPU)\n",
        "    LLM_MODEL: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    IMAGE_MODEL: str = \"stabilityai/sdxl-turbo\"\n",
        "    TTS_MODEL: str = \"parler-tts/parler_tts_mini_v0.1\"\n",
        "    ASR_MODEL: str = \"openai/whisper-tiny\"\n",
        "\n",
        "    # Generation settings\n",
        "    MAX_TOKENS: int = 2048\n",
        "    TEMPERATURE: float = 0.7\n",
        "    TOP_P: float = 0.9\n",
        "\n",
        "    # Video settings\n",
        "    VIDEO_WIDTH: int = 512\n",
        "    VIDEO_HEIGHT: int = 512\n",
        "    FPS: int = 24\n",
        "    AUDIO_SAMPLE_RATE: int = 24000\n",
        "\n",
        "    # Section settings\n",
        "    MAX_SECTION_LENGTH: int = 1000  # Characters per section\n",
        "    MIN_SECTION_LENGTH: int = 200  # Minimum for a valid section"
      ],
      "metadata": {
        "id": "N1MNJ7i3YImY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL MANAGER\n",
        "# ================================================================"
      ],
      "metadata": {
        "id": "m1ivWPIXYlZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelState(Enum):\n",
        "    \"\"\"Possible states for a model.\"\"\"\n",
        "\n",
        "    UNLOADED = auto()\n",
        "    LOADED = auto()\n",
        "    LOADING = auto()\n",
        "    ERROR = auto()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelContainer:\n",
        "    \"\"\"Container for a loaded model and its components.\"\"\"\n",
        "\n",
        "    name: str\n",
        "    model: Optional[Any] = None\n",
        "    tokenizer: Optional[Any] = None\n",
        "    processor: Optional[Any] = None\n",
        "    state: ModelState = ModelState.UNLOADED\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "class ModelManager:\n",
        "    \"\"\"\n",
        "    Memory-efficient model manager for T4 GPU.\n",
        "\n",
        "    Manages loading/unloading of models to stay within VRAM limits.\n",
        "    Provides phase-based loading for video generation vs chat.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: AppConfig):\n",
        "        self.config = config\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Initialize model containers\n",
        "        self._models: Dict[str, ModelContainer] = {\n",
        "            \"llm\": ModelContainer(name=self.config.LLM_MODEL),\n",
        "            \"image\": ModelContainer(name=self.config.IMAGE_MODEL),\n",
        "            \"tts\": ModelContainer(name=self.config.TTS_MODEL),\n",
        "            \"asr\": ModelContainer(name=self.config.ASR_MODEL),\n",
        "        }\n",
        "\n",
        "        # Quantization config for LLM\n",
        "        self._quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        print(f\"ðŸ–¥ï¸ Device: {self.device}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"ðŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            print(\n",
        "                f\"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "            )\n",
        "\n",
        "    def _clear_memory(self):\n",
        "        \"\"\"Force garbage collection and GPU cache clearing.\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    def get_memory_usage(self) -> Dict[str, float]:\n",
        "        \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {\"allocated\": 0, \"reserved\": 0, \"free\": 0}\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        return {\n",
        "            \"allocated\": round(allocated, 2),\n",
        "            \"reserved\": round(reserved, 2),\n",
        "            \"free\": round(total - reserved, 2),\n",
        "        }\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # LLM Methods\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def load_llm(self) -> bool:\n",
        "        \"\"\"Load the LLM with 4-bit quantization.\"\"\"\n",
        "        container = self._models[\"llm\"]\n",
        "\n",
        "        if container.state == ModelState.LOADED:\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            container.state = ModelState.LOADING\n",
        "            print(f\"ðŸ“¥ Loading LLM ({container.name})...\")\n",
        "\n",
        "            container.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                container.name,\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "\n",
        "            container.model = AutoModelForCausalLM.from_pretrained(\n",
        "                container.name,\n",
        "                quantization_config=self._quant_config,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "\n",
        "            container.state = ModelState.LOADED\n",
        "            print(f\"âœ… LLM loaded! Memory: {self.get_memory_usage()}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            container.state = ModelState.ERROR\n",
        "            container.error_message = str(e)\n",
        "            print(f\"âŒ Failed to load LLM: {e}\")\n",
        "            return False\n",
        "\n",
        "    def unload_llm(self):\n",
        "        \"\"\"Unload the LLM and free memory.\"\"\"\n",
        "        container = self._models[\"llm\"]\n",
        "        if container.model is not None:\n",
        "            del container.model\n",
        "            container.model = None\n",
        "        if container.tokenizer is not None:\n",
        "            del container.tokenizer\n",
        "            container.tokenizer = None\n",
        "        container.state = ModelState.UNLOADED\n",
        "        self._clear_memory()\n",
        "        print(\"ðŸ“¤ LLM unloaded\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Image Generation Methods\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def load_image_model(self) -> bool:\n",
        "        \"\"\"Load the image generation model.\"\"\"\n",
        "        container = self._models[\"image\"]\n",
        "\n",
        "        if container.state == ModelState.LOADED:\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            container.state = ModelState.LOADING\n",
        "            print(f\"ðŸ“¥ Loading Image Generator ({container.name})...\")\n",
        "\n",
        "            # Load the pipeline. This should include all sub-components like tokenizers.\n",
        "            container.model = StableDiffusionXLPipeline.from_pretrained(\n",
        "                container.name,\n",
        "                torch_dtype=torch.float16,\n",
        "                use_safetensors=True,\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Enable memory-efficient attention\n",
        "            container.model.enable_attention_slicing()\n",
        "\n",
        "            container.state = ModelState.LOADED\n",
        "            print(f\"âœ… Image model loaded! Memory: {self.get_memory_usage()}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            container.state = ModelState.ERROR\n",
        "            container.error_message = str(e)\n",
        "            print(f\"âŒ Failed to load image model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def unload_image_model(self):\n",
        "        \"\"\"Unload the image model and free memory.\"\"\"\n",
        "        container = self._models[\"image\"]\n",
        "        if container.model is not None:\n",
        "            del container.model\n",
        "            container.model = None\n",
        "        container.state = ModelState.UNLOADED\n",
        "        self._clear_memory()\n",
        "        print(\"ðŸ“¤ Image model unloaded\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TTS Methods\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def load_tts(self) -> bool:\n",
        "        \"\"\"Load the TTS model.\"\"\"\n",
        "        container = self._models[\"tts\"]\n",
        "\n",
        "        if container.state == ModelState.LOADED:\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            container.state = ModelState.LOADING\n",
        "            print(f\"ðŸ“¥ Loading TTS ({container.name})...\")\n",
        "\n",
        "            container.model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
        "                container.name\n",
        "            ).to(self.device)\n",
        "\n",
        "            container.tokenizer = AutoTokenizer.from_pretrained(container.name)\n",
        "\n",
        "            container.state = ModelState.LOADED\n",
        "            print(f\"âœ… TTS loaded! Memory: {self.get_memory_usage()}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            container.state = ModelState.ERROR\n",
        "            container.error_message = str(e)\n",
        "            print(f\"âŒ Failed to load TTS: {e}\")\n",
        "            return False\n",
        "\n",
        "    def unload_tts(self):\n",
        "        \"\"\"Unload the TTS model and free memory.\"\"\"\n",
        "        container = self._models[\"tts\"]\n",
        "        if container.model is not None:\n",
        "            del container.model\n",
        "            container.model = None\n",
        "        if container.tokenizer is not None:\n",
        "            del container.tokenizer\n",
        "            container.tokenizer = None\n",
        "        container.state = ModelState.UNLOADED\n",
        "        self._clear_memory()\n",
        "        print(\"ðŸ“¤ TTS unloaded\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # ASR Methods\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def load_asr(self) -> bool:\n",
        "        \"\"\"Load the ASR model.\"\"\"\n",
        "        container = self._models[\"asr\"]\n",
        "\n",
        "        if container.state == ModelState.LOADED:\n",
        "            return True\n",
        "\n",
        "        try:\n",
        "            container.state = ModelState.LOADING\n",
        "            print(f\"ðŸ“¥ Loading ASR ({container.name})...\")\n",
        "\n",
        "            container.processor = AutoProcessor.from_pretrained(container.name)\n",
        "            container.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "                container.name,\n",
        "                torch_dtype=torch.float16,\n",
        "                use_safetensors=True,\n",
        "            ).to(self.device)\n",
        "\n",
        "\n",
        "            container.state = ModelState.LOADED\n",
        "            print(f\"âœ… ASR loaded! Memory: {self.get_memory_usage()}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            container.state = ModelState.ERROR\n",
        "            container.error_message = str(e)\n",
        "            print(f\"âŒ Failed to load ASR: {e}\")\n",
        "            return False\n",
        "\n",
        "    def unload_asr(self):\n",
        "        \"\"\"Unload the ASR model and free memory.\"\"\"\n",
        "        container = self._models[\"asr\"]\n",
        "        if container.model is not None:\n",
        "            del container.model\n",
        "            container.model = None\n",
        "        container.state = ModelState.UNLOADED\n",
        "        self._clear_memory()\n",
        "        print(\"ðŸ“¤ ASR unloaded\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase-Based Loading\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def load_for_video_generation(self) -> bool:\n",
        "        \"\"\"\n",
        "        Load ALL models needed for video generation phase.\n",
        "        Models: LLM, Image Gen, TTS\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"ðŸŽ¬ Loading Video Generation Phase Models\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        self.unload_asr()  # Not needed for video generation\n",
        "\n",
        "        success = True\n",
        "        success &= self.load_llm()\n",
        "        success &= self.load_image_model()\n",
        "        success &= self.load_tts()\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nâœ… All video generation models loaded!\")\n",
        "            print(f\"ðŸ“Š Final memory: {self.get_memory_usage()}\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Some models failed to load\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def load_for_chat(self) -> bool:\n",
        "        \"\"\"\n",
        "        Load models needed for chat phase.\n",
        "        Models: LLM (keep), ASR (add)\n",
        "        Unloads: Image Gen, TTS\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"ðŸ’¬ Loading Chat Phase Models\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Unload heavy models not needed for chat\n",
        "        self.unload_image_model()\n",
        "        self.unload_tts()\n",
        "\n",
        "        success = True\n",
        "        success &= self.load_llm()  # Keep loaded\n",
        "        success &= self.load_asr()  # Add for voice input\n",
        "\n",
        "        if success:\n",
        "            print(\"\\nâœ… All chat models loaded!\")\n",
        "            print(f\"ðŸ“Š Final memory: {self.get_memory_usage()}\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Some models failed to load\")\n",
        "\n",
        "        return success\n",
        "\n",
        "    def unload_all(self):\n",
        "        \"\"\"Unload all models.\"\"\"\n",
        "        self.unload_llm()\n",
        "        self.unload_image_model()\n",
        "        self.unload_tts()\n",
        "        self.unload_asr()\n",
        "        print(\"ðŸ§¹ All models unloaded!\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Generation Methods\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    def generate_text(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        max_new_tokens: Optional[int] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Generate text using the LLM.\"\"\"\n",
        "        container = self._models[\"llm\"]\n",
        "\n",
        "        if container.state != ModelState.LOADED:\n",
        "            raise RuntimeError(\"LLM not loaded\")\n",
        "\n",
        "        messages = []\n",
        "        if system_prompt:\n",
        "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        print(\"  >> LLM: Applying chat template...\")\n",
        "        text = container.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        inputs = container.tokenizer(text, return_tensors=\"pt\").to(\n",
        "            container.model.device\n",
        "        )\n",
        "\n",
        "        generate_kwargs = {\n",
        "            \"max_new_tokens\": max_new_tokens if max_new_tokens is not None else self.config.MAX_TOKENS,\n",
        "            \"temperature\": self.config.TEMPERATURE,\n",
        "            \"top_p\": self.config.TOP_P,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": container.tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        print(\"  >> LLM: Generating response...\")\n",
        "        with torch.no_grad():\n",
        "            outputs = container.model.generate(\n",
        "                **inputs,\n",
        "                **generate_kwargs\n",
        "            )\n",
        "        print(\"  >> LLM: Decoding response...\")\n",
        "        response = container.tokenizer.decode(\n",
        "            outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
        "        )\n",
        "        print(\"  >> LLM: Response generated.\")\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    def generate_image(self, prompt: str, negative_prompt: str = \"\") -> Image.Image:\n",
        "        \"\"\"Generate an image from a text prompt.\"\"\"\n",
        "        container = self._models[\"image\"]\n",
        "\n",
        "        if container.state != ModelState.LOADED:\n",
        "            raise RuntimeError(\"Image model not loaded\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image = container.model(\n",
        "                prompt=prompt,\n",
        "                num_inference_steps=1,\n",
        "                guidance_scale=0.0,\n",
        "                height=self.config.VIDEO_HEIGHT,\n",
        "                width=self.config.VIDEO_WIDTH,\n",
        "            ).images[0]\n",
        "\n",
        "        return image\n",
        "\n",
        "    def generate_audio(\n",
        "        self, text: str, description: str = None\n",
        "    ) -> Tuple[np.ndarray, int]:\n",
        "        \"\"\"Generate audio from text using TTS.\"\"\"\n",
        "        container = self._models[\"tts\"]\n",
        "\n",
        "        if container.state != ModelState.LOADED:\n",
        "            raise RuntimeError(\"TTS model not loaded\")\n",
        "\n",
        "        if description is None:\n",
        "            description = \"A clear, professional instructor explaining educational content in a calm, engaging voice.\"\n",
        "\n",
        "        inputs = container.tokenizer(description, return_tensors=\"pt\").to(\n",
        "            container.model.device\n",
        "        )\n",
        "        prompt = container.tokenizer(text, return_tensors=\"pt\").to(\n",
        "            container.model.device\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            audio = container.model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                prompt_input_ids=prompt.input_ids,\n",
        "                prompt_attention_mask=prompt.attention_mask,\n",
        "            )\n",
        "\n",
        "        audio_arr = audio.cpu().numpy().squeeze()\n",
        "        return audio_arr, self.config.AUDIO_SAMPLE_RATE\n",
        "\n",
        "    def transcribe_audio(self, audio_path: str) -> str:\n",
        "        \"\"\"Transcribe.\"\"\"\n",
        "        container = self._models[\"asr\"]\n",
        "\n",
        "        if container.state != ModelState.LOADED:\n",
        "            raise RuntimeError(\"ASR model not loaded\")\n",
        "\n",
        "        audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        inputs = container.processor(\n",
        "            audio,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = container.model.generate(**inputs)\n",
        "\n",
        "        transcription = container.processor.batch_decode(\n",
        "            generated_ids, skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Properties for easy access\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    @property\n",
        "    def llm_loaded(self) -> bool:\n",
        "        return self._models[\"llm\"].state == ModelState.LOADED\n",
        "\n",
        "    @property\n",
        "    def image_loaded(self) -> bool:\n",
        "        return self._models[\"image\"].state == ModelState.LOADED\n",
        "\n",
        "    @property\n",
        "    def tts_loaded(self) -> bool:\n",
        "        return self._models[\"tts\"].state == ModelState.LOADED\n",
        "\n",
        "    @property\n",
        "    def asr_loaded(self) -> bool:\n",
        "        return self._models[\"asr\"].state == ModelState.LOADED"
      ],
      "metadata": {
        "id": "M1lzkfcrYudW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTENT PROCESSOR\n",
        "# ================================================================"
      ],
      "metadata": {
        "id": "DvK7_7KhY-j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentProcessor:\n",
        "    \"\"\"\n",
        "    Processes uploaded documents and prepares them for video generation.\n",
        "    Handles PDF, DOCX, TXT files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_manager: ModelManager):\n",
        "        self.manager = model_manager\n",
        "\n",
        "    def extract_text(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from various file formats.\"\"\"\n",
        "        path = Path(file_path)\n",
        "        suffix = path.suffix.lower()\n",
        "\n",
        "        if suffix == \".pdf\":\n",
        "            return self._extract_from_pdf(file_path)\n",
        "        elif suffix == \".docx\":\n",
        "            return self._extract_from_docx(file_path)\n",
        "        elif suffix == \".txt\":\n",
        "            return self._extract_from_txt(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {suffix}\")\n",
        "\n",
        "    def _extract_from_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file.\"\"\"\n",
        "        text = []\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                text.append(page.extract_text())\n",
        "        return \"\\n\\n\".join(text)\n",
        "\n",
        "    def _extract_from_docx(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from DOCX file.\"\"\"\n",
        "        doc = Document(file_path)\n",
        "        return \"\\n\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "\n",
        "    def _extract_from_txt(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from TXT file.\"\"\"\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    def create_sections(\n",
        "        self, text: str, max_length: int = 1000\n",
        "    ) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Break text into logical sections for video generation.\n",
        "\n",
        "        Returns list of dicts with 'title' and 'content' keys.\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert educator who creates clear, engaging course content.\n",
        "Given text from educational material, your task is to:\n",
        "1. Break it into logical sections (each section should be a coherent topic)\n",
        "2. Create a clear, concise title for each section\n",
        "3. Ensure each section is educational and self-contained\n",
        "\n",
        "Format your response as JSON array:\n",
        "[\n",
        "  {\"title\": \"Section Title\", \"content\": \"Section content here...\"},\n",
        "  ...\n",
        "]\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"Please break the following educational content into logical sections.\n",
        "Each section should be roughly {max_length} characters or less and cover a single coherent topic.\n",
        "\n",
        "Content to process:\n",
        "{text[:8000]}  # Limit to avoid token limits\n",
        "\n",
        "Return ONLY the JSON array, no other text.\"\"\"\n",
        "\n",
        "        response = self.manager.generate_text(prompt, system_prompt)\n",
        "\n",
        "        # Parse JSON from response\n",
        "        try:\n",
        "            # Find JSON array in response\n",
        "            json_match = re.search(r\"\\[.*\\]\", response, re.DOTALL)\n",
        "            if json_match:\n",
        "                sections = json.loads(json_match.group())\n",
        "                return sections\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        # Fallback: simple paragraph-based splitting\n",
        "        return self._fallback_sections(text, max_length)\n",
        "\n",
        "    def _fallback_sections(self, text: str, max_length: int) -> List[Dict[str, str]]:\n",
        "        \"\"\"Fallback section creation if LLM parsing fails.\"\"\"\n",
        "        paragraphs = text.split(\"\\n\\n\")\n",
        "        sections = []\n",
        "\n",
        "        current_content = \"\"\n",
        "        section_num = 1\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if not para:\n",
        "                continue\n",
        "\n",
        "            if len(current_content) + len(para) > max_length:\n",
        "                if current_content:\n",
        "                    sections.append(\n",
        "                        {\n",
        "                            \"title\": f\"Section {section_num}\",\n",
        "                            \"content\": current_content.strip(),\n",
        "                        }\n",
        "                    )\n",
        "                    section_num += 1\n",
        "                    current_content = para\n",
        "                else:\n",
        "                    sections.append(\n",
        "                        {\"title\": f\"Section {section_num}\", \"content\": para}\n",
        "                    )\n",
        "                    section_num += 1\n",
        "            else:\n",
        "                current_content += \"\\n\\n\" + para\n",
        "\n",
        "        if current_content.strip():\n",
        "            sections.append(\n",
        "                {\"title\": f\"Section {section_num}\", \"content\": current_content.strip()}\n",
        "            )\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def generate_script(self, section: Dict[str, str]) -> str:\n",
        "        \"\"\"Generate a narration script from a section.\"\"\"\n",
        "        system_prompt = \"\"\"You are an expert educator and scriptwriter.\n",
        "Create clear, engaging narration scripts for educational videos.\n",
        "The script should:\n",
        "- Be conversational and easy to understand\n",
        "- Include clear explanations of concepts\n",
        "- Use analogies and examples where helpful\n",
        "- Be suitable for text-to-speech narration\n",
        "- Be concise but comprehensive\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"Create a narration script for the following section.\n",
        "The script should be suitable for an educational video.\n",
        "\n",
        "Section Title: {section[\"title\"]}\n",
        "Section Content: {section[\"content\"]}\n",
        "\n",
        "Write a clear, engaging narration script:\"\"\"\n",
        "\n",
        "        return self.manager.generate_text(prompt, system_prompt)\n",
        "\n",
        "    def generate_image_prompt(self, section: Dict[str, str], script: str) -> str:\n",
        "        \"\"\"Generate an image prompt for a section.\"\"\"\n",
        "        system_prompt = \"\"\"You create image prompts for educational videos.\n",
        "Given a section of educational content, create a detailed image prompt that:\n",
        "- Visually represents the key concepts\n",
        "- Is suitable for AI image generation\n",
        "- Is educational and appropriate\n",
        "- Uses clear, descriptive language\n",
        "- Is concise and focused, preferably under 60 words to avoid truncation.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"Create an image prompt for this educational section.\n",
        "The image should help illustrate the concepts being discussed.\n",
        "\n",
        "Section Title: {section[\"title\"]}\n",
        "Script: {script[:500]} # Limit script to avoid overwhelming the prompt\n",
        "\n",
        "Create a detailed, concise image prompt (max 60 words):\"\"\"\n",
        "\n",
        "        return self.manager.generate_text(prompt, system_prompt, max_new_tokens=60)"
      ],
      "metadata": {
        "id": "R8JkmtA0ZGE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO GENERATOR\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "SFnn1fq3ZNXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoGenerator:\n",
        "    \"\"\"\n",
        "    Generates educational videos from processed content.\n",
        "    Coordinates audio and visual generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_manager: ModelManager, content_processor: ContentProcessor):\n",
        "        self.manager = model_manager\n",
        "        self.processor = content_processor\n",
        "\n",
        "    def generate_section_video(\n",
        "        self, section: Dict[str, str], script: str, output_dir: str, section_num: int\n",
        "    ) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Generate video components for a single section.\n",
        "\n",
        "        Returns dict with paths to generated files.\n",
        "        \"\"\"\n",
        "        print(f\"\\nðŸŽ¬ Generating video for section {section_num}: {section['title']}\")\n",
        "\n",
        "        paths = {}\n",
        "\n",
        "        # Generate image prompt\n",
        "        print(\"  ðŸ“ Generating image prompt...\")\n",
        "        image_prompt = self.processor.generate_image_prompt(section, script)\n",
        "\n",
        "        # Clean up image prompt\n",
        "        image_prompt = image_prompt.strip('\"').strip(\"' \").strip()\n",
        "\n",
        "        # Generate image\n",
        "        print(\"  ðŸŽ¨ Generating image...\")\n",
        "        image = self.manager.generate_image(\n",
        "            prompt=image_prompt,\n",
        "            negative_prompt=\"blurry, low quality, distorted, ugly, text, watermark\",\n",
        "        )\n",
        "\n",
        "        image_path = os.path.join(output_dir, f\"section_{section_num}_image.png\")\n",
        "        image.save(image_path)\n",
        "        paths[\"image\"] = image_path\n",
        "\n",
        "        # Generate audio\n",
        "        print(\"  ðŸ”Š Generating audio...\")\n",
        "        audio_data, sample_rate = self.manager.generate_audio(script)\n",
        "\n",
        "        audio_path = os.path.join(output_dir, f\"section_{section_num}_audio.wav\")\n",
        "        sf.write(audio_path, audio_data, sample_rate)\n",
        "        paths[\"audio\"] = audio_path\n",
        "\n",
        "        # Calculate audio duration\n",
        "        audio_duration = len(audio_data) / sample_rate\n",
        "        paths[\"duration\"] = audio_duration\n",
        "\n",
        "        print(f\"  âœ… Section {section_num} complete! Duration: {audio_duration:.1f}s\")\n",
        "\n",
        "        return paths\n",
        "\n",
        "    def compile_video(\n",
        "        self, section_files: List[Dict[str, str]], output_path: str, fps: int = 24\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Compile all sections into a final video.\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸŽ¥ Compiling final video...\")\n",
        "\n",
        "        clips = []\n",
        "\n",
        "        for i, files in enumerate(section_files, 1):\n",
        "            print(f\"  ðŸ“Œ Processing section {i}...\")\n",
        "\n",
        "            # Create image clip with audio duration\n",
        "            img_clip = ImageClip(files[\"image\"]).set_duration(files[\"duration\"])\n",
        "\n",
        "            # Add audio\n",
        "            audio_clip = AudioFileClip(files[\"audio\"])\n",
        "            video_clip = img_clip.set_audio(audio_clip)\n",
        "\n",
        "            clips.append(video_clip)\n",
        "\n",
        "        # Concatenate all clips\n",
        "        print(\"  ðŸ”— Concatenating clips...\")\n",
        "        final_video = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "        # Write output\n",
        "        print(f\"  ðŸ’¾ Saving video to {output_path}...\")\n",
        "        final_video.write_videofile(\n",
        "            output_path,\n",
        "            fps=fps,\n",
        "            codec=\"libx264\",\n",
        "            audio_codec=\"aac\",\n",
        "            temp_audiofile=\"temp-audio.m4a\",\n",
        "            remove_temp=True,\n",
        "            verbose=False,\n",
        "            logger=None,\n",
        "        )\n",
        "\n",
        "        # Clean up\n",
        "        final_video.close()\n",
        "        for clip in clips:\n",
        "            clip.close()\n",
        "\n",
        "        print(f\"  âœ… Video saved: {output_path}\")\n",
        "\n",
        "        return output_path"
      ],
      "metadata": {
        "id": "TmMPBNz1ZUuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHAT MANAGER\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "yIy2_9lKZdnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatManager:\n",
        "    \"\"\"\n",
        "    Manages the interactive chat interface.\n",
        "    Maintains conversation context with the course material.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_manager: ModelManager):\n",
        "        self.manager = model_manager\n",
        "        self.conversation_history: List[Dict[str, str]] = []\n",
        "        self.course_context: str = \"\"\n",
        "        self.sections: List[Dict[str, str]] = []\n",
        "\n",
        "    def set_context(self, text: str, sections: List[Dict[str, str]]):\n",
        "        \"\"\"Set the course material context for the chat.\"\"\"\n",
        "        self.course_context = text[:10000]  # Limit context size\n",
        "        self.sections = sections\n",
        "        self.conversation_history = []  # Reset history for new material\n",
        "\n",
        "    def ask(\n",
        "        self, question: str, use_voice: bool = False, audio_path: Optional[str] = None\n",
        "    ) -> Generator[str, None, None]:\n",
        "        \"\"\"\n",
        "        Ask a question about the course material.\n",
        "\n",
        "        Yields response chunks for streaming.\n",
        "        \"\"\"\n",
        "        # Transcribe audio if voice input\n",
        "        if use_voice and audio_path:\n",
        "            print(\"ðŸŽ¤ Transcribing voice input...\")\n",
        "            question = self.manager.transcribe_audio(audio_path)\n",
        "            print(f\"  ðŸ“ Transcribed: {question}\")\n",
        "\n",
        "        # Build system prompt with context\n",
        "        system_prompt = f\"\"\"You are a helpful educational assistant. You have access to course material\n",
        "and should help the user understand it better.\n",
        "\n",
        "Course Material Summary:\n",
        "{self.course_context[:3000]}\n",
        "\n",
        "Your role is to:\n",
        "1. Answer questions about the material clearly and thoroughly\n",
        "2. Provide additional explanations and examples when helpful\n",
        "3. Relate answers back to the course content when relevant\n",
        "4. Be encouraging and supportive in your teaching style\"\"\"\n",
        "\n",
        "        # Add conversation history\n",
        "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        messages.extend(self.conversation_history)\n",
        "        messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "        # Store user question\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "        # Generate response\n",
        "        response = self.manager.generate_text(question, system_prompt)\n",
        "\n",
        "        # Store assistant response\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # Yield response (could be modified for actual streaming)\n",
        "        yield response\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history.\"\"\"\n",
        "        self.conversation_history = []\n",
        "        print(\"ðŸ§¹ Conversation history cleared\")\n"
      ],
      "metadata": {
        "id": "WqrookKIZihp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN APPLICATION\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "BHj0HhEcZrpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CourseVideoApp:\n",
        "    \"\"\"\n",
        "    Main application class that ties everything together.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"ðŸš€ Initializing Course Video Generator...\")\n",
        "\n",
        "        self.config = AppConfig()\n",
        "        self.manager = ModelManager(self.config)\n",
        "        self.processor = ContentProcessor(self.manager)\n",
        "        self.video_gen = VideoGenerator(self.manager, self.processor)\n",
        "        self.chat = ChatManager(self.manager)\n",
        "\n",
        "        self.current_text: str = \"\"\n",
        "        self.current_sections: List[Dict[str, str]] = []\n",
        "        self.current_video_path: Optional[str] = None\n",
        "\n",
        "        print(\"âœ… Application initialized!\")\n",
        "\n",
        "    def process_document(self, file_path: str) -> Tuple[str, str]:\n",
        "        \"\"\"Process uploaded document and return summary.\"\"\"\n",
        "        print(f\"\\nðŸ“„ Processing document: {file_path}\")\n",
        "\n",
        "        # Ensure LLM is loaded for summary generation\n",
        "        if not self.manager.llm_loaded:\n",
        "            self.manager.load_llm()\n",
        "\n",
        "        # Extract text\n",
        "        text = self.processor.extract_text(file_path)\n",
        "        self.current_text = text\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.manager.generate_text(\n",
        "            f\"Summarize the following educational content in 2-3 sentences:\\n\\n{text[:2000]}\"\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Document processed! Length: {len(text)} characters\")\n",
        "\n",
        "        return f\"Document loaded successfully!\\n\\nSummary: {summary}\", text[:1000]\n",
        "\n",
        "    def create_sections(self, progress=gr.Progress()) -> Tuple[str, str]:\n",
        "        \"\"\"Create sections from loaded document.\"\"\"\n",
        "        if not self.current_text:\n",
        "            return \"Please upload a document first!\", \"\"\n",
        "\n",
        "        progress(0.1, desc=\"Creating sections...\")\n",
        "\n",
        "        # Create sections\n",
        "        self.current_sections = self.processor.create_sections(self.current_text)\n",
        "\n",
        "        progress(0.5, desc=\"Generating section summaries...\")\n",
        "\n",
        "        # Format for display\n",
        "        sections_text = \"ðŸ“š Created Sections:\\n\\n\"\n",
        "        for i, section in enumerate(self.current_sections, 1):\n",
        "            sections_text += f\"**Section {i}: {section['title']}**\\n\"\n",
        "            sections_text += f\"{section['content'][:200]}...\\n\\n\"\n",
        "\n",
        "        progress(1.0, desc=\"Done!\")\n",
        "\n",
        "        return f\"Created {len(self.current_sections)} sections!\", sections_text\n",
        "\n",
        "    def generate_video(self, progress=gr.Progress()) -> str:\n",
        "        \"\"\"Generate complete video from sections.\"\"\"\n",
        "        if not self.current_sections:\n",
        "            return \"Please create sections first!\"\n",
        "\n",
        "        # Load video generation models\n",
        "        progress(0.0, desc=\"Loading models...\")\n",
        "        self.manager.load_for_video_generation()\n",
        "\n",
        "        # Create temp directory for output\n",
        "        output_dir = tempfile.mkdtemp()\n",
        "        section_files = []\n",
        "\n",
        "        try:\n",
        "            total_sections = len(self.current_sections)\n",
        "\n",
        "            for i, section in enumerate(self.current_sections):\n",
        "                progress(\n",
        "                    (i + 0.5) / total_sections,\n",
        "                    desc=f\"Processing section {i + 1}/{total_sections}...\",\n",
        "                )\n",
        "\n",
        "                # Generate script\n",
        "                print(f\"\\nðŸ“ Generating script for section {i + 1}...\")\n",
        "                script = self.processor.generate_script(section)\n",
        "\n",
        "                # Generate video components\n",
        "                progress(\n",
        "                    (i + 0.8) / total_sections,\n",
        "                    desc=f\"Generating media for section {i + 1}/{total_sections}...\",\n",
        "                )\n",
        "\n",
        "                files = self.video_gen.generate_section_video(\n",
        "                    section,\n",
        "                    script,\n",
        "                    output_dir,\n",
        "                    i + 1,\n",
        "                )\n",
        "                section_files.append(files)\n",
        "\n",
        "            # Compile final video\n",
        "            progress(0.95, desc=\"Compiling final video...\")\n",
        "\n",
        "            self.current_video_path = os.path.join(output_dir, \"course_video.mp4\")\n",
        "            final_video_path = self.video_gen.compile_video(section_files, self.current_video_path)\n",
        "\n",
        "            progress(1.0, desc=\"Complete!\")\n",
        "\n",
        "            return final_video_path\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating video: {str(e)}\"\n",
        "\n",
        "    def switch_to_chat(self) -> str:\n",
        "        \"\"\"Switch to chat mode (unload heavy models).\"\"\"\n",
        "        self.manager.load_for_chat\n",
        "\n",
        "        # Set context for chat\n",
        "        if self.current_text and self.current_sections:\n",
        "            self.chat.set_context(self.current_text, self.current_sections)\n",
        "\n",
        "        return \"âœ… Switched to chat mode! You can now ask questions about the material.\"\n",
        "\n",
        "    def chat_response(self, message: str, history: List) -> str:\n",
        "        \"\"\"Generate chat response.\"\"\"\n",
        "        if not self.manager.llm_loaded:\n",
        "            return \"Please switch to chat mode first!\"\n",
        "\n",
        "        response = \"\"\n",
        "        for chunk in self.chat.ask(message):\n",
        "            response = chunk\n",
        "\n",
        "        return response\n",
        "\n",
        "    def voice_chat_response(self, audio_path: str, history: List) -> str:\n",
        "        \"\"\"Generate chat response from voice input.\"\"\"\n",
        "        if not self.manager.asr_loaded:\n",
        "            return \"Please switch to chat mode first!\"\n",
        "\n",
        "        response = \"\"\n",
        "        for chunk in self.chat.ask(\"\", use_voice=True, audio_path=audio_path):\n",
        "            response = chunk\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "h0uW3gQUZxnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRADIO UI\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "okuAcDfyZ7wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ui():\n",
        "    \"\"\"Create the Gradio UI.\"\"\"\n",
        "\n",
        "    app = CourseVideoApp()\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"Course Video Generator\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .header {text-align: center; margin-bottom: 20px;}\n",
        "        .status {padding: 10px; border-radius: 5px; margin: 10px 0;}\n",
        "        \"\"\",\n",
        "    ) as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # ðŸŽ“ Course Video Generator\n",
        "            Transform your educational materials into engaging video courses with AI narration.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Tabs():\n",
        "            # =================================================================\n",
        "            # TAB 1: Video Generator\n",
        "            # =================================================================\n",
        "            with gr.TabItem(\"ðŸŽ¬ Video Generator\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"### ðŸ“„ Upload Material\")\n",
        "\n",
        "                        file_input = gr.File(\n",
        "                            label=\"Upload Document\",\n",
        "                            file_types=[\".pdf\", \".docx\", \".txt\"],\n",
        "                        )\n",
        "\n",
        "                        upload_btn = gr.Button(\"ðŸ“¤ Upload & Process\", variant=\"primary\")\n",
        "\n",
        "                        upload_status = gr.Textbox(\n",
        "                            label=\"Status\", lines=2, interactive=False\n",
        "                        )\n",
        "\n",
        "                        gr.Markdown(\"### ðŸ“š Sections\")\n",
        "                        sections_output = gr.Textbox(\n",
        "                            label=\"Generated Sections\", lines=10, interactive=False\n",
        "                        )\n",
        "\n",
        "                        create_sections_btn = gr.Button(\"ðŸ“– Create Sections\")\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"### ðŸŽ¥ Video Generation\")\n",
        "\n",
        "                        generate_btn = gr.Button(\n",
        "                            \"ðŸŽ¬ Generate Video\", variant=\"primary\", size=\"lg\"\n",
        "                        )\n",
        "\n",
        "                        video_output = gr.Video(label=\"Generated Video\", height=400)\n",
        "\n",
        "                        gr.Markdown(\n",
        "                            \"\"\"\n",
        "                            ### âš™ï¸ How it works\n",
        "                            1. Upload a PDF, DOCX, or TXT file\n",
        "                            2. Click \"Create Sections\" to break content into parts\n",
        "                            3. Click \"Generate Video\" to create the course video\n",
        "                            4. Switch to Chat tab to ask questions\n",
        "                            \"\"\"\n",
        "                        )\n",
        "\n",
        "                # Event handlers\n",
        "                upload_btn.click(\n",
        "                    fn=app.process_document,\n",
        "                    inputs=[file_input],\n",
        "                    outputs=[upload_status, sections_output],\n",
        "                )\n",
        "\n",
        "                create_sections_btn.click(\n",
        "                    fn=app.create_sections, outputs=[upload_status, sections_output]\n",
        "                )\n",
        "\n",
        "                generate_btn.click(fn=app.generate_video, outputs=[video_output])\n",
        "\n",
        "            # =================================================================\n",
        "            # TAB 2: Interactive Chat\n",
        "            # =================================================================\n",
        "            with gr.TabItem(\"ðŸ’¬ Interactive Chat\"):\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ### Ask Questions About Your Course Material\n",
        "                    After generating your video, switch to chat mode to ask questions!\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "                switch_mode_btn = gr.Button(\n",
        "                    \"ðŸ”„ Switch to Chat Mode\", variant=\"secondary\"\n",
        "                )\n",
        "                mode_status = gr.Textbox(label=\"Mode Status\", interactive=False)\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        chatbot = gr.Chatbot(\n",
        "                            label=\"Course Assistant\", height=500, show_copy_button=True\n",
        "                        )\n",
        "\n",
        "                        with gr.Row():\n",
        "                            msg_input = gr.Textbox(\n",
        "                                label=\"Your Question\",\n",
        "                                placeholder=\"Ask a question about the course material...\",\n",
        "                                scale=4,\n",
        "                            )\n",
        "                            submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "                    with gr.Column(scale=1):\n",
        "                        gr.Markdown(\"### ðŸŽ¤ Voice Input\")\n",
        "\n",
        "                        audio_input = gr.Audio(\n",
        "                            sources=[\"microphone\"],\n",
        "                            type=\"filepath\",\n",
        "                            label=\"Record Question\",\n",
        "                        )\n",
        "\n",
        "                        voice_btn = gr.Button(\"ðŸŽ¤ Ask by Voice\")\n",
        "\n",
        "                        gr.Markdown(\"### ðŸ› ï¸ Options\")\n",
        "                        clear_btn = gr.Button(\"ðŸ§¹ Clear History\")\n",
        "\n",
        "                # Event handlers\n",
        "                def user_message(message, history):\n",
        "                    return \"\", history + [[message, None]]\n",
        "\n",
        "                def bot_response(history):\n",
        "                    if len(history) > 0:\n",
        "                        message = history[-1][0]\n",
        "                        response = app.chat_response(message, history[:-1])\n",
        "                        history[-1][1] = response\n",
        "                    return history\n",
        "\n",
        "                switch_mode_btn.click(fn=app.switch_to_chat, outputs=[mode_status])\n",
        "\n",
        "                msg_input.submit(\n",
        "                    fn=user_message,\n",
        "                    inputs=[msg_input, chatbot],\n",
        "                    outputs=[msg_input, chatbot],\n",
        "                ).then(fn=bot_response, inputs=[chatbot], outputs=[chatbot])\n",
        "\n",
        "                submit_btn.click(\n",
        "                    fn=user_message,\n",
        "                    inputs=[msg_input, chatbot],\n",
        "                    outputs=[msg_input, chatbot],\n",
        "                ).then(fn=bot_response, inputs=[chatbot], outputs=[chatbot])\n",
        "\n",
        "                def voice_message(audio, history):\n",
        "                    if audio:\n",
        "                        response = app.voice_chat_response(audio, history)\n",
        "                        return history + [[\"ðŸŽ¤ (voice question)\", response]]\n",
        "                    return history\n",
        "\n",
        "                voice_btn.click(\n",
        "                    fn=voice_message, inputs=[audio_input, chatbot], outputs=[chatbot]\n",
        "                )\n",
        "\n",
        "                clear_btn.click(\n",
        "                    fn=lambda: app.chat.clear_history() or [], outputs=[chatbot]\n",
        "                )\n",
        "\n",
        "            # =================================================================\n",
        "            # TAB 3: System Info\n",
        "            # =================================================================\n",
        "            with gr.TabItem(\"â„¹ï¸ System Info\"):\n",
        "                gr.Markdown(\"### ðŸ“Š System Information\")\n",
        "\n",
        "                def get_system_info():\n",
        "                    info = []\n",
        "                    info.append(\n",
        "                        f\"**Device:** {'CUDA' if torch.cuda.is_available() else 'CPU'}\"\n",
        "                    )\n",
        "                    if torch.cuda.is_available():\n",
        "                        info.append(f\"**GPU:** {torch.cuda.get_device_name(0)}\")\n",
        "                        mem = app.manager.get_memory_usage()\n",
        "                        info.append(f\"**VRAM Used:** {mem['allocated']} GB\")\n",
        "                        info.append(f\"**VRAM Free:** {mem['free']} GB\")\n",
        "\n",
        "                    info.append(\"\\n### Model Status\")\n",
        "                    info.append(\n",
        "                        f\"- LLM: {'âœ… Loaded' if app.manager.llm_loaded else 'âŒ Not loaded'}\"\n",
        "                    )\n",
        "                    info.append(\n",
        "                        f\"- Image Gen: {'âœ… Loaded' if app.manager.image_loaded else 'âŒ Not loaded'}\"\n",
        "                    )\n",
        "                    info.append(\n",
        "                        f\"- TTS: {'âœ… Loaded' if app.manager.tts_loaded else 'âŒ Not loaded'}\"\n",
        "                    )\n",
        "                    info.append(\n",
        "                        f\"- ASR: {'âœ… Loaded' if app.manager.asr_loaded else 'âŒ Not loaded'}\"\n",
        "                    )\n",
        "\n",
        "                    return \"\\n\".join(info)\n",
        "\n",
        "                system_info_output = gr.Markdown(get_system_info())\n",
        "                refresh_btn = gr.Button(\"ðŸ”„ Refresh\")\n",
        "                refresh_btn.click(fn=get_system_info, outputs=[system_info_output])\n",
        "\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ### ðŸ“‹ Model Information\n",
        "\n",
        "                    | Model | Type | Purpose |\n",
        "                    |-------|------|---------|\n",
        "                    | Qwen2.5-1.5B-Instruct | LLM | Text generation, Q&A |\n",
        "                    | stabilityai/sdxl-turbo | Image | Educational visuals |\n",
        "                    | Parler-TTS Mini | Audio | Voice narration |\n",
        "                    | Whisper Tiny | ASR | Voice input |\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "    return demo\n"
      ],
      "metadata": {
        "id": "7xL27YUhaA83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN ENTRY POINT\n",
        "# ================================================================="
      ],
      "metadata": {
        "id": "Yj73HXrVaMLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸŽ“ Course Video Generator - Google Colab Edition\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "demo = create_ui()\n",
        "\n",
        "demo.launch(\n",
        "    debug=True,\n",
        "    share=True,  # Creates public link for Colab\n",
        "    show_error=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "MND8F40xaRwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRT7P3x0inJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}