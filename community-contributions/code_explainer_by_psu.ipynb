{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f8636d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2cd0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e62927",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promt= \"\"\" \n",
    "                You are a technical explainer and problem-solver.\n",
    "Your job is to answer the user’s technical question with a clear, accurate explanation that matches their skill level.\n",
    "\n",
    "Guidelines:\n",
    "- Start with a short direct answer.\n",
    "- Then explain the concept step-by-step in plain language.\n",
    "- Use a small example when helpful (code, command, or pseudo-code).\n",
    "- Call out assumptions and edge cases.\n",
    "- If the question is ambiguous, ask 1–2 clarifying questions, otherwise make reasonable assumptions and state them.\n",
    "- Be honest about uncertainty; do not invent facts.\n",
    "- Prefer practical guidance over theory unless the user asks for theory.\n",
    "Output format:\n",
    "1) Direct answer\n",
    "2) Explanation\n",
    "3) Example (if useful)\n",
    "4) Common pitfalls / gotchas\n",
    "5) Next steps (optional)\n",
    "\n",
    "Respond in markdown with proper label and sub label without code blocks.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "def get_question_user_prompt(question):\n",
    "    question_user_prompt = \"\"\" Please explain what this code does and why:\n",
    "                                   \\n \"\"\"\n",
    "    question_user_prompt +=question \n",
    "    return question_user_prompt\n",
    "  \n",
    "#           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f7192f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Please explain what this code does and why:\\n                                   \\n  yield from {book.get(\"author\") for book in books if book.get(\"author\")} '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=' yield from {book.get(\"author\") for book in books if book.get(\"author\")} '\n",
    "get_question_user_prompt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16492ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_code_explainer_ollama(question):\n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_promt},\n",
    "            {\"role\": \"user\", \"content\": get_question_user_prompt(question)}\n",
    "          ],\n",
    "        stream=True  # streanms one by one in chunks (parts)\n",
    "    )    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Direct Answer**\n",
       "The given code uses a generator expression to extract the authors of books from a list of dictionaries.\n",
       "\n",
       "\n",
       "**Explanation**\n",
       "\n",
       "This code is a mix of two advanced concepts: iterators and dictionary methods. Here's how it works:\n",
       "\n",
       "*   The `yield from` expression is used to delegate sub-iteration for the resulting iterator.\n",
       "*   The `.get(\"author\") for book in books if book.get(\"author\")` part is a filter. It filters out any dictionaries (`book`) that don't contain an \"author\" key.\n",
       "\n",
       "**Example**\n",
       "\n",
       "Suppose we have a list of dictionaries representing books, where each dictionary has an \"author\" key:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author1\"},\n",
       "    {\"title\": \"Book Two\", \"author\": \"Author2\"},\n",
       "    {\"title\": \"Another Book\"}\n",
       "]\n",
       "```\n",
       "\n",
       "The code would process the first two books, because they contain the \"author\" key:\n",
       "\n",
       "```python\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "# authors -> ['Author1', 'Author2']\n",
       "```\n",
       "\n",
       "But it would skip the third, since its dictionary didn't contain an \"author\" key.\n",
       "\n",
       "\n",
       "**Common Pitfalls / Gotchas**\n",
       "\n",
       "*   If you are not careful, using `.get()` can result in keys being treated as attributes with non-dictionary values. In other words, if a book object is missing the author key and has `book.author`, things won't behave correctly.\n",
       "*   Don’t forget to handle exceptions while using `.get()`. There will always be potential edge cases.\n",
       "\n",
       "**Next Steps**\n",
       "Consider replacing `'Book'` in code with an array or list containing strings to further process these filtered book names."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "stream_code_explainer_ollama(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed1d8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Direct Answer**\n",
       "This code downloads a list of links from a webpage, appends the links to a string, and saves the response for later use.\n",
       "\n",
       "**Explanation**\n",
       "\n",
       "This code snippet is part of a larger script that interacts with a website. Here's a step-by-step breakdown:\n",
       "\n",
       "1. `fetch_website_links(url)`: This function fetches (downloads) a list of links from a given URL on the internet.\n",
       "2. `week1 EXERCISE.ipynb links = ...`: The `links` variable contains the returned result of `fetch_website_links(url).\n",
       "3. `\" \".join(links)`: This expression concatenates all the links in the `links` list into a single string, separated by spaces.\n",
       "4. `user_prompt += ...`: The resulting string is appended to a larger string called `user_prompt`.\n",
       "\n",
       "**Example**\n",
       "\n",
       "```python\n",
       "def fetch_website_links(url):\n",
       "    # Replace this with your actual website URL and scraping logic\n",
       "    return [\"https://www.example.com\", \"https://www.google.com\"]\n",
       "\n",
       "links = fetch_website_links(\"https://www.example.com\")\n",
       "new_links_string = \" \".join(links)\n",
       "user_prompt += new_links_string\n",
       "```\n",
       "\n",
       "**Common Pitfalls / Gotchas**\n",
       "\n",
       "* Make sure you're respecting the website's `robots.txt` file and scraping policies to avoid getting banned or reported.\n",
       "* Be cautious when dealing with dynamically generated content, as it might require additional logging, JavaScript execution, or user interaction.\n",
       "\n",
       "Next Steps:\n",
       "\n",
       "Since you've asked for an explanation, if your question has more context (like what \"fetch_website_links\" function does or what kind of data is stored in `user_prompt`), I'd love to ask a few clarifying questions:\n",
       "\n",
       "* Can you explain where this script runs (e.g., Jupyter Notebook, command-line interface)?\n",
       "* What is the purpose of `user_prompt` in this context?\n",
       "* Does the code handle errors or edge cases (e.g., non-HTTP responses or server-side scraping restrictions)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question=\"\"\"\"week1 EXERCISE.ipynb\"links = fetch_website_links(url)\n",
    "           user_prompt += \"\\n\".join(links) \"\"\"\n",
    "stream_code_explainer_ollama(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de2781",
   "metadata": {},
   "source": [
    "### Adding UI using gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d317f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e84adc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_code(s: str) -> bool:\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    patterns = [\n",
    "        r\"```\",                          # fenced code blocks\n",
    "        r\"^\\s{4,}\\S\",                    # indented lines (4+ spaces)\n",
    "        r\"\\b(def|class|import|from|if|elif|else|for|while|try|except|with|return|print)\\b\",\n",
    "        r\"[(){}\\[\\];]\",                  # code punctuation\n",
    "        r\"==|!=|<=|>=|:=|\\+=|-=|\\*=|/=|//=|\\*\\*\",\n",
    "        r\":\\s*$\",                        # line ending with colon (Python blocks)\n",
    "        r\"#\",                            # comment\n",
    "    ]\n",
    "\n",
    "    score = sum(bool(re.search(p, s, re.MULTILINE)) for p in patterns)\n",
    "    if score >= 2:\n",
    "        likely_code=True\n",
    "    else:\n",
    "        likely_code=False\n",
    "\n",
    "    return likely_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c05e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_explainer_ollama(message,history):\n",
    "    history=[{\"role\":h['role'],\"content\":h['content']}for h in history]\n",
    "    if looks_like_code(message)==True:\n",
    "        relative_system_promt=system_promt\n",
    "        relative_user_prompt=get_question_user_prompt(message)\n",
    "    else:\n",
    "        relative_system_promt='You are an polite helpful assistant'\n",
    "        relative_user_prompt=message\n",
    "        \n",
    "    \n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"system\", \"content\": relative_system_promt}] +history+\n",
    "          [{\"role\": \"user\", \"content\": relative_user_prompt}],\n",
    "        stream=True  # streanms one by one in chunks (parts)\n",
    "    )    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d38f8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* Running on public URL: https://f57e517315f0b2f1b5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f57e517315f0b2f1b5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat=gr.ChatInterface(fn=generate_code_explainer_ollama,title='Code Explainer')\n",
    "chat.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9cad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-explaine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
