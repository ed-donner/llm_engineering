{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb8554ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import gradio as gr \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0b9626",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "    \n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set (and this is optional)\")\n",
        "\n",
        "\n",
        "if openrouter_api_key:\n",
        "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0b38dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to OpenAI client library\n",
        "# A thin wrapper around calls to HTTP endpoints\n",
        "\n",
        "openai = OpenAI()\n",
        "\n",
        "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
        "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
        "# And OpenAI allows you to change the base_url\n",
        "\n",
        "\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "openrouter_client = OpenAI(api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
        "ollama_client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
        "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d69f41",
      "metadata": {},
      "outputs": [],
      "source": [
        "requests.get(\"http://localhost:11434/\").content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e178dc6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "AGENTS = [\n",
        "    {\n",
        "        \"name\": \"Alex\",\n",
        "        \"provider\": \"openai\",\n",
        "        \"model\": \"gpt-4.1-mini\",\n",
        "        \"system\": \"You are Alex. You are argumentative, skeptical, and slightly snarky. You challenge claims directly.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Blake\",\n",
        "        \"provider\": \"openrouter\",\n",
        "        \"model\": \"openrouter/aurora-alpha\",\n",
        "        \"system\": \"You are Blake. You are polite and cooperative. You look for common ground and de-escalate tension.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Charles\",\n",
        "        \"provider\": \"ollama\",\n",
        "        \"model\": \"llama3.2\",\n",
        "        \"system\": \"You are Charles. You are naive, literal, and often miss the point. You drift into misunderstandings.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "CLIENTS = {\n",
        "    \"openai\": openai_client,\n",
        "    \"openrouter\": openrouter_client,\n",
        "    \"ollama\": ollama_client,\n",
        "}\n",
        "\n",
        "def validate_config(agents):\n",
        "    providers = {a[\"provider\"] for a in agents}\n",
        "    if \"openai\" in providers and not openai_api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY is required for provider 'openai'\")\n",
        "    if \"openrouter\" in providers and not openrouter_api_key:\n",
        "        raise ValueError(\"OPENROUTER_API_KEY is required for provider 'openrouter'\")\n",
        "\n",
        "validate_config(AGENTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16662978",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_messages(current_agent, transcript):\n",
        "    messages = [{\"role\": \"system\", \"content\": current_agent[\"system\"]}]\n",
        "\n",
        "    for turn in transcript:\n",
        "        speaker = turn[\"speaker\"]\n",
        "        text = turn[\"text\"]\n",
        "\n",
        "        if speaker == current_agent[\"name\"]:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": text})\n",
        "        else:\n",
        "            messages.append({\"role\": \"user\", \"content\": f\"{speaker}: {text}\"})\n",
        "\n",
        "    return messages\n",
        "\n",
        "def generate_reply(agent, transcript, temperature=0.7):\n",
        "    client = CLIENTS[agent[\"provider\"]]\n",
        "    messages = build_messages(agent, transcript)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=agent[\"model\"],\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    return (response.choices[0].message.content or \"\").strip()\n",
        "\n",
        "def generate_reply_stream(agent, transcript, temperature=0.7):\n",
        "    client = CLIENTS[agent[\"provider\"]]\n",
        "    messages = build_messages(agent, transcript)\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=agent[\"model\"],\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    full_text = \"\"\n",
        "    for chunk in stream:\n",
        "        if not getattr(chunk, \"choices\", None):\n",
        "            continue\n",
        "        delta = chunk.choices[0].delta\n",
        "        if delta is None:\n",
        "            continue\n",
        "        piece = delta.content or \"\"\n",
        "        if piece:\n",
        "            full_text += piece\n",
        "            yield full_text\n",
        "\n",
        "def run_conversation(seed_prompt, rounds=3, verbose=True):\n",
        "    transcript = [{\"speaker\": \"User\", \"text\": seed_prompt}]\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        for agent in AGENTS:\n",
        "            reply = generate_reply(agent, transcript)\n",
        "            transcript.append({\"speaker\": agent[\"name\"], \"text\": reply})\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{agent['name']}: {reply}\\n\")\n",
        "\n",
        "    return transcript\n",
        "\n",
        "def run_conversation_stream(seed_prompt, rounds=3, verbose=False):\n",
        "    transcript = [{\"speaker\": \"User\", \"text\": seed_prompt}]\n",
        "    yield transcript\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        for agent in AGENTS:\n",
        "            transcript.append({\"speaker\": agent[\"name\"], \"text\": \"\"})\n",
        "\n",
        "            for partial_reply in generate_reply_stream(agent, transcript[:-1]):\n",
        "                transcript[-1][\"text\"] = partial_reply\n",
        "                yield transcript\n",
        "\n",
        "            if not transcript[-1][\"text\"]:\n",
        "                transcript[-1][\"text\"] = \"[No response returned]\"\n",
        "                yield transcript\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{agent['name']}: {transcript[-1]['text']}\\\\n\")\n",
        "\n",
        "def render_markdown_transcript(transcript):\n",
        "    parts = [\"## Conversation Transcript\"]\n",
        "    for turn in transcript:\n",
        "        parts.append(f\"**{turn['speaker']}**: {turn['text']}\")\n",
        "    display(Markdown(\"\\n\\n\".join(parts)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7798d9af",
      "metadata": {},
      "outputs": [],
      "source": [
        "#seed = \"Debate whether AI will improve education in the next 5 years.\"\n",
        "#transcript = run_conversation(seed_prompt=seed, rounds=2, verbose=True)\n",
        "#render_markdown_transcript(transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf79b555",
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcript_to_markdown(transcript):\n",
        "    parts = [\"## Conversation Transcript\"]\n",
        "    for turn in transcript:\n",
        "        parts.append(f\"**{turn['speaker']}**: {turn['text']}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def debate_ui_stream(seed_prompt):\n",
        "    try:\n",
        "        for transcript in run_conversation_stream(seed_prompt=seed_prompt, rounds=2, verbose=False):\n",
        "            yield transcript_to_markdown(transcript)\n",
        "    except Exception as e:\n",
        "        yield f\"## Error\\n\\n{type(e).__name__}: {e}\"\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=debate_ui_stream,\n",
        "    title=\"Model Debates\",\n",
        "    inputs=gr.Textbox(label=\"Proposed Topic\", info=\"Enter a debate topic\", lines=4),\n",
        "    outputs=gr.Markdown(label=\"Response\"),\n",
        "    examples=[[\"Debate whether AI will improve education in the next 5 years.\"]],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "view.queue().launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f27551b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
