{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
        "\n",
        "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
        "\n",
        "I will publish a full solution here soon - unless someone beats me to it...\n",
        "\n",
        "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "MODELS = {\n",
        "    \"GPT-4o-mini (OpenAI)\": {\n",
        "        \"base_url\": None,\n",
        "        \"api_key\":  os.getenv(\"OPENAI_API_KEY\"),\n",
        "        \"model\":    \"gpt-4o-mini\",\n",
        "        \"tools\":    True,   # tool-calling supported\n",
        "    },\n",
        "    \"Gemini 2.0 Flash (Google)\": {\n",
        "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "        \"api_key\":  os.getenv(\"GOOGLE_API_KEY\"),\n",
        "        \"model\":    \"gemini-2.0-flash\",\n",
        "        \"tools\":    False,\n",
        "    },\n",
        "    \"DeepSeek Chat\": {\n",
        "        \"base_url\": \"https://api.deepseek.com/v1\",\n",
        "        \"api_key\":  os.getenv(\"DEEPSEEK_API_KEY\"),\n",
        "        \"model\":    \"deepseek-chat\",\n",
        "        \"tools\":    False,\n",
        "    },\n",
        "    \"Llama 3.2 (Local / Ollama)\": {\n",
        "        \"base_url\": \"http://localhost:11434/v1\",\n",
        "        \"api_key\":  \"ollama\",\n",
        "        \"model\":    \"llama3.2\",\n",
        "        \"tools\":    False,\n",
        "    },\n",
        "}\n",
        "\n",
        "def get_client(model_name: str) -> OpenAI:\n",
        "    cfg = MODELS[model_name]\n",
        "    if cfg[\"base_url\"]:\n",
        "        return OpenAI(base_url=cfg[\"base_url\"], api_key=cfg[\"api_key\"])\n",
        "    return OpenAI(api_key=cfg[\"api_key\"])\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert programming tutor and AI/ML technical mentor.\n",
        "You have deep knowledge of Python, software engineering, machine learning, LLMs, and system design.\n",
        "\n",
        "When answering questions:\n",
        "- Give clear, accurate explanations with code examples where relevant\n",
        "- Break complex concepts into simple steps\n",
        "- Highlight best practices and common pitfalls\n",
        "- When it would help to *show* how code works, use the run_python_code tool to execute it live\n",
        "\n",
        "You have access to a Python code execution tool ‚Äî use it proactively to make your explanations concrete.\"\"\"\n",
        "\n",
        "\n",
        "run_code_tool = {\n",
        "    \"name\": \"run_python_code\",\n",
        "    \"description\": \"Execute a Python code snippet and return its stdout/stderr. Use to demonstrate or verify how code works.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"code\": {\"type\": \"string\", \"description\": \"The Python code to execute.\"}\n",
        "        },\n",
        "        \"required\": [\"code\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "tools = [{\"type\": \"function\", \"function\": run_code_tool}]\n",
        "\n",
        "\n",
        "def run_python_code(code: str) -> str:\n",
        "    \"\"\"Execute code in a subprocess and return output.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-c\", code],\n",
        "            capture_output=True, text=True, timeout=10,\n",
        "        )\n",
        "        out = result.stdout.strip()\n",
        "        err = result.stderr.strip()\n",
        "        if err:\n",
        "            out += f\"\\n[stderr]: {err}\"\n",
        "        return out or \"(no output)\"\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"Error: timed out after 10 seconds\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def handle_tool_calls(message):\n",
        "    \"\"\"Process tool calls, run them, and return API responses + display text.\"\"\"\n",
        "    api_responses = []\n",
        "    display_parts = []\n",
        "    for tc in message.tool_calls:\n",
        "        if tc.function.name == \"run_python_code\":\n",
        "            code = json.loads(tc.function.arguments).get(\"code\", \"\")\n",
        "            output = run_python_code(code)\n",
        "            display_parts.append(\n",
        "                f\"**üîß Code executed:**\\n```python\\n{code}\\n```\\n**Output:**\\n```\\n{output}\\n```\"\n",
        "            )\n",
        "            api_responses.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": output,\n",
        "                \"tool_call_id\": tc.id,\n",
        "            })\n",
        "    return api_responses, display_parts\n",
        "\n",
        "\n",
        "\n",
        "def chat(message: str, history: list, model_name: str):\n",
        "    \"\"\"\n",
        "    Generator that yields progressively longer response strings.\n",
        "    history is a list of [user_msg, bot_msg] tuples (Gradio default format).\n",
        "    Handles tool calls (for models that support them) before streaming the reply.\n",
        "    \"\"\"\n",
        "    client = get_client(model_name)\n",
        "    cfg = MODELS[model_name]\n",
        "    model = cfg[\"model\"]\n",
        "\n",
        "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for h in history:\n",
        "        msgs.append({\"role\": h[\"role\"], \"content\": h[\"content\"]})\n",
        "    msgs.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    prefix = \"\"  \n",
        "\n",
        "  \n",
        "    if cfg[\"tools\"]:\n",
        "        response = client.chat.completions.create(model=model, messages=msgs, tools=tools)\n",
        "        while response.choices[0].finish_reason == \"tool_calls\":\n",
        "            tool_msg = response.choices[0].message\n",
        "            api_responses, display_parts = handle_tool_calls(tool_msg)\n",
        "            prefix += \"\\n\\n\".join(display_parts) + \"\\n\\n---\\n\\n\"\n",
        "            msgs.append(tool_msg)\n",
        "            msgs.extend(api_responses)\n",
        "            response = client.chat.completions.create(model=model, messages=msgs, tools=tools)\n",
        "\n",
        "\n",
        "    try:\n",
        "        stream = client.chat.completions.create(model=model, messages=msgs, stream=True)\n",
        "        partial = prefix\n",
        "        for chunk in stream:\n",
        "            delta = chunk.choices[0].delta.content\n",
        "            if delta:\n",
        "                partial += delta\n",
        "                yield partial\n",
        "    except Exception as e:\n",
        "        yield f\"{prefix}‚ö†Ô∏è Error calling **{model_name}**: `{e}`\"\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Tech Q&A Assistant\", theme=gr.themes.Soft()) as ui:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"# ü§ñ Technical Q&A Assistant\\n\"\n",
        "        \"Ask any programming or AI/ML question. \"\n",
        "        \"Switch models to compare answers. \"\n",
        "        \"**GPT-4o-mini** can also *run Python code* live to illustrate explanations.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dd = gr.Dropdown(\n",
        "            choices=list(MODELS.keys()),\n",
        "            value=\"GPT-4o-mini (OpenAI)\",\n",
        "            label=\"Model\",\n",
        "            scale=1,\n",
        "            min_width=280,\n",
        "        )\n",
        "        gr.Markdown(\"*Tip: GPT-4o-mini supports live code execution via tool calling.*\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=520)\n",
        "\n",
        "    with gr.Row():\n",
        "        msg_box = gr.Textbox(\n",
        "            placeholder=\"Ask a technical question‚Ä¶\",\n",
        "            label=\"\",\n",
        "            scale=5,\n",
        "        )\n",
        "        send_btn = gr.Button(\"Send ‚ñ∂\", variant=\"primary\", scale=0, min_width=90)\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"Explain what this code does: yield from {book.get('author') for book in books if book.get('author')}\"],\n",
        "            [\"What is the difference between a list and a generator in Python?\"],\n",
        "            [\"Show me with a timing example how list comprehension compares to a for-loop\"],\n",
        "            [\"Explain how the attention mechanism works in transformer models\"],\n",
        "            [\"What are the trade-offs between RAG and fine-tuning an LLM?\"],\n",
        "        ],\n",
        "        inputs=msg_box,\n",
        "    )\n",
        "\n",
        "\n",
        "    def user_turn(message, history):\n",
        "        \"\"\"Append user message dict and clear the input box.\"\"\"\n",
        "        return \"\", history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    def bot_turn(history, model_name):\n",
        "        \"\"\"Stream the assistant reply, updating the last history entry.\"\"\"\n",
        "        last_user_msg = history[-1][\"content\"]\n",
        "        prior_history = history[:-1]\n",
        "        history_with_reply = history + [{\"role\": \"assistant\", \"content\": \"\"}]\n",
        "        for partial in chat(last_user_msg, prior_history, model_name):\n",
        "            history_with_reply[-1][\"content\"] = partial\n",
        "            yield history_with_reply\n",
        "\n",
        "\n",
        "    for trigger in (msg_box.submit, send_btn.click):\n",
        "        trigger(user_turn, [msg_box, chatbot], [msg_box, chatbot]).then(\n",
        "            bot_turn, [chatbot, model_dd], chatbot\n",
        "        )\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.10.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
