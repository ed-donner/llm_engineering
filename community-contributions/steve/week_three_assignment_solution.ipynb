{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VHmSDOEBixi"
      },
      "outputs": [],
      "source": [
        "## INSTALL DEPENDANCIES\n",
        "\n",
        "!pip install transformers diffusers accelerate gradio datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPORTS\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from diffusers import DiffusionPipeline\n",
        "from huggingface_hub import login, whoami\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Vk-d-sAGCDNL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## REPRODUCTIBILITY\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "lqPJgVNJXcho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOGIN WITH HUGGING FACE\n",
        "\n",
        "try:\n",
        "    hf_token = userdata.get(\"HF_API_KEY\")\n",
        "    if hf_token:\n",
        "        login(hf_token)\n",
        "        print(\"Logged into Hugging Face\")\n",
        "    else:\n",
        "        print(\"HF_TOKEN not found. Make sure to set it in Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"Login skipped or failed:\", e)"
      ],
      "metadata": {
        "id": "9KYBz0FRCLCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ea24a3-4154-4bb5-af8e-4672092b6147"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged into Hugging Face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## SETUP DEVICE\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using GPU\" if device == 0 else \"Using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf19WUVbEjDE",
        "outputId": "6a13ef45-cbb1-4d6c-c05d-f4a1b793c8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD MODELS  SET-UP\n",
        "\n",
        "models = {\n",
        "    \"FLAN-T5\": \"google/flan-t5-base\",\n",
        "    \"DistilGPT2\": \"distilgpt2\",\n",
        "    \"TinyLlama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "}\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "def load_model(model_name):\n",
        "    if model_name not in loaded_models:\n",
        "        loaded_models[model_name] = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=models[model_name],\n",
        "            device=device\n",
        "        )\n",
        "    return loaded_models[model_name]"
      ],
      "metadata": {
        "id": "IJK6uf3wEvP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## JSON VALIDATION\n",
        "\n",
        "def extract_json(text):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                return json.loads(match.group())\n",
        "            except:\n",
        "                return {\"error\": \"Invalid JSON format\"}\n",
        "        return {\"error\": \"No JSON detected\"}"
      ],
      "metadata": {
        "id": "HhBXiVVRNbCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SYTHENTIC DATA GENERATOR\n",
        "\n",
        "def generate_synthetic_data(task_type, model_name, num_samples):\n",
        "\n",
        "    pipe = load_model(model_name)\n",
        "\n",
        "    prompts = {\n",
        "        \"Customer Support\":\n",
        "        \"\"\"\n",
        "Generate ONE realistic customer support interaction as VALID JSON only.\n",
        "\n",
        "Return:\n",
        "{\n",
        "  \"customer_message\": \"...\",\n",
        "  \"agent_response\": \"...\",\n",
        "  \"issue_category\": \"...\",\n",
        "  \"resolution_status\": \"resolved or escalated\",\n",
        "  \"customer_sentiment\": \"positive, neutral, or negative\"\n",
        "}\n",
        "        \"\"\",\n",
        "\n",
        "        \"Product Reviews\":\n",
        "        \"\"\"\n",
        "Generate ONE realistic product review as VALID JSON only.\n",
        "\n",
        "Return:\n",
        "{\n",
        "  \"product_name\": \"...\",\n",
        "  \"category\": \"...\",\n",
        "  \"rating\": 1-5,\n",
        "  \"review_title\": \"...\",\n",
        "  \"review_text\": \"...\",\n",
        "  \"verified_purchase\": true or false,\n",
        "  \"sentiment\": \"positive, neutral, or negative\"\n",
        "}\n",
        "        \"\"\",\n",
        "\n",
        "        \"Meeting Summary\":\n",
        "        \"\"\"\n",
        "Generate ONE structured meeting summary as VALID JSON only.\n",
        "\n",
        "Return:\n",
        "{\n",
        "  \"title\": \"...\",\n",
        "  \"date\": \"YYYY-MM-DD\",\n",
        "  \"participants\": [\"name1\", \"name2\"],\n",
        "  \"key_points\": [\"point1\", \"point2\"],\n",
        "  \"decisions_made\": [\"decision1\"],\n",
        "  \"action_items\": [\n",
        "    {\"task\": \"...\", \"owner\": \"...\", \"deadline\": \"YYYY-MM-DD\"}\n",
        "  ]\n",
        "}\n",
        "        \"\"\",\n",
        "\n",
        "        \"QA Dataset\":\n",
        "        \"\"\"\n",
        "Generate ONE QA training example as VALID JSON only.\n",
        "\n",
        "Return:\n",
        "{\n",
        "  \"domain\": \"...\",\n",
        "  \"difficulty\": \"easy, medium, or hard\",\n",
        "  \"context\": \"...\",\n",
        "  \"question\": \"...\",\n",
        "  \"answer\": \"...\",\n",
        "  \"answer_type\": \"fact, explanation, or reasoning\"\n",
        "}\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _ in range(int(num_samples)):\n",
        "        output = pipe(\n",
        "            prompts[task_type],\n",
        "            max_new_tokens=180,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        validated = extract_json(output)\n",
        "        results.append(validated)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Na0i0I-PEzY_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## JSON CSV EXPORT\n",
        "\n",
        "def json_to_dataframe(json_data):\n",
        "    try:\n",
        "        return pd.DataFrame(json_data)\n",
        "    except:\n",
        "        return pd.DataFrame({\"error\": [\"Conversion failed\"]})"
      ],
      "metadata": {
        "id": "zhaX_bk2N0IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TOKENIZER INSPECTION\n",
        "\n",
        "def inspect_tokenizer(model_name, sample_text):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(models[model_name])\n",
        "    tokens = tokenizer.tokenize(sample_text)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"token_ids\": token_ids,\n",
        "        \"num_tokens\": len(tokens)\n",
        "    }"
      ],
      "metadata": {
        "id": "vgDijSnkN-gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD DIFFUSSION MODEL\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    image_pipe = DiffusionPipeline.from_pretrained(\n",
        "        \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    image_pipe.enable_attention_slicing()\n",
        "    image_pipe.enable_model_cpu_offload()\n",
        "\n",
        "    print(\"Diffusion model loaded.\")\n",
        "else:\n",
        "    print(\"GPU not available. Image mode disabled.\")\n",
        "    image_pipe = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YascCUGeWXUu",
        "outputId": "0ff5ffc4-71e2-48f0-ef8e-2eb942a53694"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available. Diffusion image mode disabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMAGE GENERATOR\n",
        "\n",
        "def generate_synthetic_image_dataset(prompt, num_images):\n",
        "\n",
        "    if image_pipe is None:\n",
        "        return [\"GPU not available ‚Äî image generation disabled.\"]\n",
        "\n",
        "    images = []\n",
        "    for _ in range(int(num_images)):\n",
        "        image = image_pipe(prompt).images[0]\n",
        "        images.append(image)\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "XVy3iPsbOeIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GRADIO APPLICATION\n",
        "\n",
        "def run_generator(task_type, model_name, num_samples):\n",
        "    data = generate_synthetic_data(task_type, model_name, num_samples)\n",
        "    df = json_to_dataframe(data)\n",
        "\n",
        "    csv_path = \"synthetic_dataset.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    return data, csv_path\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    gr.Markdown(\"# üß† Synthetic Dataset Generator Studio\")\n",
        "\n",
        "    # TEXT DATA SECTION\n",
        "    gr.Markdown(\"## üìä Text Synthetic Dataset Generator\")\n",
        "\n",
        "    task_dropdown = gr.Dropdown(\n",
        "        [\"Customer Support\", \"Product Reviews\", \"Meeting Summary\", \"QA Dataset\"],\n",
        "        label=\"Select Dataset Type\"\n",
        "    )\n",
        "\n",
        "    model_dropdown = gr.Dropdown(\n",
        "        list(models.keys()),\n",
        "        label=\"Select Model\"\n",
        "    )\n",
        "\n",
        "    num_samples = gr.Slider(1, 5, value=2, step=1, label=\"Number of Samples\")\n",
        "\n",
        "    output_json = gr.JSON(label=\"Validated JSON Output\")\n",
        "    csv_output = gr.File(label=\"Download CSV\")\n",
        "\n",
        "    generate_btn = gr.Button(\"Generate Dataset\")\n",
        "\n",
        "    generate_btn.click(\n",
        "        run_generator,\n",
        "        inputs=[task_dropdown, model_dropdown, num_samples],\n",
        "        outputs=[output_json, csv_output],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # TOKENIZER SECTION\n",
        "    gr.Markdown(\"## üîç Tokenizer Inspection\")\n",
        "\n",
        "    token_input = gr.Textbox(label=\"Enter Text\")\n",
        "    token_output = gr.JSON(label=\"Tokenizer Output\")\n",
        "\n",
        "    inspect_btn = gr.Button(\"Inspect Tokens\")\n",
        "\n",
        "    inspect_btn.click(\n",
        "        inspect_tokenizer,\n",
        "        inputs=[model_dropdown, token_input],\n",
        "        outputs=token_output,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    # IMAGE SECTION\n",
        "    gr.Markdown(\"## üñº Synthetic Image Dataset Generator\")\n",
        "\n",
        "    image_prompt = gr.Textbox(label=\"Image Prompt\")\n",
        "    num_images = gr.Slider(1, 3, value=1, step=1, label=\"Number of Images\")\n",
        "    image_gallery = gr.Gallery(label=\"Generated Images\")\n",
        "\n",
        "    image_btn = gr.Button(\"Generate Images\")\n",
        "\n",
        "    image_btn.click(\n",
        "        generate_synthetic_image_dataset,\n",
        "        inputs=[image_prompt, num_images],\n",
        "        outputs=image_gallery,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "n214LLgqOpZH",
        "outputId": "db7fa218-23ae-43d2-9e9d-dc18db8c2c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5d818ebea3cc01fd74.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5d818ebea3cc01fd74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}