{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwlhBiSuhDr3"
      },
      "outputs": [],
      "source": [
        "## IMPORTS\n",
        "\n",
        "\n",
        "import os\n",
        "import black\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install black"
      ],
      "metadata": {
        "id": "2iR46aq46yVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## API ACCESS\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    raise ValueError(\"ANTHROPIC_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "print(\"API Keys Loaded Successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNuv4Ue-hfwh",
        "outputId": "26b0dc4e-d12f-46f5-f0bb-6f6c432a54e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API Keys Loaded Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CLIENT INITILIZATION\n",
        "\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "anthropic_client = OpenAI(\n",
        "    api_key=ANTHROPIC_API_KEY,\n",
        "    base_url=\"https://api.anthropic.com/v1\"\n",
        ")\n",
        "\n",
        "print(\"Clients initialized successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTVetBSriMhA",
        "outputId": "a0480768-ebdc-4f94-a01e-fbe8b3113867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Clients initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MODEL CONFIGURATION\n",
        "\n",
        "OPENAI_HIGH = \"gpt-4o\"\n",
        "OPENAI_LOW = \"gpt-4o-mini\"\n",
        "\n",
        "CLAUDE_HIGH = \"claude-3-opus-20240229\"\n",
        "CLAUDE_LOW = \"claude-3-haiku-20240307\""
      ],
      "metadata": {
        "id": "2PXOU5WNp-hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CLIENT-MODEL SWITCHING\n",
        "\n",
        "def get_client_and_model(provider=\"openai\", tier=\"low\"):\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        model = OPENAI_HIGH if tier == \"high\" else OPENAI_LOW\n",
        "        return openai_client, model\n",
        "\n",
        "    elif provider == \"claude\":\n",
        "        model = CLAUDE_HIGH if tier == \"high\" else CLAUDE_LOW\n",
        "        return anthropic_client, model\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider.\")"
      ],
      "metadata": {
        "id": "Vg5azoSVqG1e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SYSTEM PROMPT\n",
        "\n",
        "def build_system_prompt(include_tests=False):\n",
        "\n",
        "    base_prompt = \"\"\"\n",
        "You are a senior Python engineer.\n",
        "\n",
        "Your task:\n",
        "\n",
        "1. Add professional docstrings.\n",
        "2. Add meaningful inline comments.\n",
        "3. Do NOT change logic.\n",
        "4. Do NOT wrap in markdown.\n",
        "5. Return ONLY raw executable Python code.\n",
        "\n",
        "Docstrings must include:\n",
        "- Description\n",
        "- Args\n",
        "- Returns\n",
        "\"\"\"\n",
        "\n",
        "    if include_tests:\n",
        "        base_prompt += \"\"\"\n",
        "\n",
        "6. After the updated code, generate pytest unit tests.\n",
        "   - Cover normal and edge cases.\n",
        "   - Use professional structure.\n",
        "\"\"\"\n",
        "\n",
        "    return base_prompt"
      ],
      "metadata": {
        "id": "yyuQD3OWqazo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PEP 8 FORMATTER\n",
        "\n",
        "def format_code_pep8(code):\n",
        "    try:\n",
        "        return black.format_str(code, mode=black.FileMode())\n",
        "    except Exception:\n",
        "        return code"
      ],
      "metadata": {
        "id": "7CytqxVj9maG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DOCSTRING GENERATOR\n",
        "\n",
        "def generate_docstrings(code, provider=\"openai\", tier=\"low\", include_tests=False):\n",
        "\n",
        "    client, model = get_client_and_model(provider, tier)\n",
        "    system_prompt = build_system_prompt(include_tests)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": code}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "    content = format_code_pep8(content)\n",
        "\n",
        "    usage = response.usage\n",
        "    total_tokens = usage.total_tokens\n",
        "\n",
        "    cost_per_1k_tokens = 0.005\n",
        "    estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
        "\n",
        "    metadata = f\"\"\"\n",
        "Prompt Tokens: {usage.prompt_tokens}\n",
        "Completion Tokens: {usage.completion_tokens}\n",
        "Total Tokens: {usage.total_tokens}\n",
        "Estimated Cost: ${estimated_cost:.6f}\n",
        "\"\"\"\n",
        "\n",
        "    return content, metadata"
      ],
      "metadata": {
        "id": "LKEnr-RiqvgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## STREAM DOCSTRING GENERATOR\n",
        "\n",
        "def generate_docstrings_stream(code, provider=\"openai\", tier=\"low\", include_tests=False):\n",
        "\n",
        "    client, model = get_client_and_model(provider, tier)\n",
        "    system_prompt = build_system_prompt(include_tests)\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": code}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    full_response = \"\"\n",
        "\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content:\n",
        "            content = chunk.choices[0].delta.content\n",
        "            full_response += content\n",
        "            yield full_response, \"\"   # live stream\n",
        "\n",
        "    #POST-PROCESS AFTER STREAM ENDS\n",
        "\n",
        "    formatted = format_code_pep8(full_response)\n",
        "\n",
        "    # Get usage from final chunk\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": code}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    usage = final_response.usage\n",
        "    total_tokens = usage.total_tokens\n",
        "    cost_per_1k_tokens = 0.005\n",
        "    estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
        "\n",
        "    metadata = f\"\"\"\n",
        "Prompt Tokens: {usage.prompt_tokens}\n",
        "Completion Tokens: {usage.completion_tokens}\n",
        "Total Tokens: {usage.total_tokens}\n",
        "Estimated Cost: ${estimated_cost:.6f}\n",
        "\"\"\"\n",
        "\n",
        "    yield formatted, metadata"
      ],
      "metadata": {
        "id": "h50sMgaStjbH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FILE UPLOAD SUPPORT\n",
        "\n",
        "def load_file(file):\n",
        "    if file is None:\n",
        "        return \"\"\n",
        "    return file.read().decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "MvkksRzT4ngn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## COPY TO CLIPBOARD support\n",
        "\n",
        "def copy_output(code):\n",
        "    return code"
      ],
      "metadata": {
        "id": "8LjD8Niu5Umr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TOKEN USAGE COST CALCULATOR\n",
        "\n",
        "def generate_docstrings(code, provider=\"openai\", tier=\"low\", include_tests=False):\n",
        "\n",
        "    client, model = get_client_and_model(provider, tier)\n",
        "    system_prompt = build_system_prompt(include_tests)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": code}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Token usage\n",
        "    usage = response.usage\n",
        "    prompt_tokens = usage.prompt_tokens\n",
        "    completion_tokens = usage.completion_tokens\n",
        "    total_tokens = usage.total_tokens\n",
        "\n",
        "    # Rough cost estimate (adjust per model if needed)\n",
        "    cost_per_1k_tokens = 0.005  # Example estimate\n",
        "    estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
        "\n",
        "    metadata = f\"\\n\\n# --- METADATA ---\\n# Prompt Tokens: {prompt_tokens}\\n# Completion Tokens: {completion_tokens}\\n# Total Tokens: {total_tokens}\\n# Estimated Cost: ${estimated_cost:.6f}\\n\"\n",
        "\n",
        "    return content + metadata"
      ],
      "metadata": {
        "id": "nVmAYRqz7fOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GRADIO INTERFACE\n",
        "\n",
        "def docstring_app(code, provider, tier, streaming, include_tests):\n",
        "\n",
        "    if not code or not code.strip():\n",
        "        yield \"#Please paste valid Python code.\", \"\"\n",
        "        return\n",
        "\n",
        "    if streaming:\n",
        "        yield from generate_docstrings_stream(\n",
        "            code, provider, tier, include_tests\n",
        "        )\n",
        "    else:\n",
        "        content, metadata = generate_docstrings(\n",
        "            code, provider, tier, include_tests\n",
        "        )\n",
        "        yield content, metadata\n",
        "\n",
        "def clear_fields():\n",
        "    return \"\", \"\"\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    gr.Markdown(\"## AI Docstring & Unit Test Generator\")\n",
        "    gr.Markdown(\"Production-ready multi-model AI code enhancement tool.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        provider = gr.Dropdown(\n",
        "            choices=[\"openai\", \"claude\"],\n",
        "            value=\"openai\",\n",
        "            label=\"Provider\"\n",
        "        )\n",
        "        tier = gr.Dropdown(\n",
        "            choices=[\"low\", \"high\"],\n",
        "            value=\"low\",\n",
        "            label=\"Model Tier\"\n",
        "        )\n",
        "\n",
        "    streaming = gr.Checkbox(label=\"Enable Streaming\", value=True)\n",
        "    include_tests = gr.Checkbox(label=\"Generate Unit Tests\", value=False)\n",
        "\n",
        "    file_upload = gr.File(label=\"Upload Python File (.py)\", file_types=[\".py\"])\n",
        "\n",
        "    code_input = gr.Code(label=\"Paste Python Code\", language=\"python\")\n",
        "\n",
        "    file_upload.change(load_file, inputs=file_upload, outputs=code_input)\n",
        "\n",
        "    output = gr.Code(label=\"Generated Code\", language=\"python\")\n",
        "    metadata_output = gr.Textbox(label=\"Token Usage & Cost\", lines=4)\n",
        "\n",
        "    with gr.Row():\n",
        "        generate_btn = gr.Button(\"Generate\")\n",
        "        clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "\n",
        "    generate_btn.click(\n",
        "        docstring_app,\n",
        "        inputs=[code_input, provider, tier, streaming, include_tests],\n",
        "        outputs=[output, metadata_output]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        clear_fields,\n",
        "        outputs=[code_input, output, metadata_output]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "binoanh5u-t1",
        "outputId": "2596b843-6418-493f-9ca3-2aaa2e732f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e1dd5ae17d58d5778e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e1dd5ae17d58d5778e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cp3TjFJHt1MK"
      }
    }
  ]
}