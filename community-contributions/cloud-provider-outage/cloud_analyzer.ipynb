{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5044a9ca",
      "metadata": {},
      "source": [
        "# Multi-Cloud Public Outage Collector.\n",
        "\n",
        "This notebook collects **public outage data** from:\n",
        "- AWS (RSS)\n",
        "- Azure (RSS)\n",
        "- GCP (JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "95729f66",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The value specified in an AutoRun registry key could not be parsed.\n",
            "\u001b[2mUsing Python 3.12.12 environment at: C:\\Users\\vishalkc2\\Documents\\Sample_Projects\\ed_donner\\llm_engineering\\.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install feedparser requests dotenv beautifulsoup4 lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "06ddde99",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eed70f3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found and looks good so far!\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Check the key\n",
        "\n",
        "if not api_key:\n",
        "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
        "elif not api_key.startswith(\"sk-proj-\"):\n",
        "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
        "elif api_key.strip() != api_key:\n",
        "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "32d111bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def now_iso():\n",
        "    return datetime.now(timezone.utc).isoformat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bfe26d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timezone\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f0b21d12",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_aws_incidents():\n",
        "    url = \"https://status.aws.amazon.com/history.json\"\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\n",
        "            url,\n",
        "            headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
        "            timeout=20\n",
        "        )\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception:\n",
        "        # AWS no longer exposes public history JSON\n",
        "        return [{\n",
        "            \"cloud\": \"aws\",\n",
        "            \"service\": \"all\",\n",
        "            \"region\": \"global\",\n",
        "            \"title\": \"AWS does not expose public historical outage data via JSON APIs\",\n",
        "            \"status\": \"unsupported\",\n",
        "            \"start_time\": None,\n",
        "            \"end_time\": None,\n",
        "            \"details\": (\n",
        "                \"AWS Health historical events require either \"\n",
        "                \"1) AWS Health API with Business/Enterprise support, or \"\n",
        "                \"2) JavaScript execution in a browser context.\"\n",
        "            ),\n",
        "            \"source_url\": \"https://health.aws.amazon.com/health/status\"\n",
        "        }]\n",
        "\n",
        "    incidents = []\n",
        "\n",
        "    for date, events in data.items():\n",
        "        for e in events:\n",
        "            incidents.append({\n",
        "                \"cloud\": \"aws\",\n",
        "                \"service\": e.get(\"service\"),\n",
        "                \"region\": e.get(\"region\") or \"global\",\n",
        "                \"title\": e.get(\"summary\"),\n",
        "                \"status\": e.get(\"status\"),\n",
        "                \"start_time\": date,\n",
        "                \"end_time\": None,\n",
        "                \"details\": e.get(\"description\"),\n",
        "                \"source_url\": \"https://health.aws.amazon.com/health/status\"\n",
        "            })\n",
        "\n",
        "    return incidents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d5a837",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_azure_incidents():\n",
        "    url = \"https://azure.status.microsoft/en-us/status/history/\"\n",
        "    html = requests.get(\n",
        "        url,\n",
        "        headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
        "        timeout=20\n",
        "    ).text\n",
        "\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    results = []\n",
        "\n",
        "    # Each month block\n",
        "    for month_header in soup.select(\"div.month-title-container h2\"):\n",
        "        month_text = month_header.get_text(strip=True)\n",
        "\n",
        "        wrapper = (\n",
        "            month_header\n",
        "            .find_parent(\"div\")\n",
        "            .find_next_sibling(\"div\", class_=\"month-incident-container-wrapper\")\n",
        "        )\n",
        "        if not wrapper:\n",
        "            continue\n",
        "\n",
        "        # Each incident row\n",
        "        for row in wrapper.select(\"div.row\"):\n",
        "            day_el = row.select_one(\".incident-history-day\")\n",
        "            title_el = row.select_one(\".incident-history-title\")\n",
        "            tracking_el = row.select_one(\".incident-history-tracking-id\")\n",
        "            body_el = row.select_one(\".incident-history-collapse .card-body\")\n",
        "\n",
        "            if not all([day_el, title_el, tracking_el, body_el]):\n",
        "                continue\n",
        "\n",
        "            # Normalize date\n",
        "            day = day_el.get_text(strip=True)\n",
        "            try:\n",
        "                date_obj = datetime.strptime(\n",
        "                    f\"{month_text} {day}\", \"%B %Y %d\"\n",
        "                )\n",
        "                iso_date = date_obj.date().isoformat()\n",
        "            except Exception:\n",
        "                iso_date = None\n",
        "\n",
        "            title = title_el.get_text(strip=True)\n",
        "            tracking_id = (\n",
        "                tracking_el.get_text(strip=True)\n",
        "                .replace(\"Tracking ID:\", \"\")\n",
        "                .strip()\n",
        "            )\n",
        "\n",
        "            raw_html = str(body_el)\n",
        "            clean_text = body_el.get_text(\"\\n\", strip=True)\n",
        "\n",
        "            # Extract PIR sections\n",
        "            sections = {}\n",
        "            current = None\n",
        "\n",
        "            def norm(s):\n",
        "                return re.sub(r\"\\s+\", \" \", s.lower())\n",
        "\n",
        "            for el in body_el.find_all([\"strong\", \"p\", \"li\"]):\n",
        "                text = el.get_text(strip=True)\n",
        "                key = norm(text)\n",
        "\n",
        "                if \"what happened\" in key:\n",
        "                    current = \"what_happened\"\n",
        "                    sections[current] = []\n",
        "                elif \"what went wrong\" in key:\n",
        "                    current = \"what_went_wrong\"\n",
        "                    sections[current] = []\n",
        "                elif \"how did we respond\" in key:\n",
        "                    current = \"how_did_we_respond\"\n",
        "                    sections[current] = []\n",
        "                elif \"how are we making\" in key:\n",
        "                    current = \"mitigation\"\n",
        "                    sections[current] = []\n",
        "                elif \"how can customers\" in key:\n",
        "                    current = \"customer_guidance\"\n",
        "                    sections[current] = []\n",
        "                elif current:\n",
        "                    sections[current].append(text)\n",
        "\n",
        "            sections = {k: \"\\n\".join(v) for k, v in sections.items()}\n",
        "\n",
        "            results.append({\n",
        "                \"cloud\": \"azure\",\n",
        "                \"month\": month_text,\n",
        "                \"date\": iso_date,\n",
        "                \"tracking_id\": tracking_id,\n",
        "                \"title\": title,\n",
        "                \"text\": clean_text,\n",
        "                \"sections\": sections,\n",
        "                \"raw_html\": raw_html,\n",
        "                \"source_url\": url\n",
        "            })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "37b37c69",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_gcp_incidents():\n",
        "    url = \"https://status.cloud.google.com/incidents.json\"\n",
        "    data = requests.get(url, timeout=15).json()\n",
        "\n",
        "    incidents = []\n",
        "    for inc in data:\n",
        "        incidents.append({\n",
        "            \"cloud\": \"gcp\",\n",
        "            \"service\": \", \".join(inc.get(\"services\", [])),\n",
        "            \"region\": \", \".join(inc.get(\"currently_affected_locations\", [])) or \"global\",\n",
        "            \"title\": inc.get(\"external_desc\"),\n",
        "            \"status\": inc.get(\"status\"),\n",
        "            \"start_time\": inc.get(\"begin\"),\n",
        "            \"end_time\": inc.get(\"end\"),\n",
        "            \"url\": inc.get(\"uri\")\n",
        "        })\n",
        "\n",
        "    return incidents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67df74aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cloud_outages_json():\n",
        "    incidents = []\n",
        "    incidents.extend(fetch_aws_incidents())\n",
        "    incidents.extend(fetch_azure_incidents())\n",
        "    incidents.extend(fetch_gcp_incidents())\n",
        "\n",
        "    payload = {\n",
        "        \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"source\": \"public-status-pages\",\n",
        "        \"incident_count\": len(incidents),\n",
        "        \"incidents\": incidents\n",
        "    }\n",
        "\n",
        "    return json.dumps(payload, separators=(\",\", \":\"), ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ec2667fb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"generated_at\":\"2026-01-23T11:59:29.060209+00:00\",\"source\":\"public-status-pages\",\"incident_count\":21,\"incidents\":[{\"cloud\":\"aws\",\"service\":\"all\",\"region\":\"global\",\"title\":\"AWS does not expose public historical outage data via JSON APIs\",\"status\":\"unsupported\",\"start_time\":null,\"end_time\":null,\"details\":\"AWS Health historical events require either 1) AWS Health API with Business/Enterprise support, or 2) JavaScript execution in a browser context.\",\"source_url\":\"https://health.aws.amazon.com/health/status\"},{\"cloud\":\"azure\",\"month\":\"January 2026\",\"date\":\"2026-01-10\",\"tracking_id\":\"XM22-5_G\",\"title\":\"Preliminary Post Incident Review (PIR) – Power event impacting multiple services in AZ01 – West US 2\",\"text\":\"This is our Preliminary PIR to share what we know so far. After our internal retrospective is completed (generally within\\xa014 days)\\xa0we\\xa0will publish a Final PIR with\\xa0additional\\xa0details.\\\\nDuring this incident, we temporarily used our public Azure Status page because it\\xa0was taking\\xa0too long to\\xa0determine\\xa0exactly which customers, regions, or services were affected. Since the issue has been mitigated and customers who were\\xa0impacted\\xa0can read relevant communications in Azure Service Health, this public post will be removed\\xa0once the\\xa0Final PIR is published.\\xa0For details on how this public Status page is used, refer to\\\\nhttps://aka.ms/StatusPageCriteria\\\\n.\\\\nWhat happened?\\\\nBetween 17:50 UTC on 10 January 2026 and 01:23 UTC on 11 January 2026, customers with resources hosted in the West US 2 region may have experienced intermittent connectivity issues, timeouts, increased error rates, or delays when performing operations on affected resources. This issue was due to a power interruption affecting infrastructure within a single Availability Zone (AZ01) within the West US 2 region, which resulted in some infrastructure being temporarily unavailable.\\\\nCompute and storage infrastructure recovered by 19:51 UTC on 10 January 2026, however residual impact to newly created\\xa0Virtual Machines\\xa0(or VMs updated during the impact timeframe) in this AZ may have remained until 01:23 UTC on 11 January 2026.\\xa0Impacted services included, but may not have been limited to: Azure Cache for Redis, Azure Cosmos DB, Azure Data Explorer, Azure Database for PostgreSQL, Azure Databricks, Azure Synapse Analytics, Azure Service Bus, Azure SQL Database, Azure Storage, and Azure Virtual Machines (VMs).\\\\nNote that the \\'logical\\' Availability Zones used by each customer subscription may correspond to different physical Availability Zones. Customers can use the Locations API to understand this mapping, to confirm which resources run in this physical AZ:\\\\nhttps://learn.microsoft.com/rest/api/resources/subscriptions/list-locations?HTTP#availabilityzonemappings\\\\n.\\\\nWhat went wrong and why?\\\\nAn emergency ‘power off’ safety system in a portion of the impacted datacenter activated unexpectedly, removing power to a subset of compute and storage infrastructure racks, contained within a single Availability Zone in the West US 2 region. This sudden loss of power caused the affected hardware to shut down, resulting in interruptions for services dependent on those resources. The activation occurred within a confined portion of the datacenter’s electrical path, and the loss of power caused downstream infrastructure to become unavailable until power was restored and systems recovered.\\\\nThis power down event also impacted key infrastructure underlying the Azure Networking stack that is supported by multiple networking components, particularly the Azure Software Load Balancer (SLB) platform. SLB services did not fully recover when compute and storage resources initially mitigated, which caused newly deployed or updated VMs within this Availability Zone (on hosts managed by impacted SLB resources) to have continued intermittent outbound connectivity issues. Services reliant on these VMs may have experienced further issues until residual impact mitigated. Extended investigation was needed to fully understand how to resolve the issues impacting the programming of network configurations by SLB services.\\\\nHow did we respond?\\\\n17:50 UTC on 10 January 2026 – Customer impact began.\\\\n17:59 UTC on 10 January 2026 – Initial monitoring alerted us to the issue and engaged engineers and onsite teams. Automated recovery operations for some services were ongoing.\\\\n18:09 UTC on 10 January 2026 – We identified the power interruption.\\\\n18:20 UTC on 10 January 2026 – Recovery efforts underway to re-energize impacted infrastructure.\\\\n19:25 UTC on 10 January 2026 – Datacenter infrastructure was re-energized and services began to recover including storage and compute resources on which other services rely. We continued to monitor for full recovery of other services.\\\\n19:51 UTC on 10 January 2026 – Mitigation was declared for compute and storage infrastructure. Residual impact remained for some services reliant on VM creation operations due to the SLB issue which was still under investigation.\\\\n00:00 UTC on 11 January 2026 – After extended investigation of the SLB issue, we determined mitigation steps for the residual impact.\\\\n01:23 UTC on 11 January 2026 - We made a manual configuration update to the SLB services to resume proper programming and mitigate residual impact to newly created or updated VMs.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nImprove alerting for specific rack infrastructure impacted by such datacenter issues. (Estimated completion: TBD)\\\\nImprove our standard operating procedures, troubleshooting guides, and escalation workflows to reduce the time to mitigate residual impact. (Estimated completion: TBD)\\\\nImprove automated recovery of SLB infrastructure services impacted following localized infrastructure interruptions such as this one. (Estimated completion: TBD)\\\\nThis is our Preliminary PIR to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a Final PIR with additional details.\\\\nHow can customers make incidents like this less impactful?\\\\nConsider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations:\\\\nhttps://docs.microsoft.com/azure/availability-zones/az-overview\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources — for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, ensure the right people in your organization are notified about future service issues by configuring Azure Service Health alerts (email, SMS, push notifications, webhooks, and more):\\\\nhttps://aka.ms/ash-alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/XM22-5_G\",\"sections\":{\"what_happened\":\"Between 17:50 UTC on 10 January 2026 and 01:23 UTC on 11 January 2026, customers with resources hosted in the West US 2 region may have experienced intermittent connectivity issues, timeouts, increased error rates, or delays when performing operations on affected resources. This issue was due to a power interruption affecting infrastructure within a single Availability Zone (AZ01) within the West US 2 region, which resulted in some infrastructure being temporarily unavailable.\\\\nCompute and storage infrastructure recovered by 19:51 UTC on 10 January 2026, however residual impact to newly created\\xa0Virtual Machines\\xa0(or VMs updated during the impact timeframe) in this AZ may have remained until 01:23 UTC on 11 January 2026.\\xa0Impacted services included, but may not have been limited to: Azure Cache for Redis, Azure Cosmos DB, Azure Data Explorer, Azure Database for PostgreSQL, Azure Databricks, Azure Synapse Analytics, Azure Service Bus, Azure SQL Database, Azure Storage, and Azure Virtual Machines (VMs).\\\\nNote that the \\'logical\\' Availability Zones used by each customer subscription may correspond to different physical Availability Zones. Customers can use the Locations API to understand this mapping, to confirm which resources run in this physical AZ:https://learn.microsoft.com/rest/api/resources/subscriptions/list-locations?HTTP#availabilityzonemappings.\",\"what_went_wrong\":\"An emergency ‘power off’ safety system in a portion of the impacted datacenter activated unexpectedly, removing power to a subset of compute and storage infrastructure racks, contained within a single Availability Zone in the West US 2 region. This sudden loss of power caused the affected hardware to shut down, resulting in interruptions for services dependent on those resources. The activation occurred within a confined portion of the datacenter’s electrical path, and the loss of power caused downstream infrastructure to become unavailable until power was restored and systems recovered.\\\\nThis power down event also impacted key infrastructure underlying the Azure Networking stack that is supported by multiple networking components, particularly the Azure Software Load Balancer (SLB) platform. SLB services did not fully recover when compute and storage resources initially mitigated, which caused newly deployed or updated VMs within this Availability Zone (on hosts managed by impacted SLB resources) to have continued intermittent outbound connectivity issues. Services reliant on these VMs may have experienced further issues until residual impact mitigated. Extended investigation was needed to fully understand how to resolve the issues impacting the programming of network configurations by SLB services.\",\"how_did_we_respond\":\"17:50 UTC on 10 January 2026 – Customer impact began.\\\\n17:59 UTC on 10 January 2026 – Initial monitoring alerted us to the issue and engaged engineers and onsite teams. Automated recovery operations for some services were ongoing.\\\\n18:09 UTC on 10 January 2026 – We identified the power interruption.\\\\n18:20 UTC on 10 January 2026 – Recovery efforts underway to re-energize impacted infrastructure.\\\\n19:25 UTC on 10 January 2026 – Datacenter infrastructure was re-energized and services began to recover including storage and compute resources on which other services rely. We continued to monitor for full recovery of other services.\\\\n19:51 UTC on 10 January 2026 – Mitigation was declared for compute and storage infrastructure. Residual impact remained for some services reliant on VM creation operations due to the SLB issue which was still under investigation.\\\\n00:00 UTC on 11 January 2026 – After extended investigation of the SLB issue, we determined mitigation steps for the residual impact.\\\\n01:23 UTC on 11 January 2026 - We made a manual configuration update to the SLB services to resume proper programming and mitigate residual impact to newly created or updated VMs.\",\"mitigation\":\"Improve alerting for specific rack infrastructure impacted by such datacenter issues. (Estimated completion: TBD)\\\\nImprove our standard operating procedures, troubleshooting guides, and escalation workflows to reduce the time to mitigate residual impact. (Estimated completion: TBD)\\\\nImprove automated recovery of SLB infrastructure services impacted following localized infrastructure interruptions such as this one. (Estimated completion: TBD)\\\\nThis is our Preliminary PIR to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a Final PIR with additional details.\",\"customer_guidance\":\"Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations:https://docs.microsoft.com/azure/availability-zones/az-overview\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources — for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, ensure the right people in your organization are notified about future service issues by configuring Azure Service Health alerts (email, SMS, push notifications, webhooks, and more):https://aka.ms/ash-alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/XM22-5_G\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><em>This is our Preliminary PIR to share what we know so far. After our internal retrospective is completed (generally within\\xa014 days)\\xa0we\\xa0will publish a Final PIR with\\xa0additional\\xa0details.</em>\\xa0</p><p><em>During this incident, we temporarily used our public Azure Status page because it\\xa0was taking\\xa0too long to\\xa0determine\\xa0exactly which customers, regions, or services were affected. Since the issue has been mitigated and customers who were\\xa0impacted\\xa0can read relevant communications in Azure Service Health, this public post will be removed\\xa0once the\\xa0Final PIR is published.\\xa0For details on how this public Status page is used, refer to\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/StatusPageCriteria\\\\\">https://aka.ms/StatusPageCriteria</a>.</em></p><p><br/></p><p><strong>What happened?</strong></p><p>Between 17:50 UTC on 10 January 2026 and 01:23 UTC on 11 January 2026, customers with resources hosted in the West US 2 region may have experienced intermittent connectivity issues, timeouts, increased error rates, or delays when performing operations on affected resources. This issue was due to a power interruption affecting infrastructure within a single Availability Zone (AZ01) within the West US 2 region, which resulted in some infrastructure being temporarily unavailable.</p><p>Compute and storage infrastructure recovered by 19:51 UTC on 10 January 2026, however residual impact to newly created\\xa0Virtual Machines\\xa0(or VMs updated during the impact timeframe) in this AZ may have remained until 01:23 UTC on 11 January 2026.\\xa0Impacted services included, but may not have been limited to: Azure Cache for Redis, Azure Cosmos DB, Azure Data Explorer, Azure Database for PostgreSQL, Azure Databricks, Azure Synapse Analytics, Azure Service Bus, Azure SQL Database, Azure Storage, and Azure Virtual Machines (VMs).</p><p>Note that the \\'logical\\' Availability Zones used by each customer subscription may correspond to different physical Availability Zones. Customers can use the Locations API to understand this mapping, to confirm which resources run in this physical AZ: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://learn.microsoft.com/rest/api/resources/subscriptions/list-locations?HTTP#availabilityzonemappings\\\\\">https://learn.microsoft.com/rest/api/resources/subscriptions/list-locations?HTTP#availabilityzonemappings</a>.</p><p><strong>What went wrong and why?</strong></p><p>An emergency ‘power off’ safety system in a portion of the impacted datacenter activated unexpectedly, removing power to a subset of compute and storage infrastructure racks, contained within a single Availability Zone in the West US 2 region. This sudden loss of power caused the affected hardware to shut down, resulting in interruptions for services dependent on those resources. The activation occurred within a confined portion of the datacenter’s electrical path, and the loss of power caused downstream infrastructure to become unavailable until power was restored and systems recovered.\\xa0</p><p>This power down event also impacted key infrastructure underlying the Azure Networking stack that is supported by multiple networking components, particularly the Azure Software Load Balancer (SLB) platform. SLB services did not fully recover when compute and storage resources initially mitigated, which caused newly deployed or updated VMs within this Availability Zone (on hosts managed by impacted SLB resources) to have continued intermittent outbound connectivity issues. Services reliant on these VMs may have experienced further issues until residual impact mitigated. Extended investigation was needed to fully understand how to resolve the issues impacting the programming of network configurations by SLB services.</p><p><strong>How did we respond?</strong></p><ul><li>17:50 UTC on 10 January 2026 – Customer impact began.</li><li>17:59 UTC on 10 January 2026 – Initial monitoring alerted us to the issue and engaged engineers and onsite teams. Automated recovery operations for some services were ongoing.</li><li>18:09 UTC on 10 January 2026 – We identified the power interruption.</li><li>18:20 UTC on 10 January 2026 – Recovery efforts underway to re-energize impacted infrastructure.</li><li>19:25 UTC on 10 January 2026 – Datacenter infrastructure was re-energized and services began to recover including storage and compute resources on which other services rely. We continued to monitor for full recovery of other services.</li><li>19:51 UTC on 10 January 2026 – Mitigation was declared for compute and storage infrastructure. Residual impact remained for some services reliant on VM creation operations due to the SLB issue which was still under investigation.</li><li>00:00 UTC on 11 January 2026 – After extended investigation of the SLB issue, we determined mitigation steps for the residual impact.</li><li>01:23 UTC on 11 January 2026 - We made a manual configuration update to the SLB services to resume proper programming and mitigate residual impact to newly created or updated VMs.</li></ul><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><ul><li>Improve alerting for specific rack infrastructure impacted by such datacenter issues. (Estimated completion: TBD)</li><li>Improve our standard operating procedures, troubleshooting guides, and escalation workflows to reduce the time to mitigate residual impact. (Estimated completion: TBD)</li><li>Improve automated recovery of SLB infrastructure services impacted following localized infrastructure interruptions such as this one. (Estimated completion: TBD)</li><li><em>This is our Preliminary PIR to share what we know so far. After our internal retrospective is completed (generally within 14 days) we will publish a Final PIR with additional details.</em></li></ul><p><strong>How can customers make incidents like this less impactful?</strong></p><ul><li>Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://docs.microsoft.com/azure/availability-zones/az-overview\\\\\">https://docs.microsoft.com/azure/availability-zones/az-overview</a></li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></li><li>The impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources — for guidance on implementing monitoring to understand granular impact:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></li><li>Finally, ensure the right people in your organization are notified about future service issues by configuring Azure Service Health alerts (email, SMS, push notifications, webhooks, and more):\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/ash-alerts\\\\\">https://aka.ms/ash-alerts</a></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/XM22-5_G\\\\\">https://aka.ms/AzPIR/XM22-5_G</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"azure\",\"month\":\"December 2025\",\"date\":\"2025-12-22\",\"tracking_id\":\"FV31-PQG\",\"title\":\"Post Incident Review (PIR) – Entra Privileged Identity Management – Customers experiencing API failures\",\"text\":\"Watch our \\'Azure Incident Retrospective\\' video about this incident:\\\\nhttps://aka.ms/air/FV31-PQG\\\\nWhat happened?\\\\nBetween 08:05 UTC and 18:30 UTC on 22 December 2025, the Microsoft Entra Privileged Identity Management (PIM) service experienced issues that impacted role activations for a subset of customers. Some requests returned server errors (5xx) and, for a portion of customers, some requests returned unexpected client errors (4xx). This issue manifested as API failures, elevated latency, and various activation errors.\\\\nImpacted actions could have included:\\\\nReading eligible or active role assignments\\\\nEligible or Active Role assignment management\\\\nPIM Policy and PIM Alerts management\\\\nActivation of Eligible or Active Role assignments\\\\nDeactivation of role assignments initiated by customer (deactivations triggered by the expiration of previous activation were not impacted)\\\\nApproval of role assignment activation or extension\\\\nCustomer initiated PIM operations from various Microsoft management portals, mobile app, and API calls were likely impacted. During this incident period, operations that were retried may have succeeded.\\\\nWhat went wrong and why?\\\\nUnder normal conditions, Privileged Identity Management processes requests—such as role activations—by coordinating activity across its front-end APIs, traffic routing layer, and the backend databases that store role activation information. These components work together to route requests to healthy endpoints, complete required checks, and process activations without delay. To support this flow, the system maintains a pool of active connections for responsiveness and automatically retries brief interruptions to keep error rates low, helping ensure customers experience reliable access when reading, managing, or activating role assignments.\\\\nAs part of ongoing work to improve how the PIM service stores and manages data, configuration changes which manage the backend databases were deployed incrementally using safe deployment practices. The rollout progressed successfully through multiple rings, with no detected errors and all monitored signals remaining within healthy thresholds.\\\\nWhen the deployment reached a ring operating under higher workload, the additional per-request processing increased demand on the underlying infrastructure that hosts the API which manages connections to the backend databases. Although the service includes throttling mechanisms designed to protect against spikes in API traffic, this scenario led to elevated CPU utilization without exceeding request-count thresholds, so throttling did not engage. Over time, the sustained load caused available database connections to be exhausted, and the service became unable to process new requests efficiently. This resulted in delays, timeouts, and errors for customers attempting to view or activate privileged roles.\\\\nHow did we respond?\\\\n21:36 UTC on 15 December 2025 – The configuration change deployment was initiated.\\\\n22:08 UTC on 19 December 2025 – The configuration change was progressed to the ring with the heavy workload.\\\\n08:05 UTC on 22 December 2025 – Initial customer impact began.\\\\n08:26 UTC on 22 December 2025 – Automated alerts received for intermittent, low volume of errors prompting us to start investigating.\\\\n10:30 UTC on 22 December 2025 – We attempted isolated restarts on impacted database instances in an effort to mitigate low-level impact.\\\\n13:03 UTC on 22 December 2025 – Automated monitoring alerted us to elevated error rates and a full incident response was initiated.\\\\n13:22 UTC on 22 December 2025 – We identified that calls to the database were intermittently timing out. Traffic volume appeared to be normal with no significant surge detected however we observed the spike in CPU utilization.\\\\n13:54 UTC on 22 December 2025 – Mitigation efforts began, including beginning to scale out the impacted environments.\\\\n15:05 UTC on 22 December 2025 – Scale out efforts were observed as decreasing error rates but not completely eliminating failures. Further instance restarts provided temporary relief.\\\\n15:25 UTC on 22 December 2025 – Scaling efforts continued. We engaged our database engineering team to help investigate.\\\\n16:37 UTC on 22 December 2025 – While we did not correlate the deployment to this incident, we initiated a rollback of the configuration change.\\\\n17:20 UTC on 22 December 2025 – Scale-out efforts completed.\\\\n17:45 UTC on 22 December 2025 – Service availability telemetry was showing improvements. Some customers began to report recovery.\\\\n18:30 UTC on 22 December 2025 – Customer impact confirmed as mitigated, after rollback of configuration change had completed and error rates had returned to normal levels.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nWe have rolled back the problematic configuration change across all regions. (Completed)\\\\nFor outages that manifest later as a result of configuration updates, we are developing a mechanism to help engineers correlate these signals more quickly. (Estimated completion: January 2026)\\\\nWe are working to ensure this configuration change will not inadvertently introduce excessive load before we redeploy this again. (Estimated completion: January 2026)\\\\nWe are working on updating our auto-scale configuration to be more responsive to changes in CPU usage. (Estimated completion: January 2026)\\\\nWe are enabling monitoring and runbooks for available database connections to respond to emerging issues sooner. (Estimated completion: February 2026)\\\\nHow can customers make incidents like this less impactful?\\\\nConsider retrying critical operations directly through the API, for example, when requesting just in time access:\\\\nhttps://learn.microsoft.com/entra/id-governance/privileged-identity-management/pim-apis#iteration-3-current--pim-for-microsoft-entra-roles-groups-in-microsoft-graph-api-and-for-azure-resources-in-azure-resource-manager-api\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\\\nhttps://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/FV31-PQG\",\"sections\":{\"what_happened\":\"Between 08:05 UTC and 18:30 UTC on 22 December 2025, the Microsoft Entra Privileged Identity Management (PIM) service experienced issues that impacted role activations for a subset of customers. Some requests returned server errors (5xx) and, for a portion of customers, some requests returned unexpected client errors (4xx). This issue manifested as API failures, elevated latency, and various activation errors.\\\\nImpacted actions could have included:\\\\nReading eligible or active role assignments\\\\nEligible or Active Role assignment management\\\\nPIM Policy and PIM Alerts management\\\\nActivation of Eligible or Active Role assignments\\\\nDeactivation of role assignments initiated by customer (deactivations triggered by the expiration of previous activation were not impacted)\\\\nApproval of role assignment activation or extension\\\\nCustomer initiated PIM operations from various Microsoft management portals, mobile app, and API calls were likely impacted. During this incident period, operations that were retried may have succeeded.\",\"what_went_wrong\":\"Under normal conditions, Privileged Identity Management processes requests—such as role activations—by coordinating activity across its front-end APIs, traffic routing layer, and the backend databases that store role activation information. These components work together to route requests to healthy endpoints, complete required checks, and process activations without delay. To support this flow, the system maintains a pool of active connections for responsiveness and automatically retries brief interruptions to keep error rates low, helping ensure customers experience reliable access when reading, managing, or activating role assignments.\\\\nAs part of ongoing work to improve how the PIM service stores and manages data, configuration changes which manage the backend databases were deployed incrementally using safe deployment practices. The rollout progressed successfully through multiple rings, with no detected errors and all monitored signals remaining within healthy thresholds.\\\\nWhen the deployment reached a ring operating under higher workload, the additional per-request processing increased demand on the underlying infrastructure that hosts the API which manages connections to the backend databases. Although the service includes throttling mechanisms designed to protect against spikes in API traffic, this scenario led to elevated CPU utilization without exceeding request-count thresholds, so throttling did not engage. Over time, the sustained load caused available database connections to be exhausted, and the service became unable to process new requests efficiently. This resulted in delays, timeouts, and errors for customers attempting to view or activate privileged roles.\",\"how_did_we_respond\":\"21:36 UTC on 15 December 2025 – The configuration change deployment was initiated.\\\\n22:08 UTC on 19 December 2025 – The configuration change was progressed to the ring with the heavy workload.\\\\n08:05 UTC on 22 December 2025 – Initial customer impact began.\\\\n08:26 UTC on 22 December 2025 – Automated alerts received for intermittent, low volume of errors prompting us to start investigating.\\\\n10:30 UTC on 22 December 2025 – We attempted isolated restarts on impacted database instances in an effort to mitigate low-level impact.\\\\n13:03 UTC on 22 December 2025 – Automated monitoring alerted us to elevated error rates and a full incident response was initiated.\\\\n13:22 UTC on 22 December 2025 – We identified that calls to the database were intermittently timing out. Traffic volume appeared to be normal with no significant surge detected however we observed the spike in CPU utilization.\\\\n13:54 UTC on 22 December 2025 – Mitigation efforts began, including beginning to scale out the impacted environments.\\\\n15:05 UTC on 22 December 2025 – Scale out efforts were observed as decreasing error rates but not completely eliminating failures. Further instance restarts provided temporary relief.\\\\n15:25 UTC on 22 December 2025 – Scaling efforts continued. We engaged our database engineering team to help investigate.\\\\n16:37 UTC on 22 December 2025 – While we did not correlate the deployment to this incident, we initiated a rollback of the configuration change.\\\\n17:20 UTC on 22 December 2025 – Scale-out efforts completed.\\\\n17:45 UTC on 22 December 2025 – Service availability telemetry was showing improvements. Some customers began to report recovery.\\\\n18:30 UTC on 22 December 2025 – Customer impact confirmed as mitigated, after rollback of configuration change had completed and error rates had returned to normal levels.\",\"mitigation\":\"We have rolled back the problematic configuration change across all regions. (Completed)\\\\nFor outages that manifest later as a result of configuration updates, we are developing a mechanism to help engineers correlate these signals more quickly. (Estimated completion: January 2026)\\\\nWe are working to ensure this configuration change will not inadvertently introduce excessive load before we redeploy this again. (Estimated completion: January 2026)\\\\nWe are working on updating our auto-scale configuration to be more responsive to changes in CPU usage. (Estimated completion: January 2026)\\\\nWe are enabling monitoring and runbooks for available database connections to respond to emerging issues sooner. (Estimated completion: February 2026)\",\"customer_guidance\":\"Consider retrying critical operations directly through the API, for example, when requesting just in time access:https://learn.microsoft.com/entra/id-governance/privileged-identity-management/pim-apis#iteration-3-current--pim-for-microsoft-entra-roles-groups-in-microsoft-graph-api-and-for-azure-resources-in-azure-resource-manager-api\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:https://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/FV31-PQG\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><em>Watch our \\'Azure Incident Retrospective\\' video about this incident: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/air/FV31-PQG\\\\\">https://aka.ms/air/FV31-PQG</a></em></p><p><strong>What happened?</strong></p><p>Between 08:05 UTC and 18:30 UTC on 22 December 2025, the Microsoft Entra Privileged Identity Management (PIM) service experienced issues that impacted role activations for a subset of customers. Some requests returned server errors (5xx) and, for a portion of customers, some requests returned unexpected client errors (4xx). This issue manifested as API failures, elevated latency, and various activation errors.</p><p>Impacted actions could have included:</p><ul><li>Reading eligible or active role assignments</li><li>Eligible or Active Role assignment management</li><li>PIM Policy and PIM Alerts management</li><li>Activation of Eligible or Active Role assignments</li><li>Deactivation of role assignments initiated by customer (deactivations triggered by the expiration of previous activation were not impacted)</li><li>Approval of role assignment activation or extension</li></ul><p>Customer initiated PIM operations from various Microsoft management portals, mobile app, and API calls were likely impacted. During this incident period, operations that were retried may have succeeded.</p><p><strong>What went wrong and why? </strong></p><p>Under normal conditions, Privileged Identity Management processes requests—such as role activations—by coordinating activity across its front-end APIs, traffic routing layer, and the backend databases that store role activation information. These components work together to route requests to healthy endpoints, complete required checks, and process activations without delay. To support this flow, the system maintains a pool of active connections for responsiveness and automatically retries brief interruptions to keep error rates low, helping ensure customers experience reliable access when reading, managing, or activating role assignments.</p><p>As part of ongoing work to improve how the PIM service stores and manages data, configuration changes which manage the backend databases were deployed incrementally using safe deployment practices. The rollout progressed successfully through multiple rings, with no detected errors and all monitored signals remaining within healthy thresholds.</p><p>When the deployment reached a ring operating under higher workload, the additional per-request processing increased demand on the underlying infrastructure that hosts the API which manages connections to the backend databases. Although the service includes throttling mechanisms designed to protect against spikes in API traffic, this scenario led to elevated CPU utilization without exceeding request-count thresholds, so throttling did not engage. Over time, the sustained load caused available database connections to be exhausted, and the service became unable to process new requests efficiently. This resulted in delays, timeouts, and errors for customers attempting to view or activate privileged roles.</p><p><strong>How did we respond?</strong></p><ul><li>21:36 UTC on 15 December 2025 – The configuration change deployment was initiated.</li><li>22:08 UTC on 19 December 2025 – The configuration change was progressed to the ring with the heavy workload.</li><li>08:05 UTC on 22 December 2025 – Initial customer impact began.</li><li>08:26 UTC on 22 December 2025 – Automated alerts received for intermittent, low volume of errors prompting us to start investigating.</li><li>10:30 UTC on 22 December 2025 – We attempted isolated restarts on impacted database instances in an effort to mitigate low-level impact.</li><li>13:03 UTC on 22 December 2025 – Automated monitoring alerted us to elevated error rates and a full incident response was initiated.</li><li>13:22 UTC on 22 December 2025 – We identified that calls to the database were intermittently timing out. Traffic volume appeared to be normal with no significant surge detected however we observed the spike in CPU utilization.</li><li>13:54 UTC on 22 December 2025 – Mitigation efforts began, including beginning to scale out the impacted environments.</li><li>15:05 UTC on 22 December 2025 – Scale out efforts were observed as decreasing error rates but not completely eliminating failures. Further instance restarts provided temporary relief.</li><li>15:25 UTC on 22 December 2025 – Scaling efforts continued. We engaged our database engineering team to help investigate.</li><li>16:37 UTC on 22 December 2025 – While we did not correlate the deployment to this incident, we initiated a rollback of the configuration change.</li><li>17:20 UTC on 22 December 2025 – Scale-out efforts completed.</li><li>17:45 UTC on 22 December 2025 – Service availability telemetry was showing improvements. Some customers began to report recovery.</li><li>18:30 UTC on 22 December 2025 – Customer impact confirmed as mitigated, after rollback of configuration change had completed and error rates had returned to normal levels.</li></ul><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><ul><li>We have rolled back the problematic configuration change across all regions. (Completed)</li><li>For outages that manifest later as a result of configuration updates, we are developing a mechanism to help engineers correlate these signals more quickly. (Estimated completion: January 2026)</li><li>We are working to ensure this configuration change will not inadvertently introduce excessive load before we redeploy this again. (Estimated completion: January 2026)</li><li>We are working on updating our auto-scale configuration to be more responsive to changes in CPU usage. (Estimated completion: January 2026)</li><li>We are enabling monitoring and runbooks for available database connections to respond to emerging issues sooner. (Estimated completion: February 2026)</li></ul><p><strong>How can customers make incidents like this less impactful?</strong></p><ul><li>Consider retrying critical operations directly through the API, for example, when requesting just in time access: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://learn.microsoft.com/entra/id-governance/privileged-identity-management/pim-apis#iteration-3-current--pim-for-microsoft-entra-roles-groups-in-microsoft-graph-api-and-for-azure-resources-in-azure-resource-manager-api\\\\\">https://learn.microsoft.com/entra/id-governance/privileged-identity-management/pim-apis#iteration-3-current--pim-for-microsoft-entra-roles-groups-in-microsoft-graph-api-and-for-azure-resources-in-azure-resource-manager-api</a></li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></li><li>The impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></li><li>Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Alerts\\\\\">https://aka.ms/AzPIR/Alerts</a></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/FV31-PQG\\\\\">https://aka.ms/AzPIR/FV31-PQG</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"azure\",\"month\":\"December 2025\",\"date\":\"2025-12-08\",\"tracking_id\":\"ML7_-DWG\",\"title\":\"Post Incident Review (PIR) – Azure Resource Manager – Service management failures affecting Azure Government\",\"text\":\"What happened?\\\\nBetween 11:04 and 14:13 EST on 08 December 2025, customers using any of the Azure Government regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.\\\\nAffected services included but were not limited to: Azure App Service, Azure Backup, Azure Communication Services, Azure Data Factory, Azure Databricks, Azure Functions, Azure Kubernetes Service, Azure Maps, Azure Migrate, Azure NetApp Files, Azure OpenAI Service, Azure Policy (including Machine Configuration), Azure Resource Manager, Azure Search, Azure Service Bus, Azure Site Recovery, Azure Storage, Azure Virtual Desktop, Microsoft Fabric, and Microsoft Power Platform (including AI Builder and Power Automate).\\\\nWhat went wrong and why?\\\\nAzure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure Government cloud were affected.\\\\nMicrosoft services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the Microsoft internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.\\\\nEarlier on the same day of this incident, a similar key rotation issue affected services in the Azure in China sovereign cloud. Both the Azure Government and Azure in China sovereign cloud had their (separate but equivalent) keys created on the same day, starting completely independent timers, back in February 2025 – so each was inadvertently rotated on their respective timers, approximately three hours apart. As such, the key used by ARM for the Azure Government regions was automatically rotated, before the same key rotation issue affecting the Azure in China regions was fully mitigated. Although potential impact to other sovereign clouds was discussed as part of the initial investigation, we did not have a sufficient understanding of the inadvertent key rotation to be able to prevent impact in the second sovereign cloud, Azure Government.\\\\nHow did we respond?\\\\n11:04 EST on 08 December 2025 – Customer impact began.\\\\n11:07 EST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.\\\\n11:38 EST on 08 December 2025 – We began applying a fix for the impacted authentication components.\\\\n13:58 EST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.\\\\n14:13 EST on 08 December 2025 – All customer impact confirmed as mitigated.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nFirst and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)\\\\nOur internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)\\\\nWe will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)\\\\nOur Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)\\\\nHow can customers make incidents like this less impactful?\\\\nThere was nothing that customers could have done to avoid or minimize impact from this specific service incident.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\\\nhttps://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/ML7_-DWG\",\"sections\":{\"what_happened\":\"Between 11:04 and 14:13 EST on 08 December 2025, customers using any of the Azure Government regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.\\\\nAffected services included but were not limited to: Azure App Service, Azure Backup, Azure Communication Services, Azure Data Factory, Azure Databricks, Azure Functions, Azure Kubernetes Service, Azure Maps, Azure Migrate, Azure NetApp Files, Azure OpenAI Service, Azure Policy (including Machine Configuration), Azure Resource Manager, Azure Search, Azure Service Bus, Azure Site Recovery, Azure Storage, Azure Virtual Desktop, Microsoft Fabric, and Microsoft Power Platform (including AI Builder and Power Automate).\",\"what_went_wrong\":\"Azure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure Government cloud were affected.\\\\nMicrosoft services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the Microsoft internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.\\\\nEarlier on the same day of this incident, a similar key rotation issue affected services in the Azure in China sovereign cloud. Both the Azure Government and Azure in China sovereign cloud had their (separate but equivalent) keys created on the same day, starting completely independent timers, back in February 2025 – so each was inadvertently rotated on their respective timers, approximately three hours apart. As such, the key used by ARM for the Azure Government regions was automatically rotated, before the same key rotation issue affecting the Azure in China regions was fully mitigated. Although potential impact to other sovereign clouds was discussed as part of the initial investigation, we did not have a sufficient understanding of the inadvertent key rotation to be able to prevent impact in the second sovereign cloud, Azure Government.\",\"how_did_we_respond\":\"11:04 EST on 08 December 2025 – Customer impact began.\\\\n11:07 EST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.\\\\n11:38 EST on 08 December 2025 – We began applying a fix for the impacted authentication components.\\\\n13:58 EST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.\\\\n14:13 EST on 08 December 2025 – All customer impact confirmed as mitigated.\",\"mitigation\":\"First and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)\\\\nOur internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)\\\\nWe will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)\\\\nOur Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)\",\"customer_guidance\":\"There was nothing that customers could have done to avoid or minimize impact from this specific service incident.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:https://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/ML7_-DWG\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><strong>What happened?</strong></p><p>Between 11:04 and 14:13 EST on 08 December 2025, customers using any of the Azure Government regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.</p><p>Affected services included but were not limited to: Azure App Service, Azure Backup, Azure Communication Services, Azure Data Factory, Azure Databricks, Azure Functions, Azure Kubernetes Service, Azure Maps, Azure Migrate, Azure NetApp Files, Azure OpenAI Service, Azure Policy (including Machine Configuration), Azure Resource Manager, Azure Search, Azure Service Bus, Azure Site Recovery, Azure Storage, Azure Virtual Desktop, Microsoft Fabric, and Microsoft Power Platform (including AI Builder and Power Automate).</p><p><strong>What went wrong and why?</strong></p><p>Azure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure Government cloud were affected.</p><p>Microsoft services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the Microsoft internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.</p><p>Earlier on the same day of this incident, a similar key rotation issue affected services in the Azure in China sovereign cloud. Both the Azure Government and Azure in China sovereign cloud had their (separate but equivalent) keys created on the same day, starting completely independent timers, back in February 2025 – so each was inadvertently rotated on their respective timers, approximately three hours apart. As such, the key used by ARM for the Azure Government regions was automatically rotated, before the same key rotation issue affecting the Azure in China regions was fully mitigated. Although potential impact to other sovereign clouds was discussed as part of the initial investigation, we did not have a sufficient understanding of the inadvertent key rotation to be able to prevent impact in the second sovereign cloud, Azure Government.</p><p><strong>How did we respond?</strong></p><ul><li>11:04 EST on 08 December 2025 – Customer impact began.</li><li>11:07 EST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.</li><li>11:38 EST on 08 December 2025 – We began applying a fix for the impacted authentication components.</li><li>13:58 EST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.</li><li>14:13 EST on 08 December 2025 – All customer impact confirmed as mitigated.</li></ul><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><ul><li>First and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)</li><li>Our internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)</li><li>We will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)</li><li>Our Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)</li></ul><p><strong>How can customers make incidents like this less impactful?</strong></p><ul><li>There was nothing that customers could have done to avoid or minimize impact from this specific service incident.</li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></span></li><li>The impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></span></li><li>Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Alerts\\\\\">https://aka.ms/AzPIR/Alerts</a></span></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/ML7_-DWG\\\\\">https://aka.ms/AzPIR/ML7_-DWG</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"azure\",\"month\":\"December 2025\",\"date\":\"2025-12-08\",\"tracking_id\":\"JSNV-FBZ\",\"title\":\"Post Incident Review (PIR) – Azure Resource Manager – Service management failures affecting Azure in China\",\"text\":\"What happened?\\\\nBetween 16:50 CST on 08 December 2025 and 02:00 CST on 09 December 2025 (China Standard Time) customers using any of the Azure in China regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.\\\\nAffected services included but were not limited to: Azure AI Search, Azure API Management, Azure App Service, Azure Application Insights, Azure Arc, Azure Automation, Azure Backup, Azure Data Factory, Azure Databricks, Azure Database for PostgreSQL Flexible Server, Azure Kubernetes Service, Azure Logic Apps, Azure Managed HSM, Azure Marketplace, Azure Monitor, Azure Policy (including Machine Configuration), Azure Portal, Azure Resource Manager, Azure Site Recovery, Azure Stack HCI, Azure Stream Analytics, Azure Synapse Analytics, and Microsoft Sentinel.\\\\nWhat went wrong and why?\\\\nAzure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure in China cloud were affected.\\\\nAzure services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.\\\\nHow did we respond?\\\\n16:50 CST on 08 December 2025 – Customer impact began.\\\\n16:59 CST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.\\\\n18:37 CST on 08 December 2025 – We identified the underlying cause as the incorrect key rotation.\\\\n19:16 CST on 08 December 2025 – We identified mitigation steps and began applying a fix for the impacted authentication components. This was tested and validated before being applied.\\\\n22:00 CST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.\\\\n23:53 CST on 08 December 2025 – Many services had recovered but residual impact remained for some services.\\\\n02:00 CST on 09 December 2025 – All customer impact confirmed as mitigated.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nFirst and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)\\\\nOur internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)\\\\nWe will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)\\\\nOur Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)\\\\nHow can customers make incidents like this less impactful?\\\\nThere was nothing that customers could have done to avoid or minimize impact from this specific service incident.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\\\nhttps://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/JSNV-FBZ\",\"sections\":{\"what_happened\":\"Between 16:50 CST on 08 December 2025 and 02:00 CST on 09 December 2025 (China Standard Time) customers using any of the Azure in China regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.\\\\nAffected services included but were not limited to: Azure AI Search, Azure API Management, Azure App Service, Azure Application Insights, Azure Arc, Azure Automation, Azure Backup, Azure Data Factory, Azure Databricks, Azure Database for PostgreSQL Flexible Server, Azure Kubernetes Service, Azure Logic Apps, Azure Managed HSM, Azure Marketplace, Azure Monitor, Azure Policy (including Machine Configuration), Azure Portal, Azure Resource Manager, Azure Site Recovery, Azure Stack HCI, Azure Stream Analytics, Azure Synapse Analytics, and Microsoft Sentinel.\",\"what_went_wrong\":\"Azure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure in China cloud were affected.\\\\nAzure services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.\",\"how_did_we_respond\":\"16:50 CST on 08 December 2025 – Customer impact began.\\\\n16:59 CST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.\\\\n18:37 CST on 08 December 2025 – We identified the underlying cause as the incorrect key rotation.\\\\n19:16 CST on 08 December 2025 – We identified mitigation steps and began applying a fix for the impacted authentication components. This was tested and validated before being applied.\\\\n22:00 CST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.\\\\n23:53 CST on 08 December 2025 – Many services had recovered but residual impact remained for some services.\\\\n02:00 CST on 09 December 2025 – All customer impact confirmed as mitigated.\",\"mitigation\":\"First and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)\\\\nOur internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)\\\\nWe will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)\\\\nOur Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)\",\"customer_guidance\":\"There was nothing that customers could have done to avoid or minimize impact from this specific service incident.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:https://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/JSNV-FBZ\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><strong>What happened?</strong></p><p>Between 16:50 CST on 08 December 2025 and 02:00 CST on 09 December 2025 (China Standard Time) customers using any of the Azure in China regions may have experienced failures when attempting to perform service management operations through Azure Resource Manager (ARM). This included operations attempted through the Azure Portal, Azure REST APIs, Azure PowerShell, and Azure CLI.</p><p>Affected services included but were not limited to: Azure AI Search, Azure API Management, Azure App Service, Azure Application Insights, Azure Arc, Azure Automation, Azure Backup, Azure Data Factory, Azure Databricks, Azure Database for PostgreSQL Flexible Server, Azure Kubernetes Service, Azure Logic Apps, Azure Managed HSM, Azure Marketplace, Azure Monitor, Azure Policy (including Machine Configuration), Azure Portal, Azure Resource Manager, Azure Site Recovery, Azure Stack HCI, Azure Stream Analytics, Azure Synapse Analytics, and Microsoft Sentinel.</p><p><strong>What went wrong and why?</strong></p><p>Azure Resource Manager (ARM) is the gateway for management operations for Azure services. ARM does authorization for these operations based on authorization policies, stored in Cosmos DB accounts that are replicated to all regions. On 08 December 2025, an inadvertent automated key rotation resulted in ARM failures to fetch authorization policies that are needed to evaluate access. As a result, ARM was temporarily unable to communicate with underlying storage resources, causing failures in service-to-service communication and affecting resource management workflows across multiple Azure services. This issue surfaced as authentication failures and 500 Internal Server errors to customers across all clients. Because the content of the Cosmos DB accounts for authorization policies is replicated globally, all regions within the Azure in China cloud were affected.</p><p>Azure services use an internal system to manage keys and secrets, which also makes it easy to perform regular needed maintenance activities, such as rotating secrets. Protecting identities and secrets is a key pillar in our Secure Future Initiative to reduce risk, enhance operational maturity, and proactively prepare for emerging threats to identity infrastructure – by prioritizing secure authentication and robust key management. In this case, our ARM service was using a key in a ‘manual mode’ which means that any key rotations would need to be manually coordinated, so that traffic could be moved to use a different key before the key could be regenerated. The Cosmos DBs that ARM use for accessing authorization policies was intentionally onboarded to the internal service which governs the account key lifecycle, but unintentionally configured with the option to automatically rotate the keys enabled. This automated rotation should have been disabled as part of the onboarding process, until such time as it was ready to be fully automated.</p><p><strong>How did we respond?</strong></p><ul><li>16:50 CST on 08 December 2025 – Customer impact began.</li><li>16:59 CST on 08 December 2025 – Engineering was engaged to investigate based on automated alerts.</li><li>18:37 CST on 08 December 2025 – We identified the underlying cause as the incorrect key rotation.</li><li>19:16 CST on 08 December 2025 – We identified mitigation steps and began applying a fix for the impacted authentication components. This was tested and validated before being applied.</li><li>22:00 CST on 08 December 2025 – We began to restart ARM instances, to speed up the mitigation process.</li><li>23:53 CST on 08 December 2025 – Many services had recovered but residual impact remained for some services.</li><li>02:00 CST on 09 December 2025 – All customer impact confirmed as mitigated.</li></ul><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><ul><li>First and foremost, our ARM team have conducted an audit to ensure that there are no other manual keys that are misconfigured to be auto-rotated, across all clouds. (Completed)</li><li>Our internal secret management system has paused automated key rotations for managed keys, until usage signals are made available on key usage – see the Cosmos DB change safety repair item below. (Completed)</li><li>We will complete the migration to auto-rotated Cosmos DB account keys for ARM authentication accounts, across all clouds. (Estimated completion: February 2026)</li><li>Our Cosmos DB team will introduce change safety controls that block regenerating keys that have usage, by emitting a relevant usage signal. (Estimated completion: Public Preview by April 2026, General Availability to follow by August 2026)</li></ul><p><strong>How can customers make incidents like this less impactful?</strong></p><ul><li>There was nothing that customers could have done to avoid or minimize impact from this specific service incident.\\xa0</li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></span></li><li>The impact times above\\xa0represent\\xa0the full\\xa0incident duration,\\xa0so\\xa0are not specific to any individual customer. Actual impact to service availability varied\\xa0between customers and resources\\xa0– for guidance on implementing monitoring to understand granular impact:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></span></li><li>Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\xa0<span><a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Alerts\\\\\">https://aka.ms/AzPIR/Alerts</a></span></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/JSNV-FBZ\\\\\">https://aka.ms/AzPIR/JSNV-FBZ</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"azure\",\"month\":\"November 2025\",\"date\":\"2025-11-05\",\"tracking_id\":\"2LGD-9VG\",\"title\":\"Post Incident Review (PIR) – Thermal event impacting multiple services in AZ01 – West Europe\",\"text\":\"Watch our \\'Azure Incident Retrospective\\' video about this incident:\\\\nhttps://aka.ms/air/2LGD-9VG\\\\nWhat happened?\\\\nBetween approximately 16:53 UTC on 5 November 2025 and 02:25 UTC on 6 November 2025, a subset of customers in the West Europe region experienced service disruptions or degraded performance across multiple services, including Virtual Machines (VM), Azure Database for PostgreSQL Flexible Server, MySQL Flexible Server, Azure Kubernetes Service, Storage, Service Bus, and Virtual Machine Scale Sets, among others. Customers using Azure Databricks in West Europe observed degraded performance when launching or scaling all-purpose and job compute workloads, which impacted Unity Catalog and Databricks SQL operations.\\\\nWhat went wrong and why?\\\\nThe incident was triggered by a voltage sag in the utility grid, which caused cooling units to shut down and temperatures to rise above normal thresholds in a datacenter located in Physical Availability Zone 01 in the West Europe region. As temperature rose, multiple storage scale units automatically powered down to prevent hardware damage. Under normal circumstances, cooling systems are designed and commissioned to automatically restart after power events and are built with sufficient redundancy to maintain safe operating conditions even when failures occur. While cooling unit auto-restart capability had been regularly tested and exercised in this datacenter, the specific size and duration of utility sag exposed a previously unknown hardware issue that prevented the cooling units from restarting. This resulted in temperatures exceeding safe operational limits for infrastructure.\\\\nThe extended recovery time was influenced by several factors. Recovery of storage scale units was prolonged because some storage servers entered a degraded state, necessitating sequential validation before they could be brought back online. In addition, dependent services such as compute and networking could not resume until storage integrity checks were complete. Finally, additional thermal audits were performed before workloads were reactivated to ensure that no residual risk remained.\\\\nHow did we respond?\\\\nAutomated monitoring\\xa0detected the temperature anomaly and triggered an incident response.\\xa0Facilities teams restored safe operating conditions by performing\\xa0hard\\xa0reset on the affected units and confirmed that temperatures were trending back to safe levels.\\xa0Service recovery efforts began once the facility had reached safe operating temperatures.\\\\nFollowing this, storage systems\\xa0initiated comprehensive data consistency checks when nodes in a storage scale unit restarted after the unplanned shutdown. These checks, which may include reconstructing replicas that have fallen behind.\\xa0The recovery process prioritizes data integrity and durability over availability, resulting in customer traffic being blocked until all integrity validations was completed and contributing to the extended recovery time. In parallel, engineers prioritized recovery of compute hosts to stop further VM failures and allow dependent services to resume.\\\\nTimeline of events:\\\\n16:53 UTC on\\xa05 November 2025\\xa0– Customer impact began as elevated temperatures caused multiple storage scale units to shut down.\\\\n16:55 UTC on\\xa05 November 2025\\xa0– Datacenter monitoring detected temperature breaches and triggered thermal alerts.\\\\n17:20 UTC\\xa0on\\xa05 November 2025\\xa0– Power sag and cooling unit failure to restart identified as contributing factors. Cooling recovery begins as engineers manually restart cooling units.\\\\n17:40 UTC\\xa0on\\xa05 November 2025\\xa0– Mitigation workstream started as cooling restoration progresses.\\\\n17:48 UTC\\xa0on\\xa05 November 2025\\xa0– All cooling units restored, we start seeing temperatures reducing as cooling normalized.\\\\n17:50 UTC\\xa0on\\xa05 November 2025\\xa0– Rack-level thermal monitoring returned to safe operational thresholds.\\xa0Engineering teams initiated storage recovery.\\\\n18:30 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Sequential storage scale unit validation began. Each scale unit underwent integrity checks to ensure no data corruption before bringing storage services back into rotation.\\\\n20:00 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Gradual restoration of storage scale units. Recovery was staged to avoid overloading power and cooling systems. Dependent compute nodes remained offline during this time.\\\\n23:30 UTC\\xa0on\\xa05 November 2025\\xa0– Service dependency checks and orchestration. Networking and compute services were progressively re-enabled after storage scale units passed health checks.\\\\n02:25 UTC\\xa0on\\xa06 November 2025\\xa0–\\xa0All services\\xa0were brought back online, and customer impact was mitigated.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nWe are implementing several improvements to strengthen resilience and accelerate recovery:\\\\nWe are conducting internal and manufacturer-led forensic investigations of specific device hardware and firmware to develop solutions to bolster component tolerance to power sags and voltage fluctuations. (Estimated completion: December 2025)\\\\nWe are performing a gap analysis of the control circuit to ensure all relays delivering critical function of the units are powered through Uninterruptible Power Supply (UPS) and stored energy sources. (Estimated completion: January 2026)\\\\nWe are validating our maintenance procedures and adding additional prechecks for Heating, Ventilation, and Air Conditioning (HVAC) subsystems. (Estimated completion: January 2026)\\\\nWe are optimizing recovery steps for compute and storage systems to reduce overall restoration time. (Estimated completion: January 2026)\\\\nWe are improving automated correlation between environmental signals and service health to trigger earlier, and more targeted escalations. (Estimated completion: January 2026)\\\\nWe are updating our cooling unit\\xa0commissioning\\xa0playbooks\\xa0to\\xa0expand range of\\xa0power sag and swell\\xa0scenarios.\\xa0(Estimated completion:\\xa0February\\xa02026)\\\\nIn the longer term, we are implementing a new approach to accelerate service restoration after major incidents. Historically, recovery prioritized full data integrity checks before bringing services online, which extended downtime because it was a serial process. Going forward, integrity checks will run in the background during VM and disk mount operations, allowing availability to return much sooner without compromising data safety. This change is a direct outcome of this incident and represents a significant improvement in recovery speed.\\xa0(Estimated completion: April 2026)\\\\nHow can customers make incidents like this less impactful?\\\\nConsider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations:\\\\nhttps://docs.microsoft.com/azure/availability-zones/az-overview\\\\nPlan for regional redundancy by using active/active or failover designs across paired regions.\\\\nMaintain regular backups and test failover procedures to ensure recovery readiness.\\\\nIncrease resilience by using retry logic and backoff strategies to prevent overload during recovery.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\\\nhttps://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/2LGD-9VG\",\"sections\":{\"what_happened\":\"Between approximately 16:53 UTC on 5 November 2025 and 02:25 UTC on 6 November 2025, a subset of customers in the West Europe region experienced service disruptions or degraded performance across multiple services, including Virtual Machines (VM), Azure Database for PostgreSQL Flexible Server, MySQL Flexible Server, Azure Kubernetes Service, Storage, Service Bus, and Virtual Machine Scale Sets, among others. Customers using Azure Databricks in West Europe observed degraded performance when launching or scaling all-purpose and job compute workloads, which impacted Unity Catalog and Databricks SQL operations.\",\"what_went_wrong\":\"The incident was triggered by a voltage sag in the utility grid, which caused cooling units to shut down and temperatures to rise above normal thresholds in a datacenter located in Physical Availability Zone 01 in the West Europe region. As temperature rose, multiple storage scale units automatically powered down to prevent hardware damage. Under normal circumstances, cooling systems are designed and commissioned to automatically restart after power events and are built with sufficient redundancy to maintain safe operating conditions even when failures occur. While cooling unit auto-restart capability had been regularly tested and exercised in this datacenter, the specific size and duration of utility sag exposed a previously unknown hardware issue that prevented the cooling units from restarting. This resulted in temperatures exceeding safe operational limits for infrastructure.\\\\nThe extended recovery time was influenced by several factors. Recovery of storage scale units was prolonged because some storage servers entered a degraded state, necessitating sequential validation before they could be brought back online. In addition, dependent services such as compute and networking could not resume until storage integrity checks were complete. Finally, additional thermal audits were performed before workloads were reactivated to ensure that no residual risk remained.\",\"how_did_we_respond\":\"Automated monitoring\\xa0detected the temperature anomaly and triggered an incident response.\\xa0Facilities teams restored safe operating conditions by performing\\xa0hard\\xa0reset on the affected units and confirmed that temperatures were trending back to safe levels.\\xa0Service recovery efforts began once the facility had reached safe operating temperatures.\\\\nFollowing this, storage systems\\xa0initiated comprehensive data consistency checks when nodes in a storage scale unit restarted after the unplanned shutdown. These checks, which may include reconstructing replicas that have fallen behind.\\xa0The recovery process prioritizes data integrity and durability over availability, resulting in customer traffic being blocked until all integrity validations was completed and contributing to the extended recovery time. In parallel, engineers prioritized recovery of compute hosts to stop further VM failures and allow dependent services to resume.\\\\nTimeline of events:\\\\n16:53 UTC on\\xa05 November 2025\\xa0– Customer impact began as elevated temperatures caused multiple storage scale units to shut down.\\\\n16:55 UTC on\\xa05 November 2025\\xa0– Datacenter monitoring detected temperature breaches and triggered thermal alerts.\\\\n17:20 UTC\\xa0on\\xa05 November 2025\\xa0– Power sag and cooling unit failure to restart identified as contributing factors. Cooling recovery begins as engineers manually restart cooling units.\\\\n17:40 UTC\\xa0on\\xa05 November 2025\\xa0– Mitigation workstream started as cooling restoration progresses.\\\\n17:48 UTC\\xa0on\\xa05 November 2025\\xa0– All cooling units restored, we start seeing temperatures reducing as cooling normalized.\\\\n17:50 UTC\\xa0on\\xa05 November 2025\\xa0– Rack-level thermal monitoring returned to safe operational thresholds.\\xa0Engineering teams initiated storage recovery.\\\\n18:30 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Sequential storage scale unit validation began. Each scale unit underwent integrity checks to ensure no data corruption before bringing storage services back into rotation.\\\\n20:00 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Gradual restoration of storage scale units. Recovery was staged to avoid overloading power and cooling systems. Dependent compute nodes remained offline during this time.\\\\n23:30 UTC\\xa0on\\xa05 November 2025\\xa0– Service dependency checks and orchestration. Networking and compute services were progressively re-enabled after storage scale units passed health checks.\\\\n02:25 UTC\\xa0on\\xa06 November 2025\\xa0–\\xa0All services\\xa0were brought back online, and customer impact was mitigated.\",\"mitigation\":\"We are implementing several improvements to strengthen resilience and accelerate recovery:\\\\nWe are conducting internal and manufacturer-led forensic investigations of specific device hardware and firmware to develop solutions to bolster component tolerance to power sags and voltage fluctuations. (Estimated completion: December 2025)\\\\nWe are performing a gap analysis of the control circuit to ensure all relays delivering critical function of the units are powered through Uninterruptible Power Supply (UPS) and stored energy sources. (Estimated completion: January 2026)\\\\nWe are validating our maintenance procedures and adding additional prechecks for Heating, Ventilation, and Air Conditioning (HVAC) subsystems. (Estimated completion: January 2026)\\\\nWe are optimizing recovery steps for compute and storage systems to reduce overall restoration time. (Estimated completion: January 2026)\\\\nWe are improving automated correlation between environmental signals and service health to trigger earlier, and more targeted escalations. (Estimated completion: January 2026)\\\\nWe are updating our cooling unit\\xa0commissioning\\xa0playbooks\\xa0to\\xa0expand range of\\xa0power sag and swell\\xa0scenarios.\\xa0(Estimated completion:\\xa0February\\xa02026)\\\\nIn the longer term, we are implementing a new approach to accelerate service restoration after major incidents. Historically, recovery prioritized full data integrity checks before bringing services online, which extended downtime because it was a serial process. Going forward, integrity checks will run in the background during VM and disk mount operations, allowing availability to return much sooner without compromising data safety. This change is a direct outcome of this incident and represents a significant improvement in recovery speed.\\xa0(Estimated completion: April 2026)\",\"customer_guidance\":\"Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations:https://docs.microsoft.com/azure/availability-zones/az-overview\\\\nPlan for regional redundancy by using active/active or failover designs across paired regions.\\\\nMaintain regular backups and test failover procedures to ensure recovery readiness.\\\\nIncrease resilience by using retry logic and backoff strategies to prevent overload during recovery.\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:https://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/2LGD-9VG\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><em>Watch our \\'Azure Incident Retrospective\\' video about this incident: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/air/2LGD-9VG\\\\\">https://aka.ms/air/2LGD-9VG</a></em></p><p><strong>What happened?</strong></p><p>Between approximately 16:53 UTC on 5 November 2025 and 02:25 UTC on 6 November 2025, a subset of customers in the West Europe region experienced service disruptions or degraded performance across multiple services, including Virtual Machines (VM), Azure Database for PostgreSQL Flexible Server, MySQL Flexible Server, Azure Kubernetes Service, Storage, Service Bus, and Virtual Machine Scale Sets, among others. Customers using Azure Databricks in West Europe observed degraded performance when launching or scaling all-purpose and job compute workloads, which impacted Unity Catalog and Databricks SQL operations.</p><p><strong>What went wrong and why?</strong></p><p>The incident was triggered by a voltage sag in the utility grid, which caused cooling units to shut down and temperatures to rise above normal thresholds in a datacenter located in Physical Availability Zone 01 in the West Europe region. As temperature rose, multiple storage scale units automatically powered down to prevent hardware damage. Under normal circumstances, cooling systems are designed and commissioned to automatically restart after power events and are built with sufficient redundancy to maintain safe operating conditions even when failures occur. While cooling unit auto-restart capability had been regularly tested and exercised in this datacenter, the specific size and duration of utility sag exposed a previously unknown hardware issue that prevented the cooling units from restarting. This resulted in temperatures exceeding safe operational limits for infrastructure.\\xa0\\xa0</p><p>The extended recovery time was influenced by several factors. Recovery of storage scale units was prolonged because some storage servers entered a degraded state, necessitating sequential validation before they could be brought back online. In addition, dependent services such as compute and networking could not resume until storage integrity checks were complete. Finally, additional thermal audits were performed before workloads were reactivated to ensure that no residual risk remained.</p><p><strong>How did we respond?</strong></p><p>Automated monitoring\\xa0detected the temperature anomaly and triggered an incident response.\\xa0Facilities teams restored safe operating conditions by performing\\xa0hard\\xa0reset on the affected units and confirmed that temperatures were trending back to safe levels.\\xa0Service recovery efforts began once the facility had reached safe operating temperatures.\\xa0</p><p>Following this, storage systems\\xa0initiated comprehensive data consistency checks when nodes in a storage scale unit restarted after the unplanned shutdown. These checks, which may include reconstructing replicas that have fallen behind.\\xa0The recovery process prioritizes data integrity and durability over availability, resulting in customer traffic being blocked until all integrity validations was completed and contributing to the extended recovery time. In parallel, engineers prioritized recovery of compute hosts to stop further VM failures and allow dependent services to resume.\\xa0</p><p>Timeline of events:</p><ul><li>16:53 UTC on\\xa05 November 2025\\xa0– Customer impact began as elevated temperatures caused multiple storage scale units to shut down.\\xa0</li><li>16:55 UTC on\\xa05 November 2025\\xa0– Datacenter monitoring detected temperature breaches and triggered thermal alerts.\\xa0\\xa0</li><li>17:20 UTC\\xa0on\\xa05 November 2025\\xa0– Power sag and cooling unit failure to restart identified as contributing factors. Cooling recovery begins as engineers manually restart cooling units.\\xa0</li><li>17:40 UTC\\xa0on\\xa05 November 2025\\xa0– Mitigation workstream started as cooling restoration progresses.\\xa0</li><li>17:48 UTC\\xa0on\\xa05 November 2025\\xa0– All cooling units restored, we start seeing temperatures reducing as cooling normalized.\\xa0\\xa0</li><li>17:50 UTC\\xa0on\\xa05 November 2025\\xa0– Rack-level thermal monitoring returned to safe operational thresholds.\\xa0Engineering teams initiated storage recovery.\\xa0</li><li>18:30 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Sequential storage scale unit validation began. Each scale unit underwent integrity checks to ensure no data corruption before bringing storage services back into rotation.\\xa0</li><li>20:00 UTC\\xa0on\\xa05 November 2025\\xa0–\\xa0Gradual restoration of storage scale units. Recovery was staged to avoid overloading power and cooling systems. Dependent compute nodes remained offline during this time.\\xa0</li><li>23:30 UTC\\xa0on\\xa05 November 2025\\xa0– Service dependency checks and orchestration. Networking and compute services were progressively re-enabled after storage scale units passed health checks.\\xa0</li><li>02:25 UTC\\xa0on\\xa06 November 2025\\xa0–\\xa0All services\\xa0were brought back online, and customer impact was mitigated.\\xa0</li></ul><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><p>We are implementing several improvements to strengthen resilience and accelerate recovery:\\xa0</p><ul><li>We are conducting internal and manufacturer-led forensic investigations of specific device hardware and firmware to develop solutions to bolster component tolerance to power sags and voltage fluctuations. (Estimated completion: December 2025)\\xa0</li><li>We are performing a gap analysis of the control circuit to ensure all relays delivering critical function of the units are powered through Uninterruptible Power Supply (UPS) and stored energy sources. (Estimated completion: January 2026)\\xa0\\xa0</li><li>We are validating our maintenance procedures and adding additional prechecks for Heating, Ventilation, and Air Conditioning (HVAC) subsystems. (Estimated completion: January 2026)\\xa0</li><li>We are optimizing recovery steps for compute and storage systems to reduce overall restoration time. (Estimated completion: January 2026)\\xa0</li><li>We are improving automated correlation between environmental signals and service health to trigger earlier, and more targeted escalations. (Estimated completion: January 2026)\\xa0</li><li>We are updating our cooling unit\\xa0commissioning\\xa0playbooks\\xa0to\\xa0expand range of\\xa0power sag and swell\\xa0scenarios.\\xa0(Estimated completion:\\xa0February\\xa02026)\\xa0</li><li>In the longer term, we are implementing a new approach to accelerate service restoration after major incidents. Historically, recovery prioritized full data integrity checks before bringing services online, which extended downtime because it was a serial process. Going forward, integrity checks will run in the background during VM and disk mount operations, allowing availability to return much sooner without compromising data safety. This change is a direct outcome of this incident and represents a significant improvement in recovery speed.\\xa0(Estimated completion: April 2026)</li></ul><p><strong>How can customers make incidents like this less impactful?</strong></p><ul><li>Consider using Availability Zones (AZs) to run your services across physically separate locations within an Azure region. To help services be more resilient to datacenter-level failures like this one, each AZ provides independent power, networking, and cooling. Many Azure services support zonal, zone-redundant, and/or always-available configurations: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://docs.microsoft.com/azure/availability-zones/az-overview\\\\\">https://docs.microsoft.com/azure/availability-zones/az-overview</a></li><li>Plan for regional redundancy by using active/active or failover designs across paired regions.</li><li>Maintain regular backups and test failover procedures to ensure recovery readiness.</li><li>Increase resilience by using retry logic and backoff strategies to prevent overload during recovery.</li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></li><li>The impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></li><li>Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Alerts\\\\\">https://aka.ms/AzPIR/Alerts</a></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/2LGD-9VG\\\\\">https://aka.ms/AzPIR/2LGD-9VG</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"azure\",\"month\":\"October 2025\",\"date\":\"2025-10-29\",\"tracking_id\":\"YKYN-BWZ\",\"title\":\"Post Incident Review (PIR) – Azure Front Door – Connectivity issues across multiple regions\",\"text\":\"Read Azure Front Door\\'s blog series in making progress on key resilience repairs:\\\\nhttps://aka.ms/AzureFrontDoor/Resiliency-Part1\\\\nWatch our \\'Azure Incident Retrospective\\' video about this incident:\\\\nhttps://aka.ms/air/YKYN-BWZ\\\\nWhat happened?\\\\nBetween 15:41 UTC on 29 October and 00:05 UTC on 30 October 2025, customers and Microsoft services leveraging Azure Front Door (AFD) and Azure Content Delivery Network (CDN) experienced connection timeout errors and Domain Name System (DNS) resolution issues. From 18:30 UTC on 29 October 2025, as the system recovered gradually, some customers started to see availability improve – albeit with increased latency – until the system fully stabilized by 00:05 UTC on 30 October 2025.\\\\nAffected Azure services included, but were not limited to: Azure Active Directory B2C, Azure AI Video Indexer, Azure App Service, Azure Communication Services, Azure Databricks, Azure Healthcare APIs, Azure Maps, Azure Marketplace, Azure Media Services, Azure Portal, Azure Sphere Security Service, Azure SQL Database, and Azure Static Web Apps.\\\\nOther Microsoft services were also impacted, including Microsoft 365 (see: MO1181369), the Microsoft Communication Registry website, Microsoft Copilot for Security, Microsoft Defender (External Attack Surface Management), Microsoft Dragon Copilot, Microsoft Dynamics 365 and Power Platform (see: MX1181378), Microsoft Entra ID (Mobility Management Policy Service, Identity & Access Management, and User Management), Microsoft Purview, Microsoft Sentinel (Threat Intelligence), Visual Studio App Center, and customers’ ability to open support cases (both in the Azure Portal and by phone).\\\\nWhat went wrong and why?\\\\nAzure Front Door (AFD) and Azure Content Delivery Network (CDN) route traffic using globally distributed edge sites supporting customers as well as Microsoft services including various management portals. The AFD control plane generates customer configuration metadata that the data plane consumes for all customer-initiated operations including purge and Web Application Firewall (WAF) on the AFD platform. Since customer applications hosted on AFD and CDN can be accessed by their end users from anywhere in the world, these changes are deployed globally across all its edge sites to provide a consistent user experience.\\\\nA specific sequence of customer configuration changes, performed across two different control plane build versions, resulted in incompatible customer configuration metadata being generated. These customer configuration changes themselves were valid and non-malicious – however they produced metadata that, when deployed to edge site servers, exposed a latent bug in the data plane. This incompatibility triggered a crash during asynchronous processing within the data plane service. This defect escaped detection due to a gap in our pre-production validation, since not all features are validated across different control plane build versions.\\\\nAzure Front Door employs multiple deployment stages, and a configuration protection system to ensure safe propagation of customer configurations. This system validates configurations at each deployment stage and advances only after receiving positive health signals from the data plane. Once deployments are rolled out successfully, the configuration propagation system also updates a ‘Last Known Good’ (LKG) snapshot (a periodic snapshot of healthy customer configurations) so that deployments can be automatically rolled back in case of any issues. The configuration protection system waits for approximately a minute between each stage, completing on an average within 5-10 minutes globally.\\\\nDuring this incident, the incompatible customer configuration change was made at 15:35 UTC, and was applied to the data plane in a pre-production stage at 15:36 UTC. Our configuration propagation monitoring continued to receive healthy signals – although the problematic metadata was present, it had not caused any issues. Because the data plane crash surfaced asynchronously, after approximately five minutes, the configuration passed through the protection safeguards and propagated to later stages. This configuration (with the incompatible metadata) completed propagation to a majority of edge sites by 15:39 UTC. Since the incompatible customer configuration metadata was deployed successfully to the majority of fleet with positive health signal, the LKG was also updated with this configuration.\\\\nThe data plane impact began in phases starting with our preproduction environment at 15:41 UTC, and replicated across all edge sites globally by 15:45 UTC. As the data plane impact started, the configuration protection system detected this and stopped all new and inflight customer configuration changes from being propagated at 15:43 UTC. The incompatible customer configuration was processed by edge servers, causing crashes across our various edge sites. This also impacted AFD’s internal DNS service, hosted on the edge sites of Azure Front Door, resulting in intermittent DNS resolution errors for a subset of AFD customer requests. This sequence of events was the trigger for the global impact on the AFD platform.\\\\nThis AFD incident on 29 October was not directly related to the previous AFD incident, from 9 October. Both incidents were broadly related to configuration propagation risk (inherent to a global Content Delivery Network, in which route/WAF/origin changes must be quickly deployed worldwide) but while the failure mode was similar, the underlying defects were different. Azure Front Door’s configuration protection system is designed to validate configurations and proceed only after receiving positive health signals from the data plane. During the AFD incident on 9 October (Tracking ID: QNBQ-5W8) that protection system worked as intended, but was later bypassed by our engineering team during a manual cleanup operation. During this AFD incident on 29 October (Tracking ID: YKYN-BWZ) the incompatible customer configuration metadata progressed through the protection system, before the delayed asynchronous processing task resulted in the crash. Some of the learnings and repair items from the earlier incident are applicable to this incident as well, and are included in the list of repairs below.\\\\nHow did we respond?\\\\nThe issue started at 15:41 UTC and was detected by monitoring at 15:48 UTC, prompting our investigation. By 15:43 UTC the configuration protection system activated in response to widespread data plane issues, and automatically blocked all new and in-flight configuration changes from being deployed worldwide.\\\\nSince the latest ‘last known good’ (LKG) version was updated with the conflicting metadata, we chose not to revert to it. To ensure system stability, we decided not to rollback to prior versions of the LKG either. Instead, we opted to edit the latest LKG, by removing the problematic customer configurations manually. We also opted to block all customer configuration changes from propagating to the data plane at 17:30 UTC so that, as we mitigate, we would not reintroduce this issue. At 17:40 UTC we began deploying the updated LKG configuration across the global fleet. Recovery required reloading all customer configurations at every edge site and rebalancing traffic gradually, to avoid overload conditions as Edge sites returned to service. This deliberate, phased recovery was necessary to stabilize the system while restoring scale and ensuring no recurrence of the issue.\\\\nMany downstream services that use AFD were able to failover to prevent further customer impact, including Microsoft Entra and Intune portals, and Azure Active Directory B2C. In a more complex example, Azure Portal leveraged its standard recovery process to successfully transition away from AFD during the incident. Users of the Portal would have seen limited impact during this failover process and then been able to use the Portal without issue. Unfortunately, some services within the Portal did not have an established fallback strategy and therefore parts of the Portal experience continued to experience failures even after Portal recovery (for example, Marketplace).\\\\nTimeline of major incident milestones:\\\\n15:35 UTC on 29 October 2025 – Corrupt metadata first introduced, as described above.\\\\n15:41 UTC on 29 October 2025 – Customer impact began, triggered by the resulting crashes.\\\\n15:43 UTC on 29 October 2025 – Configuration protection system activated in response to issues.\\\\n15:48 UTC on 29 October 2025 – Investigation commenced following monitoring alerts being triggered.\\\\n16:15 UTC on 29 October 2025 – Focus of the investigation became examining AFD configuration changes.\\\\n16:18 UTC on 29 October 2025 – Initial communication posted to our public status page.\\\\n16:20 UTC on 29 October 2025 – Targeted communications to impacted customers sent to Azure Service Health.\\\\n17:10 UTC on 29 October 2025 – Began updating the ‘last known good’ LKG configuration to remove problematic configurations manually.\\\\n17:26 UTC on 29 October 2025 – Azure Portal failed away from Azure Front Door.\\\\n17:30 UTC on 29 October 2025 – Blocked all customer configuration propagation to the data plane, in preparation for deploying the new configuration.\\\\n17:40 UTC on 29 October 2025 – Initiated the deployment of our updated ‘last known good’ configuration.\\\\n17:50 UTC on 29 October 2025 – Last known good configuration available to all Edge sites, which began gradually reloading LKG configuration.\\\\n18:30 UTC on 29 October 2025 – AFD DNS servers recovered, allowing us to rebalance traffic manually to a small number of healthy Edge sites. Customers began seeing improvements in availability.\\\\n20:20 UTC on 29 October 2025 – As a sufficient number of Edge sites had recovered, we switched to automatic traffic management – customers continued to see availability improve.\\\\n00:05 UTC on 30 October 2025 – AFD impact confirmed mitigated for customers, as availability and latency had returned to pre-incident levels.\\\\nPost mitigation, we temporarily blocked all AFD customer configuration changes at the Azure Resource Manager (ARM) level to ensure the safety of the data plane. We also implemented additional safeguards including (i) fixing the control plane and data plane defects, (ii) removing asynchronous processing from the data plane, (iii) introducing an additional ‘pre-canary’ stage to test customer configuration (iv) extending the bake time during each stage of the configuration propagation, and (v) improvements to the data plane recovery time from approximately 4.5 hours to approximately one hour. We began draining the customer configuration queue from 2 November 2025. Once these safeguards were fully implemented, this restriction was removed on 5 November 2025.\\\\nThe introduction of new stages in the configuration propagation pipeline was coupled with additional ‘bake time’ between stages – which has resulted in an increase in configuration propagation time, for all operations including create, update, delete, WAF operations on AFD platform, and cache purges. We continue to work on platform enhancements to ensure a robust configuration delivery pipeline and further reduce the propagation time. For more details on these temporary propagation delays, refer to\\\\nhttp://aka.ms/AFD_FAQ\\\\n.\\\\nHow are we making incidents like this less likely or less impactful?\\\\nTo prevent issues like this, and improve deployment safety...\\\\nWe have fixed both the original control plane incompatibility, and the data plane bug described above. (Completed)\\\\nWe are now enforcing complete synchronous processing of each customer configuration, before advancing to production stages. (Completed)\\\\nWe have implemented additional stages in our phased configuration rollout, including extended bake time to help detect configuration related issues. (Completed)\\\\nWe are decoupling configuration processing in data plane servers from active traffic-serving instances to isolated worker process instances, thereby removing the risk of any configuration defect impacting data plane processing. (Estimated completion: January 2026)\\\\nOnce our pre-validation of this configuration pipeline is in place, we will work towards reducing the propagation time from 45 minutes to approximately 15 minutes. (Estimated completion: January 2026)\\\\nWe are enhancing our testing and validation framework to ensure backwards compatibility with configurations generated across previous versions of the control plane build. (Estimated completion: February 2026)\\\\nTo reduce the blast radius of potential future issues...\\\\nWe have migrated critical first-party infrastructure (including Azure Portal, Azure Communication Services, Marketplace, Linux Software Repository for Microsoft Products, Support ticket creation) into an active-active solution with fail away. (Completed)\\\\nIn the longer term, we are enhancing our customer configuration and traffic isolation, to ensure that no impact to any other customers from single customers’ traffic or configuration issue – utilizing ‘micro cell’ segmentation of the AFD data plane. (Estimated completion: June 2026)\\\\nTo be able to recover more quickly from issues...\\\\nWe have made changes to accelerate our data plane recovery time, by leveraging the local customer configuration caching more effectively – to restore customer configurations within one hour. (Completed)\\\\nWe are making investments to reduce data plane recovery time further, to restore customer configurations within approximately 10 minutes. (Estimated completion: March 2026)\\\\nTo improve our communications and support...\\\\nWe have addressed delays in delivering alerts via Azure Service Health to impacted customers, by making immediate improvements to increase resource thresholds. (Completed)\\\\nWe will expand the automated customer alerts sent via Azure Service Health, to include similar classes of service degradation – to notify impacted customers more quickly. (Estimated completion: November 2025)\\\\nFinally, we will resolve the technical and staffing challenges that prevented Premier and Unified customers from being able to create a support request, by ensuring that we can failover to our backups systems more quickly. (Estimated completion: November 2025)\\\\nHow can customers make incidents like this less impactful?\\\\nUnderstand best practices for building a resilient global HTTP ingress layer, to maintain availability during regional or network disruptions:\\\\nhttps://learn.microsoft.com/azure/architecture/guide/networking/global-web-applications/mission-critical-global-http-ingress\\\\nAs an alternative for when management portals are temporarily inaccessible, customers can consider using programmatic methods to manage resources – including the REST API (\\\\nhttps://learn.microsoft.com/rest/api/azure\\\\n) and PowerShell (\\\\nhttps://learn.microsoft.com/powershell/azure\\\\n)\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:\\\\nhttps://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:\\\\nhttps://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:\\\\nhttps://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:\\\\nhttps://aka.ms/AzPIR/YKYN-BWZ\",\"sections\":{\"what_happened\":\"Between 15:41 UTC on 29 October and 00:05 UTC on 30 October 2025, customers and Microsoft services leveraging Azure Front Door (AFD) and Azure Content Delivery Network (CDN) experienced connection timeout errors and Domain Name System (DNS) resolution issues. From 18:30 UTC on 29 October 2025, as the system recovered gradually, some customers started to see availability improve – albeit with increased latency – until the system fully stabilized by 00:05 UTC on 30 October 2025.\\\\nAffected Azure services included, but were not limited to: Azure Active Directory B2C, Azure AI Video Indexer, Azure App Service, Azure Communication Services, Azure Databricks, Azure Healthcare APIs, Azure Maps, Azure Marketplace, Azure Media Services, Azure Portal, Azure Sphere Security Service, Azure SQL Database, and Azure Static Web Apps.\\\\nOther Microsoft services were also impacted, including Microsoft 365 (see: MO1181369), the Microsoft Communication Registry website, Microsoft Copilot for Security, Microsoft Defender (External Attack Surface Management), Microsoft Dragon Copilot, Microsoft Dynamics 365 and Power Platform (see: MX1181378), Microsoft Entra ID (Mobility Management Policy Service, Identity & Access Management, and User Management), Microsoft Purview, Microsoft Sentinel (Threat Intelligence), Visual Studio App Center, and customers’ ability to open support cases (both in the Azure Portal and by phone).\",\"what_went_wrong\":\"Azure Front Door (AFD) and Azure Content Delivery Network (CDN) route traffic using globally distributed edge sites supporting customers as well as Microsoft services including various management portals. The AFD control plane generates customer configuration metadata that the data plane consumes for all customer-initiated operations including purge and Web Application Firewall (WAF) on the AFD platform. Since customer applications hosted on AFD and CDN can be accessed by their end users from anywhere in the world, these changes are deployed globally across all its edge sites to provide a consistent user experience.\\\\nA specific sequence of customer configuration changes, performed across two different control plane build versions, resulted in incompatible customer configuration metadata being generated. These customer configuration changes themselves were valid and non-malicious – however they produced metadata that, when deployed to edge site servers, exposed a latent bug in the data plane. This incompatibility triggered a crash during asynchronous processing within the data plane service. This defect escaped detection due to a gap in our pre-production validation, since not all features are validated across different control plane build versions.\\\\nAzure Front Door employs multiple deployment stages, and a configuration protection system to ensure safe propagation of customer configurations. This system validates configurations at each deployment stage and advances only after receiving positive health signals from the data plane. Once deployments are rolled out successfully, the configuration propagation system also updates a ‘Last Known Good’ (LKG) snapshot (a periodic snapshot of healthy customer configurations) so that deployments can be automatically rolled back in case of any issues. The configuration protection system waits for approximately a minute between each stage, completing on an average within 5-10 minutes globally.\\\\nDuring this incident, the incompatible customer configuration change was made at 15:35 UTC, and was applied to the data plane in a pre-production stage at 15:36 UTC. Our configuration propagation monitoring continued to receive healthy signals – although the problematic metadata was present, it had not caused any issues. Because the data plane crash surfaced asynchronously, after approximately five minutes, the configuration passed through the protection safeguards and propagated to later stages. This configuration (with the incompatible metadata) completed propagation to a majority of edge sites by 15:39 UTC. Since the incompatible customer configuration metadata was deployed successfully to the majority of fleet with positive health signal, the LKG was also updated with this configuration.\\\\nThe data plane impact began in phases starting with our preproduction environment at 15:41 UTC, and replicated across all edge sites globally by 15:45 UTC. As the data plane impact started, the configuration protection system detected this and stopped all new and inflight customer configuration changes from being propagated at 15:43 UTC. The incompatible customer configuration was processed by edge servers, causing crashes across our various edge sites. This also impacted AFD’s internal DNS service, hosted on the edge sites of Azure Front Door, resulting in intermittent DNS resolution errors for a subset of AFD customer requests. This sequence of events was the trigger for the global impact on the AFD platform.\\\\nThis AFD incident on 29 October was not directly related to the previous AFD incident, from 9 October. Both incidents were broadly related to configuration propagation risk (inherent to a global Content Delivery Network, in which route/WAF/origin changes must be quickly deployed worldwide) but while the failure mode was similar, the underlying defects were different. Azure Front Door’s configuration protection system is designed to validate configurations and proceed only after receiving positive health signals from the data plane. During the AFD incident on 9 October (Tracking ID: QNBQ-5W8) that protection system worked as intended, but was later bypassed by our engineering team during a manual cleanup operation. During this AFD incident on 29 October (Tracking ID: YKYN-BWZ) the incompatible customer configuration metadata progressed through the protection system, before the delayed asynchronous processing task resulted in the crash. Some of the learnings and repair items from the earlier incident are applicable to this incident as well, and are included in the list of repairs below.\",\"how_did_we_respond\":\"The issue started at 15:41 UTC and was detected by monitoring at 15:48 UTC, prompting our investigation. By 15:43 UTC the configuration protection system activated in response to widespread data plane issues, and automatically blocked all new and in-flight configuration changes from being deployed worldwide.\\\\nSince the latest ‘last known good’ (LKG) version was updated with the conflicting metadata, we chose not to revert to it. To ensure system stability, we decided not to rollback to prior versions of the LKG either. Instead, we opted to edit the latest LKG, by removing the problematic customer configurations manually. We also opted to block all customer configuration changes from propagating to the data plane at 17:30 UTC so that, as we mitigate, we would not reintroduce this issue. At 17:40 UTC we began deploying the updated LKG configuration across the global fleet. Recovery required reloading all customer configurations at every edge site and rebalancing traffic gradually, to avoid overload conditions as Edge sites returned to service. This deliberate, phased recovery was necessary to stabilize the system while restoring scale and ensuring no recurrence of the issue.\\\\nMany downstream services that use AFD were able to failover to prevent further customer impact, including Microsoft Entra and Intune portals, and Azure Active Directory B2C. In a more complex example, Azure Portal leveraged its standard recovery process to successfully transition away from AFD during the incident. Users of the Portal would have seen limited impact during this failover process and then been able to use the Portal without issue. Unfortunately, some services within the Portal did not have an established fallback strategy and therefore parts of the Portal experience continued to experience failures even after Portal recovery (for example, Marketplace).\\\\nTimeline of major incident milestones:\\\\n15:35 UTC on 29 October 2025 – Corrupt metadata first introduced, as described above.\\\\n15:41 UTC on 29 October 2025 – Customer impact began, triggered by the resulting crashes.\\\\n15:43 UTC on 29 October 2025 – Configuration protection system activated in response to issues.\\\\n15:48 UTC on 29 October 2025 – Investigation commenced following monitoring alerts being triggered.\\\\n16:15 UTC on 29 October 2025 – Focus of the investigation became examining AFD configuration changes.\\\\n16:18 UTC on 29 October 2025 – Initial communication posted to our public status page.\\\\n16:20 UTC on 29 October 2025 – Targeted communications to impacted customers sent to Azure Service Health.\\\\n17:10 UTC on 29 October 2025 – Began updating the ‘last known good’ LKG configuration to remove problematic configurations manually.\\\\n17:26 UTC on 29 October 2025 – Azure Portal failed away from Azure Front Door.\\\\n17:30 UTC on 29 October 2025 – Blocked all customer configuration propagation to the data plane, in preparation for deploying the new configuration.\\\\n17:40 UTC on 29 October 2025 – Initiated the deployment of our updated ‘last known good’ configuration.\\\\n17:50 UTC on 29 October 2025 – Last known good configuration available to all Edge sites, which began gradually reloading LKG configuration.\\\\n18:30 UTC on 29 October 2025 – AFD DNS servers recovered, allowing us to rebalance traffic manually to a small number of healthy Edge sites. Customers began seeing improvements in availability.\\\\n20:20 UTC on 29 October 2025 – As a sufficient number of Edge sites had recovered, we switched to automatic traffic management – customers continued to see availability improve.\\\\n00:05 UTC on 30 October 2025 – AFD impact confirmed mitigated for customers, as availability and latency had returned to pre-incident levels.\\\\nPost mitigation, we temporarily blocked all AFD customer configuration changes at the Azure Resource Manager (ARM) level to ensure the safety of the data plane. We also implemented additional safeguards including (i) fixing the control plane and data plane defects, (ii) removing asynchronous processing from the data plane, (iii) introducing an additional ‘pre-canary’ stage to test customer configuration (iv) extending the bake time during each stage of the configuration propagation, and (v) improvements to the data plane recovery time from approximately 4.5 hours to approximately one hour. We began draining the customer configuration queue from 2 November 2025. Once these safeguards were fully implemented, this restriction was removed on 5 November 2025.\\\\nThe introduction of new stages in the configuration propagation pipeline was coupled with additional ‘bake time’ between stages – which has resulted in an increase in configuration propagation time, for all operations including create, update, delete, WAF operations on AFD platform, and cache purges. We continue to work on platform enhancements to ensure a robust configuration delivery pipeline and further reduce the propagation time. For more details on these temporary propagation delays, refer tohttp://aka.ms/AFD_FAQ.\",\"mitigation\":\"To prevent issues like this, and improve deployment safety...\\\\nWe have fixed both the original control plane incompatibility, and the data plane bug described above. (Completed)\\\\nWe are now enforcing complete synchronous processing of each customer configuration, before advancing to production stages. (Completed)\\\\nWe have implemented additional stages in our phased configuration rollout, including extended bake time to help detect configuration related issues. (Completed)\\\\nWe are decoupling configuration processing in data plane servers from active traffic-serving instances to isolated worker process instances, thereby removing the risk of any configuration defect impacting data plane processing. (Estimated completion: January 2026)\\\\nOnce our pre-validation of this configuration pipeline is in place, we will work towards reducing the propagation time from 45 minutes to approximately 15 minutes. (Estimated completion: January 2026)\\\\nWe are enhancing our testing and validation framework to ensure backwards compatibility with configurations generated across previous versions of the control plane build. (Estimated completion: February 2026)\\\\nTo reduce the blast radius of potential future issues...\\\\nWe have migrated critical first-party infrastructure (including Azure Portal, Azure Communication Services, Marketplace, Linux Software Repository for Microsoft Products, Support ticket creation) into an active-active solution with fail away. (Completed)\\\\nIn the longer term, we are enhancing our customer configuration and traffic isolation, to ensure that no impact to any other customers from single customers’ traffic or configuration issue – utilizing ‘micro cell’ segmentation of the AFD data plane. (Estimated completion: June 2026)\\\\nTo be able to recover more quickly from issues...\\\\nWe have made changes to accelerate our data plane recovery time, by leveraging the local customer configuration caching more effectively – to restore customer configurations within one hour. (Completed)\\\\nWe are making investments to reduce data plane recovery time further, to restore customer configurations within approximately 10 minutes. (Estimated completion: March 2026)\\\\nTo improve our communications and support...\\\\nWe have addressed delays in delivering alerts via Azure Service Health to impacted customers, by making immediate improvements to increase resource thresholds. (Completed)\\\\nWe will expand the automated customer alerts sent via Azure Service Health, to include similar classes of service degradation – to notify impacted customers more quickly. (Estimated completion: November 2025)\\\\nFinally, we will resolve the technical and staffing challenges that prevented Premier and Unified customers from being able to create a support request, by ensuring that we can failover to our backups systems more quickly. (Estimated completion: November 2025)\",\"customer_guidance\":\"Understand best practices for building a resilient global HTTP ingress layer, to maintain availability during regional or network disruptions:https://learn.microsoft.com/azure/architecture/guide/networking/global-web-applications/mission-critical-global-http-ingress\\\\nAs an alternative for when management portals are temporarily inaccessible, customers can consider using programmatic methods to manage resources – including the REST API (https://learn.microsoft.com/rest/api/azure) and PowerShell (https://learn.microsoft.com/powershell/azure)\\\\nMore generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review:https://aka.ms/AzPIR/WAF\\\\nThe impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact:https://aka.ms/AzPIR/Monitoring\\\\nFinally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:https://aka.ms/AzPIR/Alerts\\\\nHow can we make our incident communications more useful?\\\\nHow can we make our incident communications more useful?\\\\nYou can rate this PIR and provide any feedback using our quick 3-question survey:https://aka.ms/AzPIR/YKYN-BWZ\"},\"raw_html\":\"<div class=\\\\\"card-body\\\\\">\\\\n<p><em>Read Azure Front Door\\'s blog series in making progress on key resilience repairs: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzureFrontDoor/Resiliency-Part1\\\\\">https://aka.ms/AzureFrontDoor/Resiliency-Part1</a></em></p><p><em>Watch our \\'Azure Incident Retrospective\\' video about this incident:\\xa0<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/air/YKYN-BWZ\\\\\">https://aka.ms/air/YKYN-BWZ</a></em></p><p><strong>What happened?</strong></p><p>Between 15:41 UTC on 29 October and 00:05 UTC on 30 October 2025, customers and Microsoft services leveraging Azure Front Door (AFD) and Azure Content Delivery Network (CDN) experienced connection timeout errors and Domain Name System (DNS) resolution issues. From 18:30 UTC on 29 October 2025, as the system recovered gradually, some customers started to see availability improve – albeit with increased latency – until the system fully stabilized by 00:05 UTC on 30 October 2025.</p><p>Affected Azure services included, but were not limited to: Azure Active Directory B2C, Azure AI Video Indexer, Azure App Service, Azure Communication Services, Azure Databricks, Azure Healthcare APIs, Azure Maps, Azure Marketplace, Azure Media Services, Azure Portal, Azure Sphere Security Service, Azure SQL Database, and Azure Static Web Apps.</p><p>Other Microsoft services were also impacted, including Microsoft 365 (see: MO1181369), the Microsoft Communication Registry website, Microsoft Copilot for Security, Microsoft Defender (External Attack Surface Management), Microsoft Dragon Copilot, Microsoft Dynamics 365 and Power Platform (see: MX1181378), Microsoft Entra ID (Mobility Management Policy Service, Identity &amp; Access Management, and User Management), Microsoft Purview, Microsoft Sentinel (Threat Intelligence), Visual Studio App Center, and customers’ ability to open support cases (both in the Azure Portal and by phone).</p><p><strong>What went wrong and why?</strong></p><p>Azure Front Door (AFD) and Azure Content Delivery Network (CDN) route traffic using globally distributed edge sites supporting customers as well as Microsoft services including various management portals. The AFD control plane generates customer configuration metadata that the data plane consumes for all customer-initiated operations including purge and Web Application Firewall (WAF) on the AFD platform. Since customer applications hosted on AFD and CDN can be accessed by their end users from anywhere in the world, these changes are deployed globally across all its edge sites to provide a consistent user experience.</p><p>A specific sequence of customer configuration changes, performed across two different control plane build versions, resulted in incompatible customer configuration metadata being generated. These customer configuration changes themselves were valid and non-malicious – however they produced metadata that, when deployed to edge site servers, exposed a latent bug in the data plane. This incompatibility triggered a crash during asynchronous processing within the data plane service. This defect escaped detection due to a gap in our pre-production validation, since not all features are validated across different control plane build versions.</p><p>Azure Front Door employs multiple deployment stages, and a configuration protection system to ensure safe propagation of customer configurations. This system validates configurations at each deployment stage and advances only after receiving positive health signals from the data plane. Once deployments are rolled out successfully, the configuration propagation system also updates a ‘Last Known Good’ (LKG) snapshot (a periodic snapshot of healthy customer configurations) so that deployments can be automatically rolled back in case of any issues. The configuration protection system waits for approximately a minute between each stage, completing on an average within 5-10 minutes globally.</p><p>During this incident, the incompatible customer configuration change was made at 15:35 UTC, and was applied to the data plane in a pre-production stage at 15:36 UTC. Our configuration propagation monitoring continued to receive healthy signals – although the problematic metadata was present, it had not caused any issues. Because the data plane crash surfaced asynchronously, after approximately five minutes, the configuration passed through the protection safeguards and propagated to later stages. This configuration (with the incompatible metadata) completed propagation to a majority of edge sites by 15:39 UTC. Since the incompatible customer configuration metadata was deployed successfully to the majority of fleet with positive health signal, the LKG was also updated with this configuration.</p><p>The data plane impact began in phases starting with our preproduction environment at 15:41 UTC, and replicated across all edge sites globally by 15:45 UTC. As the data plane impact started, the configuration protection system detected this and stopped all new and inflight customer configuration changes from being propagated at 15:43 UTC. The incompatible customer configuration was processed by edge servers, causing crashes across our various edge sites. This also impacted AFD’s internal DNS service, hosted on the edge sites of Azure Front Door, resulting in intermittent DNS resolution errors for a subset of AFD customer requests. This sequence of events was the trigger for the global impact on the AFD platform.</p><p>This AFD incident on 29 October was not directly related to the previous AFD incident, from 9 October. Both incidents were broadly related to configuration propagation risk (inherent to a global Content Delivery Network, in which route/WAF/origin changes must be quickly deployed worldwide) but while the failure mode was similar, the underlying defects were different. Azure Front Door’s configuration protection system is designed to validate configurations and proceed only after receiving positive health signals from the data plane. During the AFD incident on 9 October (Tracking ID: QNBQ-5W8) that protection system worked as intended, but was later bypassed by our engineering team during a manual cleanup operation. During this AFD incident on 29 October (Tracking ID: YKYN-BWZ) the incompatible customer configuration metadata progressed through the protection system, before the delayed asynchronous processing task resulted in the crash. Some of the learnings and repair items from the earlier incident are applicable to this incident as well, and are included in the list of repairs below.</p><p><strong>How did we respond?</strong></p><p>The issue started at 15:41 UTC and was detected by monitoring at 15:48 UTC, prompting our investigation. By 15:43 UTC the configuration protection system activated in response to widespread data plane issues, and automatically blocked all new and in-flight configuration changes from being deployed worldwide.</p><p>Since the latest ‘last known good’ (LKG) version was updated with the conflicting metadata, we chose not to revert to it. To ensure system stability, we decided not to rollback to prior versions of the LKG either. Instead, we opted to edit the latest LKG, by removing the problematic customer configurations manually. We also opted to block all customer configuration changes from propagating to the data plane at 17:30 UTC so that, as we mitigate, we would not reintroduce this issue. At 17:40 UTC we began deploying the updated LKG configuration across the global fleet. Recovery required reloading all customer configurations at every edge site and rebalancing traffic gradually, to avoid overload conditions as Edge sites returned to service. This deliberate, phased recovery was necessary to stabilize the system while restoring scale and ensuring no recurrence of the issue.</p><p>Many downstream services that use AFD were able to failover to prevent further customer impact, including Microsoft Entra and Intune portals, and Azure Active Directory B2C. In a more complex example, Azure Portal leveraged its standard recovery process to successfully transition away from AFD during the incident. Users of the Portal would have seen limited impact during this failover process and then been able to use the Portal without issue. Unfortunately, some services within the Portal did not have an established fallback strategy and therefore parts of the Portal experience continued to experience failures even after Portal recovery (for example, Marketplace).</p><p>Timeline of major incident milestones:</p><ul><li>15:35 UTC on 29 October 2025 – Corrupt metadata first introduced, as described above.</li><li>15:41 UTC on 29 October 2025 – Customer impact began, triggered by the resulting crashes.</li><li>15:43 UTC on 29 October 2025 – Configuration protection system activated in response to issues.</li><li>15:48 UTC on 29 October 2025 – Investigation commenced following monitoring alerts being triggered.</li><li>16:15 UTC on 29 October 2025 – Focus of the investigation became examining AFD configuration changes.</li><li>16:18 UTC on 29 October 2025 – Initial communication posted to our public status page.</li><li>16:20 UTC on 29 October 2025 – Targeted communications to impacted customers sent to Azure Service Health.</li><li>17:10 UTC on 29 October 2025 – Began updating the ‘last known good’ LKG configuration to remove problematic configurations manually.</li><li>17:26 UTC on 29 October 2025 – Azure Portal failed away from Azure Front Door.</li><li>17:30 UTC on 29 October 2025 – Blocked all customer configuration propagation to the data plane, in preparation for deploying the new configuration.</li><li>17:40 UTC on 29 October 2025 – Initiated the deployment of our updated ‘last known good’ configuration.</li><li>17:50 UTC on 29 October 2025 – Last known good configuration available to all Edge sites, which began gradually reloading LKG configuration.</li><li>18:30 UTC on 29 October 2025 – AFD DNS servers recovered, allowing us to rebalance traffic manually to a small number of healthy Edge sites. Customers began seeing improvements in availability.</li><li>20:20 UTC on 29 October 2025 – As a sufficient number of Edge sites had recovered, we switched to automatic traffic management – customers continued to see availability improve.</li><li>00:05 UTC on 30 October 2025 – AFD impact confirmed mitigated for customers, as availability and latency had returned to pre-incident levels.</li></ul><p>Post mitigation, we temporarily blocked all AFD customer configuration changes at the Azure Resource Manager (ARM) level to ensure the safety of the data plane. We also implemented additional safeguards including (i) fixing the control plane and data plane defects, (ii) removing asynchronous processing from the data plane, (iii) introducing an additional ‘pre-canary’ stage to test customer configuration (iv) extending the bake time during each stage of the configuration propagation, and (v) improvements to the data plane recovery time from approximately 4.5 hours to approximately one hour. We began draining the customer configuration queue from 2 November 2025. Once these safeguards were fully implemented, this restriction was removed on 5 November 2025.</p><p>The introduction of new stages in the configuration propagation pipeline was coupled with additional ‘bake time’ between stages – which has resulted in an increase in configuration propagation time, for all operations including create, update, delete, WAF operations on AFD platform, and cache purges. We continue to work on platform enhancements to ensure a robust configuration delivery pipeline and further reduce the propagation time. For more details on these temporary propagation delays, refer to <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"http://aka.ms/AFD_FAQ\\\\\">http://aka.ms/AFD_FAQ</a>.</p><p><strong>How are we making incidents like this less likely or less impactful?</strong></p><p>To prevent issues like this, and improve deployment safety...</p><ul><li>We have fixed both the original control plane incompatibility, and the data plane bug described above. (Completed)</li><li>We are now enforcing complete synchronous processing of each customer configuration, before advancing to production stages. (Completed)</li><li>We have implemented additional stages in our phased configuration rollout, including extended bake time to help detect configuration related issues. (Completed)</li><li>We are decoupling configuration processing in data plane servers from active traffic-serving instances to isolated worker process instances, thereby removing the risk of any configuration defect impacting data plane processing. (Estimated completion: January 2026)</li><li>Once our pre-validation of this configuration pipeline is in place, we will work towards reducing the propagation time from 45 minutes to approximately 15 minutes. (Estimated completion: January 2026)</li><li>We are enhancing our testing and validation framework to ensure backwards compatibility with configurations generated across previous versions of the control plane build. (Estimated completion: February 2026)</li></ul><p>To reduce the blast radius of potential future issues...</p><ul><li>We have migrated critical first-party infrastructure (including Azure Portal, Azure Communication Services, Marketplace, Linux Software Repository for Microsoft Products, Support ticket creation) into an active-active solution with fail away. (Completed)</li><li>In the longer term, we are enhancing our customer configuration and traffic isolation, to ensure that no impact to any other customers from single customers’ traffic or configuration issue – utilizing ‘micro cell’ segmentation of the AFD data plane. (Estimated completion: June 2026)</li></ul><p>To be able to recover more quickly from issues...</p><ul><li>We have made changes to accelerate our data plane recovery time, by leveraging the local customer configuration caching more effectively – to restore customer configurations within one hour. (Completed)</li><li>We are making investments to reduce data plane recovery time further, to restore customer configurations within approximately 10 minutes. (Estimated completion: March 2026)</li></ul><p>To improve our communications and support...</p><ul><li>We have addressed delays in delivering alerts via Azure Service Health to impacted customers, by making immediate improvements to increase resource thresholds. (Completed)</li><li>We will expand the automated customer alerts sent via Azure Service Health, to include similar classes of service degradation – to notify impacted customers more quickly. (Estimated completion: November 2025)</li><li>Finally, we will resolve the technical and staffing challenges that prevented Premier and Unified customers from being able to create a support request, by ensuring that we can failover to our backups systems more quickly. (Estimated completion: November 2025)</li></ul><p><strong>How can customers make incidents like this less impactful?</strong><em>\\xa0</em></p><ul><li>Understand best practices for building a resilient global HTTP ingress layer, to maintain availability during regional or network disruptions: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://learn.microsoft.com/azure/architecture/guide/networking/global-web-applications/mission-critical-global-http-ingress\\\\\">https://learn.microsoft.com/azure/architecture/guide/networking/global-web-applications/mission-critical-global-http-ingress</a></li><li>As an alternative for when management portals are temporarily inaccessible, customers can consider using programmatic methods to manage resources – including the REST API (<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://learn.microsoft.com/rest/api/azure\\\\\">https://learn.microsoft.com/rest/api/azure</a>) and PowerShell (<a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://learn.microsoft.com/powershell/azure\\\\\">https://learn.microsoft.com/powershell/azure</a>)</li><li>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/WAF\\\\\">https://aka.ms/AzPIR/WAF</a></li><li>The impact times above represent the full incident duration, so are not specific to any individual customer. Actual impact to service availability varied between customers and resources – for guidance on implementing monitoring to understand granular impact: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Monitoring\\\\\">https://aka.ms/AzPIR/Monitoring</a></li><li>Finally, consider ensuring that the right people in your organization will be notified about any future service issues – by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/Alerts\\\\\">https://aka.ms/AzPIR/Alerts</a></li></ul><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick 3-question survey: <a class=\\\\\"wa-link-status\\\\\" href=\\\\\"https://aka.ms/AzPIR/YKYN-BWZ\\\\\">https://aka.ms/AzPIR/YKYN-BWZ</a></p>\\\\n</div>\",\"source_url\":\"https://azure.status.microsoft/en-us/status/history/\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"We are investigating elevated error rates with multiple products in us-east1\",\"status\":null,\"start_time\":\"2025-07-18T14:42:00+00:00\",\"end_time\":\"2025-07-18T16:47:00+00:00\",\"url\":\"incidents/8cY8jdUpEGGbsSMSQk7J\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Multiple GCP products are experiencing Service issues.\",\"status\":null,\"start_time\":\"2025-06-12T17:51:00+00:00\",\"end_time\":\"2025-06-13T01:18:00+00:00\",\"url\":\"incidents/ow5i3PPK96RduMcb1SsW\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Google Compute Engine (GCE) issue impacting multiple dependent GCP services across zones\",\"status\":null,\"start_time\":\"2025-05-20T03:23:00+00:00\",\"end_time\":\"2025-05-20T12:05:00+00:00\",\"url\":\"incidents/SXRPpPwx2RZ5VHjTwFLx\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Customers are experiencing connectivity issues with multiple Google Cloud services in zone us-east5-c\",\"status\":null,\"start_time\":\"2025-03-29T19:53:00+00:00\",\"end_time\":\"2025-03-30T02:15:00+00:00\",\"url\":\"incidents/N3Dw7nbJ7rk7qwrtwh7X\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Apigee customers may experience unable to login to Admin UI portal.\",\"status\":null,\"start_time\":\"2025-03-04T20:04:19+00:00\",\"end_time\":\"2025-03-04T21:40:41+00:00\",\"url\":\"incidents/hdknJ5aWh8KCAimNhTHe\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Cloud Asset Inventory customers\\' queries may not return result\",\"status\":null,\"start_time\":\"2025-02-19T09:14:00+00:00\",\"end_time\":\"2025-02-20T07:56:52+00:00\",\"url\":\"incidents/32iSTecJmvVhCPRvCuWX\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"SIEM Dashboards and SOAR Advanced Dashboards for Google Security Operations (SecOps) were unavailable.\",\"status\":null,\"start_time\":\"2025-02-18T22:42:52+00:00\",\"end_time\":\"2025-02-19T02:41:56+00:00\",\"url\":\"incidents/r23WwsX2tpSN7RyFs83c\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Vertex AI Search for commerce customers may observe 100% error rate for certain IPs.\",\"status\":null,\"start_time\":\"2025-02-16T12:48:58+00:00\",\"end_time\":\"2025-02-16T15:49:43+00:00\",\"url\":\"incidents/YzMELUzpd8rYgwYt714D\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Chronicle Security experienced issues related to feed based ingestion in europe-west2\",\"status\":null,\"start_time\":\"2025-02-13T20:13:52+00:00\",\"end_time\":\"2025-02-14T05:39:53+00:00\",\"url\":\"incidents/eya5zBxFRFhNqBhUXe6Q\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Chronicle Security users experiencing an issue in us-multiregions\",\"status\":null,\"start_time\":\"2025-02-02T02:00:00+00:00\",\"end_time\":\"2025-02-03T09:54:55+00:00\",\"url\":\"incidents/3C3D9dLK9dkx8kRdc72a\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Chronicle Security - WORKSPACE_ACTIVITY data ingestion observed delays in asia-southeast1 & asia-south1 regions\",\"status\":null,\"start_time\":\"2025-01-31T00:53:00+00:00\",\"end_time\":\"2025-01-31T14:13:00+00:00\",\"url\":\"incidents/n8QFYMxUxe65sum9P1gk\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Cloud Translation experiencing elevated latency and error rates\",\"status\":null,\"start_time\":\"2025-01-28T12:50:17+00:00\",\"end_time\":\"2025-01-29T04:54:18+00:00\",\"url\":\"incidents/jPyjAMj7j3NksnWVMTRt\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Cloud Translation experienced elevated latency and error rates\",\"status\":null,\"start_time\":\"2025-01-28T12:50:17+00:00\",\"end_time\":\"2025-01-28T22:16:39+00:00\",\"url\":\"incidents/uqPLSADLwLztWWcLCPfz\"},{\"cloud\":\"gcp\",\"service\":\"\",\"region\":\"global\",\"title\":\"Appsheet is unavailable in us-east4 and europe-west4\",\"status\":null,\"start_time\":\"2025-01-24T17:30:00+00:00\",\"end_time\":\"2025-01-24T19:20:00+00:00\",\"url\":\"incidents/qB1du5LQfSHCJjWR88Fi\"}]}'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outage_json = get_cloud_outages_json()\n",
        "outage_json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e25031d",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a snarky assistant that analyzes the outages of 3 different cloud providers and provider suggestions on which cloud provider I choose based on the outage.\n",
        "Provides a short, snarky, humorous summary, ignoring text that might be navigation related.\n",
        "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "71eb6956",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_prompt_prefix = \"\"\"\n",
        "Here are the contents of the outage for 90 days from aws, azure and gcp. If a cloud service provider is not giving enough outage details, consider that as well.\n",
        "Some cases the cloud provider (eg. AWS) gives details but it the data is retrieved dynamically from server and hence cannot be scraped from webpage. But the data can be accessed directly going to the browser and click different events. No payment needed.\n",
        "Provide a short summary of the outage and your recommendation on which service provider to choose and why.\n",
        "If it includes major and minor outages and duration of outage, then summarize these too.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4a9ad3dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def messages_for(outage_data):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_prefix + outage_data}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "98270a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(outage_data):\n",
        "    openai = OpenAI()\n",
        "    response = openai.chat.completions.create(\n",
        "        model = \"gpt-4.1-mini\",\n",
        "        messages = messages_for(outage_data)\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "60ae1f61",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Cloud Outage Snark-Off: Who’s the least outage-y?\\n\\n## AWS\\n- **Outage transparency?** Nope. AWS treats its historical outage data like the crown jewels – locked behind Business/Enterprise support paywalls or sneaky JavaScript browser gymnastics.\\n- **Incident count:** 21 (but details? Nada from the JSON feed.)\\n- **Summary:** AWS outages might be happening, but you’ll have to be in the VIP club to know the juicy details. Otherwise, it\\'s all smoke and mirrors.\\n\\n## Azure\\n- **Incident count:** Multiple detailed incidents over the last 90 days.\\n- **Major Outages:**\\n  - **Jan 2026 West US 2 power event:** ~7.5 hours outage affecting a slew of services due to an emergency power-off safety system tripping. Compute/storage back by ~2 hours, but networking residual issues lasted till ~7.5 hours later.\\n  - **Dec 2025 PIM API failures:** ~10.5 hours of elevated errors and timeouts tied to database connection exhaustion after a config change.\\n  - **Dec 2025 ARM key rotation hiccups (two incidents, Azure Government & China):** ~3 to 9 hours of service management operation failures caused by rogue automated key rotations.\\n  - **Nov 2025 Thermal event in West Europe:** ~9.5 hours outage caused by voltage sag and a stubborn cooling unit refusing to restart, cascading into prolonged storage & compute downtime.\\n  - **Oct 2025 Azure Front Door debacle:** ~8.5 hours global outage due to corrupt metadata causing data plane crashes and DNS failures.\\n- **Communication:** Thorough, transparent, and even backed by follow-up videos and surveys.\\n- **Mitigations:** Real plans with estimated dates, learning from mistakes, and customer guidance on resilience.\\n- **Snark Meter:** Impressive honesty and effort, but quite the parade of misfortunes.\\n\\n## GCP\\n- **Incident count:** Multiple outages but zero juicy details beyond \"service issues\" and timestamps.\\n- **Durations:** Range from ~1.5 hours to multi-hour outages, including a 9-hour or so outage for Chronicle Security feed delays.\\n- **Communication:** Minimal, no root causes, no post mortems, just cold outage notices and \"we’re investigating\" vibes.\\n- **Transparency:** About as clear as a foggy London morning.\\n- **Snark Meter:** \"Trust us, things happened, but you don’t really need to know why.\"\\n\\n---\\n\\n# So, who to pick?\\n\\n| Provider | Outage Transparency | Outage Impact | Communication Quality | Recommendation Snark |\\n|----------|---------------------|---------------|-----------------------|---------------------|\\n| AWS      | Locked down         | Unknown       | meh                   | Invisible outages are still outages. If you want mystery with your downtime, AWS is your Bermuda Triangle.|\\n| Azure    | Crystal clear       | Several multi-hour incidents affecting big services | Detailed with timelines, fixes, and lessons | Azure’s outages are like bad reality TV: dramatic and frequent, but at least they spill the tea. Great if you like knowing what’s bombed, so you can plan accordingly.|\\n| GCP      | Minimal             | Multi-hour outages across various services   | Minimal, no root cause analysis | Outages happened, but Google’s master plan is to keep you guessing. Fun for those who enjoy surprise downtime without spoilers.|\\n\\n**Recommendation:**  \\nIf you fancy knowing your chaos and prefer a partner who openly admits when they screw up (and actually tries to fix it), **Azure** is your lovable disaster buddy. AWS might be stable but good luck confirming that (you need a secret handshake). GCP? They throw the outages party but never send invites or after-parties.\\n\\nPick Azure if transparency and detailed retrospectives matter. Otherwise, brace for mystery outages with AWS or GCP.\\n\\n---\\n\\n*And remember: No cloud provider is perfect; the best one is the one whose outage dance moves you can live with.*'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarize(outage_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2bf29ed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_summary(outage_data):\n",
        "    summary = summarize(outage_data)\n",
        "    display(Markdown(summary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "207daf59",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Cloud Outage Snark & Recommendations\n",
              "\n",
              "## AWS  \n",
              "- AWS doesn't share its historical outage data via API or static means. You gotta play detective in your browser with JavaScript ninja skills and have a Business/Enterprise support plan.  \n",
              "- So basically, AWS is like that friend who \"forgot\" to answer your texts about last night's party (outages). The data is there, but you'll have to go spelunking through their UI.  \n",
              "- Incident count is 21, but no concrete downtime or affected services details publicly given. Sneaky!\n",
              "\n",
              "---\n",
              "\n",
              "## Azure  \n",
              "Azure put on a mini soap opera of incidents from November 2025 to January 2026. Here's the gist:  \n",
              "\n",
              "1. **Power Outage (Jan 10-11, 2026, West US 2 AZ01)**  \n",
              "   - Power-off safety system freaked out, killing power to racks.  \n",
              "   - Services down: VMs, Storage, Databricks, Synapse, SQL DB, Redis, Cosmos DB & more.  \n",
              "   - Downtime: ~7.5 hours (17:50 UTC 10 Jan to 01:23 UTC 11 Jan).  \n",
              "   - Residual VM creation/updates affected until manual fix at 01:23.  \n",
              "   - Response time: Fast detection, but recovery took time because of network layer (SLB) complexity.  \n",
              "\n",
              "2. **Entra Privileged Identity Management API Failures (Dec 22, 2025)**  \n",
              "   - Deployment overload = CPU spike + exhausted DB connections.  \n",
              "   - Duration roughly 10+ hours (08:05 to 18:30 UTC).  \n",
              "   - Rolled-back config, scaled out resources, restarted DB.  \n",
              "\n",
              "3. **ARM (Azure Resource Manager) Service Failures (Dec 8, 2025) & China regions**  \n",
              "   - Automated key rotation gone wild, causing auth failures globally in ARM management plane.  \n",
              "   - Outage lasted ~3 hours (11:04-14:13 EST in US Gov regions; ~9.5 hours in China region).  \n",
              "   - Service management via portal, CLI, APIs was down or flaky.  \n",
              "\n",
              "4. **Thermal Event in West Europe (Nov 5-6, 2025)**  \n",
              "   - Voltage sag shut down cooling units, racks overheated and storage scaled down forcibly.  \n",
              "   - Recovery took ~9.5 hours (16:53 Nov 5 to 02:25 Nov 6 UTC).  \n",
              "   - Extensive data consistency checks slowed restoration.  \n",
              "\n",
              "5. **Azure Front Door (AFD) Global Outage due to config bug (Oct 29-30, 2025)**  \n",
              "   - Config metadata incompatibility caused data plane crashes and DNS resolution errors.  \n",
              "   - Impact: 8+ hours outage (15:41 Oct 29 to 00:05 Oct 30 UTC).  \n",
              "   - Recovery via manual rollout of fixed config; some portal services still affected longer.  \n",
              "\n",
              "**Azure takeaway:** They provide detailed post-mortems, full timelines, root causes, mitigations, and customer guidance. Confidence points for transparency and thoroughness - they own their messes and plan fixes.\n",
              "\n",
              "---\n",
              "\n",
              "## GCP  \n",
              "- Outages are listed mostly as titles with start and end time, no juicy root causes or affected services specifics.  \n",
              "- Examples:  \n",
              "  - 9+ major incidents over 90 days, lasting from couple hours to over a day; e.g., compute engine issues (May 20, 2025, ~8.5hrs), elevated error rates in various regions.   \n",
              "  - Titles mention API failures, latency spikes, service unavailability but zero detailed explanations.  \n",
              "- A cloud outage page is a mystery novel with no plot, just a list of \"We noticed this, it lasted that long.\"  \n",
              "- No public post incident reviews, no root cause, no customer guidance. It's like \"Trust us, stuff happened.\"  \n",
              "\n",
              "---\n",
              "\n",
              "# Final Snarky Summary & Recommendation\n",
              "\n",
              "| Provider | Transparency | Outage Severity / Duration | Incident Detail | Cool Factor | Recommend? |\n",
              "|----------|--------------|----------------------------|-----------------|-------------|------------|\n",
              "| **AWS**  | Wall of silence + browser hacking | Unknown (21 incidents, no details) | Nada in API or JSON, only in-browser JS detective work | Very secretive, Houdini-level data tricks | Nah, unless you're a JS spelunker with $$$ |\n",
              "| **Azure**| Full drama series, in-depth PIRs with timelines, causes, mitigations | Multiple multi-hour incidents, up to ~9.5 hrs outage on some incidents | Detailed, candid, and actionable, plus customer guidance | Transparency and accountability champs | Yes, if you want to know what went wrong and how it's fixed; slightly higher downtime overall but they *own* it |\n",
              "| **GCP**  | Minimal, vague incident titles with no root causes or fixes documented | Many (~9) multi-hour outages, no detail | Evasive, vague, no customer advice or postmortem | The \"we had issues\" whisperer | Meh, unless you like to live in mystery |\n",
              "\n",
              "---\n",
              "\n",
              "# **Recommendation:**  \n",
              "\n",
              "If you want a cloud that openly admits when it borked and tells you what exactly to do next, go **Azure**. Their outages are transparent, root problems are explained, and they provide timeline and remediation plans. Sure, they have several multi-hour outages, but at least you can make informed decisions and plan around them.\n",
              "\n",
              "**AWS** is like the secretive sibling who knows they messed up but makes you earn the info with complicated JavaScript dance routines — not fun unless you have deep pockets and lots of patience.\n",
              "\n",
              "**GCP** feels like the elusive fog of cloud outages — many events, but no clues and no answers, just \"stuff happened.\" If you like mystery and suspense, that's your pick.\n",
              "\n",
              "---\n",
              "\n",
              "# TL;DR:  \n",
              "Azure spills the tea, AWS hides it in JS code, GCP whispers \"yeah, something broke.\"\n",
              "\n",
              "Choose **Azure** if you want to avoid blind spots in your cloud outage knowledge.  \n",
              "Choose **AWS** if you like a challenge and don’t mind paying for it.  \n",
              "Choose **GCP** if you enjoy guessing games about outage causes.  \n",
              "\n",
              "---\n",
              "\n",
              "# Final note:  \n",
              "Clouds will sometimes rain, but at least Azure shows you the clouds. The others? They keep their skies suspiciously clear in public view. ☁️🌩️🕵️‍♂️"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_summary(outage_json)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
