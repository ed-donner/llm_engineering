{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2d20a1",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d259f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from scraper import fetch_website_contents\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e59a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Ollama is running\n",
    "requests.get(f'{OLLAMA_BASE_URL}').content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8eead",
   "metadata": {},
   "source": [
    "### Download llama3.2 from meta\n",
    "\n",
    "Change this to llama3.2:1b if your computer is smaller.\n",
    "\n",
    "Don't use llama3.3 or llama4! They are too big for your computer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31230c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama3.2 to your machine \n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28995faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenAI client with Ollama\n",
    "ollama = OpenAI(base_url=f'{OLLAMA_BASE_URL}/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f97e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteSummarizer:\n",
    "  \n",
    "    self.system_prompt =  \"\"\"\n",
    "You are a snarky assistant that analyzes the contents of a website,\n",
    "and provides a short, snarky, humorous summary, ignoring text that might be navigation related.\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\"\n",
    "    self.user_prompt_prefix =  \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.  \n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.contents = fetch_website_contents(url)\n",
    "        self.summary = self.summarize()\n",
    "        self.client = ollama\n",
    "          \n",
    "        \n",
    "\n",
    "    def messages_for(self):\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f'{self.user_prompt_prefix}/n{self.contents}'}\n",
    "        ]\n",
    "\n",
    "    def summarize(self):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama3.2\",\n",
    "            messages=self.messages_for()\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "      \n",
    "    def display_summary(self):\n",
    "        display(Markdown(self.summary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
