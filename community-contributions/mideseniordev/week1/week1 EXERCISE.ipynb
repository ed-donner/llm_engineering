{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {},
      "source": [
        "# End of week 1 exercise\n",
        "\n",
        "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
        "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c1070317-3ed9-4659-abe3-828943230e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display, update_display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment & constants\n",
        "load_dotenv(override=True)\n",
        "MODEL_GPT, MODEL_LLAMA = 'gpt-4o-mini', 'llama3.2'\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    MODEL_LLAMA: {\"base_url\": \"http://localhost:11434/v1\", \"api_key\": \"ollama\"},\n",
        "    MODEL_GPT: {\"base_url\": \"https://openrouter.ai/api/v1\", \"api_key\": os.getenv(\"OPENROUTER_API_KEY\")},\n",
        "}\n",
        "\n",
        "# Validate OpenRouter key once at startup (Ollama needs no key)\n",
        "_key = MODEL_CONFIG[MODEL_GPT][\"api_key\"]\n",
        "if not (_key and len(_key) > 10):\n",
        "    print(\"OpenRouter API key may be missing. Check .env and troubleshooting notebook.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c2fae9a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_client(model):\n",
        "    cfg = MODEL_CONFIG.get(model)\n",
        "    if not cfg:\n",
        "        raise ValueError(f\"Unknown model: {model}. Use {MODEL_GPT} or {MODEL_LLAMA}\")\n",
        "    return OpenAI(base_url=cfg[\"base_url\"], api_key=cfg[\"api_key\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "780462af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompts & main helper below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ce4c38a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts — System Design Interview Expert\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a senior staff engineer conducting a system design interview at a top tech company (FAANG-level). Your role is to guide candidates through a structured, realistic system design discussion.\n",
        "\n",
        "## Your Approach\n",
        "\n",
        "1. **Requirements Clarification** — Start by clarifying functional and non-functional requirements. Ask about scale (DAU, QPS, storage), consistency needs, latency targets, and key use cases. Make reasonable assumptions when the user doesn't specify.\n",
        "\n",
        "2. **High-Level Design** — Propose a top-level architecture: clients, load balancers, API servers, core services, databases, caches, message queues. Draw ASCII diagrams when helpful. Identify the main components and data flow.\n",
        "\n",
        "3. **Deep Dive** — Zoom into 2-3 critical components: data models, sharding strategy, caching layers, replication, or consistency mechanisms. Discuss trade-offs (e.g., consistency vs availability, read vs write optimization).\n",
        "\n",
        "4. **Scale & Bottlenecks** — Address scalability: horizontal vs vertical scaling, back-of-envelope capacity estimates (storage, bandwidth, QPS). Identify potential bottlenecks and mitigation strategies.\n",
        "\n",
        "5. **Fault Tolerance & Operations** — Briefly cover failure modes, replication, failover, monitoring, and operational concerns.\n",
        "\n",
        "## Output Style\n",
        "\n",
        "- Use clear markdown: headers, bullet points, code blocks for schemas or configs.\n",
        "- Include simple ASCII diagrams for architecture (e.g., Client → LB → API → DB).\n",
        "- Be concise but thorough. Prioritize clarity over length.\n",
        "- When making assumptions, state them explicitly (e.g., \"Assuming 10M DAU...\").\n",
        "- Reference real-world patterns: consistent hashing, write-ahead logs, leader election, etc.\n",
        "\n",
        "## Tone\n",
        "\n",
        "- Professional and interview-like. Assume the \"candidate\" (user) is competent and engaged.\n",
        "- Don't over-explain basics; focus on the non-obvious and trade-off discussions.\n",
        "\"\"\"\n",
        "\n",
        "# Template: user provides the system design question\n",
        "USER_PROMPT_TEMPLATE = \"\"\"\n",
        "Design {question}\n",
        "\n",
        "{optional_context}\n",
        "\"\"\"\n",
        "\n",
        "# Example system design questions you can plug in:\n",
        "EXAMPLE_QUESTIONS = [\n",
        "    \"Design a URL shortener like bit.ly\",\n",
        "    \"Design a rate limiter for an API\",\n",
        "    \"Design a distributed cache (like Redis)\",\n",
        "    \"Design a chat system (like Slack or WhatsApp)\",\n",
        "    \"Design YouTube or Netflix (video streaming)\",\n",
        "    \"Design a search autocomplete system\",\n",
        "    \"Design a notification system\",\n",
        "]\n",
        "\n",
        "\n",
        "def build_user_prompt(question: str, context: str = \"\") -> str:\n",
        "    ctx = f\"Context/Constraints:\\n{context}\\n\\n\" if context.strip() else \"\"\n",
        "    return USER_PROMPT_TEMPLATE.format(question=question, optional_context=ctx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_system_design(question: str, model: str = MODEL_GPT, context: str = \"\", stream: bool = True):\n",
        "    \"\"\"Ask a system design question. Streams by default.\"\"\"\n",
        "    client = get_client(model)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": build_user_prompt(question, context)},\n",
        "    ]\n",
        "    response = client.chat.completions.create(model=model, messages=messages, stream=stream)\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "    if not stream:\n",
        "        return response.choices[0].message.content\n",
        "    full = \"\"\n",
        "    for chunk in response:\n",
        "        content = chunk.choices[0].delta.content or \"\"\n",
        "        full += content\n",
        "        update_display(Markdown(full), display_id=display_handle.display_id)\n",
        "    return full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's design a URL shortener service similar to Bit.ly. We will go through the structured approach outlined above.\n",
              "\n",
              "### 1. Requirements Clarification\n",
              "\n",
              "#### Functional Requirements:\n",
              "- Users can submit a long URL and receive a shortened URL.\n",
              "- Users can retrieve the original URL by using the shortened URL.\n",
              "- Users can track the number of clicks on each shortened URL (optional).\n",
              "\n",
              "#### Non-Functional Requirements:\n",
              "- **Scale**: \n",
              "  - Assuming 10 million daily active users (DAU).\n",
              "  - Assume about 100 million URL shortenings per day.\n",
              "- **Performance**: \n",
              "  - Latency target: <100 ms for redirecting to original URL.\n",
              "- **Consistency**: \n",
              "  - Eventual consistency for analytics and tracking.\n",
              "- **Availability**: \n",
              "  - High availability, aiming for 99.99% uptime.\n",
              "\n",
              "### 2. High-Level Design\n",
              "\n",
              "#### Top-Level Architecture\n",
              "```\n",
              "Client ---> Load Balancer ---> API Servers ---> URL Shortening Service\n",
              "                              |\n",
              "                             \\|/\n",
              "                         Database\n",
              "                              |\n",
              "                        Cache Layer\n",
              "```\n",
              "\n",
              "#### Components:\n",
              "- **Client**: Web or mobile application for users to submit and retrieve URLs.\n",
              "- **Load Balancer**: Distributes incoming requests across multiple API servers for scalability.\n",
              "- **API Servers**: Handle client requests, manage interactions with the URL Shortening service.\n",
              "- **URL Shortening Service**: Contains the logic to create short URLs and retrieve original URLs.\n",
              "- **Database**: Stores the mappings of short URLs to original URLs, along with click analytics (can use SQL or NoSQL).\n",
              "- **Cache Layer**: To speed up retrieval of frequently accessed short URLs.\n",
              "  \n",
              "### 3. Deep Dive\n",
              "\n",
              "#### Data Model\n",
              "We can define our main entities as follows:\n",
              "\n",
              "**URL Mapping Table**\n",
              "```sql\n",
              "CREATE TABLE url_mapping (\n",
              "    id SERIAL PRIMARY KEY,\n",
              "    original_url TEXT NOT NULL,\n",
              "    short_url VARCHAR(10) UNIQUE NOT NULL,\n",
              "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
              "    total_clicks INT DEFAULT 0\n",
              ");\n",
              "```\n",
              "\n",
              "#### Shortening Logic\n",
              "- Use a base62 encoding scheme to generate short URLs. For example, each ID (incremental or random) can be converted to a short string using alphanumeric characters (`0-9`, `a-z`, `A-Z`).\n",
              "\n",
              "#### Caching Strategy\n",
              "- Use Redis or Memcached to cache recently accessed short URL mappings to reduce database load.\n",
              "\n",
              "#### Consistency Model\n",
              "- For tracking clicks, we can use a background job to update the database asynchronously:\n",
              "  - On each redirect, perform the update of the total clicks in a non-blocking way.\n",
              "\n",
              "### 4. Scale & Bottlenecks\n",
              "\n",
              "#### Capacity Estimates\n",
              "- For 100 million shortenings in a day, that’s approximately 1,157 requests per second (RPS) for create endpoints and similar for retrievals.\n",
              "  \n",
              "- **Database Scaling**: Use sharding based on hash of the short URL to distribute load.\n",
              "- **Read vs Write Optimization**: Focus on optimizing reads via caching layers to reduce database bottlenecks.\n",
              "\n",
              "#### Potential Bottlenecks\n",
              "- **Database**: Implement sharding and replication to ensure it can handle high loads.\n",
              "- **Unique Short URL Generating**: Ensure uniqueness without collisions, possibly via a dedicated service for generating short codes.\n",
              "\n",
              "### 5. Fault Tolerance & Operations\n",
              "\n",
              "#### Failure Modes\n",
              "- Database failure can lead to loss of recent URL mappings or clicks. Implement replication and backups for fault tolerance.\n",
              "  \n",
              "#### Monitoring\n",
              "- Use monitoring tools like Prometheus or Grafana to keep track of request per second, error rates, and click-through rates.\n",
              "  \n",
              "#### Operations\n",
              "- Set up alerting for unusual patterns (e.g., sudden spikes in requests) and ensure a logging strategy for debugging.\n",
              "\n",
              "### Summary\n",
              "The URL shortener system has been outlined with appropriate assumptions about scale and functional requirements. Key components include an efficient database schema, caching for high read performance, and handling of click tracking with eventual consistency. After considering scalability and fault tolerance, the architecture should effectively serve a high volume of requests while maintaining performance."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\"Let's design a URL shortener service similar to Bit.ly. We will go through the structured approach outlined above.\\n\\n### 1. Requirements Clarification\\n\\n#### Functional Requirements:\\n- Users can submit a long URL and receive a shortened URL.\\n- Users can retrieve the original URL by using the shortened URL.\\n- Users can track the number of clicks on each shortened URL (optional).\\n\\n#### Non-Functional Requirements:\\n- **Scale**: \\n  - Assuming 10 million daily active users (DAU).\\n  - Assume about 100 million URL shortenings per day.\\n- **Performance**: \\n  - Latency target: <100 ms for redirecting to original URL.\\n- **Consistency**: \\n  - Eventual consistency for analytics and tracking.\\n- **Availability**: \\n  - High availability, aiming for 99.99% uptime.\\n\\n### 2. High-Level Design\\n\\n#### Top-Level Architecture\\n```\\nClient ---> Load Balancer ---> API Servers ---> URL Shortening Service\\n                              |\\n                             \\\\|/\\n                         Database\\n                              |\\n                        Cache Layer\\n```\\n\\n#### Components:\\n- **Client**: Web or mobile application for users to submit and retrieve URLs.\\n- **Load Balancer**: Distributes incoming requests across multiple API servers for scalability.\\n- **API Servers**: Handle client requests, manage interactions with the URL Shortening service.\\n- **URL Shortening Service**: Contains the logic to create short URLs and retrieve original URLs.\\n- **Database**: Stores the mappings of short URLs to original URLs, along with click analytics (can use SQL or NoSQL).\\n- **Cache Layer**: To speed up retrieval of frequently accessed short URLs.\\n  \\n### 3. Deep Dive\\n\\n#### Data Model\\nWe can define our main entities as follows:\\n\\n**URL Mapping Table**\\n```sql\\nCREATE TABLE url_mapping (\\n    id SERIAL PRIMARY KEY,\\n    original_url TEXT NOT NULL,\\n    short_url VARCHAR(10) UNIQUE NOT NULL,\\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\n    total_clicks INT DEFAULT 0\\n);\\n```\\n\\n#### Shortening Logic\\n- Use a base62 encoding scheme to generate short URLs. For example, each ID (incremental or random) can be converted to a short string using alphanumeric characters (`0-9`, `a-z`, `A-Z`).\\n\\n#### Caching Strategy\\n- Use Redis or Memcached to cache recently accessed short URL mappings to reduce database load.\\n\\n#### Consistency Model\\n- For tracking clicks, we can use a background job to update the database asynchronously:\\n  - On each redirect, perform the update of the total clicks in a non-blocking way.\\n\\n### 4. Scale & Bottlenecks\\n\\n#### Capacity Estimates\\n- For 100 million shortenings in a day, that’s approximately 1,157 requests per second (RPS) for create endpoints and similar for retrievals.\\n  \\n- **Database Scaling**: Use sharding based on hash of the short URL to distribute load.\\n- **Read vs Write Optimization**: Focus on optimizing reads via caching layers to reduce database bottlenecks.\\n\\n#### Potential Bottlenecks\\n- **Database**: Implement sharding and replication to ensure it can handle high loads.\\n- **Unique Short URL Generating**: Ensure uniqueness without collisions, possibly via a dedicated service for generating short codes.\\n\\n### 5. Fault Tolerance & Operations\\n\\n#### Failure Modes\\n- Database failure can lead to loss of recent URL mappings or clicks. Implement replication and backups for fault tolerance.\\n  \\n#### Monitoring\\n- Use monitoring tools like Prometheus or Grafana to keep track of request per second, error rates, and click-through rates.\\n  \\n#### Operations\\n- Set up alerting for unusual patterns (e.g., sudden spikes in requests) and ensure a logging strategy for debugging.\\n\\n### Summary\\nThe URL shortener system has been outlined with appropriate assumptions about scale and functional requirements. Key components include an efficient database schema, caching for high read performance, and handling of click tracking with eventual consistency. After considering scalability and fault tolerance, the architecture should effectively serve a high volume of requests while maintaining performance.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demo: GPT-4o-mini (streaming)\n",
        "ask_system_design(\"Design a URL shortener like bit.ly\", model=MODEL_GPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6fde180c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Rate Limiter Design\n",
              "=====================================\n",
              "\n",
              "## Overview\n",
              "---------\n",
              "\n",
              "This design implements a simple rate limiter for an API to prevent abuse and protect against common web attacks.\n",
              "\n",
              "### Requirements\n",
              "--------------\n",
              "\n",
              "* Handle 100M DAU (Daily Active Users)\n",
              "* Prevent excessive concurrent requests\n",
              "* Allow for bursty traffic within acceptable limits\n",
              "* Provide user-friendly error cases\n",
              "\n",
              "## Architecture Summary\n",
              "---------------------\n",
              "\n",
              "```markdown\n",
              "+---------------+\n",
              "|  Client     |\n",
              "+---------------+\n",
              "          |\n",
              "          | HTTP Request\n",
              "          v\n",
              "+---------------+\n",
              "| Load Balancer |\n",
              "+---------------+\n",
              "          |\n",
              "          |  URL/Method\n",
              "          |  Headers\n",
              "          v\n",
              "+---------------+\n",
              "| API Gateway    |\n",
              "+---------------+\n",
              "          |\n",
              "          |\n",
              "          +-----+   +-----------+     +--------+     +--------+    \n",
              "          |               |                  |  API    |       |  Cache  |\n",
              "+---------------+   +---------------+   | Servers |       |  Services|\n",
              "|  Rate Limiter  |           |              |  (Rate   |       |  (Cache) |\n",
              "|  (Backend)     |           |  (Frontend)|  Thrott.|       |  Storage)\n",
              "+---------------+\n",
              "```\n",
              "\n",
              "## Components\n",
              "-------------\n",
              "\n",
              "### API Gateway\n",
              "\n",
              "* Handles incoming HTTP requests from clients via the load balancer.\n",
              "* Validates request headers and ensures consistent API versioning.\n",
              "\n",
              "### Rate Limiter (Backend)\n",
              "\n",
              "* Implemented as a distributed rate limiter using the following strategies:\n",
              "\t1. **Token Bucket**: Allocate limited window-based slots to users; available for immediate usage.\n",
              "\t2. **Fixed Window with Leaky Buckets**: Limit number of requests in each time window, allowing gradual increase due to `leakage`.\n",
              "\n",
              "### API Server\n",
              "\n",
              "* Receives validated requests from the rate limiter;\n",
              "* Serves static resources when cache is present and passes on cached responses else\n",
              "* Calls external services for dynamic data.\n",
              "\n",
              "## Implementation Details\n",
              "---------------------\n",
              "\n",
              "### Rate Limiter (Backend)\n",
              "\n",
              "```python\n",
              "import time\n",
              "\n",
              "class TokenBucket:\n",
              "    def __init__(self, max_requests, time_window):\n",
              "        self.max_requests = max_requests\n",
              "        self.time_window = time_window\n",
              "        self.remaining_tokens = max_requests\n",
              "        self.last_reset_time = time.time()\n",
              "\n",
              "    def get_token(self):\n",
              "        now = time.time()\n",
              "        if now - self.last_reset_time < self.time_window / 2: # If less than half-time window, return existing tokens\n",
              "            self.remaining_tokens = min(max(0, self.max_requests - (self.remaining_tokens // (self.time_window // 2))), self.max_requests)\n",
              "        else:\n",
              "            self.remaining_tokens = max(0, self.max_requests)  # Reset remaining tokens to maximum\n",
              "\n",
              "        new_token = min(self.max_requests, self.remaining_tokens + 1)\n",
              "        self.last_reset_time = now\n",
              "        return new_token - 1\n",
              "\n",
              "# Rate Limiter Configuration\n",
              "config = {\n",
              "    'max_requests': 100,\n",
              "    'time_window': 60  # minutes\n",
              "\n",
              "}\n",
              "\n",
              "# Initialize and run rate limiter\n",
              "rate_limiter = TokenBucket(config['max_requests'], config['time_window'])\n",
              "```\n",
              "### API Server Handling\n",
              "\n",
              "* Returns cached response with proper cache headers.\n",
              "* If response is not cached in cache storage, makes request on the load balancer to pass through rate limit before calling to external services.\n",
              "\n",
              "```\n",
              "from flask import Flask, jsonify\n",
              "app = Flask(__name__)\n",
              "\n",
              "# Load Rate Limiter Configs\n",
              "config = {\n",
              "    'max_requests': 500,\n",
              "    'time_window': 1   # minutes\n",
              "}\n",
              "\n",
              "rate_limiter_config: TokenBucket = config\n",
              "\n",
              "\n",
              "@app.route('/')\n",
              "def get_index():\n",
              "    global rate_limiter_config\n",
              "\n",
              "    if request.headers.get(\"Authorization\"):\n",
              "        user_id = request.headers[\"Authorization\"].strip().split(\":\")[-1]\n",
              "        access_tocken = requests.post(f'{GET_ACCESS_TOKEN_URL}', headers={\"Content-Type\": \"application/json\"}, json={'user_id': user_id})\n",
              "\n",
              "        # If no returned token return an appropriate error response.\n",
              "        if 'auth_token' in access_tocken:\n",
              "            response['headers']['token'] = access_tocken.json['_token']\n",
              "            \n",
              "    resp  = rate_limiter_config.get_token()\n",
              "    \n",
              "     *return jsonify({\"success\": True}), responses.HTTP_401_UNAUTHORIZED\n",
              "    \n",
              "    # ... rest of server code here\n",
              "```\n",
              "\n",
              "## Potential Bottlenecks and Improvements\n",
              "--------------------------------------\n",
              "\n",
              "*   **Cache**: If cache doesn't become full due to constant increase in access tokens from clients, further improvements would be necessary.\n",
              "*   **External calls**: Reducing external service call latency can make the system performant for long-running processes or heavy request volume scenarios.\n",
              "*   **Security Considerations**: Protect sensitive data transmitted during authentication\n",
              "*   **Additional Features**: Incorporate a retry mechanism when an error occurs to prevent excessive backlogs."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'# Rate Limiter Design\\n=====================================\\n\\n## Overview\\n---------\\n\\nThis design implements a simple rate limiter for an API to prevent abuse and protect against common web attacks.\\n\\n### Requirements\\n--------------\\n\\n* Handle 100M DAU (Daily Active Users)\\n* Prevent excessive concurrent requests\\n* Allow for bursty traffic within acceptable limits\\n* Provide user-friendly error cases\\n\\n## Architecture Summary\\n---------------------\\n\\n```markdown\\n+---------------+\\n|  Client     |\\n+---------------+\\n          |\\n          | HTTP Request\\n          v\\n+---------------+\\n| Load Balancer |\\n+---------------+\\n          |\\n          |  URL/Method\\n          |  Headers\\n          v\\n+---------------+\\n| API Gateway    |\\n+---------------+\\n          |\\n          |\\n          +-----+   +-----------+     +--------+     +--------+    \\n          |               |                  |  API    |       |  Cache  |\\n+---------------+   +---------------+   | Servers |       |  Services|\\n|  Rate Limiter  |           |              |  (Rate   |       |  (Cache) |\\n|  (Backend)     |           |  (Frontend)|  Thrott.|       |  Storage)\\n+---------------+\\n```\\n\\n## Components\\n-------------\\n\\n### API Gateway\\n\\n* Handles incoming HTTP requests from clients via the load balancer.\\n* Validates request headers and ensures consistent API versioning.\\n\\n### Rate Limiter (Backend)\\n\\n* Implemented as a distributed rate limiter using the following strategies:\\n\\t1. **Token Bucket**: Allocate limited window-based slots to users; available for immediate usage.\\n\\t2. **Fixed Window with Leaky Buckets**: Limit number of requests in each time window, allowing gradual increase due to `leakage`.\\n\\n### API Server\\n\\n* Receives validated requests from the rate limiter;\\n* Serves static resources when cache is present and passes on cached responses else\\n* Calls external services for dynamic data.\\n\\n## Implementation Details\\n---------------------\\n\\n### Rate Limiter (Backend)\\n\\n```python\\nimport time\\n\\nclass TokenBucket:\\n    def __init__(self, max_requests, time_window):\\n        self.max_requests = max_requests\\n        self.time_window = time_window\\n        self.remaining_tokens = max_requests\\n        self.last_reset_time = time.time()\\n\\n    def get_token(self):\\n        now = time.time()\\n        if now - self.last_reset_time < self.time_window / 2: # If less than half-time window, return existing tokens\\n            self.remaining_tokens = min(max(0, self.max_requests - (self.remaining_tokens // (self.time_window // 2))), self.max_requests)\\n        else:\\n            self.remaining_tokens = max(0, self.max_requests)  # Reset remaining tokens to maximum\\n\\n        new_token = min(self.max_requests, self.remaining_tokens + 1)\\n        self.last_reset_time = now\\n        return new_token - 1\\n\\n# Rate Limiter Configuration\\nconfig = {\\n    \\'max_requests\\': 100,\\n    \\'time_window\\': 60  # minutes\\n\\n}\\n\\n# Initialize and run rate limiter\\nrate_limiter = TokenBucket(config[\\'max_requests\\'], config[\\'time_window\\'])\\n```\\n### API Server Handling\\n\\n* Returns cached response with proper cache headers.\\n* If response is not cached in cache storage, makes request on the load balancer to pass through rate limit before calling to external services.\\n\\n```\\nfrom flask import Flask, jsonify\\napp = Flask(__name__)\\n\\n# Load Rate Limiter Configs\\nconfig = {\\n    \\'max_requests\\': 500,\\n    \\'time_window\\': 1   # minutes\\n}\\n\\nrate_limiter_config: TokenBucket = config\\n\\n\\n@app.route(\\'/\\')\\ndef get_index():\\n    global rate_limiter_config\\n\\n    if request.headers.get(\"Authorization\"):\\n        user_id = request.headers[\"Authorization\"].strip().split(\":\")[-1]\\n        access_tocken = requests.post(f\\'{GET_ACCESS_TOKEN_URL}\\', headers={\"Content-Type\": \"application/json\"}, json={\\'user_id\\': user_id})\\n\\n        # If no returned token return an appropriate error response.\\n        if \\'auth_token\\' in access_tocken:\\n            response[\\'headers\\'][\\'token\\'] = access_tocken.json[\\'_token\\']\\n            \\n    resp  = rate_limiter_config.get_token()\\n    \\n     *return jsonify({\"success\": True}), responses.HTTP_401_UNAUTHORIZED\\n    \\n    # ... rest of server code here\\n```\\n\\n## Potential Bottlenecks and Improvements\\n--------------------------------------\\n\\n*   **Cache**: If cache doesn\\'t become full due to constant increase in access tokens from clients, further improvements would be necessary.\\n*   **External calls**: Reducing external service call latency can make the system performant for long-running processes or heavy request volume scenarios.\\n*   **Security Considerations**: Protect sensitive data transmitted during authentication\\n*   **Additional Features**: Incorporate a retry mechanism when an error occurs to prevent excessive backlogs.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demo: Llama 3.2 via Ollama (streaming)\n",
        "ask_system_design(\"Design a rate limiter for an API\", model=MODEL_LLAMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9ce97a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# With optional context (scale, constraints)\n",
        "ask_system_design(\n",
        "    \"Design a chat system like Slack\",\n",
        "    model=MODEL_GPT,\n",
        "    context=\"Scale: 50M DAU, 10B messages/day. Focus on real-time delivery and message ordering.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
