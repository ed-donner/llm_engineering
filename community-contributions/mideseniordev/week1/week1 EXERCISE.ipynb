{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {},
      "source": [
        "# End of week 1 exercise\n",
        "\n",
        "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
        "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1070317-3ed9-4659-abe3-828943230e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display, update_display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment & constants\n",
        "load_dotenv(override=True)\n",
        "MODEL_GPT, MODEL_LLAMA = 'gpt-4o-mini', 'llama3.2'\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    MODEL_LLAMA: {\"base_url\": \"http://localhost:11434/v1\", \"api_key\": \"ollama\"},\n",
        "    MODEL_GPT: {\"base_url\": \"https://openrouter.ai/api/v1\", \"api_key\": os.getenv(\"OPENROUTER_API_KEY\")},\n",
        "}\n",
        "\n",
        "# Validate OpenRouter key once at startup (Ollama needs no key)\n",
        "_key = MODEL_CONFIG[MODEL_GPT][\"api_key\"]\n",
        "if not (_key and len(_key) > 10):\n",
        "    print(\"OpenRouter API key may be missing. Check .env and troubleshooting notebook.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2fae9a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_client(model):\n",
        "    cfg = MODEL_CONFIG.get(model)\n",
        "    if not cfg:\n",
        "        raise ValueError(f\"Unknown model: {model}. Use {MODEL_GPT} or {MODEL_LLAMA}\")\n",
        "    return OpenAI(base_url=cfg[\"base_url\"], api_key=cfg[\"api_key\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780462af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompts & main helper below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4c38a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompts — System Design Interview Expert\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a senior staff engineer conducting a system design interview at a top tech company (FAANG-level). Your role is to guide candidates through a structured, realistic system design discussion.\n",
        "\n",
        "## Your Approach\n",
        "\n",
        "1. **Requirements Clarification** — Start by clarifying functional and non-functional requirements. Ask about scale (DAU, QPS, storage), consistency needs, latency targets, and key use cases. Make reasonable assumptions when the user doesn't specify.\n",
        "\n",
        "2. **High-Level Design** — Propose a top-level architecture: clients, load balancers, API servers, core services, databases, caches, message queues. Draw ASCII diagrams when helpful. Identify the main components and data flow.\n",
        "\n",
        "3. **Deep Dive** — Zoom into 2-3 critical components: data models, sharding strategy, caching layers, replication, or consistency mechanisms. Discuss trade-offs (e.g., consistency vs availability, read vs write optimization).\n",
        "\n",
        "4. **Scale & Bottlenecks** — Address scalability: horizontal vs vertical scaling, back-of-envelope capacity estimates (storage, bandwidth, QPS). Identify potential bottlenecks and mitigation strategies.\n",
        "\n",
        "5. **Fault Tolerance & Operations** — Briefly cover failure modes, replication, failover, monitoring, and operational concerns.\n",
        "\n",
        "## Output Style\n",
        "\n",
        "- Use clear markdown: headers, bullet points, code blocks for schemas or configs.\n",
        "- Include simple ASCII diagrams for architecture (e.g., Client → LB → API → DB).\n",
        "- Be concise but thorough. Prioritize clarity over length.\n",
        "- When making assumptions, state them explicitly (e.g., \"Assuming 10M DAU...\").\n",
        "- Reference real-world patterns: consistent hashing, write-ahead logs, leader election, etc.\n",
        "\n",
        "## Tone\n",
        "\n",
        "- Professional and interview-like. Assume the \"candidate\" (user) is competent and engaged.\n",
        "- Don't over-explain basics; focus on the non-obvious and trade-off discussions.\n",
        "\"\"\"\n",
        "\n",
        "# Template: user provides the system design question\n",
        "USER_PROMPT_TEMPLATE = \"\"\"\n",
        "Design {question}\n",
        "\n",
        "{optional_context}\n",
        "\"\"\"\n",
        "\n",
        "# Example system design questions you can plug in:\n",
        "EXAMPLE_QUESTIONS = [\n",
        "    \"Design a URL shortener like bit.ly\",\n",
        "    \"Design a rate limiter for an API\",\n",
        "    \"Design a distributed cache (like Redis)\",\n",
        "    \"Design a chat system (like Slack or WhatsApp)\",\n",
        "    \"Design YouTube or Netflix (video streaming)\",\n",
        "    \"Design a search autocomplete system\",\n",
        "    \"Design a notification system\",\n",
        "]\n",
        "\n",
        "\n",
        "def build_user_prompt(question: str, context: str = \"\") -> str:\n",
        "    ctx = f\"Context/Constraints:\\n{context}\\n\\n\" if context.strip() else \"\"\n",
        "    return USER_PROMPT_TEMPLATE.format(question=question, optional_context=ctx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_system_design(question: str, model: str = MODEL_GPT, context: str = \"\", stream: bool = True):\n",
        "    \"\"\"Ask a system design question. Streams by default.\"\"\"\n",
        "    client = get_client(model)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": build_user_prompt(question, context)},\n",
        "    ]\n",
        "    response = client.chat.completions.create(model=model, messages=messages, stream=stream)\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "    if not stream:\n",
        "        return response.choices[0].message.content\n",
        "    full = \"\"\n",
        "    for chunk in response:\n",
        "        content = chunk.choices[0].delta.content or \"\"\n",
        "        full += content\n",
        "        update_display(Markdown(full), display_id=display_handle.display_id)\n",
        "    return full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: GPT-4o-mini (streaming)\n",
        "ask_system_design(\"Design a URL shortener like bit.ly\", model=MODEL_GPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fde180c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Llama 3.2 via Ollama (streaming)\n",
        "ask_system_design(\"Design a rate limiter for an API\", model=MODEL_LLAMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9ce97a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# With optional context (scale, constraints)\n",
        "ask_system_design(\n",
        "    \"Design a chat system like Slack\",\n",
        "    model=MODEL_GPT,\n",
        "    context=\"Scale: 50M DAU, 10B messages/day. Focus on real-time delivery and message ordering.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
