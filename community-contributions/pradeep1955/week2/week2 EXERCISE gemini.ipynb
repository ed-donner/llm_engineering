{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667ad775-7e05-4415-9357-0bbfd24d5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.protos import FunctionResponse, Part\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828306bf-12db-4d14-9375-b2121fb9a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# --- IMPORTANT: Configure the Gemini API Key ---\n",
    "# It's best practice to set this as an environment variable\n",
    "try:\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Gemini API: {e}\")\n",
    "    # Exit if the API key isn't configured, as the app cannot run.\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e442c4b5-43ad-4885-aac4-9b2b05d2e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The absolute path to the root directory the LLM is allowed to access.\n",
    "ALLOWED_DIRECTORY = os.path.expanduser(\"~/projects/pleasurewebsite/mysite\")\n",
    "\n",
    "if not os.path.isdir(ALLOWED_DIRECTORY):\n",
    "    print(f\"Error: The specified directory '{ALLOWED_DIRECTORY}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# --- Tool Functions (with type hints for better schema generation) ---\n",
    "\n",
    "def list_files_in_directory(path: str = \".\"):\n",
    "    \"\"\"\n",
    "    Lists files and directories within a specified path inside the allowed directory.\n",
    "    The path is relative to the project root.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_path = ALLOWED_DIRECTORY\n",
    "        requested_path = os.path.abspath(os.path.join(base_path, path))\n",
    "        if not requested_path.startswith(base_path):\n",
    "            return {\"error\": \"Access denied.\"}\n",
    "        if not os.path.isdir(requested_path):\n",
    "            return {\"error\": f\"'{path}' is not a valid directory.\"}\n",
    "        items = os.listdir(requested_path)\n",
    "        dirs = sorted([d for d in items if os.path.isdir(os.path.join(requested_path, d))])\n",
    "        files = sorted([f for f in items if os.path.isfile(os.path.join(requested_path, f))])\n",
    "        \n",
    "        response_str = f\"Contents of '{path}':\\n\\n\"\n",
    "        if dirs:\n",
    "            response_str += \"Directories:\\n\" + \"\\n\".join(f\"- {d}/\" for d in dirs) + \"\\n\\n\"\n",
    "        if files:\n",
    "            response_str += \"Files:\\n\" + \"\\n\".join(f\"- {f}\" for f in files) + \"\\n\"\n",
    "        \n",
    "        return {\"content\": response_str if items else f\"The directory '{path}' is empty.\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An error occurred: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "424b0803-eb15-4e18-9f68-cfd2de644a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path: str):\n",
    "    \"\"\"\n",
    "    Reads the text content of a specific file within the allowed directory.\n",
    "    The file_path is relative to the project root.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_path = ALLOWED_DIRECTORY\n",
    "        requested_path = os.path.abspath(os.path.join(base_path, file_path))\n",
    "        if not requested_path.startswith(base_path):\n",
    "            return {\"error\": \"Access denied.\"}\n",
    "        if not os.path.isfile(requested_path):\n",
    "            return {\"error\": f\"File not found at '{file_path}'\"}\n",
    "        with open(requested_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read(5000)\n",
    "            final_content = content + \"\\n\\n[... file content truncated ...]\" if len(content) == 5000 else content\n",
    "            return {\"content\": final_content}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An error occurred while reading the file: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24138538-df34-4a83-b81d-8dec12617c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Definitions for the OpenAI API ---\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"list_files_in_directory\",\n",
    "            \"description\": \"Get a list of files and directories in a specified path within the Django project. Use '.' for the root.\",\n",
    "            \"parameters\": { \"type\": \"object\", \"properties\": { \"path\": { \"type\": \"string\", \"description\": \"The relative path to the directory. E.g., 'home/views'.\"}}, \"required\": [\"path\"]},\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"read_file_content\",\n",
    "            \"description\": \"Read the text content of a specific file within the Django project.\",\n",
    "            \"parameters\": { \"type\": \"object\", \"properties\": { \"file_path\": { \"type\": \"string\", \"description\": \"The relative path to the file. E.g., 'mysite/settings.py'.\"}}, \"required\": [\"file_path\"]},\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8a8d17-7b8b-41cf-b0aa-a2ae1afde48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Chat Logic (Rewritten for Gemini) ---\n",
    "\n",
    "def chat_with_tools(user_message, history):\n",
    "    \"\"\"\n",
    "    Handles the conversation using Google Gemini, including tool calls and streaming.\n",
    "    \"\"\"\n",
    "    system_instruction = f\"You are a helpful AI assistant with access to a local Django project file system. The root of the project is '{ALLOWED_DIRECTORY}'. You can list files and read their content. When asked about the project, use your tools to find information before answering. Be concise.\"\n",
    "    \n",
    "    # Pass the Python functions directly to the model for automatic schema generation.\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name='gemini-1.5-flash',\n",
    "        system_instruction=system_instruction,\n",
    "        tools=[list_files_in_directory, read_file_content],\n",
    "    )\n",
    "\n",
    "    # A dictionary to map function names (strings) to the actual functions\n",
    "    available_functions = {\n",
    "        \"list_files_in_directory\": list_files_in_directory,\n",
    "        \"read_file_content\": read_file_content,\n",
    "    }\n",
    "\n",
    "    # Convert Gradio history to Gemini's message format\n",
    "    messages = []\n",
    "    for human_msg, ai_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"parts\": [human_msg]})\n",
    "        messages.append({\"role\": \"model\", \"parts\": [ai_msg]})\n",
    "    \n",
    "    # Start a chat session to maintain conversation history\n",
    "    chat_session = model.start_chat(history=messages)\n",
    "\n",
    "    try:\n",
    "        # Send the message to Gemini (NON-STREAMING to check for tool calls)\n",
    "        response = chat_session.send_message(user_message)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Safely check for a function call in the response parts\n",
    "        function_call = None\n",
    "        # Ensure the response has candidates and parts before trying to access them\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            part = response.candidates[0].content.parts[0]\n",
    "            # Check if the part contains a function_call and that it has a name\n",
    "            if 'function_call' in part and part.function_call.name:\n",
    "                function_call = part.function_call\n",
    "\n",
    "        if function_call:\n",
    "            # The model wants to use a tool\n",
    "            function_name = function_call.name\n",
    "            function_args = {key: value for key, value in function_call.args.items()}\n",
    "            \n",
    "            print(f\" Calling tool: {function_name}({function_args})\")\n",
    "            \n",
    "            # Call the actual Python function\n",
    "            function_response_data = available_functions[function_name](**function_args)\n",
    "            \n",
    "            # Send the tool's output back to the model for the final answer\n",
    "            response_stream = chat_session.send_message(\n",
    "                Part(\n",
    "                    function_response=FunctionResponse(\n",
    "                        name=function_name,\n",
    "                        response=function_response_data,\n",
    "                    )\n",
    "                ),\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            for chunk in response_stream:\n",
    "                if chunk.text:\n",
    "                    yield chunk.text\n",
    "\n",
    "        else:\n",
    "            # No tool call, the model responded directly. Stream this response.\n",
    "            # Safely check for text content before yielding it.\n",
    "            if response.candidates and response.candidates[0].content.parts and response.candidates[0].content.parts[0].text:\n",
    "                 yield response.text\n",
    "            else:\n",
    "                 yield \"The model did not provide a text response or a tool call.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the Gemini chat logic: {e}\")\n",
    "        yield \"Sorry, an error occurred with the AI model. Please check the console for details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d632dc72-7d81-4057-90c9-5cd94a4ab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336180/2592309743.py:7: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=600, bubble_full_width=False)\n",
      "/tmp/ipykernel_336180/2592309743.py:7: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=600, bubble_full_width=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Gradio Interface (This part remains the same) ---\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"Django Project Assistant (Gemini) \") as demo:\n",
    "    gr.Markdown(\"# Django Project Assistant (with Gemini) \")\n",
    "    gr.Markdown(\"Ask me questions about your Django project. I can list files and read their contents to help you.\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=600, bubble_full_width=False) \n",
    "    \n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"e.g., What files are in mysite/?\",\n",
    "            scale=7,\n",
    "            autofocus=True,\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "    def handle_chat(user_input, history):\n",
    "        history.append([user_input, \"\"])\n",
    "        response_stream = chat_with_tools(user_input, history[:-1])\n",
    "        for chunk in response_stream:\n",
    "            history[-1][1] += chunk\n",
    "            yield history, \"\"\n",
    "        yield history, \"\"\n",
    "\n",
    "    submit_btn.click(handle_chat, [msg_textbox, chatbot], [chatbot, msg_textbox])\n",
    "    msg_textbox.submit(handle_chat, [msg_textbox, chatbot], [chatbot, msg_textbox])\n",
    "\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"What files are in the root directory?\"],\n",
    "            [\"Can you show me the contents of 'mysite/settings.py'?\"],\n",
    "            [\"What's inside the 'home/templates/home' directory?\"]\n",
    "        ],\n",
    "        inputs=msg_textbox,\n",
    "        outputs=[chatbot, msg_textbox],\n",
    "        fn=handle_chat,\n",
    "        cache_examples=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623800d-a096-4c9d-8a45-1e78c2036d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Calling tool: list_files_in_directory({'path': '/home/pradeep/projects/pleasurewebsite/mysite'})\n",
      " Calling tool: read_file_content({'file_path': 'mysite/settings.py'})\n",
      " Calling tool: list_files_in_directory({'path': 'home/templates/home'})\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd999e3d-cf13-496c-9ca3-fd95b79355e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "llmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
