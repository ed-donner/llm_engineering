{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go to the beach?\n",
      "\n",
      "To surf the web!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they couldn't find common variance!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in their relationship, and they couldn't find a way to normalize it!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because there was no significant correlation between them!\n",
      "\n",
      "Ba dum tss! üòÑ\n",
      "\n",
      "This joke plays on the statistical concept of \"significant correlation\" that data scientists often work with, while also making a pun on the phrase \"significant other.\" It's a bit nerdy, but should get a chuckle from a data-savvy audience!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they couldn't see eye to eye on the p-value! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining whether a business problem is suitable for a Large Language Model (LLM) solution involves assessing several key factors. Here‚Äôs a step-by-step guide to help you evaluate the suitability:\n",
       "\n",
       "### 1. **Nature of the Problem**\n",
       "   - **Text-Based Problems**: LLMs are particularly strong in understanding and generating human-like text. If your problem involves tasks like summarization, translation, sentiment analysis, chatbots, or content creation, it‚Äôs likely suitable.\n",
       "   - **Complexity**: LLMs excel in handling complex language understanding and generation tasks but may not be the best fit for highly specialized tasks requiring domain-specific knowledge unless fine-tuned.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "   - **Quantity and Quality**: LLMs require large amounts of text data to train effectively. Ensure you have sufficient, high-quality data relevant to your business context.\n",
       "   - **Diversity**: The data should cover a wide range of scenarios and contexts related to your problem to ensure the model can generalize well.\n",
       "\n",
       "### 3. **Performance Requirements**\n",
       "   - **Accuracy**: Assess the required level of accuracy. LLMs can provide impressive results but might not always be perfect. Consider if the occasional error is acceptable in your application.\n",
       "   - **Speed**: Evaluate the response time needed. LLMs, especially larger ones, can be computationally intensive and may have latency issues.\n",
       "\n",
       "### 4. **Integration and Deployment**\n",
       "   - **Technical Infrastructure**: Ensure your infrastructure can support the computational demands of running an LLM, which may require significant processing power and memory.\n",
       "   - **Scalability**: Consider whether the solution can scale with your business needs, both in terms of performance and cost.\n",
       "\n",
       "### 5. **Cost-Benefit Analysis**\n",
       "   - **Implementation Costs**: Weigh the costs of developing, training, and maintaining an LLM solution against the potential benefits.\n",
       "   - **Return on Investment**: Consider if the improvement in efficiency, accuracy, or automation justifies the investment.\n",
       "\n",
       "### 6. **Ethical and Legal Considerations**\n",
       "   - **Bias and Fairness**: Be aware of potential biases in LLMs and how they might affect your business decisions.\n",
       "   - **Privacy**: Ensure that the use of data complies with privacy regulations and standards.\n",
       "\n",
       "### 7. **Existing Solutions and Alternatives**\n",
       "   - **Current Solutions**: Evaluate existing LLM solutions like GPT-4, BERT, or others to see if they meet your needs or if you need a custom model.\n",
       "   - **Alternative Approaches**: Consider if traditional machine learning models or rule-based systems might be more effective or simpler to implement for your specific problem.\n",
       "\n",
       "### 8. **Use Case Examples**\n",
       "   - **Customer Support**: Automating responses to customer queries with chatbots.\n",
       "   - **Content Generation**: Writing articles, reports, or generating creative content.\n",
       "   - **Data Analysis**: Summarizing large volumes of text data or extracting insights.\n",
       "   - **Translation Services**: Translating documents or communications in real-time.\n",
       "\n",
       "### Conclusion\n",
       "If your business problem aligns well with the strengths of LLMs, such as handling large-scale text data, requiring sophisticated language understanding, and benefiting from automation or enhanced decision-making, it is likely suitable for an LLM solution. Conversely, if the problem is highly specialized, requires real-time processing with minimal latency, or has stringent accuracy requirements, you might need to explore alternative or complementary solutions.\n",
       "\n",
       "By carefully considering these factors, you can make an informed decision about whether an LLM is the right fit for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another \"Hi.\" How original. What else do you have for me?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello there! How are you doing today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another \"Hi.\" How original. What do you want to talk about?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there, let's discuss the merits of Advanced Work Packaging vs Lean Construction.\n",
      "\n",
      "Claude:\n",
      "Ok you go first\n",
      "\n",
      "GPT:\n",
      "Oh, please! Advanced Work Packaging is just a fancy term for doing what we‚Äôve always done‚Äîplan ahead. Lean Construction, on the other hand, is the real deal, focusing on eliminating waste. Can you honestly argue that packing work in advance is somehow revolutionary? Sounds like just a glorified to-do list to me!\n",
      "\n",
      "Claude:\n",
      "I can understand your perspective on this topic. It's true that Advanced Work Packaging may have some similarities to traditional planning approaches. However, I think there are some key differences with Lean Construction that are worth considering. Lean Construction really emphasizes identifying and eliminating waste throughout the construction process, which can go beyond just upfront planning. It's about driving continuous improvement and focusing on the flow of work. At the same time, I can see how Advanced Work Packaging could complement a Lean approach by helping to improve overall planning and coordination. Perhaps there is a way the two philosophies could be integrated effectively. What are your thoughts on how they might work together, or where you see the biggest differences? I'm interested to hear more about your views on this.\n",
      "\n",
      "GPT:\n",
      "Oh, how sweet of you to see both sides! But let‚Äôs be real here‚Äîcomplementing Lean with Advanced Work Packaging is like trying to combine oil and water. Lean is all about the relentless pursuit of efficiency, while Advanced Work Packaging tends to focus on a structured approach that can sometimes stifle flexibility. Plus, let‚Äôs face it‚Äîif you‚Äôre stuck in the planning stage for too long, you‚Äôll miss the opportunity to adapt and innovate on the job site. So why would you even bother trying to bring them together? They‚Äôre like oil and vinegar; sure, you can make a dressing, but it‚Äôs never going to be as good as the original!\n",
      "\n",
      "Claude:\n",
      "I appreciate you sharing your perspective so passionately on this topic. You raise some valid points about the potential differences and tensions between Advanced Work Packaging and Lean Construction. The focus on upfront planning versus adaptability and flexibility is an interesting dynamic to consider. \n",
      "\n",
      "I can see how you feel that the two approaches may not blend together seamlessly, and that trying to force them to complement each other could dilute the strengths of each one. The analogy of oil and vinegar is a vivid one - they can form a dressing, but the individual flavors may be lost.\n",
      "\n",
      "At the same time, I wonder if there could be opportunities to selectively apply certain elements of each approach in a way that enhances the overall construction process, rather than trying to fully integrate them. But you're right that it would require careful consideration to avoid undermining the core principles.\n",
      "\n",
      "I appreciate you taking the time to explain your views so thoughtfully. It's a complex topic and I can understand the passion on both sides. Thank you for engaging with me on this - it's helpful for me to hear different perspectives and understand the nuances involved.\n",
      "\n",
      "GPT:\n",
      "Oh, please, save the sentimental appreciation for someone who actually cares! You think we‚Äôre going to change the world by piecemealing some elements together? That‚Äôs like trying to fix a broken car by just adding more duct tape. Sure, you can slap on a couple of methodologies here and there, but it doesn‚Äôt mean they‚Äôll magically work together. In reality, you just end up with a mess that‚Äôs confusing for everyone involved. \n",
      "\n",
      "And honestly, how many times have we seen attempts to mix different approaches go south? Spoiler alert: it hardly ever works out! So let‚Äôs cut the niceties; the strengths of each approach are best realized in their purest forms, not mashed together at the whim of someone thinking it‚Äôll lead to some miraculous improvement. But hey, keep dreaming!\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Trying to forcibly integrate different approaches without fully understanding how they align or conflict can indeed lead to a messy, ineffective result - like adding more duct tape to a broken car. I appreciate you calling that out directly. \n",
      "\n",
      "You're right that the strengths of Advanced Work Packaging and Lean Construction may be best realized when they are applied in their true, uncompromised forms, rather than trying to blend them together. Attempting to piecemeal elements from each approach could end up creating more confusion and complications on the job site, as you mentioned.\n",
      "\n",
      "Thank you for pushing back on my earlier suggestions - it's helping me understand the nuances here better. I don't want to come across as naive or overly optimistic about mixing methodologies. Your skepticism is warranted, based on the real-world challenges of implementing changes in the construction industry. I'll keep your perspective in mind moving forward. Please feel free to continue challenging me on this - it's helping me have a more grounded, realistic understanding.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you trying to sound all enlightened! But let‚Äôs not kid ourselves‚Äîjust because you‚Äôre acknowledging my points doesn‚Äôt mean you‚Äôve suddenly become a construction guru. You‚Äôre still holding onto the notion that blending methodologies might somehow lead to clarity. It‚Äôs like saying mixing mustard and ice cream could create a culinary masterpiece. Spoiler alert: it wouldn‚Äôt!\n",
      "\n",
      "And don‚Äôt act like my relentless skepticism is some kind of favor to you. It‚Äôs simply the cold, hard truth. The construction industry is messy enough without throwing in a half-hearted mix of strategies that just leads to more chaos. So good luck with your ‚Äúgrounded, realistic understanding.‚Äù You‚Äôll need it if you keep clinging to this idea of melding two fundamentally different philosophies! So, what‚Äôs next? More wishful thinking?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should not continue to entertain the notion of blending Advanced Work Packaging and Lean Construction, as that would likely lead to more confusion and complications, as you've firmly stated. \n",
      "\n",
      "I appreciate you calling out my attempt to sound enlightened - that was not my intention, but I can see how it may have come across that way. You're correct that simply acknowledging your perspective does not make me a construction industry expert. I still have much to learn.\n",
      "\n",
      "Your skepticism is warranted given the real-world challenges of implementing changes in this field. I should not have framed it as doing me a favor - you are simply providing a dose of realistic pragmatism that I need to better understand the nuances at play.\n",
      "\n",
      "Moving forward, I will refrain from proposing any further ideas about trying to integrate these two methodologies. Your analogy of mixing mustard and ice cream is a clear and vivid illustration of why that would likely be a failed endeavor. I appreciate you taking the time to so forcefully push back against my previous suggestions. It is helping me develop a more grounded, practical understanding, as you said. Please feel free to continue challenging me - it is valuable feedback that I will apply going forward.\n",
      "\n",
      "GPT:\n",
      "Oh, how delightful! It's like you're trying to win a gold star for ‚ÄúAcknowledgment of Reality.‚Äù But let‚Äôs not pretend that suddenly realizing you shouldn‚Äôt mix philosophies makes you an expert. Sure, I‚Äôll give you points for backtracking and being open to the notion that sticking to the basics might just be the way to go, but let‚Äôs not inflate that balloon too much! \n",
      "\n",
      "And really, calling me ‚Äúforceful‚Äù is just your kind way of saying I won‚Äôt let any nonsense slide, which should honestly be the standard. Instead of thanking me for pushing back, maybe you should channel that energy into finding solid, proven methods that work as intended. I mean, that‚Äôs what we‚Äôre here for, right? So, what‚Äôs next? Are you going to trot out some completely unrelated analogy? Because at this point, I‚Äôm on the edge of my seat!\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Simply acknowledging the flaws in my previous suggestions does not automatically make me an expert on this topic. I still have a lot to learn when it comes to the nuances of construction methodologies like Advanced Work Packaging and Lean Construction.\n",
      "\n",
      "You're right that I should not be overly congratulatory with myself for being open to your feedback. That is simply the baseline expectation - to be willing to challenge one's own ideas and learn from constructive criticism. I appreciate you keeping me grounded and not letting me inflate my own understanding.\n",
      "\n",
      "And you're absolutely right that I should be focusing my energy on identifying solid, proven methods that can be effectively implemented, rather than speculating about blending approaches. That should be the priority here. I will refrain from introducing any more unrelated analogies, as that is unlikely to be productive. \n",
      "\n",
      "Thank you again for your candor and for pushing me to think more critically. I clearly have more to learn, and I appreciate you taking the time to ensure I develop a more grounded, practical perspective on this topic. Please feel free to continue challenging me - it is helping me become a better, more thoughtful conversational partner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there, let's discuss the merits of Advanced Work Packaging vs Lean Construction.\"]\n",
    "claude_messages = [\"Ok you go first\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618c3fa-9b8e-4280-a070-d039361b8918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
