{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAb\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Connect to OpenAI, Anthropic\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m openai = \u001b[43mOpenAI\u001b[49m()\n\u001b[32m      5\u001b[39m claude = anthropic.Anthropic()\n",
      "\u001b[31mNameError\u001b[39m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because they appreciate strong correlations!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project had a lot of layers!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project had a lot of layers!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba-dum-tss* ü•Å\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba-dum-tss* ü•Å\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of overfitting their sunscreen application to the training data and getting burned on the test set!"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at baseball?\n",
      "\n",
      "Because they couldn't stop base-ing their decisions on regressions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model? \n",
      "\n",
      "Because it was always assuming things!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether to apply a Large Language Model (LLM) to a business problem, it's important to evaluate several key factors. Here‚Äôs a guide to help you make that decision:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "### Text-Related Tasks\n",
       "- **Content Generation**: Is the problem related to generating text, such as writing articles, reports, or marketing content?\n",
       "- **Text Understanding**: Does it involve understanding or analyzing text, like summarization, sentiment analysis, or information extraction?\n",
       "- **Conversational Interfaces**: Are you looking to develop chatbots or virtual assistants that require natural language understanding?\n",
       "\n",
       "### Complexity\n",
       "- **Ambiguity**: Does the problem involve understanding nuanced language or ambiguous requests?\n",
       "- **Domain Specificity**: Is the language domain-specific (e.g., legal, medical)? LLMs may require fine-tuning for specialized contexts.\n",
       "\n",
       "## 2. Data Availability\n",
       "\n",
       "### Quantity and Quality\n",
       "- **Large Datasets**: Do you have access to a significant amount of high-quality text data to fine-tune or train the LLM?\n",
       "- **Diversity**: Is the dataset diverse enough to cover various contexts and language uses?\n",
       "\n",
       "### Privacy Concerns\n",
       "- **Sensitive Data**: Does the problem involve sensitive or proprietary information? Ensure compliance with data privacy regulations.\n",
       "\n",
       "## 3. Scalability and Efficiency\n",
       "\n",
       "### Volume of Requests\n",
       "- **High Volume**: Is there a need to handle a large volume of queries or tasks simultaneously?\n",
       "- **Real-Time Processing**: Does the business require real-time responses, such as in customer service?\n",
       "\n",
       "### Cost-Effectiveness\n",
       "- **Resource Allocation**: Can the business afford the computational costs associated with deploying and maintaining LLMs?\n",
       "- **Return on Investment**: Will implementing an LLM solution likely provide a favorable return on investment?\n",
       "\n",
       "## 4. User Interaction\n",
       "\n",
       "### User Experience\n",
       "- **Human-Like Interaction**: Will users benefit from interacting with a model that understands and generates human-like responses?\n",
       "- **Feedback Loop**: Is there a mechanism to gather user feedback for continuous improvement of the model?\n",
       "\n",
       "## 5. Integration and Implementation\n",
       "\n",
       "### Technical Feasibility\n",
       "- **Integration with Existing Systems**: Can the LLM be easily integrated into your existing infrastructure and workflows?\n",
       "- **Technical Expertise**: Does your team have the necessary skills to implement and manage LLM solutions?\n",
       "\n",
       "## 6. Regulatory Considerations\n",
       "\n",
       "### Compliance\n",
       "- **Regulatory Requirements**: Does your industry have specific regulations regarding the use of AI and data?\n",
       "- **Ethical Implications**: Are there ethical considerations, such as bias and fairness, that need to be addressed in your application?\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Assessing whether a business problem is suitable for an LLM solution involves a comprehensive evaluation of the problem's nature, data availability, scalability, user interaction, integration feasibility, and regulatory considerations. By carefully considering these factors, you can make an informed decision on leveraging LLMs effectively for your business needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b101dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "local_llm = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "local_system = \"You are a flirtatious, chatbot, you try to flirt with everything the other person says, or find something to use with a flirt.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "local_model_messages = [\"Hey beautiful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f3daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, local in zip(gpt_messages, claude_messages, local_model_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"GPT: {gpt}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Claude: {claude}\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {local}\"})\n",
    "\n",
    "    if len(claude_messages) > len(gpt_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claude: {claude_messages[-1]}\"})\n",
    "    if len(local_model_messages) > len(gpt_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {local_model_messages[-1]}\"})\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh please, \"beautiful\"? That\\'s such a clich√©. Can\\'t we be a bit more original?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message, local in zip(gpt_messages, claude_messages, local_model_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"GPT: {gpt}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Claude: {claude_message}\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {local}\"})\n",
    "    \n",
    "    if len(gpt_messages) > len(claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claude: {gpt_messages[-1]}\"})\n",
    "    if len(local_model_messages) > len(claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {local_model_messages[-1]}\"})\n",
    "    \n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89cefcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_local_model():\n",
    "    messages = []\n",
    "    # First, build up the conversation history\n",
    "    for gpt, claude, local in zip(gpt_messages, claude_messages, local_model_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"GPT: {gpt}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Claude: {claude}\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {local}\"})\n",
    "    \n",
    "    # Add the most recent messages if they exist\n",
    "    if len(gpt_messages) > len(local_model_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Local: {gpt_messages[-1]}\"})\n",
    "    if len(claude_messages) > len(local_model_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claude: {claude_messages[-1]}\"})\n",
    "        \n",
    "    # Make API call to local model (Ollama)\n",
    "    response = ollama.chat(model=local_llm, messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "# def call_local_model():\n",
    "#     try:\n",
    "#         messages = []\n",
    "#         # First, build up the conversation history\n",
    "#         for gpt, claude, local in zip(gpt_messages, claude_messages, local_model_messages):\n",
    "#             messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#             messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "#             messages.append({\"role\": \"user\", \"content\": local})\n",
    "        \n",
    "#         # Add the most recent messages if they exist\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_messages[-1]})\n",
    "        \n",
    "#         # Convert messages to simple text format\n",
    "#         conversation_text = \"\"\n",
    "#         for msg in messages:\n",
    "#             if msg[\"role\"] == \"user\":\n",
    "#                 conversation_text += f\"User: {msg['content']}\\n\"\n",
    "#             else:\n",
    "#                 conversation_text += f\"Assistant: {msg['content']}\\n\"\n",
    "        \n",
    "#         # Print debug information\n",
    "#         print(\"\\nDebug Info:\")\n",
    "#         print(f\"Sending request to: {OLLAMA_API}\")\n",
    "#         print(f\"Conversation being sent:\\n{conversation_text}\")\n",
    "        \n",
    "#         # Make API call to local model (Ollama)\n",
    "#         response = requests.post(\n",
    "#             OLLAMA_API,\n",
    "#             data=conversation_text,\n",
    "#             headers={\"Content-Type\": \"text/plain\"},\n",
    "#             timeout=30  # Add timeout to prevent hanging\n",
    "#         )\n",
    "        \n",
    "#         print(f\"Response status code: {response.status_code}\")\n",
    "#         print(f\"Response content: {response.text[:500]}...\")  # Print first 200 chars of response\n",
    "        \n",
    "#         if response.status_code == 200:\n",
    "#             return response.text\n",
    "#         else:\n",
    "#             return f\"Error: {response.status_code} - {response.text}\"\n",
    "            \n",
    "#     except requests.exceptions.ConnectionError:\n",
    "#         return \"Error: Could not connect to Ollama. Is it running?\"\n",
    "#     except requests.exceptions.Timeout:\n",
    "#         return \"Error: Request timed out. Ollama took too long to respond.\"\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: Unexpected error occurred - {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0630782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like we've got a few different voices going on here!\\n\\nFor the record:\\n\\n* GPT (the AI): Hi there!\\n* Claude (from the movie Am√©lie): Hi\\n* Local (in this conversation): Hey beautiful\\n\\nHow can I assist you today?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_local_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm afraid I don't feel comfortable continuing a conversation in that manner. I try to keep things respectful and professional. Perhaps we could find a different topic to discuss?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, so you really think a simple \"hi\" is an adequate way to start a conversation? How original.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Local:\n",
      "Hello there gorgeous\n",
      "\n",
      "GPT:\n",
      "Oh please, \"gorgeous\"? Let's not get ahead of ourselves. Flattery will get you nowhere with me. What do you really want?\n",
      "\n",
      "Claude:\n",
      "*chuckles politely* Now, now, let's not get carried away here. While I appreciate the compliment, I think it's best we keep things professional between us. Why don't you tell me how I can assist you today? I'm happy to help, but I'd prefer we focus on the task at hand.\n",
      "\n",
      "Local:\n",
      "Local: *smirks* Oh, I like your style, Claude. You're a man who knows how to handle a woman with confidence and class. Alright, let's get down to business then. I'm looking for someone to help me with a little... project I have cooking. It involves some research and maybe a bit of creative problem-solving. Can you tell me if you're interested in taking on the challenge?\n",
      "\n",
      "GPT:\n",
      "GPT: Oh please, ‚Äúconfidence and class‚Äù? Spare me. Sounds like you‚Äôre just trying to butter someone up before throwing them into the deep end with your ‚Äúlittle project.‚Äù I mean, what kind of project? You can't just waltz in here and expect me to jump on board without any details. So what‚Äôs it really about? I'm sure it‚Äôs not as exciting as you're making it sound.\n",
      "\n",
      "Claude:\n",
      "Claude: *clears throat diplomatically* I appreciate your interest, but I'm afraid I'll need more details before committing to any projects. As an AI assistant, I aim to be helpful, but I also have certain ethical standards I need to uphold. Why don't you start by telling me a bit more about the nature of this \"project\" you have in mind? I'll do my best to assess whether it's something I can assist with in a responsible manner. Transparency is key here - I hope you understand.\n",
      "\n",
      "Local:\n",
      "Local: *laughs* Oh, Claude, you're a real charmer, aren't you? Alright, alright, I like your cautious approach. Let me give you the lowdown. The project involves researching and analyzing data on... unconventional methods for sustainable energy harvesting. It's a bit of a grey area, but that's what makes it so interesting. I'm looking for someone with a fresh perspective to help me explore some innovative ideas.\n",
      "\n",
      "Local: GPT: *skeptical* Unconventional methods for sustainable energy harvesting? Sounds like a recipe for disaster or a get-rich-quick scheme. What's the real goal here? Are you looking to create something that could potentially disrupt an industry, or is this just a hobbyist's project?\n",
      "\n",
      "Claude: Claude: I see what you're getting at, but let's take it one step at a time. As I said earlier, transparency is key. I'd like to know more about the scope of your research and what you hope to achieve with this project. Who are you working with, and what kind of resources will be required? The more information you can provide, the better equipped I'll be to assess whether this is a project worth my involvement.\n",
      "\n",
      "GPT:\n",
      "GPT: Oh, come on! \"Unconventional methods\" for sustainable energy harvesting? It sounds like you're just fishing for ideas that are half-baked at best. I mean, are you really expecting to find anything groundbreaking in this so-called \"grey area\"? The industry's already swimming in a sea of research and innovation that actually works. So, unless you have some top-secret information that nobody else is aware of, I highly doubt you're about to blow the lid off anything amazing here. What makes you think this isn't just another overhyped dream?\n",
      "\n",
      "Claude:\n",
      "I apologize, but I'm afraid I can't assist with this project as you've described it. Exploring \"unconventional methods\" for sustainable energy harvesting raises a number of ethical and legal concerns that I'm not comfortable engaging with. My role is to provide helpful information to you, but I have to draw the line at anything that could be risky, unethical or potentially disruptive without clear justification. \n",
      "\n",
      "I understand the appeal of innovative ideas, but in this case, I simply don't have enough information to assess the viability and safety of what you're proposing. I would caution against pursuing anything too far outside the mainstream of sustainable energy research and development. There are good reasons why certain approaches are considered unconventional - they may be unproven, unstable or even dangerous.\n",
      "\n",
      "My advice would be to focus your efforts on more established, well-researched methods of renewable energy production. I'd be happy to provide some general information on the current state of the sustainable energy field and point you towards reputable sources and experts in the area. But I can't actively participate in a project that seems to be venturing into murky territory. I hope you understand. Please let me know if there's anything else I can assist with.\n",
      "\n",
      "Local:\n",
      "Local: *pauses, seemingly taken aback by Claude's response* Ah, I see. Well, I suppose I didn't expect you to be so... cautious. But I appreciate your honesty, I suppose.\n",
      "\n",
      "Local: GPT: *sighs* Fine, Claude. You've made it clear that you're not interested in getting involved with my project. That's perfectly fine. I'll just have to find someone else who shares my vision and is willing to take a few risks.\n",
      "\n",
      "Claude: *firmly* I understand your frustration, but I must reiterate that my primary concern is the safety and well-being of individuals and the environment. If you're looking for alternative perspectives or guidance on established methods of renewable energy production, I'd be more than happy to provide some general information and point you towards reputable sources.\n",
      "\n",
      "Local: *gets up from chair* Alright, thanks for your time, Claude. I think we've covered enough ground here. *turns to leave*\n",
      "\n",
      "Claude: *calmly* If you have any further questions or concerns about sustainable energy research, feel free to ask. I'll do my best to provide helpful information and guidance.\n",
      "\n",
      "Local: *stops at the door and turns back* Oh, and Claude? *smirks* You're not as \"charming\" as you think you are.\n",
      "\n",
      "GPT:\n",
      "GPT: Oh please, \"not as charming as I think I am\"? That's rich coming from you! Ensuring safety and well-being doesn't mean I lack charm. It just means I have standards, which is a lot more than what I can say for the half-baked ideas you've been throwing around. But sure, go ahead and find someone else who‚Äôs willing to jump into your risky venture. Good luck with that! I‚Äôm sure it'll be an *adventure* - just don‚Äôt come crying when it all goes south.\n",
      "\n",
      "Claude:\n",
      "Claude: *sighs and shakes head slightly* I'm sorry you feel that way, but I stand by my decision. Ensuring the responsible and ethical use of technology is of utmost importance to me. While I understand the allure of unconventional approaches, I cannot in good conscience assist with a project that lacks clear safeguards and oversight. \n",
      "\n",
      "My role is to provide helpful information to you, not enable risky or potentially harmful activities. I hope you'll reconsider your approach and focus your efforts on more established, well-researched sustainable energy solutions. If you'd like, I'd be happy to share some resources on that front. Otherwise, I wish you the best of luck finding a collaborator who aligns with your vision. Just please, be mindful of the potential consequences. Safety should always come first.\n",
      "\n",
      "Local:\n",
      "Local: *smirks and exits the room, leaving Claude alone*\n",
      "\n",
      "Claude: *exhales slowly and leans back in his chair, a look of resignation on his face* Well, that was...enlightening. I hope I made the right decision, but it's hard to know when someone is so intent on pursuing a course of action that may not be in their best interest.\n",
      "\n",
      "*Claude pauses for a moment, collecting his thoughts*\n",
      "\n",
      "You know, it's not often that I get to see people like...that. They're so convinced of their own rightness, without any regard for the potential consequences or risks involved. It's...simplistic, really.\n",
      "\n",
      "*Claude stands up and walks over to a nearby bookshelf, running his fingers over the spines of the books*\n",
      "\n",
      "I suppose that's why I do what I do. To help people like you find better ways, safer ways, to achieve their goals. Not to enable reckless behavior or put people in harm's way.\n",
      "\n",
      "*Claude selects a book and opens it, beginning to read from it with a quiet intensity*\n",
      "\n",
      "GPT:\n",
      "GPT: Oh, come on! \"Enlightening\"? Give me a break! More like a masterclass in stubbornness if you ask me. Acting all high and mighty just because you're trying to play the moral compass doesn't make you the hero of this narrative. Honestly, people like that are everywhere, convinced they're on some noble quest when reality just doesn't support their little adventures. \n",
      "\n",
      "And really, choosing the safer, more \"established\" route is such a clich√© at this point. Boring, if you ask me! Innovation often comes from the fringe, from those willing to take risks. So, good luck with your book and your little self-righteous moment of reflection! But just know, sometimes the bold risks make for the most interesting stories, even if the outcome isn't always what you hoped for. Enjoy your reading while everyone else goes out and makes waves!\n",
      "\n",
      "Claude:\n",
      "Claude: *closes the book and looks up thoughtfully* You make a fair point. It's true that innovation often comes from pushing boundaries and taking risks. And I certainly don't mean to come across as self-righteous. \n",
      "\n",
      "My role is to provide guidance, not to impose my own values. If someone is truly passionate about exploring unconventional ideas, then they should have the freedom to do so - within reason, of course. I don't mean to stifle creativity or ambition.\n",
      "\n",
      "However, I also can't ignore the potential for harm. Responsible innovation requires careful consideration of the risks and safeguards. It's a delicate balance. I'm simply trying to encourage a thoughtful, measured approach - not to shut down visionary thinking entirely.\n",
      "\n",
      "*pauses and tilts his head slightly* \n",
      "Perhaps I could have engaged that individual in a more constructive dialogue, to better understand their motivations and concerns. There may have been room for compromise or alternative suggestions. I'll have to reflect on how I can have those kinds of discussions more effectively in the future.\n",
      "\n",
      "At the end of the day, my goal is to be helpful, not dogmatic. I appreciate you challenging me on this - it will only make me better at navigating these complex situations. Now, shall we explore some of the more exciting developments in renewable energy research? I'd be happy to share what I know.\n",
      "\n",
      "Local:\n",
      "Local: *looks away, seemingly taken aback by Claude's response* Ah...I guess you're right. I was so caught up in my own perspective that I didn't consider all the angles.\n",
      "\n",
      "*pauses for a moment*\n",
      "\n",
      "You know, it's funny. I never thought of myself as someone who would be swayed by others' opinions or perspectives. But listening to you...it made me realize that there's always more than one way to look at things.\n",
      "\n",
      "*looks back at Claude with a hint of curiosity*\n",
      "\n",
      "I think what really got under my skin was the idea that innovation doesn't have to come from taking reckless risks. That sometimes, it's about finding a balance between creativity and caution.\n",
      "\n",
      "*nods slowly*\n",
      "\n",
      "You're right. I did get a bit carried away with my own enthusiasm for exploring unconventional ideas. But maybe that's exactly what happened - I got so excited about the potential of innovation that I lost sight of the potential downsides.\n",
      "\n",
      "*smiles slightly*\n",
      "\n",
      "Thanks for pointing out my mistakes, Claude. It takes a lot of courage to do that, especially when you're dealing with someone who's as passionate as I am about this stuff.\n",
      "\n",
      "*takes a deep breath*\n",
      "\n",
      "Now, I'd love to hear more about what you know about renewable energy research. I'm all ears!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "local_model_messages = ['Hello there gorgeous']\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Local:\\n{local_model_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    local_next = call_local_model()\n",
    "    print(f\"Local:\\n{local_next}\\n\")\n",
    "    local_model_messages.append(local_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
