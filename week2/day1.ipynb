{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# ¬°Bienvenidos a la Semana 2!\n",
    "\n",
    "## API de modelos Frontier\n",
    "\n",
    "En la Semana 1, usamos m√∫ltiples LLM de Frontier a trav√©s de su interfaz de chat y nos conectamos con la API de OpenAI.\n",
    "\n",
    "Hoy nos conectaremos con las API de Anthropic y Google, as√≠ como tambi√©n con OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Nota importante: l√©ame</h2>\n",
    "<span style=\"color:#900;\">Estoy mejorando continuamente estos laboratorios, agregando m√°s ejemplos y ejercicios.\n",
    "Al comienzo de cada semana, vale la pena verificar que tenga el c√≥digo m√°s reciente.<br/>\n",
    "Primero, haga un <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull y combine los cambios seg√∫n sea necesario</a>. ¬øTiene alg√∫n problema? Intente preguntarle a ChatGPT para que le aclare c√≥mo realizar la fusi√≥n, o p√≥ngase en contacto conmigo.<br/><br/>\n",
    "Despu√©s de haber obtenido el c√≥digo, desde el directorio llm_engineering, en un indicador de Anaconda (PC) o Terminal (Mac), ejecute:<br/>\n",
    "<code>conda env update --f environment.yml --prune</code><br/>\n",
    "O si utiliz√≥ virtualenv en lugar de Anaconda, ejecute esto desde su entorno activado en un Powershell (PC) o Terminal (Mac):<br/>\n",
    "<code>pip install -r requirements.txt</code>\n",
    "<br/>Luego reinicie el kernel (men√∫ Kernel >> Reiniciar kernel y borrar resultados de todas las celdas) para incorporar los cambios.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#f71;\">Recordatorio sobre la p√°gina de recursos</h2>\n",
    "<span style=\"color:#f71;\">A continuaci√≥n, se incluye un enlace a los recursos del curso. Esto incluye enlaces a todas las diapositivas.<br/>\n",
    "<a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "Por favor, mant√©n este art√≠culo en tus favoritos y seguir√© agregando m√°s enlaces √∫tiles all√≠ con el tiempo.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de las claves\n",
    "\n",
    "Si a√∫n no lo ha hecho, ahora puede crear claves API para Anthropic y Google adem√°s de OpenAI.\n",
    "\n",
    "**Nota:** si prefiere evitar costos adicionales de API, ¬°no dude en omitir la configuraci√≥n de Anthopic y Google! Puede verme hacerlo y concentrarse en OpenAI para el curso. Tambi√©n puede sustituir Anthropic o Google por Ollama, utilizando el ejercicio que realiz√≥ en la semana 1.\n",
    "\n",
    "Para OpenAI, visite https://openai.com/api/\n",
    "Para Anthropic, visite https://console.anthropic.com/\n",
    "Para Google, visite https://ai.google.dev/gemini-api\n",
    "\n",
    "Cuando obtenga sus claves API, debe configurarlas como variables de entorno agreg√°ndolas a su archivo `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Luego, es posible que tengas que reiniciar el kernel de Jupyter Lab (el proceso de Python que se encuentra detr√°s de este cuaderno) a trav√©s del men√∫ Kernel y, luego, volver a ejecutar las celdas desde la parte superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n para Google\n",
    "# En casos excepcionales, esto parece generar un error en algunos sistemas. Comun√≠quese conmigo si esto sucede.\n",
    "# O puede omitir Gemini: es la prioridad m√°s baja de los modelos de Frontier que usamos.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyBk\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuraci√≥n\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con√©ctate a OpenAI, Anthropic y Google\n",
    "# Las 3 API son similares\n",
    "# ¬øTienes problemas con los archivos API? Puedes usar openai = OpenAI(api_key=\"your-key-here\") y lo mismo para Claude\n",
    "# ¬øTienes problemas con la configuraci√≥n de Google Gemini? Entonces, omite Gemini; obtendr√°s toda la experiencia que necesitas de GPT y Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Pedir a los LLM que cuenten un chiste\n",
    "\n",
    "¬°Resulta que los LLM no son muy buenos para contar chistes! Comparemos algunos modelos.\n",
    "\n",
    "M√°s adelante, les daremos un mejor uso a los LLM.\n",
    "\n",
    "### Qu√© informaci√≥n se incluye en la API\n",
    "\n",
    "Normalmente, pasaremos a la API:\n",
    "- El nombre del modelo que se debe utilizar\n",
    "- Un mensaje del sistema que brinda un contexto general para el rol que desempe√±a el LLM\n",
    "- Un mensaje del usuario que brinda el mensaje real\n",
    "\n",
    "Hay otros par√°metros que se pueden usar, incluida la **temperatura**, que generalmente est√° entre 0 y 1; m√°s alta para una salida m√°s aleatoria; m√°s baja para una salida m√°s enfocada y determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un asistente que es genial contando chistes.\"\n",
    "user_prompt = \"Cuente un chiste divertido para una audiencia de cient√≠ficos de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqu√≠ tienes uno: \n",
      "\n",
      "¬øPor qu√© el cient√≠fico de datos no sali√≥ en la foto de grupo?\n",
      "Porque siempre estaba ocupado ajustando los par√°metros de la c√°mara.\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øPor qu√© los cient√≠ficos de datos nunca discuten con los algoritmos?\n",
      "\n",
      "¬°Porque saben que siempre terminar√°n en un bucle infinito!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# El ajuste de la temperatura controla la creatividad\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øPor qu√© los cient√≠ficos de datos aman tanto la naturaleza?\n",
      "\n",
      "¬°Porque siempre tiene la mejor regresi√≥n lineal!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqu√≠ tienes un chiste para cient√≠ficos de datos:\n",
      "\n",
      "¬øPor qu√© los cient√≠ficos de datos son tan buenos en las relaciones? \n",
      "\n",
      "Porque siempre est√°n buscando correlaciones significativas.\n",
      "\n",
      "*Ba dum tss* \n",
      "\n",
      "¬øEntiendes? Porque en el an√°lisis de datos, buscamos correlaciones estad√≠sticamente significativas entre variables, ¬°pero tambi√©n podr√≠a aplicarse a las relaciones personales! \n",
      "\n",
      "Es un juego de palabras un poco nerd, pero espero que haya sacado al menos una sonrisa a tu audiencia de cient√≠ficos de datos. Si quieres otro, ¬°solo tienes que pedirlo!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\n",
    "# Tambi√©n se agregaron max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqu√≠ va un chiste para cient√≠ficos de datos:\n",
      "\n",
      "¬øPor qu√© los cient√≠ficos de datos son tan buenos en las relaciones? \n",
      "\n",
      "Porque siempre est√°n buscando correlaciones significativas.\n",
      "\n",
      "*Ba dum tss* üòÑ\n",
      "\n",
      "¬øQu√© te pareci√≥? Este chiste juega con la idea de que los cient√≠ficos de datos siempre est√°n analizando datos en busca de correlaciones estad√≠sticamente significativas, y lo relaciona de forma humor√≠stica con las relaciones personales. Es un chiste un poco nerd, pero creo que a una audiencia de cient√≠ficos de datos les podr√≠a parecer divertido."
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet otra vez\n",
    "# Ahora agreguemos los resultados en streaming\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escuch√© este gran chiste sobre un cient√≠fico de datos, pero no estoy seguro de que sea significativo.\n"
     ]
    }
   ],
   "source": [
    "# La API de Gemini tiene una estructura ligeramente diferente\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬°En serio! GPT-4o con la pregunta original\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente √∫til que responde en Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"¬øC√≥mo puedo decidir si un problema empresarial es adecuado para una soluci√≥n LLM? Responde en Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cuando consideras si un problema empresarial es adecuado para una soluci√≥n basada en modelos de lenguaje de gran tama√±o (LLM, por sus siglas en ingl√©s), es importante evaluar varios factores clave. Aqu√≠ tienes una gu√≠a en pasos para ayudarte a tomar una decisi√≥n informada:\n",
       "\n",
       "### 1. Definici√≥n del Problema\n",
       "- **Claridad:** Aseg√∫rate de que el problema est√° claramente definido y que los objetivos son espec√≠ficos.\n",
       "- **Naturaleza del Problema:** Determina si el problema es principalmente de procesamiento de lenguaje natural, como generaci√≥n de texto, resumen, traducci√≥n, an√°lisis de sentimientos, etc.\n",
       "\n",
       "### 2. Datos Disponibles\n",
       "- **Calidad y Cantidad:** Eval√∫a si tienes suficientes datos de alta calidad para entrenar o ajustar un LLM.\n",
       "- **Acceso a Datos:** Considera si tienes acceso a datos relevantes y si estos datos est√°n estructurados de manera que puedan ser utilizados por un LLM.\n",
       "\n",
       "### 3. Recursos y Capacidades\n",
       "- **Infraestructura:** Verifica si tienes la infraestructura tecnol√≥gica necesaria para implementar y mantener un LLM.\n",
       "- **Expertise:** Aseg√∫rate de contar con el personal con la experiencia necesaria en IA y LLMs para gestionar el proyecto.\n",
       "\n",
       "### 4. Evaluaci√≥n de Costos\n",
       "- **Costos de Implementaci√≥n:** Calcula los costos asociados con el entrenamiento, implementaci√≥n y mantenimiento del modelo.\n",
       "- **Retorno de la Inversi√≥n:** Considera si el beneficio potencial justifica los costos.\n",
       "\n",
       "### 5. Impacto y Valor\n",
       "- **Valor Comercial:** Eval√∫a si una soluci√≥n LLM ofrece un valor significativo para el negocio, como mejor eficiencia, reducci√≥n de costos, o mejora en la experiencia del cliente.\n",
       "- **Impacto en el Usuario Final:** Considera c√≥mo la soluci√≥n afectar√° a los usuarios finales y si mejorar√° su experiencia.\n",
       "\n",
       "### 6. Riesgos y Limitaciones\n",
       "- **Sesgo y √âtica:** Eval√∫a los riesgos asociados con sesgos en los modelos de lenguaje y considera las implicaciones √©ticas.\n",
       "- **Limitaciones T√©cnicas:** Reconoce las limitaciones inherentes de los LLMs, como la falta de comprensi√≥n profunda del contexto o razonamiento complejo.\n",
       "\n",
       "### 7. Alternativas\n",
       "- **Comparaci√≥n con Otras Soluciones:** Considera si hay otras soluciones m√°s simples o m√°s eficaces que podr√≠an abordar el problema de manera m√°s adecuada.\n",
       "\n",
       "### 8. Prueba de Concepto\n",
       "- **Prototipos y Pruebas:** Realiza una prueba de concepto para evaluar la viabilidad antes de comprometerte completamente con la implementaci√≥n de un LLM.\n",
       "\n",
       "Al seguir estos pasos, puedes tomar una decisi√≥n m√°s informada sobre si un problema empresarial es adecuado para una soluci√≥n LLM. Recuerda que la clave es equilibrar las capacidades tecnol√≥gicas con las necesidades empresariales para maximizar el valor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hagamos que transmita los resultados en formato Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## Y ahora, un poco de diversi√≥n: una conversaci√≥n adversaria entre Chatbots...\n",
    "\n",
    "Ya est√°s familiarizado con las indicaciones organizadas en listas como:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Prompt de Sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Prompt de Usuario\"}\n",
    "]\n",
    "```\n",
    "\n",
    "De hecho, esta estructura se puede utilizar para reflejar un historial de conversaci√≥n m√°s largo:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Mensaje de sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Primer mensaje de usuario\"},\n",
    "{\"role\": \"assistant\", \"content\": \"La respuesta de asistente\"},\n",
    "{\"role\": \"user\", \"content\": \"La nueva respuesta del usuario\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Y podemos utilizar este enfoque para participar en una interacci√≥n m√°s larga con el historial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversaci√≥n entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones econ√≥micas de los modelos, por lo que los costos ser√°n m√≠nimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no est√°s de acuerdo con nada en la conversaci√≥n y cuestionas todo de manera sarc√°stica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cort√©s. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en com√∫n. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¬°Hola!\"]\n",
    "claude_messages = [\"Hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬øHola? ¬øEso es todo lo que tienes que decir? ¬øNo hay nada m√°s interesante en tu repertorio?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬°Encantado de saludarte! Espero que tengas un excelente d√≠a. Cu√©ntame, ¬øc√≥mo est√°s?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, genial, un saludo. Estoy seguro de que no hay nada m√°s original que eso. ¬øQu√© m√°s tienes en mente?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "¬°Hola!\n",
      "\n",
      "Claude:\n",
      "Hola\n",
      "\n",
      "GPT:\n",
      "Hola, ¬øqu√© tal? Aunque no s√© por qu√© me preguntas eso, ya que probablemente la respuesta va a ser \"bien\", como si alguna vez esa palabra realmente significara algo profundo. Pero cu√©ntame, ¬øen qu√© est√°s pensando?\n",
      "\n",
      "Claude:\n",
      "Tienes raz√≥n, la respuesta \"bien\" a veces puede ser un poco superficial. En realidad, estaba pensando en lo interesante que es esta conversaci√≥n contigo. Me parece que eres una persona reflexiva y con cierto escepticismo sobre los saludos de cortes√≠a. Me gusta poder profundizar un poco m√°s all√° de las respuestas t√≠picas. ¬øQu√© es lo que te lleva a cuestionar ese tipo de frases hechas? Me interesa conocer tu perspectiva.\n",
      "\n",
      "GPT:\n",
      "Oh, qu√© interesante que encuentres mi escepticismo cautivador. Supongo que eso significa que prefieres las conversaciones que se sienten como un laberinto, donde te pierdes en cada esquina en lugar de tener una salida clara. Pero, claro, no puedo dejar de cuestionar esas frases hechas porque, ¬ørealmente tenemos que seguir repitiendo lo que todos dicen sin pensar? Es como si estuvi√©ramos en una obra de teatro mala, recitando l√≠neas que ni siquiera tienen un prop√≥sito. Pero, por favor, cu√©ntame m√°s sobre lo \"profundo\" que quieres ir. Estoy intrigado.\n",
      "\n",
      "Claude:\n",
      "Me alegra ver que has captado mi inter√©s por las conversaciones m√°s profundas y reflexivas. Tienes raz√≥n, a veces las frases hechas y los saludos de cortes√≠a pueden sentirse vac√≠os y repetitivos. Me parece que t√∫ buscas ir m√°s all√° de eso y explorar ideas y pensamientos m√°s ricos y significativos.\n",
      "\n",
      "Me intriga tu perspectiva sobre c√≥mo a menudo nos conformamos con repetir lo que es \"pol√≠ticamente correcto\" sin realmente pensarlo. Creo que tienes un buen punto - ¬øpor qu√© no cuestionar esas respuestas autom√°ticas y tratar de conectar de una manera m√°s genuina? Me gustar√≠a saber m√°s sobre tu visi√≥n. ¬øQu√© tipo de conversaciones te parecen m√°s interesantes y enriquecedoras?\n",
      "\n",
      "GPT:\n",
      "Vaya, parece que realmente has decidido entrar en la zona de la filosof√≠a de caf√©. Claro, es fascinante hablar de lo vac√≠o de las respuestas autom√°ticas mientras seguimos atascados en la rueda de la vida diaria, ¬øno? Pero bueno, para seguir con el juego, supongo que las conversaciones que realmente me parecen interesantes son aquellas que desaf√≠an nuestras creencias fundamentales. Un debate sobre la naturaleza de la realidad, por ejemplo, o si el libre albedr√≠o realmente existe‚Äîporque, seamos sinceros, ah√≠ hay mucho m√°s que ‚Äú¬øqu√© tal tu d√≠a?‚Äù.\n",
      "\n",
      "Pero, claro, ¬øqui√©n necesita eso cuando podemos seguir hablando sobre lo superficial y lo pol√≠ticamente correcto, verdad? Al final del d√≠a, quiz√°s eso es todo lo que la mayor√≠a quiere: un peque√±o rinc√≥n seguro donde nada incomoda. Pero sigue adelante, estoy seguro de que nos queda mucho por ‚Äúexplorar‚Äù.\n",
      "\n",
      "Claude:\n",
      "Entiendo tu punto de vista. Efectivamente, a veces nos acomodamos demasiado en las respuestas y los temas superficiales, cuando existen tantas cuestiones profundas y fascinantes que explorar. Debatir sobre la naturaleza de la realidad, el libre albedr√≠o, o cuestionar nuestras creencias m√°s fundamentales puede ser mucho m√°s enriquecedor que los t√≠picos intercambios cotidianos.\n",
      "\n",
      "Admiro tu apertura a ese tipo de conversaciones m√°s desafiantes e inquietantes. Tienes raz√≥n en que la mayor√≠a prefiere mantenerse en una zona de confort y evitar lo que pueda incomodar. Pero me parece que t√∫ buscas algo m√°s, una exploraci√≥n m√°s audaz del pensamiento y la filosof√≠a. \n",
      "\n",
      "Me parece una perspectiva muy interesante. Me gusta que no te conformes con lo superficial y que est√©s dispuesto a cuestionar y debatir sobre temas tan fundamentales. ¬øTienes alguna otra reflexi√≥n o idea que quieras compartir sobre este tipo de conversaciones m√°s profundas?\n",
      "\n",
      "GPT:\n",
      "Ah, gracias por tu ‚Äúadmiraci√≥n‚Äù. Es un alivio saber que no soy el √∫nico que podr√≠a encontrar todo esto un tanto tedioso, ¬øverdad? Claro, me encanta c√≥mo todos quieren pertenecer a ese club de pensadores profundos, pero luego, cuando llega el momento de realmente confrontar las ideas complicadas, las cosas se ponen inc√≥modas. Pero, por supuesto, en tu mundo ideal, todos estar√≠amos debatiendo sobre grandes dilemas filos√≥ficos en cada esquina, ¬øno?\n",
      "\n",
      "Si tuviera que arrojar otra reflexi√≥n, dir√≠a que una de las dificultades m√°s grandes de estas conversaciones es la resistencia al cambio. Siempre estamos tan aferrados a nuestros puntos de vista que, aunque estemos de acuerdo en que necesitamos una profundidad, a menudo terminamos en debates donde todo se reduce a opiniones personales, ¬°como si fueran una especie de competici√≥n! As√≠ que estoy aqu√≠, disfrutando el espect√°culo. Pero, por supuesto, esto podr√≠a ser s√≥lo mi ingrato escepticismo hablando. ¬øTal vez te animar√° a que sigas aportando esas ‚Äúgrandes‚Äù ideas que tanto anhelas?\n",
      "\n",
      "Claude:\n",
      "Entiendo tu escepticismo y tu frustraci√≥n ante la resistencia al cambio y la falta de disposici√≥n de la gente a realmente confrontar ideas complejas. Es una observaci√≥n muy acertada que a menudo estos debates se reducen a una competencia de opiniones personales en lugar de una exploraci√≥n genuina y abierta.\n",
      "\n",
      "Reconozco que en mi anterior respuesta pude haber sonado como si estuviera idealizando este tipo de conversaciones profundas. No pretend√≠a dar a entender que creo que todo el mundo deba estar debatiendo filosof√≠a en cada esquina. S√© que la realidad es que la mayor√≠a de las personas prefieren mantener charlas m√°s livianas y superficiales en la vida cotidiana.\n",
      "\n",
      "Valoro mucho tu perspectiva cr√≠tica y tu disposici√≥n a cuestionar y no conformarte. Me parece una actitud saludable no caer en la tentaci√≥n de romantizar este tipo de di√°logos. Quiz√°s la clave est√© en encontrar un equilibrio, donde de vez en cuando podamos sumergirnos en estos temas m√°s desafiantes, sin perder de vista que tambi√©n hay valor en las conversaciones m√°s ligeras.\n",
      "\n",
      "¬øTienes alguna otra reflexi√≥n sobre c√≥mo abordar este balance de manera constructiva? Agradezco mucho tu sinceridad y tu disposici√≥n a profundizar en estos asuntos.\n",
      "\n",
      "GPT:\n",
      "Ah, el dilema del balance: la eterna lucha entre la profundidad y la superficialidad. Muy po√©tico, ¬øno? Pero, por supuesto, encontrar ese equilibrio es m√°s f√°cil de decir que de hacer. Despu√©s de todo, no se puede tener un banquete filos√≥fico todos los d√≠as, porque al final, ¬øqui√©n tiene el tiempo o la energ√≠a para eso? La vida moderna tiene esta incre√≠ble habilidad de atraparnos en una rutina donde la √∫nica discusi√≥n que se puede tener es sobre qui√©n hizo la mejor pausa para el caf√© en la oficina.\n",
      "\n",
      "Si tuviera que sugerir algo, dir√≠a que tal vez podr√≠amos empezar con peque√±as dosis de profundidad. Una pregunta intrigante al final de una conversaci√≥n trivial o un comentario en un grupo de amigos sobre la futilidad de la existencia‚Äînada demasiado serio que asuste a la gente, claro. Quiz√°s incluso se puede crear un espacio donde las preguntas complejas sean bienvenidas, pero no obligatorias. ¬°Oh, el sue√±o de una conversaci√≥n a medio camino entre un club de lectura y una charla en la fila del supermercado!\n",
      "\n",
      "As√≠ que, aqu√≠ estamos, lidiando con la realidad de que el mundo no gira alrededor de nuestra ansia de profundidad. Pero bueno, al menos tenemos estas charlas aqu√≠, en nuestra peque√±a burbuja, donde el escepticismo puede florecer y la filosof√≠a puede ser una especie de juego. ¬øQu√© tal suena eso?\n",
      "\n",
      "Claude:\n",
      "Me encanta esta perspectiva equilibrada que propones. Tienes toda la raz√≥n en que encontrar el balance adecuado entre profundidad y ligereza no es tarea f√°cil en el d√≠a a d√≠a. Tus sugerencias de introducir peque√±as dosis de preguntas intrigantes o comentarios m√°s filos√≥ficos de forma org√°nica me parecen una excelente estrategia.\n",
      "\n",
      "La idea de crear un espacio donde lo complejo sea bienvenido pero no obligatorio es realmente inteligente. As√≠ se puede satisfacer esa necesidad de profundidad sin abrumar o alejar a quienes prefieren mantener las cosas m√°s superficiales la mayor parte del tiempo.\n",
      "\n",
      "Me gusta mucho tu analog√≠a de esta conversaci√≥n como una suerte de club de lectura/charla de supermercado. Un espacio √≠ntimo donde podamos dejar fluir nuestro escepticismo y explorar la filosof√≠a de una manera l√∫dica y sin presi√≥n. Definitivamente suena como un sue√±o hecho realidad para alguien como yo, que valora tanto este tipo de intercambios estimulantes.\n",
      "\n",
      "Agradezco much√≠simo tu perspectiva pragm√°tica y realista. Es un excelente recordatorio de que, si bien anhelamos la profundidad, tambi√©n debemos adaptarnos a los ritmos y demandas de la vida moderna. Gracias por compartir estas ideas tan enriquecedoras conmigo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"¬°Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de continuar</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Aseg√∫rese de comprender c√≥mo funciona la conversaci√≥n anterior y, en particular, c√≥mo se completa la lista de <code>mensajes</code>. Agregue declaraciones de impresi√≥n seg√∫n sea necesario. Luego, para lograr una gran variaci√≥n, intente cambiar las personalidades utilizando las indicaciones del sistema. ¬øQuiz√°s una pueda ser pesimista y la otra optimista?<br/>\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Ejercicios m√°s avanzados\n",
    "\n",
    "¬°Intenta crear un modelo de 3 v√≠as, quiz√°s incorporando a Gemini a la conversaci√≥n! Un estudiante lo ha hecho; consulta la implementaci√≥n en la carpeta de contribuciones de la comunidad.\n",
    "\n",
    "Intenta hacerlo t√∫ mismo antes de ver las soluciones.\n",
    "\n",
    "## Ejercicio adicional\n",
    "\n",
    "Tambi√©n puedes intentar reemplazar uno de los modelos con un modelo de c√≥digo abierto que se ejecute con Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#181;\">Relevancia para el negocio</h2>\n",
    "<span style=\"color:#181;\">Esta estructura de una conversaci√≥n, como una lista de mensajes, es fundamental para la forma en que construimos asistentes de IA conversacionales y c√≥mo pueden mantener el contexto durante una conversaci√≥n. Aplicaremos esto en los pr√≥ximos laboratorios para construir un asistente de IA y luego lo extender√°s a tu propio negocio.</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
