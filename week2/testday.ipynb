{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac27721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LiteLLM are both tools in the LLM development ecosystem, but they operate at different layers and serve distinct primary purposes. They are often complementary rather than competing.\n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "---\n",
       "\n",
       "### LangChain\n",
       "\n",
       "**What it is:** A comprehensive framework for developing applications powered by large language models (LLMs). It provides a structured way to combine LLMs with other sources of data and computation.\n",
       "\n",
       "**Primary Purpose:** To enable developers to build complex, stateful, and data-aware LLM applications by orchestrating various components. It focuses on the \"how\" of building the application logic around LLMs.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "*   **Chains:** Sequences of calls to LLMs or other utilities, allowing for multi-step processes.\n",
       "*   **Agents:** LLMs that can reason about which tools to use and when, allowing them to perform complex tasks by interacting with their environment.\n",
       "*   **Prompt Management:** Tools for creating, managing, and optimizing prompts (PromptTemplates).\n",
       "*   **Retrieval:** Integrating LLMs with external data sources (databases, documents, APIs) for context augmentation (e.g., RAG - Retrieval Augmented Generation).\n",
       "*   **Memory:** Giving LLMs the ability to remember past interactions within a conversation.\n",
       "*   **Tooling:** Abstractions for calling external APIs, custom code, or other services.\n",
       "*   **Integrations:** Connectors to numerous LLM providers, vector stores, databases, and other services.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "*   **Orchestration Power:** Excellent for building sophisticated, multi-step LLM workflows.\n",
       "*   **Modularity:** Highly customizable and extensible.\n",
       "*   **Rich Ecosystem:** Large community, many integrations, active development.\n",
       "*   **Abstraction:** Simplifies complex interactions with LLMs and external data/tools.\n",
       "\n",
       "**Weaknesses:**\n",
       "\n",
       "*   **Learning Curve:** Can be complex and overwhelming for beginners due to its breadth.\n",
       "*   **Overhead:** For simple use cases, it might introduce unnecessary complexity or performance overhead.\n",
       "*   **Debugging:** Debugging complex chains and agents can sometimes be challenging.\n",
       "\n",
       "---\n",
       "\n",
       "### LiteLLM\n",
       "\n",
       "**What it is:** A lightweight proxy that unifies the API calls for various large language models from different providers (OpenAI, Azure, Anthropic, Cohere, Hugging Face, etc.) into a single, consistent API.\n",
       "\n",
       "**Primary Purpose:** To simplify and standardize access to multiple LLM providers, making it easy to switch between models, manage costs, handle retries/fallbacks, and avoid vendor lock-in. It focuses on the \"which\" LLM and \"how\" to reliably connect to it.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "*   **Unified API:** A single `completion()` call works across all supported models/providers.\n",
       "*   **Provider Agnostic:** Easily switch between OpenAI, Azure, Anthropic, Google, Cohere, etc., by changing a model string.\n",
       "*   **Cost Tracking:** Built-in cost logging for API calls across providers.\n",
       "*   **Retries & Fallbacks:** Automatic retries for failed requests and the ability to define fallback models if a primary one fails or is unavailable.\n",
       "*   **Caching:** Basic caching for responses to reduce costs and latency.\n",
       "*   **Load Balancing:** Distribute requests across multiple keys or models.\n",
       "*   **Streaming:** Supports streaming responses from all providers.\n",
       "\n",
       "**Strengths:**\n",
       "\n",
       "*   **Simplicity & Consistency:** Dramatically simplifies multi-provider LLM API interactions.\n",
       "*   **Vendor Agnostic:** Reduces lock-in and makes experimenting with different models easy.\n",
       "*   **Reliability:** Built-in features for retries and fallbacks improve application resilience.\n",
       "*   **Cost Management:** Provides visibility and tools for managing LLM API costs.\n",
       "*   **Easy Integration:** Extremely easy to drop into existing Python projects.\n",
       "\n",
       "**Weaknesses:**\n",
       "\n",
       "*   **No Application Logic:** LiteLLM itself does not help you build complex chains, agents, memory, or retrieval systems. It's purely an API abstraction layer.\n",
       "*   **Limited Beyond API Calls:** Its scope is focused on the LLM API interaction, not the broader application architecture.\n",
       "\n",
       "---\n",
       "\n",
       "### Direct Comparison\n",
       "\n",
       "| Feature/Aspect      | LangChain                                         | LiteLLM                                                   |\n",
       "| :------------------ | :------------------------------------------------ | :-------------------------------------------------------- |\n",
       "| **Core Purpose**    | Build complex, stateful LLM applications.         | Standardize and simplify LLM API access across providers. |\n",
       "| **Focus Area**      | Orchestration, application logic, data integration. | API unification, reliability, cost management.             |\n",
       "| **Value Proposition** | Enables sophisticated LLM use cases.             | Reduces vendor lock-in, improves API resilience.          |\n",
       "| **Complexity**      | Can be high, depending on the application.        | Relatively low; primarily a wrapper around API calls.     |\n",
       "| **LLM Integrations**| Connects to many providers, but requires configuration per provider for each call type. | Unifies *all* providers under a single API.              |\n",
       "| **Application Logic**| Provides components for building agents, chains, memory, RAG. | None; solely handles the direct LLM API call.            |\n",
       "| **Reliability**     | Relies on external mechanisms or custom code for retries/fallbacks for individual LLM calls. | Built-in retries, fallbacks, load balancing.             |\n",
       "| **Cost Management** | Indirectly, by optimizing prompts/calls.         | Direct cost logging, helps in model selection for cost.  |\n",
       "| **Example Use Case**| Building a conversational AI agent that can search a database and use external tools. | Switching between OpenAI's GPT-4 and Anthropic's Claude 3 for a basic text generation task with unified code. |\n",
       "\n",
       "---\n",
       "\n",
       "### When to Use Which (or Both)\n",
       "\n",
       "*   **Use LangChain when:**\n",
       "    *   You need to build a complex LLM application involving multiple steps, tools, memory, or external data sources (e.g., a chatbot that answers questions based on your company's documents, an agent that can book flights, or an autonomous task executor).\n",
       "    *   You require powerful orchestration capabilities.\n",
       "    *   You are comfortable with a framework approach to development.\n",
       "\n",
       "*   **Use LiteLLM when:**\n",
       "    *   You need a simple, consistent way to access various LLM providers without rewriting code for each.\n",
       "    *   You want to easily switch between models or providers based on cost, performance, or availability.\n",
       "    *   You need built-in reliability features like retries and fallbacks for your LLM calls.\n",
       "    *   You want to track costs across different LLM APIs easily.\n",
       "    *   You're building a simpler LLM interaction or integrating LLMs into an existing application without needing LangChain's full orchestration power.\n",
       "\n",
       "*   **Use Them Together:** This is a very common and powerful combination!\n",
       "    *   You can configure LangChain to use LiteLLM as its underlying model provider. This gives you the best of both worlds:\n",
       "        *   **LangChain** handles the **application logic, chains, agents, memory, and retrieval.**\n",
       "        *   **LiteLLM** handles the **reliable, unified, and cost-aware connection to the actual LLM APIs.**\n",
       "    *   For example, you'd use LangChain to define how your agent uses tools and searches for information, and LiteLLM would be the layer that abstractly provides the LLM (e.g., `litellm.ChatOpenAI(model=\"azure/gpt-4\")` or `litellm.ChatOpenAI(model=\"claude-3-opus\")`) that LangChain's agent interacts with.\n",
       "\n",
       "---\n",
       "\n",
       "**In essence:**\n",
       "\n",
       "*   **LangChain helps you build the \"brain\" (application logic) around your LLMs.**\n",
       "*   **LiteLLM helps you build a robust and flexible \"mouth\" (API access) for your LLMs.**\n",
       "\n",
       "They operate at different levels of abstraction and are very effective when used in conjunction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display , Markdown\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gemini-2.5-flash',base_url='https://generativelanguage.googleapis.com/v1beta/openai/',api_key = google_api_key)\n",
    "response=llm.invoke('Langchain vs litellm?')\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineering (UV)",
   "language": "python",
   "name": "llm-engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
