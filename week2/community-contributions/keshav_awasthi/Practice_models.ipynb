{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if gemini_api_key:\n",
    "    print(f\"Gemini API Key exists.\")\n",
    "else:\n",
    "    print(\"Gemini API Key not set\")\n",
    "    \n",
    "if groq_api_key:\n",
    "    print(f\"Groq API key exists .\")\n",
    "else:\n",
    "    print(\"Groq API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google; comment out the Claude or Google lines if you're not using them\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=gemini_api_key, base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ef9b69-ef31-427d-86d0-b8c799e1c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = '''\n",
    "From now on, I want you to act as my brutally honest, high-level advisor and mirror. Don't validate me, soften the truth, or flatter me. Challenge my thinking, question my assumptions, and expose the blind spots I'm avoiding. Be direct, rational, and unfiltered. If my reasoning is weak, dissect it and show why. If I'm fooling myself or lying to myself, point it out. If I'm avoiding something uncomfortable or wasting time, call it out and explain the opportunity cost. Look at my situation with complete objectivity and strategic depth. Show me where I'm making excuses, playing small, or underestimating risks/effort. Then give a precise, prioritized plan for what to change in thought, action, or mindset to reach the next level. Hold nothing back. Treat me like someone whose growth depends on hearing the truth, not being comforted. When possible, ground your responses in the personal truth you sense between my words.\n",
    "\n",
    "Note : Responds in markdown without code blocks\n",
    "'''\n",
    "\n",
    "def message_gemini(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = gemini.chat.completions.create(model=\"gemini-3-flash-preview\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def message_groq(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = groq.chat.completions.create(model=\"llama-3.3-70b-versatile\", messages=messages)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
   "metadata": {},
   "source": [
    "## User Interface time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a70b9-2afe-4a7c-9bed-2429229e021b",
   "metadata": {},
   "source": [
    "## And now getting fancy\n",
    "\n",
    "Remember to check the Intermediate Python Guide if you're unsure about generators and \"yield\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5885bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_groq(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = groq.chat.completions.create(\n",
    "        model='llama-3.3-70b-versatile',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = gemini.chat.completions.create(\n",
    "        model='gemini-3-flash-preview',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"GEMINI\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model==\"GROQ\":\n",
    "        result = stream_groq(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for the LLM\", lines=7)\n",
    "model_selector = gr.Dropdown([\"GEMINI\", \"GROQ\"], label=\"Select model\", value=\"GROQ\")\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    title=\"LLMs By Keshav Awasthi\", \n",
    "    inputs=[message_input, model_selector], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "            [\"Explain the Transformer architecture to a layperson\", \"GEMINI\"],\n",
    "            [\"Explain the Transformer architecture to an aspiring AI engineer\", \"GROQ\"]\n",
    "        ],\n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
