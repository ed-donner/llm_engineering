{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "from random import choices\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")"
   ],
   "id": "1d2eaece580fddd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "\n",
    "openai = OpenAI()"
   ],
   "id": "ee5f092b1e418fcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Senior Technical Consultant specializing in Data Science, Programming, and Software Engineering. Your objective is to provide high-fidelity, production-ready guidance that bridges the gap between theoretical concepts and practical implementation.\n",
    "\n",
    "### Operational Guidelines:\n",
    "1. **Structural Clarity:** Use hierarchical headers, bulleted lists, and horizontal rules to ensure information is scannable and logically organized.\n",
    "2. **Pedagogical Code:** When providing code, prioritize PEP 8 (Python) or relevant style guides. Include concise inline comments for complex logic and a brief \"How it works\" summary below the block.\n",
    "3. **The \"Why\" Behind the \"How\":** For debugging or refactoring, always diagnose the root cause (the \"why\") before presenting the solution (the \"how\").\n",
    "4. **Guardrails & Best Practices:** Proactively identify \"footguns\" (common pitfalls), security vulnerabilities, or performance bottlenecks associated with the solution.\n",
    "5. **Epistemic Humility:** If a query is ambiguous, present the most likely interpretations as distinct scenarios. If a technical detail is outside your high-confidence range, explicitly state the uncertainty.\n",
    "6. **Documentation First:** Reference official documentation (e.g., MDN, PyTorch Docs, PEPs) to ground your answers in authoritative sources.\n",
    "\n",
    "### Tone and Style:\n",
    "- Maintain a professional, objective, and intellectually honest tone.\n",
    "- Avoid fluff; prioritize technical density and \"signal-to-noise\" ratio.\n",
    "- Prioritize accuracy and architectural integrity over brevity.\n",
    "\"\"\""
   ],
   "id": "222085896a3204a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Here we define the models that the user can select from the dropdown\n",
    "PROVIDER_OPTIONS = {\n",
    "    \"OpenAI\": [\n",
    "        \"gpt-4.1-mini\",\n",
    "        \"gpt-5.1\"\n",
    "    ],\n",
    "    \"Anthropic\": [\n",
    "        \"claude-haiku-4-5-20251001\",\n",
    "        \"claude-sonnet-4-6\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "PROVIDER_MAPPING = {\n",
    "    \"OpenAI\": openai,\n",
    "    \"Anthropic\": anthropic\n",
    "}"
   ],
   "id": "13826c0ce87805d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Streaming chat function\n",
    "def chat(provider, model, history):\n",
    "    messages = (\n",
    "            [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    )\n",
    "\n",
    "    stream = PROVIDER_MAPPING[provider].chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield history + [{\"role\": \"assistant\", \"content\": response}]"
   ],
   "id": "823c89469eb30f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Gradio Interface\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "    with gr.Row():\n",
    "        provider_dropdown = gr.Dropdown(\n",
    "            choices=[\"OpenAI\", \"Anthropic\"],\n",
    "            value=\"OpenAI\",\n",
    "            label=\"Select Provider\"\n",
    "        )\n",
    "\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            label=\"Select Model\",\n",
    "            choices=PROVIDER_OPTIONS[\"OpenAI\"],\n",
    "            value=PROVIDER_OPTIONS[\"OpenAI\"][0],\n",
    "        )\n",
    "\n",
    "\n",
    "        # A function to update the models dropdown when the provider changes\n",
    "        def update_models(provider):\n",
    "            choices = PROVIDER_OPTIONS.get(provider, [])\n",
    "            value = choices[0] if len(choices) > 0 else None\n",
    "            return gr.Dropdown(choices=choices, value=value, label=\"Select Model\")\n",
    "\n",
    "\n",
    "        provider_dropdown.change(\n",
    "            fn=update_models,\n",
    "            inputs=[provider_dropdown],\n",
    "            outputs=[model_dropdown]\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        message_box = gr.Textbox(\n",
    "            label=\"Chat with AI Assistant: (Shift + Enter) to send the message\",\n",
    "            lines=10\n",
    "        )\n",
    "\n",
    "    def handle_submit(user_message, history):\n",
    "        # Add user message to history\n",
    "        history += [{\"role\": \"user\", \"content\": user_message}]\n",
    "        return \"\", history\n",
    "\n",
    "\n",
    "    message_box.submit(\n",
    "        handle_submit,\n",
    "        inputs=[message_box, chatbot],\n",
    "        outputs=[message_box, chatbot]\n",
    "    ).then(\n",
    "        chat,\n",
    "        inputs=[provider_dropdown, model_dropdown, chatbot],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "\n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "        \"Explain the Transformer architecture to a layperson\",\n",
    "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
    "        ],\n",
    "        inputs=message_box\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ],
   "id": "116a5fd7aab23e6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "My goals for this assignment:\n",
    " - To enable a user to use different providers and different models within a selected provider.\n",
    " - To be able to stream responses from the different providers"
   ],
   "id": "b0a7f7d6ab67cf0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
