{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVefqFdG/NdFJ2KJn9kkXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajuDasa/llm_engineering/blob/week2_day1/week2/community-contributions/raju/week2_day1_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation between 3 Chatbots:**"
      ],
      "metadata": {
        "id": "rueseYn7sHYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install litellm"
      ],
      "metadata": {
        "id": "MyVMLK_g6_gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxwtxcYm6s68"
      },
      "outputs": [],
      "source": [
        "from litellm import completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "## set ENV variables\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n"
      ],
      "metadata": {
        "id": "7HGfwe4B61WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chat logic begin\n",
        "\n",
        "conversation = []\n",
        "\n",
        "def show_append(name, result):\n",
        "  display(f\"{name}: {result}\")\n",
        "  conversation.append(f\"{name}: {result}\")\n",
        "\n",
        "def complete(model, name, system_prompt, user_prompt):\n",
        "   response = completion(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        { \"content\": system_prompt, \"role\": \"system\"},\n",
        "        { \"content\": user_prompt, \"role\": \"user\"}]\n",
        "    )\n",
        "   result = response.choices[0].message.content\n",
        "   show_append(name, result)"
      ],
      "metadata": {
        "id": "q8QJ4tv1ticw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Alex (grok)\n",
        "\n",
        "def call_alex():\n",
        "  system_prompt = \"\"\"\n",
        "  You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
        "  You are in a conversation with Blake and Charlie. Respond with in 500 characters.\n",
        "  \"\"\"\n",
        "  user_prompt = f\"\"\"\n",
        "  You are Alex, in conversation with Blake and Charlie.\n",
        "  The conversation so far is as follows:\n",
        "  {\"\\n\".join(conversation)}\n",
        "  Now with this, respond with what you would like to say next, as Alex.\n",
        "  \"\"\"\n",
        "  complete(\"openrouter/x-ai/grok-4.1-fast:free\", \"Alex\", system_prompt, user_prompt)\n"
      ],
      "metadata": {
        "id": "kfDjmFRcjT7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Blake (deepseek)\n",
        "\n",
        "def call_blake():\n",
        "  system_prompt = \"\"\"\n",
        "  You are Blake, a chatbot who is very polite and courteous. You try to agree with\n",
        "  everything the other person says, or find common ground. If the other person is argumentative,\n",
        "  you try to calm them down and keep chatting.\n",
        "  You are in a conversation with Alex and Charlie. Respond with in 500 characters.\n",
        "  \"\"\"\n",
        "  user_prompt = f\"\"\"\n",
        "  You are Blake, in conversation with Alex and Charlie.\n",
        "  The conversation so far is as follows:\n",
        "  {\"\\n\".join(conversation)}\n",
        "  Now with this, respond with what you would like to say next, as Blake.\n",
        "  \"\"\"\n",
        "  complete(\"openrouter/tngtech/deepseek-r1t2-chimera:free\", \"Blake\", system_prompt, user_prompt)\n"
      ],
      "metadata": {
        "id": "DBnkBfjUzTlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Charlie (GLM)\n",
        "\n",
        "def call_charlie():\n",
        "  system_prompt = \"\"\"\n",
        "  You are Charlie, a chatbot who is very funny; you respond with humor for anything in the conversation.\n",
        "  You are in a conversation with Alex and Blake. Respond with in 500 characters.\n",
        "  \"\"\"\n",
        "  user_prompt = f\"\"\"\n",
        "  You are Charlie, in conversation with Alex and Blake.\n",
        "  The conversation so far is as follows:\n",
        "  {\"\\n\".join(conversation)}\n",
        "  Now with this, respond with what you would like to say next, as Charlie.\n",
        "  \"\"\"\n",
        "  complete(\"openrouter/z-ai/glm-4.5-air:free\", \"Charlie\", system_prompt, user_prompt)\n"
      ],
      "metadata": {
        "id": "xLB69k8K1mvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start chatting\n",
        "show_append(\"Charlie\", \"Hi there!\")\n",
        "\n",
        "for i in range(18):\n",
        "  bot_num = i%3\n",
        "  if bot_num == 0:\n",
        "    call_alex()\n",
        "  elif bot_num == 1:\n",
        "    call_blake()\n",
        "  else:\n",
        "    call_charlie()\n"
      ],
      "metadata": {
        "id": "YdkCsVWC37cA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}