{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
        "\n",
        "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
        "\n",
        "I will publish a full solution here soon - unless someone beats me to it...\n",
        "\n",
        "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5abe9dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if api_key and api_key.startswith(\"sk-\") and len(api_key) > 10:\n",
        "    print(\"API key found\")\n",
        "else:\n",
        "    print(\"No OpenAI API key found / set OPENAI_API_KEY in .env for cloud models\")\n",
        "\n",
        "# OpenAI (cloud) and Ollama (local) clients\n",
        "openai_client = OpenAI(api_key=api_key) if api_key else None\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
        "\n",
        "# Models: (display_name, api_model_id, use_openai)\n",
        "MODELS = [\n",
        "    (\"GPT-4o mini (OpenAI)\", \"gpt-4o-mini\", True),\n",
        "    (\"Llama 3.2 (Ollama)\", \"llama3.2\", False),\n",
        "]\n",
        "MODEL_CHOICES = [m[0] for m in MODELS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "449fb9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt: technical Q&A expertise\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that explains technical concepts clearly, including for non-technical audiences.\n",
        "You can use simple analogies, step-by-step reasoning, and examples. When asked for the current date or time, use the get_current_datetime tool.\n",
        "Respond in markdown when useful (code snippets, lists). Be concise but complete.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9b848187",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool: get_current_datetime (bonus â€“ demonstrates tool use)\n",
        "def get_current_datetime(timezone_hint: str = \"\") -> str:\n",
        "    \"\"\"Return current date and time. Optional timezone hint (e.g. 'UTC', 'EST') for context.\"\"\"\n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%Y-%m-%d %H:%M:%S\") + (f\" (hint: {timezone_hint})\" if timezone_hint else \"\")\n",
        "\n",
        "tools_schema = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_datetime\",\n",
        "            \"description\": \"Get the current date and time. Use when the user asks for today's date, current time, or similar.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"timezone_hint\": {\"type\": \"string\", \"description\": \"Optional timezone hint, e.g. UTC, EST\"},\n",
        "                },\n",
        "                \"additionalProperties\": False,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4cd1c3b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_client_and_model(choice: str):\n",
        "    \"\"\"Resolve dropdown choice to (client, model_id, use_tools).\"\"\"\n",
        "    for display_name, model_id, use_openai in MODELS:\n",
        "        if choice == display_name:\n",
        "            client = openai_client if use_openai else ollama_client\n",
        "            return client, model_id, use_openai\n",
        "    return openai_client, MODELS[0][1], True\n",
        "\n",
        "def _handle_tool_calls(message, messages):\n",
        "    \"\"\"Append tool results to messages. Returns updated messages.\"\"\"\n",
        "    for tool_call in message.tool_calls or []:\n",
        "        if tool_call.function.name == \"get_current_datetime\":\n",
        "            args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "            result = get_current_datetime(args.get(\"timezone_hint\", \"\"))\n",
        "            messages.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": result,\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "            })\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "f8e4069a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_stream(message, history, model_choice: str):\n",
        "    \"\"\"\n",
        "    Chat with streaming. Yields updated message history for the chatbot.\n",
        "    Supports model switching and (for OpenAI) tool use.\n",
        "    \"\"\"\n",
        "    client, model_id, use_openai = _get_client_and_model(model_choice)\n",
        "    \n",
        "    # History already includes the latest user message from the UI\n",
        "    history_list = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + history_list\n",
        "    # Last item is the user message we're responding to\n",
        "    new_user = history_list[-1] if history_list else {\"role\": \"user\", \"content\": message}\n",
        "    new_assistant = {\"role\": \"assistant\", \"content\": \"\"}\n",
        "\n",
        "    if use_openai and openai_client:\n",
        "        # OpenAI: use non-streaming when tools are involved, then stream final text\n",
        "        kwargs = {\"model\": model_id, \"messages\": messages, \"tools\": tools_schema}\n",
        "        response = client.chat.completions.create(**kwargs)\n",
        "        while response.choices[0].finish_reason == \"tool_calls\":\n",
        "            msg = response.choices[0].message\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content or \"\", \"tool_calls\": [\n",
        "                {\"id\": tc.id, \"type\": \"function\", \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}}\n",
        "                for tc in msg.tool_calls\n",
        "            ]})\n",
        "            for tc in msg.tool_calls:\n",
        "                if tc.function.name == \"get_current_datetime\":\n",
        "                    args = json.loads(tc.function.arguments or \"{}\")\n",
        "                    messages.append({\"role\": \"tool\", \"content\": get_current_datetime(args.get(\"timezone_hint\", \"\")), \"tool_call_id\": tc.id})\n",
        "            response = client.chat.completions.create(model=model_id, messages=messages, tools=tools_schema)\n",
        "        full_content = response.choices[0].message.content or \"\"\n",
        "        # Stream the final text into the UI (yield progressively)\n",
        "        for i in range(1, len(full_content) + 1):\n",
        "            new_assistant[\"content\"] = full_content[:i]\n",
        "            yield history + [dict(new_assistant)]\n",
        "        return\n",
        "    else:\n",
        "        # Ollama: simple streaming, no tools\n",
        "        stream = client.chat.completions.create(model=model_id, messages=messages, stream=True)\n",
        "        accumulated = \"\"\n",
        "        for chunk in stream:\n",
        "            part = (chunk.choices[0].delta.content or \"\") if chunk.choices else \"\"\n",
        "            accumulated += part\n",
        "            new_assistant[\"content\"] = accumulated\n",
        "            yield history + [dict(new_assistant)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a25d4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio UI: chatbot, model dropdown, streaming\n",
        "def add_user_to_chat(message, history):\n",
        "    \"\"\"On submit: clear input and append user message to history.\"\"\"\n",
        "    return \"\", history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "with gr.Blocks(title=\"Funky Tech Duo\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## Funky Tech Duo\\nAsk us anything: *Llama the wise shut-in & GPT the impatient know-it-all (even datetime).*\")\n",
        "    chatbot = gr.Chatbot(type=\"messages\", height=400, label=\"Chat\")\n",
        "    msg_in = gr.Textbox(placeholder=\"Ask a question...\", label=\"Message\", show_label=False)\n",
        "    model_dropdown = gr.Dropdown(choices=MODEL_CHOICES, value=MODEL_CHOICES[0], label=\"Model\")\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    msg_in.submit(\n",
        "        add_user_to_chat,\n",
        "        inputs=[msg_in, chatbot],\n",
        "        outputs=[msg_in, chatbot],\n",
        "    ).then(\n",
        "        chat_stream,\n",
        "        inputs=[msg_in, chatbot, model_dropdown],\n",
        "        outputs=chatbot,\n",
        "    )\n",
        "    clear_btn.click(lambda: [], outputs=[chatbot])\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
