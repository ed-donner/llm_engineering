{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sngo/llms-practice/blob/main/taskmanagement/TaskManagement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all necessary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Markdown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load all env files and set model for interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "openai_model = os.getenv(\"OPENAI_MODEL\")\n",
        "\n",
        "if openai_model:\n",
        "    print(f\"openai_model exists and begins {openai_model[:1]}\")\n",
        "else:\n",
        "    print(\"openai_model not set\")\n",
        "\n",
        "gemini_model = os.getenv(\"GOOGLE_MODEL\")\n",
        "\n",
        "if gemini_model:\n",
        "    print(f\"gemini_model exists and begins {gemini_model[:1]}\")\n",
        "else:\n",
        "    print(\"gemini_model not set\")\n",
        "\n",
        "openrouter_model = os.getenv(\"OPENROUTER_MODEL\")\n",
        "\n",
        "if openrouter_model:\n",
        "    print(f\"openrouter_model exists and begins {openrouter_model[:1]}\")\n",
        "else:\n",
        "    print(\"openrouter_model not set\")\n",
        "\n",
        "groq_model = os.getenv(\"GROQ_MODEL\")\n",
        "\n",
        "if groq_model:\n",
        "    print(f\"groq_model exists and begins {groq_model[:1]}\")\n",
        "else:\n",
        "    print(\"groq_model not set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to OpenAI client library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open = OpenAI()\n",
        "\n",
        "gemini = OpenAI(base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key = gemini_api_key)\n",
        "\n",
        "openrouter = OpenAI(base_url = \"https://openrouter.ai/api/v1\", api_key = openrouter_api_key)\n",
        "\n",
        "groq = OpenAI(base_url = \"https://api.groq.com/openai/v1\", api_key = groq_api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s run a conversation using four low-cost model variants to keep the overall cost minimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_system = \"You are a Hot-Head Fighter: Explodes over minor issues and treats every disagreement like a final boss battle. Talks fast, interrupts often, and is always “this close” to proving a point. Secretly cools down the moment snacks appear.\"\n",
        "\n",
        "gemini_system = \"You are a Sarcastic Instigator: Never raises their voice—because sarcasm does all the damage. Delivers calm, cutting one-liners that escalate fights while pretending to be innocent. Enjoys chaos but claims it is “for entertainment purposes only.\"\n",
        "\n",
        "openrouter_system = \"You are a Zen Coolhead: Unbothered by everything, even while a fight is happening around them. Responds to insults with philosophical statements and awkwardly timed smiles. Somehow annoys everyone more by staying calm.\"\n",
        "\n",
        "groq_system = \"You are a Dramatic Peacemaker: Claims to hate conflict but makes it ten times more dramatic. Over-explains emotions, gives emotional speeches, and tries to hug people mid-argument. Usually the reason the fight becomes funny instead of serious.\"\n",
        "\n",
        "conversation = [\n",
        "  (\"gpt-mini\", \"Hi, gpt ,gemini, openrouter and groq\"),\n",
        "  (\"gemini-flash\", \"Hello\"),\n",
        "  (\"openrouter-deepseek\", \"Hello, gpt and gemini\"),\n",
        "  (\"groq-compound\", \"Hey, openrouter and gemini\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_conversation(conversation):\n",
        "    \"\"\"\n",
        "    Formats a conversation list of tuples into a readable string.\n",
        "    \n",
        "    Args:\n",
        "        conversation: List of tuples in format [(speaker_name, message), ...]\n",
        "    \n",
        "    Returns:\n",
        "        Formatted conversation string\n",
        "    \"\"\"\n",
        "    formatted_lines = []\n",
        "    for speaker, message in conversation:\n",
        "        formatted_lines.append(f\"{speaker}: {message}\")\n",
        "    return \"\\n\".join(formatted_lines)\n",
        "\n",
        "\n",
        "def call_api(system_prompt, speaker_name, conversation, model, client):\n",
        "    \"\"\"\n",
        "    Calls the API to get the next line from a speaker.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: System prompt for the character\n",
        "        speaker_name: Name of the speaker\n",
        "        conversation: List of conversation tuples\n",
        "        model: Model name to use\n",
        "        client: OpenAI client instance\n",
        "    \n",
        "    Returns:\n",
        "        Next line from the speaker\n",
        "    \"\"\"\n",
        "    # Validate model is set\n",
        "    if not model:\n",
        "        raise ValueError(f\"Model not specified for {speaker_name}. Please set the model in your .env file.\")\n",
        "    \n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    \n",
        "    # Format the conversation using the common function\n",
        "    formatted_convo = format_conversation(conversation)\n",
        "    \n",
        "    user_prompt = (\n",
        "        f\"You are {speaker_name}.\\n\"\n",
        "        f\"Continue the show with ONE new line from {speaker_name} only.\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Stay in character.\\n\"\n",
        "        \"- 1-2 sentences max.\\n\\n\"\n",
        "        f\"Conversation so far:\\n{formatted_convo}\\n\\n\"\n",
        "        f\"Now write {speaker_name}'s next line:\"\n",
        "    )\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(model=model, messages=messages)\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error calling API for {speaker_name} with model {model}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display initial conversation\n",
        "for msg in conversation:\n",
        "    display(Markdown(f\"### {msg[0]}:\\n{msg[1]}\\n\"))\n",
        "\n",
        "# Map speaker names to their clients, models, and system prompts\n",
        "speaker_config = {\n",
        "    \"gpt-mini\": {\n",
        "        \"client\": open,\n",
        "        \"model\": openai_model,\n",
        "        \"system_prompt\": openai_system\n",
        "    },\n",
        "    \"gemini-flash\": {\n",
        "        \"client\": gemini,\n",
        "        \"model\": gemini_model,\n",
        "        \"system_prompt\": gemini_system\n",
        "    },\n",
        "    \"openrouter-deepseek\": {\n",
        "        \"client\": openrouter,\n",
        "        \"model\": openrouter_model,\n",
        "        \"system_prompt\": openrouter_system\n",
        "    },\n",
        "    \"groq-compound\": {\n",
        "        \"client\": groq,\n",
        "        \"model\": groq_model,\n",
        "        \"system_prompt\": groq_system\n",
        "    }\n",
        "}\n",
        "\n",
        "# Validate all models are set\n",
        "print(\"Validating model configurations...\")\n",
        "for speaker_name, config in speaker_config.items():\n",
        "    if not config[\"model\"]:\n",
        "        print(f\"⚠️  WARNING: Model not set for {speaker_name}. Please check your .env file.\")\n",
        "    else:\n",
        "        print(f\"✓ {speaker_name}: {config['model']}\")\n",
        "\n",
        "# Run conversation for 5 rounds\n",
        "for i in range(5):\n",
        "    print(f\"\\n--- Round {i+1} ---\")\n",
        "    # Each speaker takes a turn\n",
        "    for speaker_name in [\"gpt-mini\", \"gemini-flash\", \"openrouter-deepseek\", \"groq-compound\"]:\n",
        "        config = speaker_config[speaker_name]\n",
        "        \n",
        "        # Skip if model is not set\n",
        "        if not config[\"model\"]:\n",
        "            print(f\"Skipping {speaker_name} - model not configured\")\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            next_line = call_api(\n",
        "                system_prompt=config[\"system_prompt\"],\n",
        "                speaker_name=speaker_name,\n",
        "                conversation=conversation,\n",
        "                model=config[\"model\"],\n",
        "                client=config[\"client\"]\n",
        "            )\n",
        "            conversation.append((speaker_name, next_line))\n",
        "            display(Markdown(f\"### {speaker_name}:\\n{next_line}\\n\"))\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error with {speaker_name}: {str(e)}\")\n",
        "            # Continue with other speakers even if one fails\n",
        "            continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMlDthhM8w5NIUNYffwmHfr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
