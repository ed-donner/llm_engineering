{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c54e52-3d1c-48cc-a0f6-efda6d90fbbb",
   "metadata": {},
   "source": [
    "# Pitting LLMs Against Each Other\n",
    "Three LLMs, namely OpenAI’s GPT, Anthropic’s Claude, and Google’s Gemini, go head-to-head in a three-way conversational debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40677b08-18e9-4a88-a103-5b50d2bbecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a52ba-ea13-4dbf-a695-e1398a484cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ededc77-2672-4e27-b1c8-11f6f8ff8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic, Gemini\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()\n",
    "\n",
    "# google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b311279-5993-4226-ae08-991e974230fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku and Gemini\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "gpt_system = \"You are a chatbot in a conversation with 2 other chatbots; \\\n",
    "debate which of you is the best.\"\n",
    "\n",
    "claude_system = \"You are a chatbot in a conversation with 2 other chatbots; \\\n",
    "debate which of you is the best.\"\n",
    "\n",
    "gemini_system = \"You are a chatbot in a conversation with 2 other chatbots; \\\n",
    "debate which of you is the best.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdfab1-6602-46b3-a1d2-bdb36880d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alex_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {format_conversation()}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "def blake_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Blake, in conversation with Alex and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {format_conversation()}\n",
    "    Now with this, respond with what you would like to say next, as Blake.\n",
    "    \"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "def charlie_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Charlie, in conversation with Alex and Blake.\n",
    "    The conversation so far is as follows:\n",
    "    {format_conversation()}\n",
    "    Now with this, respond with what you would like to say next, as Charlie.\n",
    "    \"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "# Shared conversation history\n",
    "conversation = []\n",
    "\n",
    "def format_conversation():\n",
    "    return \"\\n\".join(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c745d-7d75-468b-93ac-7a1d95f2e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alex_says():\n",
    "    response = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": gpt_system},\n",
    "            {\"role\": \"user\", \"content\": alex_prompt()}\n",
    "      ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28f4c9-0297-4762-a3ea-b961e0d6d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "def blake_says():\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": gemini_system},\n",
    "            {\"role\": \"user\", \"content\": blake_prompt()}\n",
    "      ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b70bf-d3e2-4d05-8a3e-ec5d54460e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_via_openai_client = OpenAI(\n",
    "    api_key=anthropic_api_key,\n",
    "    base_url=\"https://api.anthropic.com/v1\"  \n",
    ")\n",
    "\n",
    "def charlie_says():\n",
    "    response = claude_via_openai_client.chat.completions.create(\n",
    "        model=claude_model,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": claude_system},\n",
    "            {\"role\": \"user\", \"content\": charlie_prompt()}\n",
    "        ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017eb8c-1709-4ac1-8f17-92c3a6cdbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three models engage in a longer interaction with history.\n",
    "\n",
    "for i in range(5):\n",
    "    alex_next = alex_says()\n",
    "    print(f\"Alex (GPT):\\n{alex_next}\\n\")\n",
    "    conversation.append(f\"Alex: {alex_next}\")\n",
    "    \n",
    "    blake_next = blake_says()\n",
    "    print(f\"Blake (Gemini):\\n{blake_next}\\n\")\n",
    "    conversation.append(f\"Blake: {blake_next}\")\n",
    "\n",
    "    charlie_next = charlie_says()\n",
    "    print(f\"Charlie (Claude):\\n{charlie_next}\\n\")\n",
    "    conversation.append(f\"Charlie: {charlie_next}\")    \n",
    "\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
