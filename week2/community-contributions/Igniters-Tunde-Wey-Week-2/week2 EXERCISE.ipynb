{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
        "\n",
        "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
        "\n",
        "I will publish a full solution here soon - unless someone beats me to it...\n",
        "\n",
        "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c6f34ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and clients\n",
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "OLLAMA_URL = \"http://localhost:11434/v1\"\n",
        "\n",
        "# OpenAI (default API) — uses OPENAI_API_KEY\n",
        "openai = OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
        "# OpenRouter (OpenAI-compatible) — use OPENROUTER_API_KEY from https://openrouter.ai/keys\n",
        "openrouter = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=openrouter_api_key) if openrouter_api_key else None\n",
        "ollama = OpenAI(api_key=\"ollama\", base_url=OLLAMA_URL)\n",
        "\n",
        "# Only include models whose client is available\n",
        "MODELS = {}\n",
        "if openai_api_key:\n",
        "    MODELS[\"GPT-4.1-mini (OpenAI)\"] = \"gpt-4.1-mini\"\n",
        "if openrouter_api_key:\n",
        "    # OpenRouter model IDs: https://openrouter.ai/docs#models\n",
        "    MODELS[\"GPT-4o-mini (OpenRouter)\"] = \"openai/gpt-4o-mini\"\n",
        "MODELS[\"Llama 3.2 (Ollama)\"] = \"llama3.2\"\n",
        "DEFAULT_MODEL_KEY = list(MODELS.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8352dbc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt: adds technical expertise (Week 1 style)\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful technical tutor who answers questions about Python, software engineering, data science, and LLMs.\n",
        "Give clear, accurate explanations. If you need the current date to answer (e.g. \"when was X released\"), use the get_current_date tool.\n",
        "Keep answers focused and use examples when helpful.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4e34bd90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool: get current date (bonus tool for the assistant)\n",
        "def get_current_date():\n",
        "    \"\"\"Return today's date in ISO format for the assistant to use when answering questions about dates.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Tool definition for OpenAI-compatible API\n",
        "get_current_date_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_current_date\",\n",
        "        \"description\": \"Get today's date in YYYY-MM-DD format. Use when the user asks about 'today', 'current date', or when something was released.\",\n",
        "        \"parameters\": {\"type\": \"object\", \"properties\": {}, \"additionalProperties\": False},\n",
        "    },\n",
        "}\n",
        "TOOLS = [get_current_date_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3f2a1244",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_tool_calls(message):\n",
        "    \"\"\"Execute tool calls and return tool response messages.\"\"\"\n",
        "    responses = []\n",
        "    for tool_call in message.tool_calls:\n",
        "        if tool_call.function.name == \"get_current_date\":\n",
        "            result = get_current_date()\n",
        "            responses.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": result,\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "            })\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "20d98a52",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_client(model_key):\n",
        "    \"\"\"Return the right API client for the selected model.\"\"\"\n",
        "    if \"OpenRouter\" in model_key:\n",
        "        return openrouter\n",
        "    if \"Ollama\" in model_key or \"llama\" in model_key.lower():\n",
        "        return ollama\n",
        "    return openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8626c8a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_stream(message, history, model_key):\n",
        "    \"\"\"\n",
        "    Chat with streaming. Uses system prompt for expertise, supports model switch, and tools (OpenAI only).\n",
        "    History already includes the latest user message from the UI.\n",
        "    \"\"\"\n",
        "    client = get_client(model_key)\n",
        "    model_name = MODELS.get(model_key, list(MODELS.values())[0])\n",
        "    use_tools = (client == openai and openai is not None) or (client == openrouter and openrouter is not None)\n",
        "    tools_arg = TOOLS if use_tools else None\n",
        "\n",
        "    # History already includes the new user message (added by user_submit)\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for h in history:\n",
        "        messages.append({\"role\": h[\"role\"], \"content\": h[\"content\"]})\n",
        "    new_history = list(history)\n",
        "    assistant_content = \"\"\n",
        "\n",
        "    if use_tools:\n",
        "        while True:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_name, messages=messages, tools=tools_arg\n",
        "            )\n",
        "            msg = response.choices[0].message\n",
        "            if getattr(msg, \"finish_reason\", None) == \"tool_calls\" and getattr(msg, \"tool_calls\", None):\n",
        "                messages.append(msg)\n",
        "                messages.extend(handle_tool_calls(msg))\n",
        "                continue\n",
        "            assistant_content = msg.content or \"\"\n",
        "            break\n",
        "        for i in range(1, len(assistant_content) + 1, 3):\n",
        "            yield new_history + [{\"role\": \"assistant\", \"content\": assistant_content[:i]}]\n",
        "        yield new_history + [{\"role\": \"assistant\", \"content\": assistant_content}]\n",
        "    else:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model_name, messages=messages, stream=True\n",
        "        )\n",
        "        for chunk in stream:\n",
        "            if chunk.choices and chunk.choices[0].delta.content:\n",
        "                assistant_content += chunk.choices[0].delta.content\n",
        "                yield new_history + [{\"role\": \"assistant\", \"content\": assistant_content}]\n",
        "        if assistant_content:\n",
        "            yield new_history + [{\"role\": \"assistant\", \"content\": assistant_content}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "21a772c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7871\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1635, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 760, in async_iteration\n",
            "    return await anext(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 751, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 734, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 898, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_19220\\3319233512.py\", line 20, in chat_stream\n",
            "    response = client.chat.completions.create(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1156, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\USER\\Andela\\llm_engineering\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-or-v1*************************************************************4250. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n"
          ]
        }
      ],
      "source": [
        "# Gradio UI: model dropdown + chat with streaming\n",
        "with gr.Blocks(title=\"Technical Q&A Assistant\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## Technical Q&A Assistant (Week 2 Exercise)\\nAsk questions about Python, software engineering, data science, or LLMs. Use the dropdown to switch models.\")\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=list(MODELS.keys()),\n",
        "            value=DEFAULT_MODEL_KEY,\n",
        "            label=\"Model\",\n",
        "        )\n",
        "    chatbot = gr.Chatbot(type=\"messages\", label=\"Chat\", height=500)\n",
        "    msg = gr.Textbox(label=\"Your question\", placeholder=\"e.g. Explain list comprehensions in Python\")\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "    def user_submit(message, history):\n",
        "        if not message.strip():\n",
        "            return history\n",
        "        return history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    msg.submit(\n",
        "        user_submit,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot],\n",
        "    ).then(\n",
        "        chat_stream,\n",
        "        inputs=[msg, chatbot, model_dropdown],\n",
        "        outputs=[chatbot],\n",
        "        queue=True,\n",
        "    ).then(lambda: \"\", None, [msg])\n",
        "\n",
        "    submit_btn.click(\n",
        "        user_submit,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[chatbot],\n",
        "    ).then(\n",
        "        chat_stream,\n",
        "        inputs=[msg, chatbot, model_dropdown],\n",
        "        outputs=[chatbot],\n",
        "        queue=True,\n",
        "    ).then(lambda: \"\", None, [msg])\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16040f29",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
