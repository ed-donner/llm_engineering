{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Short News Lab\n",
        "Use `AdvancedWebsite` to collect links from a source page, ask Ollama to keep likely news articles, and render 5-line summaries in a Gradio feed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import textwrap\n",
        "import io\n",
        "import contextlib\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import gradio as gr\n",
        "import sys\n",
        "\n",
        "# Add path to access AdvancedWebsite, even when the notebook is run from a nested directory\n",
        "\n",
        "def _find_repo_root() -> Path:\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for p in [cwd, *cwd.parents]:\n",
        "        candidate = p / \"week1\" / \"community-contributions\" / \"aryaman\" / \"advanced_website_scraper.py\"\n",
        "        if candidate.exists():\n",
        "            return p\n",
        "    return cwd\n",
        "\n",
        "ROOT = _find_repo_root()\n",
        "SCRAPER_PATH = ROOT / \"week1\" / \"community-contributions\" / \"aryaman\"\n",
        "if str(SCRAPER_PATH) not in sys.path:\n",
        "    sys.path.append(str(SCRAPER_PATH))\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))\n",
        "\n",
        "from advanced_website_scraper import AdvancedWebsite\n",
        "\n",
        "# Ollama / OpenAI-compatible settings\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "OLLAMA_MODEL = \"llama3.1\"\n",
        "OLLAMA_API_KEY = \"ollama\"  # default key for local Ollama\n",
        "\n",
        "# Generation knobs\n",
        "SUMMARY_TEMPERATURE = 0\n",
        "SUMMARY_MAX_TOKENS = 256  # tight for 5 lines\n",
        "\n",
        "# Filtering prompt template\n",
        "FILTER_PROMPT = \"\"\"\n",
        "You are a news assistant. Given a list of URLs, return only those that look like real news articles.\n",
        "Keep URLs that likely contain news, announcements, or blog posts. Remove home pages, category pages,\n",
        "PDFs, images, videos, or obvious non-news links. Return JSON with a top-level array of URLs only.\n",
        "\"\"\".strip()\n",
        "\n",
        "SUMMARY_SYSTEM = \"\"\"\n",
        "You are a concise news summarizer. Produce exactly 5 bullet lines capturing headline facts,\n",
        "who/what/when/where/why, and key numbers. No preamble, no trailing notes. If content is empty,\n",
        "reply with: 'No content available.'\n",
        "\"\"\".strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "def get_client() -> OpenAI:\n",
        "    return OpenAI(base_url=OLLAMA_BASE_URL, api_key=OLLAMA_API_KEY)\n",
        "\n",
        "\n",
        "def _strip_fences(text: str) -> str:\n",
        "    # Remove Markdown code fences if present\n",
        "    if text.startswith(\"```\"):\n",
        "        text = text.strip().strip(\"`\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def _extract_links_from_text(raw: str) -> List[str]:\n",
        "    # Try JSON first (after removing code fences)\n",
        "    candidate = _strip_fences(raw)\n",
        "    try:\n",
        "        data = json.loads(candidate)\n",
        "        if isinstance(data, list):\n",
        "            return [u for u in data if isinstance(u, str)]\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    # Regex fallback for URLs\n",
        "    urls = re.findall(r\"https?://[^\\s\\]\\\"'`]+\", raw)\n",
        "    return [u.rstrip('.,') for u in urls]\n",
        "\n",
        "\n",
        "def filter_links_with_ollama(links: List[str]) -> List[str]:\n",
        "    if not links:\n",
        "        return []\n",
        "    print(f\"[ollama] filtering {len(links)} links ...\")\n",
        "    client = get_client()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": FILTER_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": \"URLs:\\n\" + \"\\n\".join(links)},\n",
        "    ]\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=OLLAMA_MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=256,\n",
        "        )\n",
        "        raw = resp.choices[0].message.content.strip()\n",
        "        print(f\"[ollama] raw filter response:\\n{raw}\")\n",
        "        cleaned = _extract_links_from_text(raw)\n",
        "        print(f\"[ollama] kept {len(cleaned)} links after parsing\")\n",
        "        return cleaned\n",
        "    except Exception as e:\n",
        "        print(f\"Link filtering failed: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def summarize_link(url: str) -> Dict[str, Any]:\n",
        "    print(f\"[ollama] summarizing: {url}\")\n",
        "    try:\n",
        "        site = AdvancedWebsite(url, use_js=False)\n",
        "        title = site.title or \"(no title)\"\n",
        "        content = site.text\n",
        "        if not content:\n",
        "            print(f\"[ollama] no content at {url}\")\n",
        "            return {\"url\": url, \"title\": title, \"summary\": \"No content available.\"}\n",
        "\n",
        "        summary = site.summarize_with_ollama(\n",
        "            model=OLLAMA_MODEL,\n",
        "            base_url=OLLAMA_BASE_URL,\n",
        "            api_key=OLLAMA_API_KEY,\n",
        "            temperature=SUMMARY_TEMPERATURE,\n",
        "            max_tokens=SUMMARY_MAX_TOKENS,\n",
        "            system_prompt=SUMMARY_SYSTEM,\n",
        "        )\n",
        "        return {\"url\": url, \"title\": title, \"summary\": summary}\n",
        "    except Exception as e:\n",
        "        print(f\"[ollama] summarize failed for {url}: {e}\")\n",
        "        return {\"url\": url, \"title\": \"(error)\", \"summary\": f\"Failed: {e}\"}\n",
        "\n",
        "\n",
        "def fetch_loading_tips(count: int = 10, retries: int = 3, delay: float = 3.0) -> List[str]:\n",
        "    \"\"\"Get short one-liners about how the pipeline works using Ollama, with retry/backoff.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            client = get_client()\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\":\n",
        "                        \"You concisely describe how this short_news pipeline works. One line per fact, \"\n",
        "                        \"under 80 chars, no bullets or numbering. Facts are as follows:\\n\\n\"\n",
        "                        \"Detailed flow of the short_news lab:\\n\\n\"\n",
        "                        \"Inputs/controls: You pick a preset source (TechCrunch or Hacker News). Optional sliders \"\n",
        "                        \"bound the total links considered and the number of articles summarized. A checkbox can \"\n",
        "                        \"show run-time logs. After choosing a source, the sliders/log checkbox and the 'Fetch & Summarize' \"\n",
        "                        \"button appear; controls hide during execution.\\n\\n\"\n",
        "                        \"Fetch & parse: AdvancedWebsite fetches the source page (requests first; Playwright optional if needed) \"\n",
        "                        \"and parses links.\\n\\n\"\n",
        "                        \"Link filtering via Ollama: The scraped links are sent to Ollama (OpenAI-compatible API) with a system \"\n",
        "                        \"prompt to keep only news-like URLs. The model response is parsed (JSON or regex) to a filtered list, \"\n",
        "                        \"capped by the max-links slider.\\n\\n\"\n",
        "                        \"Summarization: For each filtered link (capped by max-articles slider), AdvancedWebsite fetches and extracts \"\n",
        "                        \"text, then summarize_with_ollama produces a concise 5-line summary. If a page is empty or fails, a fallback \"\n",
        "                        \"message is set per article.\\n\\n\"\n",
        "                        \"Loading UX: While the background thread runs, the UI cycles through short Ollama-generated hints about how \"\n",
        "                        \"the pipeline works. If 'Show logs' is enabled, stdout (fetch/filter/summarize logs) is captured and displayed \"\n",
        "                        \"above the feed after completion. Loading and tips are yielded via Gradio queue/generator.\\n\\n\"\n",
        "                        \"Output/feed: Results render as social-style cards (avatar initial, source domain, title, bullet list, and 'Read more' \"\n",
        "                        \"link). Dark gradient styling for cards and panels; logs (if enabled) appear in a monospace block above the summaries.\\n\\n\"\n",
        "                        \"Defaults/config: Uses Ollama at http://localhost:11434/v1 with model llama3.1, temperature 0 for summaries, max_tokens \"\n",
        "                        \"256 for summaries, and a 5-line bullet format.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Give {count} distinct one-line facts about: scraping with AdvancedWebsite, filtering links with Ollama, summarizing news. Return plain lines separated by newlines.\"\n",
        "                },\n",
        "            ]\n",
        "            resp = client.chat.completions.create(\n",
        "                model=OLLAMA_MODEL,\n",
        "                messages=messages,\n",
        "                temperature=0.3,\n",
        "                max_tokens=128,\n",
        "            )\n",
        "            raw = resp.choices[0].message.content.strip()\n",
        "            lines = [ln.strip(\"-• \\t\") for ln in raw.splitlines() if ln.strip()]\n",
        "            return lines[:count] or [\"Fetching links, filtering with Ollama, summarizing to 5 lines.\"]\n",
        "        except Exception as e:\n",
        "            print(f\"[ollama] tip fetch failed (attempt {attempt + 1}/{retries}): {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            return [\n",
        "                \"Scrapes links, filters via Ollama, then summarizes to 5 lines.\",\n",
        "                \"AdvancedWebsite handles fetch + parse; Ollama filters URLs.\",\n",
        "                \"Each kept link is summarized with Ollama chat into 5 bullets.\",\n",
        "            ][:count]\n",
        "\n",
        "\n",
        "def build_news_feed(source_url: str, max_links: int = 15, max_articles: int = 8) -> List[Dict[str, Any]]:\n",
        "    if not source_url:\n",
        "        return []\n",
        "    try:\n",
        "        print(f\"[pipeline] fetching source: {source_url}\")\n",
        "        source = AdvancedWebsite(source_url, use_js=False)\n",
        "        links = source.get_links()\n",
        "        print(f\"[pipeline] found {len(links)} links\")\n",
        "    except Exception as e:\n",
        "        print(f\"[pipeline] failed to fetch source: {e}\")\n",
        "        return [{\"url\": source_url, \"title\": \"(error)\", \"summary\": f\"Failed to fetch source: {e}\"}]\n",
        "\n",
        "    filtered = filter_links_with_ollama(links)\n",
        "    if max_links:\n",
        "        filtered = filtered[:max_links]\n",
        "    print(f\"[pipeline] using {len(filtered)} filtered links (cap {max_links})\")\n",
        "\n",
        "    articles = []\n",
        "    for link in filtered[:max_articles]:\n",
        "        articles.append(summarize_link(link))\n",
        "    print(f\"[pipeline] summarized {len(articles)} articles\")\n",
        "    return articles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7870\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pipeline] fetching source: https://techcrunch.com\n",
            "[pipeline] found 88 links\n",
            "[ollama] filtering 88 links ...\n",
            "[ollama] raw filter response:\n",
            "Here is the JSON output with only the URLs that likely contain news, announcements, or blog posts:\n",
            "\n",
            "```\n",
            "[\n",
            "  \"https://techcrunch.com/2025/12/20/it-felt-so-wrong-colin-angle-on-irobot-the-ftc-and-the-amazon-deal-that-never-was/\",\n",
            "  \"https://techcrunch.com/2025/12/20/new-york-governor-kathy-hochul-signs-raise-act-to-regulate-ai-safety/\",\n",
            "  \"https://techcrunch.com/2025/12/19/sequoia-partner-spreads-debunked-brown-shooting-theory-testing-new-leadership/\",\n",
            "  \"https://techcrunch.com/2025/12/20/openai-allows-users-to-directly-adjust-chatgpts-warmth-and-enthusiasm/\",\n",
            "  \"https://techcrunch.com/2025/12/19/ex-splunk-execs-startup-resolve-ai-hits-1-billion-valuation-with-series-a/\",\n",
            "  \"https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme\n",
            "[ollama] kept 6 links after parsing\n",
            "[pipeline] using 6 filtered links (cap 15)\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/20/it-felt-so-wrong-colin-angle-on-irobot-the-ftc-and-the-amazon-deal-that-never-was/\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/20/new-york-governor-kathy-hochul-signs-raise-act-to-regulate-ai-safety/\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/19/sequoia-partner-spreads-debunked-brown-shooting-theory-testing-new-leadership/\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/20/openai-allows-users-to-directly-adjust-chatgpts-warmth-and-enthusiasm/\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/19/ex-splunk-execs-startup-resolve-ai-hits-1-billion-valuation-with-series-a/\n",
            "[ollama] summarizing: https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme\n",
            "Attempt 1 failed: 404 Client Error: Not Found for url: https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme. Retrying...\n",
            "Attempt 2 failed: 404 Client Error: Not Found for url: https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme. Retrying...\n",
            "Failed to fetch https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme after 3 attempts: 404 Client Error: Not Found for url: https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme\n",
            "Requests method didn't work well. Trying Playwright...\n",
            "Error fetching with Playwright: Page.goto: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "  - navigating to \"https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme\", waiting until \"networkidle\"\n",
            "\n",
            "[ollama] no content at https://techcrunch.com/2025/12/19/elon-musks-56b-tesla-pay-package-restored-by-delaware-supreme\n",
            "[pipeline] summarized 6 articles\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "def _clean_summary(summary: str, max_lines: int = 5) -> List[str]:\n",
        "    lines = [ln.strip() for ln in summary.splitlines() if ln.strip()]\n",
        "    cleaned = []\n",
        "    for ln in lines:\n",
        "        low = ln.lower()\n",
        "        if \"here are\" in low and \"bullet\" in low:\n",
        "            continue\n",
        "        cleaned.append(ln.lstrip(\"-• \"))\n",
        "        if len(cleaned) >= max_lines:\n",
        "            break\n",
        "    return cleaned or [\"No summary available.\"]\n",
        "\n",
        "\n",
        "def _profile_badge(text: str) -> str:\n",
        "    letter = (text.strip()[:1] or \"N\").upper()\n",
        "    return f\"<div class='avatar'>{letter}</div>\"\n",
        "\n",
        "\n",
        "def format_articles_html(articles: List[Dict[str, Any]]) -> str:\n",
        "    if not articles:\n",
        "        return \"<div class='empty'>No articles found.</div>\"\n",
        "    blocks = []\n",
        "    for art in articles:\n",
        "        title = art.get(\"title\") or \"(no title)\"\n",
        "        summary = art.get(\"summary\") or \"No summary.\"\n",
        "        url = art.get(\"url\") or \"\"\n",
        "        lines = _clean_summary(summary)\n",
        "        bullets = \"\".join(f\"<li>• {ln}</li>\" for ln in lines)\n",
        "        badge = _profile_badge(title)\n",
        "        domain = url.split(\"//\")[-1].split(\"/\")[0] if url else \"source\"\n",
        "        block = f\"\"\"\n",
        "        <div class=\"card\">\n",
        "          <div class=\"profile\">\n",
        "            {badge}\n",
        "            <div class=\"meta\">\n",
        "              <div class=\"name\">{domain}</div>\n",
        "              <div class=\"title\">{title}</div>\n",
        "            </div>\n",
        "          </div>\n",
        "          <div class=\"post-content\">\n",
        "            <ul class=\"bullets\">{bullets}</ul>\n",
        "          </div>\n",
        "          <div class=\"interactions\">\n",
        "            <button>Like</button>\n",
        "            <button>Comment</button>\n",
        "            <a class=\"link\" href=\"{url}\" target=\"_blank\">Read more ↗</a>\n",
        "          </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        blocks.append(block)\n",
        "    return \"\".join(blocks)\n",
        "\n",
        "\n",
        "PRESET_SOURCES = {\n",
        "    \"TechCrunch\": \"https://techcrunch.com\",\n",
        "    \"Hacker News\": \"https://news.ycombinator.com/\",\n",
        "}\n",
        "\n",
        "def run_pipeline(source_url, max_links=15, max_articles=8):\n",
        "    hide = gr.update(visible=False)\n",
        "\n",
        "    loading_html = \"<div class='loading'>Loading summaries, please wait...</div>\"\n",
        "    fallback_tips = [\"Fetching links, filtering with Ollama, summarizing to 5 lines.\"]\n",
        "    tips_ready: List[str] = []\n",
        "    tips_done = threading.Event()\n",
        "\n",
        "    def load_tips():\n",
        "        # Fetch tips in the background so the first yield is immediate.\n",
        "        tips_ready.extend(fetch_loading_tips(10))\n",
        "        tips_done.set()\n",
        "\n",
        "    threading.Thread(target=load_tips, daemon=True).start()\n",
        "\n",
        "    done = threading.Event()\n",
        "    result = {}\n",
        "\n",
        "    def worker():\n",
        "        articles = build_news_feed(source_url, max_links=max_links, max_articles=max_articles)\n",
        "        result[\"articles\"] = articles\n",
        "        done.set()\n",
        "\n",
        "    threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "    tip_idx = 0\n",
        "\n",
        "    def next_tip():\n",
        "        nonlocal tip_idx\n",
        "        pool = tips_ready if tips_ready else fallback_tips\n",
        "        tip = pool[tip_idx % len(pool)]\n",
        "        tip_idx += 1\n",
        "        return tip\n",
        "\n",
        "    # immediate first yield without waiting for Ollama tip generation\n",
        "    tip = next_tip()\n",
        "    tip_html = loading_html + f\"<div class='tip-line'>ℹ️ {tip}</div>\"\n",
        "    yield \"⏳ Fetching & summarizing...\", tip_html, hide, hide, hide\n",
        "\n",
        "    while not done.is_set():\n",
        "        tip = next_tip()\n",
        "        tip_html = loading_html + f\"<div class='tip-line'>ℹ️ {tip}</div>\"\n",
        "        yield \"⏳ Fetching & summarizing...\", tip_html, hide, hide, hide\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    articles = result.get(\"articles\", [])\n",
        "    html = format_articles_html(articles)\n",
        "    yield \"✅ Done\", html, hide, hide, hide\n",
        "\n",
        "def set_source(name: str):\n",
        "    url = PRESET_SOURCES.get(name, \"\")\n",
        "    if not url:\n",
        "        return \"❗ Invalid source\", \"\"\n",
        "    return f\"Source: {name} — {url}\", url\n",
        "\n",
        "demo = gr.Blocks(css=\"\"\"\n",
        "#feed {max-height: 640px; overflow-y: auto;}\n",
        ".card {border: 1px solid #1f2937; border-radius: 14px; padding: 16px; margin-bottom: 14px; background: linear-gradient(135deg, #0b1220, #0f172a); color: #e6edf3; box-shadow: 0 6px 16px rgba(0,0,0,0.3);}\n",
        ".profile {display: flex; align-items: center; margin-bottom: 10px; gap: 10px;}\n",
        ".avatar {width: 42px; height: 42px; border-radius: 50%; background: #111827; border: 1px solid #1f2937; display: flex; align-items: center; justify-content: center; font-weight: 700; color: #9fcef8;}\n",
        ".meta {display: flex; flex-direction: column;}\n",
        ".title {font-weight: 700; font-size: 1.05rem; margin: 2px 0;}\n",
        ".name {font-weight: 600; font-size: 0.9rem; color: #9ca3af;}\n",
        ".post-content {margin-bottom: 10px; font-size: 0.95rem; color: #e6edf3;}\n",
        ".bullets {list-style: none; padding-left: 0; margin: 0 0 10px 0;}\n",
        ".bullets li {margin: 4px 0;}\n",
        ".interactions {display: flex; gap: 12px; align-items: center; font-size: 0.9rem; color: #9ca3af;}\n",
        ".interactions button {background: none; border: none; color: #58a6ff; cursor: pointer; padding: 0;}\n",
        ".interactions button:hover {text-decoration: underline;}\n",
        ".link {color: #58a6ff; text-decoration: none; font-weight: 600;}\n",
        ".link:hover {text-decoration: underline;}\n",
        ".empty {padding: 12px; color: #9ca3af;}\n",
        "#status {color: #9ca3af; font-style: italic;}\n",
        ".loading {padding: 16px; border-radius: 14px; background: linear-gradient(135deg, #0b1628, #0f172a); color: #bcd7ff; border: 1px solid #1f2a44; box-shadow: 0 6px 16px rgba(0,0,0,0.35); text-align: center;}\n",
        ".tip-line {margin-top: 8px; color: #cdd9e5; font-size: 0.95rem;}\n",
        "\n",
        "/* Input panel styling */\n",
        "#source_row, #sliders_row, #fetch_row {border: 1px solid #1f2937; border-radius: 14px; padding: 14px 16px; background: linear-gradient(135deg, #0b1220, #0f172a); box-shadow: 0 6px 16px rgba(0,0,0,0.25);}\n",
        "#sliders_row label {color: #e6edf3; font-weight: 600;}\n",
        "#fetch_row .gr-button {width: 100%; font-weight: 700; background: linear-gradient(135deg, #2563eb, #1d4ed8); color: white; border: none;}\n",
        "#fetch_row .gr-button:hover {filter: brightness(1.05);}\n",
        "#next_btn, .source-btn {width: 100%; background: #111827; color: #e6edf3; border: 1px solid #1f2937;}\n",
        "#next_btn:hover, .source-btn:hover {filter: brightness(1.05);}\n",
        "\n",
        "/* Status area */\n",
        "#status {padding: 8px 0;}\n",
        ".pre {font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", \"Courier New\", monospace; font-size: 0.9rem; color: #cdd9e5; background: #0b1220; padding: 12px; border-radius: 10px; border: 1px solid #1f2937; max-height: 300px; overflow-y: auto;}\n",
        "\"\"\")\n",
        "\n",
        "with demo:\n",
        "    gr.Markdown(\"## Short News Feed\")\n",
        "    gr.Markdown(\"Choose a source and summarize its latest links.\")\n",
        "    with gr.Column(visible=True, elem_id=\"source_row\") as source_row:\n",
        "        gr.Markdown(\"Select a preset source:\")\n",
        "        with gr.Row():\n",
        "            tc_btn = gr.Button(\"TechCrunch\", elem_classes=[\"source-btn\"])\n",
        "            hn_btn = gr.Button(\"Hacker News\", elem_classes=[\"source-btn\"])\n",
        "    with gr.Row(visible=False, elem_id=\"sliders_row\") as sliders_row:\n",
        "        max_links_in = gr.Slider(5, 50, value=15, step=1, label=\"Max links to consider\")\n",
        "        max_articles_in = gr.Slider(1, 20, value=8, step=1, label=\"Max articles to summarize\")\n",
        "    with gr.Row(visible=False, elem_id=\"fetch_row\") as fetch_row:\n",
        "        run_btn = gr.Button(\"Fetch & Summarize\", variant=\"primary\")\n",
        "    status = gr.Markdown(\"\", elem_id=\"status\")\n",
        "    feed = gr.HTML(label=\"News Feed\", elem_id=\"feed\")\n",
        "    source_url = gr.State(\"\")\n",
        "\n",
        "    tc_btn.click(set_source, inputs=gr.State(\"TechCrunch\"), outputs=[status, source_url])\n",
        "    hn_btn.click(set_source, inputs=gr.State(\"Hacker News\"), outputs=[status, source_url])\n",
        "\n",
        "    tc_btn.click(lambda: gr.update(visible=True), None, sliders_row)\n",
        "    tc_btn.click(lambda: gr.update(visible=True), None, fetch_row)\n",
        "    hn_btn.click(lambda: gr.update(visible=True), None, sliders_row)\n",
        "    hn_btn.click(lambda: gr.update(visible=True), None, fetch_row)\n",
        "\n",
        "    run_btn.click(\n",
        "        run_pipeline,\n",
        "        inputs=[source_url, max_links_in, max_articles_in],\n",
        "        outputs=[status, feed, source_row, sliders_row, fetch_row],\n",
        "        queue=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def launch_demo(**kwargs):\n",
        "    \"\"\"Launch the Gradio app, closing any existing server to avoid duplicates.\"\"\"\n",
        "    try:\n",
        "        demo.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return demo.queue().launch(**kwargs)\n",
        "\n",
        "# Uncomment to launch in notebook\n",
        "launch_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage\n",
        "- Ensure Ollama is running: `ollama serve` and model `llama3.1` is pulled.\n",
        "- Optional: install Playwright browsers if sites need JS: `python -m playwright install`.\n",
        "- In this notebook, run the cells, then uncomment and run `demo.launch()` to start Gradio.\n",
        "- Input a source URL (e.g., a news index). The app fetches links, filters to likely articles via Ollama, and shows 5-line summaries.\n",
        "- If link filtering or summarization fails for some items, the feed will note the failure per article.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
