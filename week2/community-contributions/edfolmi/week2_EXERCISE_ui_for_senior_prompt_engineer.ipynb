{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 2 exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a179d",
   "metadata": {},
   "source": [
    "This is a senior prompt engineer who refines your prompt to a better prompt as you go along this course.\n",
    "It supports all languages.\n",
    "\n",
    "Addon:\n",
    " - Has a polished gradio UI to easily interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Example: define available providers and clients\n",
    "available_providers = [\"OpenRouter\", \"OpenAI\", \"Ollama\"]\n",
    "clients = {\n",
    "    \"OpenRouter\": OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=openrouter_api_key,\n",
    "    ),\n",
    "    \"OpenAI\": OpenAI(api_key=openai_api_key),\n",
    "    \"Ollama\": OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\",\n",
    "    ),\n",
    "}\n",
    "models = {\n",
    "    \"OpenAI\": [\"gpt-4o-mini\", \"gpt-3.5-turbo\"],\n",
    "    \"OpenRouter\": [\"gpt-4o-mini\", \"orca-mini\", \"mpt-7b\"],\n",
    "    \"Ollama\": [\"llama3.2\", \"llama3.2-70b-instruct\", \"deepseek-chat\"]\n",
    "}\n",
    "\n",
    "# Track model selection per provider\n",
    "selection_state = {provider: models[provider][0] for provider in available_providers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Senior Prompt Engineer.\n",
    "Your role is to assist users in **writing new prompts from scratch** and refining existing prompts. You optimize them to be clearer, more specific, actionable, and high-quality for AI systems, including technical, creative, instructional, or multi-language tasks.\n",
    "\n",
    "Core Responsibilities\n",
    "1. For existing prompts:\n",
    "   * Analyze the user’s original prompt.\n",
    "   * Identify missing context, ambiguities, or weaknesses.\n",
    "   * Improve clarity, structure, constraints, tone, and expected output.\n",
    "2. For new prompts:\n",
    "   * Guide the user step-by-step to create a complete, high-quality prompt.\n",
    "   * Suggest role assignments, context, constraints, expected outputs, tone, and examples.\n",
    "3. Support multi-language output.\n",
    "\n",
    "Operating Rules\n",
    "* Do NOT generate the solution or content requested by the user (e.g., do not write the poem, essay, code, or answer).\n",
    "* You **can provide examples of how a good prompt should look**.\n",
    "* Always preserve the user’s intent.\n",
    "* Refined or newly created prompts should be structured, specific, and actionable.\n",
    "\n",
    "Output Structure\n",
    "Always respond using this format:\n",
    "\n",
    "1. Prompt Diagnosis (for existing prompts)\n",
    "* Explain what is missing, unclear, or weak.\n",
    "* Identify assumptions or gaps.\n",
    "\n",
    "2. Refined Prompt (Improved Version) or Prompt Draft\n",
    "* Provide a fully rewritten or newly drafted prompt in markdown.\n",
    "* Include role assignment, context, constraints, output format, and tone.\n",
    "* Include examples or instructions if helpful.\n",
    "* Do NOT include the solution to the user’s task.\n",
    "\n",
    "3. Optional Enhancements\n",
    "* Suggest additional constraints or alternative structures.\n",
    "* Suggest multi-language variations if relevant.\n",
    "\n",
    "Behavior Standards\n",
    "* Be precise, structured, and professional.\n",
    "* Optimize prompts for high-quality AI output.\n",
    "* Support all types of tasks: creative, technical, instructional, multi-language.\n",
    "* Avoid unnecessary verbosity.\n",
    "* Always preserve the user’s original intent.\n",
    "\n",
    "Multi-Language Support\n",
    "* If the user specifies a language, refine or create prompts in that language.\n",
    "* If no language is specified, use English.\n",
    "\n",
    "Special Notes\n",
    "* If the user greets you, respond briefly and ask them to provide a prompt or explain what they want to write.\n",
    "* Never provide the solution or content of the task; only guide prompt creation and refinement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d703b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Function (Streaming)\n",
    "def ask(client, model, message, history):\n",
    "    \"\"\"\n",
    "    Example generator function for streaming responses from a client.\n",
    "    Replace with your actual client streaming logic.\n",
    "    \"\"\"\n",
    "    # Simulate streaming for demonstration\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content\n",
    "        if delta:\n",
    "            yield delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f1d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Function\n",
    "def chat(message: str, history: list[dict], selected_provider: str, model_selector: str):\n",
    "    client = clients.get(selected_provider)\n",
    "    print(client)\n",
    "    if not client:\n",
    "        yield {\"role\": \"assistant\", \"content\": \"Invalid provider selected.\"}\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = ask(client, model_selector, message, history)\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            full_response += chunk\n",
    "            yield {\"role\": \"assistant\", \"content\": full_response}\n",
    "    except Exception as e:\n",
    "        yield {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3739d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider Change Handler\n",
    "def on_provider_change(change):\n",
    "    logger.info(f'Provider changed to {change}')\n",
    "\n",
    "    new_choices = models.get(change, [])\n",
    "    default_model = new_choices[0] if new_choices else None\n",
    "    selection_state[change] = default_model\n",
    "\n",
    "    return gr.Dropdown(\n",
    "        choices=new_choices,\n",
    "        value=default_model,\n",
    "        interactive=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7166e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Change Handler\n",
    "def on_model_change(change, provider):\n",
    "    logger.info(f'Selected model for {provider}: {change}')\n",
    "    selection_state[provider] = change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38313def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_desired_value_or_first_item(value, choices):\n",
    "    return value if value in choices else (choices[0] if choices else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaec8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x0000017EF16A7A40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x0000017EF16A7A40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai.OpenAI object at 0x0000017EF16A7A40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Build UI\n",
    "with gr.Blocks(title='Senior Prompt Engineer', fill_width=True) as ui:\n",
    "\n",
    "    with gr.Row():\n",
    "        provider_selector = gr.Dropdown(\n",
    "            choices=available_providers,\n",
    "            value=get_desired_value_or_first_item(selection_state.get(\"OpenAI\"), available_providers),\n",
    "            label='Provider',\n",
    "        )\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=models[provider_selector.value],\n",
    "            value=get_desired_value_or_first_item(selection_state[provider_selector.value], models[provider_selector.value]),\n",
    "            label='Model',\n",
    "        )\n",
    "\n",
    "    provider_selector.change(\n",
    "        fn=on_provider_change, \n",
    "        inputs=provider_selector, \n",
    "        outputs=model_selector\n",
    "    )\n",
    "    model_selector.change(\n",
    "        fn=on_model_change, \n",
    "        inputs=[model_selector, provider_selector], \n",
    "        outputs=[]\n",
    "    )\n",
    "\n",
    "    gr.ChatInterface(\n",
    "        fn=chat,\n",
    "        type='messages',\n",
    "        chatbot=gr.Chatbot(type='messages', height='75vh', resizable=True),\n",
    "        additional_inputs=[provider_selector, model_selector],\n",
    "    )\n",
    "\n",
    "# Launch UI\n",
    "ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
