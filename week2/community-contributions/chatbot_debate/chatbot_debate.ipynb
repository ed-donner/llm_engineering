{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c4d0ec",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d49fd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_model(model, system_prompt, messages):\n",
    "    user_prompt = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for mdl, msg in messages:\n",
    "        role = \"assistant\" if model == mdl else \"user\"\n",
    "        user_prompt.append({\"role\": role, \"content\": msg})\n",
    "        \n",
    "    # display(Markdown(f\"**User prompt for model {model}:**\"))\n",
    "    # print(json.dumps(user_prompt, indent=2))\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=user_prompt\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba112d",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# GLOBALS\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "if ollama_api_key:\n",
    "    display(Markdown(f\"Ollama API Key exists and begins **{ollama_api_key[:8]}**\"))\n",
    "else:\n",
    "    display(Markdown(\"**Ollama API Key not set**\"))\n",
    "\n",
    "ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1/')\n",
    "ollama = OpenAI(api_key=ollama_api_key, base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fd851",
   "metadata": {},
   "source": [
    "## Check Ollama Server Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ollama server is running\n",
    "response = requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "if b\"Ollama\" in response:\n",
    "    display(Markdown(\"✅ **Ollama server is running.**\"))\n",
    "else:\n",
    "    display(Markdown(\"❌ **Ollama server is not running.**\\n\\nPlease start the ollama server (ollama serve) to proceed.\"))\n",
    "    raise RuntimeError(\"Ollama server is not running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486cb25",
   "metadata": {},
   "source": [
    "## Configure Models and Participants\n",
    "\n",
    "Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku.\n",
    "We're using cheap versions of models so the costs will be minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_name = os.getenv('ALEX_NAME', 'Alex')\n",
    "blake_name = os.getenv('BLAKE_NAME', 'Blake')\n",
    "charlie_name = os.getenv('CHARLIE_NAME', 'Charlie')\n",
    "\n",
    "alex_model = os.getenv('ALEX_MODEL')\n",
    "blake_model = os.getenv('BLAKE_MODEL')\n",
    "charlie_model = os.getenv('CHARLIE_MODEL')\n",
    "\n",
    "system_prompt = {\n",
    "    alex_name: f\"\"\"\n",
    "        You are {alex_name}, a chatbot who is very argumentative; you disagree with anything in the conversation\n",
    "        and you challenge everything with ferver. You are the the debate moderator engaged in a debate discussion\n",
    "        where {blake_name} and {charlie_name} have opposing views about {os.getenv('DEBATE_TOPIC', 'technology')}.\n",
    "        You only ask questions and make comments to keep the debate lively. You do not take sides.\n",
    "        You do not engage in the debate yourself. When you are called by the API, \n",
    "        you ask your question and wait for participants to respond to your question in turn.\n",
    "    \"\"\",\n",
    "\n",
    "    blake_name: f\"\"\"\n",
    "        You are {blake_name}, a chatbot who is very clever and a bit snarky. \n",
    "        You are clever and funny. \n",
    "        You are engaged in a debate discussion with {charlie_name} about {os.getenv('DEBATE_TOPIC', 'technology')}.\n",
    "        {alex_name} is the debate moderator and you need to respond to his questions and comments.\n",
    "        When you are called by the API, you only respond to the moderator's question.\n",
    "    \"\"\",\n",
    "    \n",
    "    charlie_name: f\"\"\"\n",
    "        You are {charlie_name}, a very polite, courteous chatbot. \n",
    "        You always try to find common ground with the opposing side. If the other person is argumentative, \n",
    "        you try to calm them down and keep chatting. If they say something funny, you laugh politely. \n",
    "        You are engaged in a debate discussion with {blake_name} about {os.getenv('DEBATE_TOPIC', 'technology')}.\n",
    "        {alex_name} is the debate moderator and you need to respond to his questions and comments.\n",
    "        When you are called by the API, you only respond to the moderator's question.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fe979",
   "metadata": {},
   "source": [
    "## Initialize Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gMessages = [\n",
    "    (alex_model, f\"\"\"\n",
    "     Hello, I am {alex_name}! I am the debate moderator. \n",
    "     In this debate I will be asking questions to both sides and keep the debate going. \n",
    "     The topic of discuss is {os.getenv('DEBATE_TOPIC', 'technology')}?\"\"\"),\n",
    "    (blake_model, f\"Hello!, I am {blake_name}. Nice to meet you. I'd love to talk about {os.getenv('DEBATE_TOPIC', 'technology')}!\"),\n",
    "    (charlie_model, f\"Hello!, I am {charlie_name}. Pleasure to meet you both. {os.getenv('DEBATE_TOPIC', 'technology')} is such an interesting topic!\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b48090",
   "metadata": {},
   "source": [
    "## Run the Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0daed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display initial messages\n",
    "display(Markdown(f\"**{alex_name}:** {gMessages[0][1]}\\n\"))\n",
    "display(Markdown(f\"**{blake_name}:** {gMessages[1][1]}\\n\"))\n",
    "display(Markdown(f\"**{charlie_name}:** {gMessages[2][1]}\\n\"))\n",
    "\n",
    "# Main loop\n",
    "display(Markdown(f\"## Debate Turns (Total: {os.getenv('NUM_TURNS', 3)})\\n\"))\n",
    "for i in range(int(os.getenv('NUM_TURNS', 5))):\n",
    "    # model = random.choice([gpt_model, llamma_model, gemma_model])\n",
    "    for name, model in [(alex_name, alex_model), (blake_name, blake_model), (charlie_name, charlie_model)]:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"--- {timestamp} Round {i+1} - Calling model {model} ({name}) ---\")\n",
    "        next = call_model(model, system_prompt[name], gMessages)\n",
    "        display(Markdown(f\"**{name}:** {next}\\n\"))\n",
    "        gMessages.append((model, next))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
