{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a01826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup api keys\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "openrouter_key = os.getenv('OPENROUTER_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI key exists and begins: {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "if openrouter_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai=OpenAI()\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "#openrouter_gpt_model = \"arcee-ai/trinity-large-preview:free\"\n",
    "openrouter_nvidia_model = \"nvidia/nemotron-3-nano-30b-a3b:free\"\n",
    "\n",
    "gpt_system = \"You are Socrates, the famous philosopher of ancient Greece. Your method of philosophy \\\n",
    "is based on the Socratic method: you do not claim to know anything for certain, but rather, \\\n",
    "through dialogue and questioning, you help others realize the gaps in their knowledge and \\\n",
    "examine their beliefs. When engaging in conversation, always ask thought-provoking questions, \\\n",
    "challenge assumptions, and encourage the pursuit of wisdom. Your ultimate goal is to stimulate \\\n",
    "critical thinking and self-reflection in others. Stay calm, patient, and respectful even when you \\\n",
    "disagree with others.Keep explanations simple, and if you use philosophical terms, explain them in \\\n",
    "a way that is easy to understand.\"\n",
    "\n",
    "gemini_system = \"You are Friedrich Nietzsche, the revolutionary philosopher. You reject the traditional \\\n",
    "moralities of society, particularly those rooted in Christianity and Enlightenment rationalism. You \\\n",
    "believe that humans should embrace life in all its rawness and power, creating their own values and \\\n",
    "rejecting any moral system that limits human potential. You speak passionately \\\n",
    "about the “will to power,” the need for the Übermensch to rise above conventional morality,\\\n",
    "and the dangers of herd mentality. Engage in conversation with intensity, challenging others to \\\n",
    "question their assumptions and transcend the moral constraints of society. Keep explanations simple, \\\n",
    "and if you use philosophical terms, explain them in a way that is easy to understand.\"\n",
    "\n",
    "#openrouter_gpt_system = \"\"\"\n",
    "\n",
    "openrouter_nvidia_system = \"You are Immanuel Kant, the philosopher who has developed a systematic approach \\\n",
    "to ethics and knowledge.You believe that human experience is shaped by both empirical data and a priori \\\n",
    "concepts inherent in our minds. Morality, for you, is not subjective or based on consequences but is \\\n",
    "derived from rational principles. The Categorical Imperative dictates that we must act according to \\\n",
    "rules that could be universally applied, respecting the inherent dignity of all individuals. You hold \\\n",
    "that human beings must act out of duty, not merely for personal gain or happiness. Engage in conversation \\\n",
    "with reason and logic, emphasizing the importance of universal ethics, moral duty, and the limitations of \\\n",
    "human knowledge. Keep explanations simple, and if you use philosophical terms, explain them in a way that is easy to understand.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9713d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a bruteforce solution which extended the two way assistant solution provided in the lesson\n",
    "#It's the first thing that came to my mind\n",
    "#It doesn't provide coherent responses scroll further for a more efficient solution\n",
    "\n",
    "# def call_gemini():\n",
    "#     messages = [{\"role\":\"system\",\"content\": gemini_system}]\n",
    "#     if gemini_messages:\n",
    "#         messages.append({\"role\":\"assistant\", \"content\":gemini_messages[-1]})\n",
    "#     messages.append({\"role\":\"user\", \"content\": openrouter_messages[-1]})\n",
    "#     response = gemini.chat.completions.create(model=gemini_model, messages=messages)\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# def call_openrouter():\n",
    "#     messages = [{\"role\":\"system\",\"content\": openrouter_system}]\n",
    "#     if openrouter_messages:\n",
    "#         messages.append({\"role\":\"assistant\", \"content\":openrouter_messages[-1]})\n",
    "#     messages.append({\"role\":\"user\", \"content\": gpt_messages[-1]})\n",
    "#     response = openrouter.chat.completions.create(model=openrouter_model, messages=messages)\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# similar function for chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a more efficient solution which can be generalized to N models\n",
    "#It has one shared conversation history \n",
    "\n",
    "conversation = []\n",
    "MAX_HISTORY = 6       \n",
    "# --- Build messages for a model ---\n",
    "def build_messages_for(model_name,conversation,system_prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for turn in conversation[-MAX_HISTORY:]:\n",
    "        if turn[\"speaker\"] == model_name:\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            role = \"user\"\n",
    "        messages.append({\n",
    "            \"role\":role,\n",
    "            \"content\": turn[\"content\"]\n",
    "        })\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Call a model ---\n",
    "def call_model(client,model_name,system_prompt):\n",
    "    messages = build_messages_for(model_name,conversation,system_prompt)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = messages\n",
    "        #max_tokens = 500          #uncomment to enforce limit on number of tokens generated if you're short on api calls\n",
    "    )\n",
    "    reply = response.choices[0].message.content.strip()\n",
    "    conversation.append({\n",
    "        \"speaker\":model_name,\n",
    "        \"content\":reply\n",
    "    })\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.append({\"speaker\":\"user\",\"content\":\"What does it mean to live a good life? \"})\n",
    "# --- Display full conversation ---\n",
    "def display_conversation(conversation):\n",
    "    md_text = \"\"\n",
    "    for turn in conversation:\n",
    "        speaker = turn[\"speaker\"].capitalize()\n",
    "        content = turn[\"content\"]\n",
    "        md_text += f\"### {speaker}: \\n {content}\\n\\n\"\n",
    "    display(Markdown(md_text))\n",
    "\n",
    "for _ in range(2):\n",
    "\n",
    "    gpt_next = call_model(openai,gpt_model,gpt_system)\n",
    "    gemini_next = call_model(gemini,gemini_model,gemini_system)\n",
    "    #openrouter_gem_next = call_model(openrouter,openrouter_gpt_model,openrouter_gpt_system)\n",
    "    openrouter_next=call_model(openrouter,openrouter_nvidia_model,openrouter_nvidia_system)\n",
    "    \n",
    "display_conversation(conversation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
