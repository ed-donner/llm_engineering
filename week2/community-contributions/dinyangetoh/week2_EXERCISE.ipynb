{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Week 2 Exercise - Website AI Assistant\n",
        "\n",
        "## What I Learned in Week 2\n",
        "\n",
        "Week 2 covered the core building blocks of working LLM applications:\n",
        "- Calling frontier model APIs (OpenAI, Anthropic, Ollama) using the OpenAI-compatible client pattern\n",
        "- Streaming responses token-by-token using `stream=True` and Python generators with `yield`\n",
        "- Crafting effective system prompts to shape model behaviour and ground responses\n",
        "- Building conversational chat with multi-turn message history\n",
        "- Creating interactive UIs with Gradio — `gr.Blocks`, `gr.Chatbot`, `gr.Textbox`, and event wiring\n",
        "- Fetching and cleaning website content using `requests` and `BeautifulSoup` (the scraper pattern)\n",
        "\n",
        "## How I Applied It — The Website AI Assistant\n",
        "\n",
        "This notebook brings all of the above together in a single self-contained prototype. A user pastes any public website URL into a chat window and the assistant reads the site, summarises it, and answers questions — grounded only in what it found.\n",
        "\n",
        "## Components Used\n",
        "\n",
        "| Component | Purpose |\n",
        "|---|---|\n",
        "| `requests` + `BeautifulSoup` | Crawl same-domain pages and extract clean readable text (the Week 2 scraper pattern) |\n",
        "| `OpenAI` client (OpenAI-compatible) | Connect to OpenAI, Anthropic Claude, and Ollama using the same client interface |\n",
        "| System prompt with injected context | All crawled page text is appended to the system prompt so the model reads it as knowledge |\n",
        "| Streaming with `yield` | LLM replies stream token-by-token into the chat window in real time |\n",
        "| Multi-turn message history | Full conversation history is passed on every call so follow-up questions work naturally |\n",
        "| `gr.Blocks` + `gr.Chatbot` | Gradio chat UI launches with a welcome message pre-loaded, no page reload needed |\n",
        "| URL detection in chat | Detects when a message is a URL and triggers the crawl inline — no separate input box |\n",
        "| Loading indicator | A status bubble appears in the chat during the crawl and is overwritten with the summary when done |\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Launch** — Gradio opens with a welcome message asking the user to paste a URL.\n",
        "2. **Paste URL** — The assistant detects the URL, announces it is reading the site, and shows a loading indicator.\n",
        "3. **Crawl** — Up to 20 same-domain pages are fetched and cleaned into plain text documents.\n",
        "4. **Summarise** — The model reads a sample of pages and returns a 2-3 sentence summary plus 3-5 key findings, then asks what to explore.\n",
        "5. **Conversation** — Every follow-up message passes the full crawled text as system context. The model answers from the site only and says so when it cannot find an answer.\n",
        "6. **Switch site** — Pasting any new URL at any point resets the knowledge base and starts a fresh crawl.\n",
        "7. **Model choice** — Set `MODEL_CHOICE = \"gpt\"`, `\"anthropic\"`, or `\"llama\"` in the constants cell to switch between OpenAI gpt-4.1-mini, Anthropic Claude Sonnet, and Ollama llama3.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "8a83b403",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_GPT = \"gpt-4.1-mini\"\n",
        "MODEL_LLAMA = \"llama3.2\"\n",
        "MODEL_ANTHROPIC = \"claude-sonnet-4-5-20250929\"\n",
        "\n",
        "# Change to \"llama\" to use Ollama instead\n",
        "MODEL_CHOICE = \"anthropic\"\n",
        "\n",
        "MAX_PAGES = 20\n",
        "MAX_CHARS_PER_PAGE = 1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e63d3f27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Anthropic API Key exists and begins sk-ant-a\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "\n",
        "openai_client = OpenAI()\n",
        "anthropic_client = OpenAI(base_url=\"https://api.anthropic.com/v1\", api_key=anthropic_api_key)\n",
        "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "dceae091",
      "metadata": {},
      "outputs": [],
      "source": [
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "\n",
        "def get_client_and_model():\n",
        "    if MODEL_CHOICE == \"gpt\":\n",
        "        return openai_client, MODEL_GPT\n",
        "    elif MODEL_CHOICE == \"anthropic\":\n",
        "        return anthropic_client, MODEL_ANTHROPIC\n",
        "    return ollama_client, MODEL_LLAMA\n",
        "\n",
        "\n",
        "def fetch_page_text(url):\n",
        "    response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    title = soup.title.string.strip() if soup.title and soup.title.string else url\n",
        "    if soup.body:\n",
        "        for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            tag.decompose()\n",
        "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        text = \"\"\n",
        "    return title, re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "\n",
        "def is_same_domain(url, base_url):\n",
        "    return urlparse(url).netloc == urlparse(base_url).netloc\n",
        "\n",
        "\n",
        "def extract_links(soup, base_url):\n",
        "    links = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = urljoin(base_url, a[\"href\"])\n",
        "        clean_url = urlparse(href)._replace(fragment=\"\").geturl()\n",
        "        if is_same_domain(clean_url, base_url) and clean_url.startswith(\"http\"):\n",
        "            links.add(clean_url)\n",
        "    return links\n",
        "\n",
        "\n",
        "def crawl_website(base_url, max_pages=MAX_PAGES):\n",
        "    visited = set()\n",
        "    queue = [base_url]\n",
        "    documents = []\n",
        "\n",
        "    while queue and len(visited) < max_pages:\n",
        "        url = queue.pop(0)\n",
        "        if url in visited:\n",
        "            continue\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=10, headers=HEADERS)\n",
        "            if resp.status_code != 200 or \"text/html\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
        "                visited.add(url)\n",
        "                continue\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            title, text = fetch_page_text(url)\n",
        "            if len(text) > 150:\n",
        "                documents.append({\"url\": url, \"title\": title, \"text\": text})\n",
        "            visited.add(url)\n",
        "            for link in extract_links(soup, base_url):\n",
        "                if link not in visited and link not in queue:\n",
        "                    queue.append(link)\n",
        "        except Exception:\n",
        "            visited.add(url)\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "903bb28c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: claude-sonnet-4-5-20250929\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm doing well, thank you for asking. How are you doing today? Is there anything I can help you with?\""
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client, model = get_client_and_model()\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "\n",
        "print(f\"Model: {model}\")\n",
        "\n",
        "responseRaw = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "response = responseRaw.choices[0].message.content\n",
        "\n",
        "response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "232030c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_website_context(documents, max_chars=MAX_CHARS_PER_PAGE):\n",
        "    parts = [\n",
        "        f\"Page: {doc['title']}\\nURL: {doc['url']}\\n{doc['text'][:max_chars]}\"\n",
        "        for doc in documents\n",
        "    ]\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "d7643c5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a knowledgeable assistant that has studied a specific website in depth.\n",
        "You ONLY answer questions based on information found on that website.\n",
        "When answering, naturally cite specific pages using markdown links like [page name](url).\n",
        "Be conversational, concise, and accurate.\n",
        "If the answer is not in the website content provided, say so clearly — do not speculate or use outside knowledge.\"\"\"\n",
        "\n",
        "\n",
        "def stream_chat(messages, client, model):\n",
        "    stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content or \"\"\n",
        "        response += delta\n",
        "        yield response\n",
        "\n",
        "\n",
        "def generate_site_summary(documents):\n",
        "    client, model = get_client_and_model()\n",
        "    sample = \"\\n\\n\".join(\n",
        "        f\"Page: {d['title']}\\nURL: {d['url']}\\n{d['text'][:800]}\"\n",
        "        for d in documents[:6]\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a website analyst. Be concise and friendly.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Based on these pages from the website, write:\\n\"\n",
        "                \"1. A 2-3 sentence summary of what this website is about\\n\"\n",
        "                \"2. 3-5 key topics or findings as bullet points\\n\\n\"\n",
        "                f\"{sample}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "    responseRaw = client.chat.completions.create(model=model, messages=messages)\n",
        "    response = responseRaw.choices[0].message.content\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "01872e04",
      "metadata": {},
      "outputs": [],
      "source": [
        "WEBSITE_CONTEXT = \"\"\n",
        "IS_INDEXED = False\n",
        "\n",
        "\n",
        "def is_url(text):\n",
        "    return text.startswith(\"http://\") or text.startswith(\"https://\")\n",
        "\n",
        "\n",
        "def handle_chat(message, history):\n",
        "    global WEBSITE_CONTEXT, IS_INDEXED\n",
        "\n",
        "    message = message.strip()\n",
        "    if not message:\n",
        "        yield history, gr.update(value=\"\")\n",
        "        return\n",
        "\n",
        "    history = history + [{\"role\": \"user\", \"content\": message}]\n",
        "    yield history, gr.update(value=\"\")\n",
        "\n",
        "    if is_url(message):\n",
        "        WEBSITE_CONTEXT = \"\"\n",
        "        IS_INDEXED = False\n",
        "\n",
        "        history = history + [{\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": (\n",
        "                f\"Give me a few minutes — I'm visiting **{message}** and building my knowledge of the site. \"\n",
        "                \"I'll come back with a summary once I've finished reading through the pages!\"\n",
        "            )\n",
        "        }]\n",
        "        yield history, gr.update(value=\"\")\n",
        "\n",
        "        history = history + [{\"role\": \"assistant\", \"content\": \"⏳ Reading through the pages, please wait...\"}]\n",
        "        yield history, gr.update(value=\"\")\n",
        "\n",
        "        documents = crawl_website(message)\n",
        "\n",
        "        if not documents:\n",
        "            history[-1][\"content\"] = (\n",
        "                \"I wasn't able to access that website. \"\n",
        "                \"Please check the URL and make sure the site is publicly accessible, then try again.\"\n",
        "            )\n",
        "            yield history, gr.update(value=\"\")\n",
        "            return\n",
        "\n",
        "        WEBSITE_CONTEXT = build_website_context(documents)\n",
        "        IS_INDEXED = True\n",
        "\n",
        "        summary = generate_site_summary(documents)\n",
        "        page_word = \"page\" if len(documents) == 1 else \"pages\"\n",
        "        summary_msg = (\n",
        "            f\"{summary}\\n\\n\"\n",
        "            f\"I've read **{len(documents)} {page_word}** from the site. \"\n",
        "            \"What would you like to learn more about?\"\n",
        "        )\n",
        "        history[-1][\"content\"] = summary_msg\n",
        "        yield history, gr.update(value=\"\")\n",
        "        return\n",
        "\n",
        "    if not IS_INDEXED:\n",
        "        history = history + [{\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": (\n",
        "                \"It looks like that isn't a URL. \"\n",
        "                \"Please paste a website address starting with `https://` and I'll get reading!\"\n",
        "            )\n",
        "        }]\n",
        "        yield history, gr.update(value=\"\")\n",
        "        return\n",
        "\n",
        "    system_with_context = f\"{SYSTEM_PROMPT}\\n\\nWebsite knowledge:\\n\\n{WEBSITE_CONTEXT}\"\n",
        "    messages_for_llm = (\n",
        "        [{\"role\": \"system\", \"content\": system_with_context}]\n",
        "        + [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "    )\n",
        "\n",
        "    client, model = get_client_and_model()\n",
        "    history = history + [{\"role\": \"assistant\", \"content\": \"\"}]\n",
        "\n",
        "    for partial in stream_chat(messages_for_llm, client, model):\n",
        "        history[-1][\"content\"] = partial\n",
        "        yield history, gr.update(value=\"\")\n",
        "\n",
        "    yield history, gr.update(value=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "f4074a6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7899\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WELCOME_MESSAGE = [{\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": (\n",
        "        \"Hi! I'm your **Website AI Assistant**.\\n\\n\"\n",
        "        \"I can read any public website and have a conversation about what I find there — \"\n",
        "        \"from products and services to pricing, team, and more.\\n\\n\"\n",
        "        \"To get started, paste a website address (starting with `https://`) into the chat below.\"\n",
        "    )\n",
        "}]\n",
        "\n",
        "with gr.Blocks(title=\"Website AI Assistant\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"# Website AI Assistant\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        type=\"messages\",\n",
        "        value=WELCOME_MESSAGE,\n",
        "        height=500,\n",
        "        show_label=False,\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        msg_input = gr.Textbox(\n",
        "            placeholder=\"Paste a website URL or ask a question...\",\n",
        "            show_label=False,\n",
        "            scale=5,\n",
        "        )\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=handle_chat,\n",
        "        inputs=[msg_input, chatbot],\n",
        "        outputs=[chatbot, msg_input],\n",
        "    )\n",
        "\n",
        "    msg_input.submit(\n",
        "        fn=handle_chat,\n",
        "        inputs=[msg_input, chatbot],\n",
        "        outputs=[chatbot, msg_input],\n",
        "    )\n",
        "\n",
        "demo.queue().launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
