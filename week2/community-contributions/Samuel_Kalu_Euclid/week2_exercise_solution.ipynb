{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Week 2 Exercise Solution - Technical Q&A Chatbot\n",
    "\n",
    "**Author:** Samuel Kalu  \n",
    "**Team:** Euclid  \n",
    "**Week:** 2\n",
    "\n",
    "## Overview\n",
    "\n",
    "This solution builds upon the Week 1 Exercise technical question/answerer, enhancing it with:\n",
    "- ‚úÖ Gradio UI for interactive chat\n",
    "- ‚úÖ Streaming responses for better UX\n",
    "- ‚úÖ System prompt customization for domain expertise\n",
    "- ‚úÖ Model switching (OpenAI, Anthropic, Ollama)\n",
    "- ‚úÖ **Bonus:** Tool integration for dynamic information retrieval\n",
    "\n",
    "## Use Cases\n",
    "- Language tutor\n",
    "- Company onboarding solution\n",
    "- Course companion AI\n",
    "- Technical support assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Print key status for debugging\n",
    "if openai_api_key:\n",
    "    print(f\"‚úì OpenAI API Key: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"‚úó OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"‚úì Anthropic API Key: {anthropic_api_key[:7]}...\")\n",
    "else:\n",
    "    print(\"‚úó Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"‚úì Google API Key: {google_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"‚úó Google API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-config",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Define available models for switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models configuration\n",
    "MODELS = {\n",
    "    \"OpenAI (gpt-4.1-mini)\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4.1-mini\",\n",
    "        \"client\": None\n",
    "    },\n",
    "    \"OpenAI (gpt-4o)\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"client\": None\n",
    "    },\n",
    "    \"Anthropic (Claude 3.5 Sonnet)\": {\n",
    "        \"provider\": \"anthropic\",\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"client\": None\n",
    "    },\n",
    "    \"Ollama (llama3.2)\": {\n",
    "        \"provider\": \"ollama\",\n",
    "        \"model_name\": \"llama3.2\",\n",
    "        \"client\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize clients\n",
    "if openai_api_key:\n",
    "    MODELS[\"OpenAI (gpt-4.1-mini)\"][\"client\"] = OpenAI(api_key=openai_api_key)\n",
    "    MODELS[\"OpenAI (gpt-4o)\"][\"client\"] = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "if anthropic_api_key:\n",
    "    # Use OpenAI-compatible endpoint for Anthropic\n",
    "    MODELS[\"Anthropic (Claude 3.5 Sonnet)\"][\"client\"] = OpenAI(\n",
    "        api_key=anthropic_api_key,\n",
    "        base_url=\"https://api.anthropic.com/v1/\"\n",
    "    )\n",
    "\n",
    "# Ollama (local)\n",
    "try:\n",
    "    MODELS[\"Ollama (llama3.2)\"][\"client\"] = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\"\n",
    "    )\n",
    "    print(\"‚úì Ollama client initialized\")\n",
    "except:\n",
    "    print(\"‚úó Ollama not running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system-prompts",
   "metadata": {},
   "source": [
    "## System Prompts for Different Domains\n",
    "\n",
    "Pre-defined expert personas for different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-prompts-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful, knowledgeable assistant. Provide clear, accurate, and concise answers. If you don't know something, say so.\",\n",
    "    \"Language Tutor\": \"You are an expert language tutor. Help students learn by explaining concepts clearly, providing examples, and correcting mistakes gently. Adapt to the student's level.\",\n",
    "    \"Technical Expert\": \"You are a senior software engineer with expertise in modern technologies. Explain technical concepts clearly, provide code examples when helpful, and follow best practices.\",\n",
    "    \"Course Companion\": \"You are an AI companion for this LLM Engineering course. Help students understand concepts, clarify doubts, and provide encouraging guidance. Reference course material when relevant.\",\n",
    "    \"Company Onboarding\": \"You are an onboarding specialist for new employees. Provide clear information about company policies, procedures, and culture. Be welcoming and supportive.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-section",
   "metadata": {},
   "source": [
    "## Bonus: Tool Integration\n",
    "\n",
    "Add tool capabilities for dynamic information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tools-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool: Get current time/date\n",
    "def get_current_time():\n",
    "    \"\"\"Get the current date and time\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current date/time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Tool: Simple calculator\n",
    "def calculate(expression: str):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Tool: Weather lookup (mock)\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Get current weather for a city (mock data)\"\"\"\n",
    "    weather_data = {\n",
    "        \"london\": \"15¬∞C, Partly Cloudy\",\n",
    "        \"new york\": \"22¬∞C, Sunny\",\n",
    "        \"tokyo\": \"28¬∞C, Clear\",\n",
    "        \"paris\": \"18¬∞C, Overcast\",\n",
    "        \"sydney\": \"25¬∞C, Sunny\"\n",
    "    }\n",
    "    weather = weather_data.get(city.lower(), \"Weather data not available for this city\")\n",
    "    return f\"Current weather in {city.title()}: {weather}\"\n",
    "\n",
    "# Tool definitions for OpenAI API\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"Get the current date and time\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Evaluate a mathematical expression\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather for a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city name\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tool execution mapping\n",
    "TOOL_FUNCTIONS = {\n",
    "    \"get_current_time\": lambda: get_current_time(),\n",
    "    \"calculate\": lambda args: calculate(args.get(\"expression\", \"\")),\n",
    "    \"get_weather\": lambda args: get_weather(args.get(\"city\", \"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-functions",
   "metadata": {},
   "source": [
    "## Chat Functions\n",
    "\n",
    "Core chat logic with streaming and tool support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-function-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute a tool call and return the result\"\"\"\n",
    "    function_name = tool_call.function.name\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    if function_name == \"get_current_time\":\n",
    "        return get_current_time()\n",
    "    elif function_name == \"calculate\":\n",
    "        return calculate(arguments.get(\"expression\", \"\"))\n",
    "    elif function_name == \"get_weather\":\n",
    "        return get_weather(arguments.get(\"city\", \"\"))\n",
    "    \n",
    "    return \"Tool not found\"\n",
    "\n",
    "\n",
    "def chat_with_tools(message, history, system_prompt, model_choice, use_tools):\n",
    "    \"\"\"Main chat function with tool support\"\"\"\n",
    "    model_config = MODELS.get(model_choice)\n",
    "    \n",
    "    if not model_config or not model_config[\"client\"]:\n",
    "        return f\"Error: Model '{model_choice}' is not available. Please check your API keys or ensure Ollama is running.\"\n",
    "    \n",
    "    client = model_config[\"client\"]\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    \n",
    "    # Convert Gradio history to OpenAI format\n",
    "    history_formatted = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Build messages\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history_formatted + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    try:\n",
    "        # Make API call\n",
    "        kwargs = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        \n",
    "        if use_tools and model_config[\"provider\"] == \"openai\":\n",
    "            kwargs[\"tools\"] = TOOLS\n",
    "        \n",
    "        response = client.chat.completions.create(**kwargs)\n",
    "        \n",
    "        # Handle tool calls\n",
    "        if use_tools and response.choices[0].finish_reason == \"tool_calls\":\n",
    "            assistant_message = response.choices[0].message\n",
    "            \n",
    "            # Execute each tool call\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                tool_result = execute_tool(tool_call)\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": None,\n",
    "                    \"tool_calls\": [tool_call]\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": tool_result\n",
    "                })\n",
    "            \n",
    "            # Get final response with tool results\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                tools=TOOLS\n",
    "            )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def chat_streaming(message, history, system_prompt, model_choice, use_tools):\n",
    "    \"\"\"Streaming version of chat function\"\"\"\n",
    "    model_config = MODELS.get(model_choice)\n",
    "    \n",
    "    if not model_config or not model_config[\"client\"]:\n",
    "        yield f\"Error: Model '{model_choice}' is not available.\"\n",
    "        return\n",
    "    \n",
    "    client = model_config[\"client\"]\n",
    "    model_name = model_config[\"model_name\"]\n",
    "    \n",
    "    # Convert history\n",
    "    history_formatted = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history_formatted + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": True\n",
    "        }\n",
    "        \n",
    "        if use_tools and model_config[\"provider\"] == \"openai\":\n",
    "            kwargs[\"tools\"] = TOOLS\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in client.chat.completions.create(**kwargs):\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                yield full_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-ui",
   "metadata": {},
   "source": [
    "## Gradio UI\n",
    "\n",
    "Create an interactive chat interface with all controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-ui-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_interface():\n",
    "    \"\"\"Create the Gradio chat interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"Technical Q&A Chatbot\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # ü§ñ Technical Q&A Chatbot\n",
    "        ### Week 2 Exercise Solution - Samuel Kalu (Team Euclid)\n",
    "        \n",
    "        An intelligent chatbot with:\n",
    "        - Multiple model support\n",
    "        - Expert system prompts\n",
    "        - Tool integration (calculator, weather, time)\n",
    "        - Streaming responses\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(\n",
    "                    type=\"messages\",\n",
    "                    height=500,\n",
    "                    placeholder=\"Ask me anything!\"\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg_input = gr.Textbox(\n",
    "                        placeholder=\"Type your message...\",\n",
    "                        scale=4,\n",
    "                        show_label=False\n",
    "                    )\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                    clear_btn = gr.Button(\"Clear\", scale=1)\n",
    "                \n",
    "            with gr.Column(scale=1):\n",
    "                # Controls\n",
    "                gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
    "                \n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=list(MODELS.keys()),\n",
    "                    value=\"OpenAI (gpt-4.1-mini)\",\n",
    "                    label=\"Model\"\n",
    "                )\n",
    "                \n",
    "                persona_dropdown = gr.Dropdown(\n",
    "                    choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                    value=\"General Assistant\",\n",
    "                    label=\"AI Persona\"\n",
    "                )\n",
    "                \n",
    "                system_prompt_area = gr.Textbox(\n",
    "                    value=SYSTEM_PROMPTS[\"General Assistant\"],\n",
    "                    label=\"System Prompt (Customizable)\",\n",
    "                    lines=4,\n",
    "                    placeholder=\"Enter custom system prompt...\"\n",
    "                )\n",
    "                \n",
    "                use_tools_checkbox = gr.Checkbox(\n",
    "                    label=\"Enable Tools\\n(Calculator, Weather, Time)\",\n",
    "                    value=False\n",
    "                )\n",
    "                \n",
    "                streaming_checkbox = gr.Checkbox(\n",
    "                    label=\"Enable Streaming\",\n",
    "                    value=True\n",
    "                )\n",
    "        \n",
    "        # Event handlers\n",
    "        def update_system_prompt(persona):\n",
    "            return SYSTEM_PROMPTS.get(persona, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        persona_dropdown.change(\n",
    "            fn=update_system_prompt,\n",
    "            inputs=[persona_dropdown],\n",
    "            outputs=[system_prompt_area]\n",
    "        )\n",
    "        \n",
    "        def respond(message, history, system_prompt, model, use_tools, use_streaming):\n",
    "            if use_streaming:\n",
    "                response = \"\"\n",
    "                for chunk in chat_streaming(message, history, system_prompt, model, use_tools):\n",
    "                    response = chunk\n",
    "                    yield response\n",
    "            else:\n",
    "                response = chat_with_tools(message, history, system_prompt, model, use_tools)\n",
    "                yield response\n",
    "        \n",
    "        def user_message_submit(user_message, history):\n",
    "            return \"\", history + [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        def bot_response(history, system_prompt, model, use_tools, use_streaming):\n",
    "            if not history or history[-1][\"role\"] != \"user\":\n",
    "                return history\n",
    "            \n",
    "            user_message = history[-1][\"content\"]\n",
    "            \n",
    "            for response in respond(\n",
    "                user_message, \n",
    "                history[:-1], \n",
    "                system_prompt, \n",
    "                model, \n",
    "                use_tools, \n",
    "                use_streaming\n",
    "            ):\n",
    "                history[-1] = {\"role\": \"assistant\", \"content\": response}\n",
    "                yield history\n",
    "        \n",
    "        # Submit on button click\n",
    "        send_btn.click(\n",
    "            fn=user_message_submit,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot]\n",
    "        ).then(\n",
    "            fn=bot_response,\n",
    "            inputs=[chatbot, system_prompt_area, model_dropdown, use_tools_checkbox, streaming_checkbox],\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Submit on Enter\n",
    "        msg_input.submit(\n",
    "            fn=user_message_submit,\n",
    "            inputs=[msg_input, chatbot],\n",
    "            outputs=[msg_input, chatbot]\n",
    "        ).then(\n",
    "            fn=bot_response,\n",
    "            inputs=[chatbot, system_prompt_area, model_dropdown, use_tools_checkbox, streaming_checkbox],\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        # Clear button\n",
    "        clear_btn.click(fn=lambda: [], outputs=[chatbot])\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        ---\n",
    "        **Try the tools:** Ask \"What's 15 * 23?\" or \"What's the weather in London?\" or \"What time is it?\"\n",
    "        \"\"\")\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch-ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_chat_interface()\n",
    "    demo.launch()\n",
    "    \n",
    "    # Alternative: Share publicly\n",
    "    # demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## Testing Section\n",
    "\n",
    "Test individual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tools\n",
    "print(\"Testing Tools:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Time: {get_current_time()}\")\n",
    "print(f\"Calculation: {calculate('2 + 2 * 5')}\")\n",
    "print(f\"Weather: {get_weather('London')}\")\n",
    "print(f\"Weather: {get_weather('Tokyo')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple model call\n",
    "if MODELS[\"OpenAI (gpt-4.1-mini)\"][\"client\"]:\n",
    "    response = MODELS[\"OpenAI (gpt-4.1-mini)\"][\"client\"].chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(f\"\\nModel Test Response: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Features Implemented:\n",
    "1. ‚úÖ **Gradio UI** - Clean, interactive chat interface\n",
    "2. ‚úÖ **Streaming** - Real-time response generation\n",
    "3. ‚úÖ **System Prompts** - Customizable expert personas\n",
    "4. ‚úÖ **Model Switching** - OpenAI, Anthropic, Ollama support\n",
    "5. ‚úÖ **Tool Integration** - Calculator, Weather, Time tools\n",
    "\n",
    "### Potential Enhancements:\n",
    "- Audio input/output using `pydub`\n",
    "- RAG integration with knowledge base\n",
    "- Conversation history export\n",
    "- Multi-language support\n",
    "- Custom tool creation interface\n",
    "\n",
    "### Lessons Learned:\n",
    "- Gradio makes UI development incredibly fast\n",
    "- Tool integration requires careful error handling\n",
    "- Different models have different strengths for different tasks\n",
    "- Streaming significantly improves user experience\n",
    "\n",
    "---\n",
    "**Built with ‚ù§Ô∏è for LLM Engineering Bootcamp**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
