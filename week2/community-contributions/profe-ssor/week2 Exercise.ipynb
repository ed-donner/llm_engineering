{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Week 2 Exercise — Technical Q&A Prototype (profe-ssor)\n",
        "\n",
        "Full prototype of the **technical question/answerer** from Week 1, with:\n",
        "\n",
        "- **Gradio UI** — chat with history\n",
        "- **Streaming** — token-by-token responses\n",
        "- **System prompt** — technical tutor expertise\n",
        "- **Model switch** — GPT (OpenAI/OpenRouter) or Llama (Ollama)\n",
        "- **Tool** — `look_up_definition(term)` for quick definitions\n",
        "\n",
        "**Run all cells in order**, then use the app. Optional: add audio (Whisper + TTS) for voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "if openrouter_api_key:\n",
        "    openai_client = OpenAI(api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
        "    GPT_MODEL = \"openai/gpt-4o-mini\"\n",
        "    print(\"GPT: OpenRouter\")\n",
        "else:\n",
        "    openai_client = OpenAI()\n",
        "    GPT_MODEL = \"gpt-4o-mini\"\n",
        "    print(\"GPT: OpenAI\")\n",
        "\n",
        "OLLAMA_MODEL = \"llama3.2\"\n",
        "print(\\\"Llama:\\\", OLLAMA_MODEL, \\\"(Ollama)\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sysprompt-header",
      "metadata": {},
      "source": [
        "## System prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sysprompt",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a helpful technical tutor. You answer questions about Python code, software engineering, data science, and LLMs.\n",
        "Give clear, accurate explanations. If you don't know something, say so.\n",
        "You may call look_up_definition(term) to get a quick definition before explaining.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tool-header",
      "metadata": {},
      "source": [
        "## Tool: look_up_definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tool-def",
      "metadata": {},
      "outputs": [],
      "source": [
        "def look_up_definition(term: str) -> str:\n",
        "    glossary = {\n",
        "        \"lru_cache\": \"functools.lru_cache: decorator that caches recent calls (LRU = Least Recently Used).\",\n",
        "        \"transformer\": \"Neural network architecture based on self-attention (GPT, BERT).\",\n",
        "        \"token\": \"Unit of text (word or subword) that an LLM processes.\",\n",
        "        \"streaming\": \"Sending model output incrementally as it is generated.\",\n",
        "        \"api\": \"Application Programming Interface: a way for programs to talk to each other.\",\n",
        "    }\n",
        "    return glossary.get(term.strip().lower(), f\"No definition for '{term}'.\")\n",
        "\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"look_up_definition\",\n",
        "        \"description\": \"Get a brief technical definition (e.g. lru_cache, transformer, token).\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"term\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"term\"],\n",
        "            \"additionalProperties\": False,\n",
        "        },\n",
        "    },\n",
        "}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handle-tools",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_tool_calls(message):\n",
        "    out = []\n",
        "    for tc in message.tool_calls:\n",
        "        if tc.function.name == \"look_up_definition\":\n",
        "            args = json.loads(tc.function.arguments)\n",
        "            out.append({\"role\": \"tool\", \"content\": look_up_definition(args.get(\"term\", \"\")), \"tool_call_id\": tc.id})\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chat-header",
      "metadata": {},
      "source": [
        "## Chat (streaming + model switch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chat-stream",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_stream(message, history, model_choice):\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for h in history:\n",
        "        messages.append({\"role\": h[\"role\"], \"content\": h[\"content\"] or \"\"})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    if model_choice == \"Llama (Ollama)\":\n",
        "        reply = \"\"\n",
        "        try:\n",
        "            for chunk in ollama.chat(model=OLLAMA_MODEL, messages=messages, stream=True):\n",
        "                part = (chunk.get(\"message\") or {}).get(\"content\") or \"\"\n",
        "                reply += part\n",
        "                yield reply\n",
        "        except Exception as e:\n",
        "            yield f\"Ollama error: {e}. Is Ollama running? Model {OLLAMA_MODEL} pulled?\"\n",
        "        return\n",
        "\n",
        "    # GPT + tools\n",
        "    r = openai_client.chat.completions.create(model=GPT_MODEL, messages=messages, tools=tools, tool_choice=\"auto\")\n",
        "    while r.choices[0].finish_reason == \"tool_calls\":\n",
        "        msg = r.choices[0].message\n",
        "        messages.append(msg)\n",
        "        messages.extend(handle_tool_calls(msg))\n",
        "        r = openai_client.chat.completions.create(model=GPT_MODEL, messages=messages, tools=tools, tool_choice=\"auto\")\n",
        "\n",
        "    content = (r.choices[0].message.content or \"\").strip()\n",
        "    if content:\n",
        "        yield content\n",
        "        return\n",
        "    stream = openai_client.chat.completions.create(model=GPT_MODEL, messages=messages, stream=True)\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += (chunk.choices[0].delta.content or \"\")\n",
        "        yield result\n",
        "    if not result:\n",
        "        yield \"(No text reply.)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gradio-header",
      "metadata": {},
      "source": [
        "## Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gradio-ui",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(message, history, model_choice):\n",
        "    \"\"\"Gradio ChatInterface: yield streamed assistant reply.\"\"\"\n",
        "    if not message or not message.strip():\n",
        "        return\n",
        "    for partial in chat_stream(message, history, model_choice):\n",
        "        yield partial\n",
        "\n",
        "with gr.Blocks(title=\"Technical Q&A\", theme=gr.themes.Soft(), css=\".main { max-width: 700px; margin: auto; }\") as demo:\n",
        "    gr.Markdown(\"### Technical tutor — Python, ML, LLMs. Ask e.g. *What is lru_cache?*\")\n",
        "    model_dropdown = gr.Dropdown(\n",
        "        [\"GPT (OpenAI/OpenRouter)\", \"Llama (Ollama)\"],\n",
        "        value=\"GPT (OpenAI/OpenRouter)\",\n",
        "        label=\"Model\",\n",
        "    )\n",
        "    gr.ChatInterface(\n",
        "        fn=chat,\n",
        "        type=\"messages\",\n",
        "        additional_inputs=[model_dropdown],\n",
        "        chatbot=gr.Chatbot(type=\"messages\", height=400),\n",
        "        textbox=gr.Textbox(placeholder=\"Ask a technical question...\", lines=2, label=\"Message\"),\n",
        "        submit_btn=\"Send\",\n",
        "    )\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
