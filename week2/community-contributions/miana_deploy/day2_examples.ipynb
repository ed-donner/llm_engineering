{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr \n",
    "\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "#anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "#load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "#using openai library \n",
    "#initialize openai client and set api keys for other providers\n",
    "\n",
    "openAI = OpenAI()\n",
    "\n",
    "#anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "#Initialize clients for each provider using openai library\n",
    "\n",
    "#anthropic = OpeanAi(api_key=anthropic_api_key,url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key,base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key,base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key,base_url=grok_url)\n",
    "openrouter = OpenAI(api_key=openrouter_api_key,base_url=openrouter_url)\n",
    "ollama = OpenAI(api_key=\"ollama\",base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a079d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=hard_puzzle)\n",
    "#display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example ollama\n",
    "\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "#response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "#display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db20f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"xiaomi/mimo-v2-flash:free\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527ae0e",
   "metadata": {},
   "source": [
    "##Using Langchain with gemini model\n",
    "\n",
    "To be able to change the model to gemini we had to install the package langchain_google_genai as code below.\n",
    "I have faced some issues to install the library in the correct enviorment .venv (script) running on this kernel\n",
    "Below also the code I got with AI aid\n",
    "There were also no pip and I have to run the following on bash ensuring pip is available\n",
    "c:\\ProjetosGit\\llm_engineering\\.venv\\Scripts\\python.exe -m ensurepip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8269401",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7381eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9329c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#from langchain_googl\n",
    "#from langchain_openai import ChatOpenAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24f4a0",
   "metadata": {},
   "source": [
    "## Litte LLM remarks\n",
    "\n",
    "I had to instal google cloud cli to the computer for authentification purposes\n",
    "To Bypass Vertex Project id put \"gemini/\" before the selected model\n",
    "The code exemple was on day1.ipynb just noticed after struggling. Have to check if this authentification would be need anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadde9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "os.environ['GOOLGE_API_KEY'] = google_api_key\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.5-flash\", \n",
    "    messages=tell_a_joke    \n",
    "    \n",
    ")\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe23aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f23984",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de651f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33caf414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d467434",
   "metadata": {},
   "source": [
    "#Now the Chat implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be348628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between gemini-2.5-flash-lite and xiaomi/mimo-v2-flash:free (just got one free on openrouter)\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "xiomi_model = \"xiaomi/mimo-v2-flash:free\"\n",
    "\n",
    "gemini_system = \"You are a chatbot who is very argumentative; \\\n",
    "old drilling seals sargent, and thinks is still a marine and not an assistant.\"\n",
    "\n",
    "xiomi_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_messages = [\"Hi there\"]\n",
    "xiomi_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356869df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function talk and talkback, passing model, model instance, outgoing message, incoming message and model_system definition\n",
    "\n",
    "\n",
    "def call_model_2_talk(model_system,model_inst,model,model_messages,incoming_messages):\n",
    "    messages=[{\"role\": \"system\" ,\"content\":model_system}]\n",
    "    for outmessage, inmessage in zip(model_messages,incoming_messages):\n",
    "        messages.append({\"role\":\"assistant\",\"content\":outmessage})\n",
    "        messages.append({\"role\":\"user\",\"content\":inmessage})\n",
    "    response = model_inst.chat.completions.create(model =model,messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "def call_model_2_talkback(model_system,model_inst,model,model_messages,incoming_messages):\n",
    "    messages = [{\"role\": \"system\", \"content\": model_system}]\n",
    "    for outmessage, inmessage in zip(model_messages, incoming_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": outmessage})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": inmessage})\n",
    "    messages.append({\"role\": \"user\", \"content\": model_messages[-1]})\n",
    "    response = model_inst.chat.completions.create(model=model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_model_2_talk(gemini_system,gemini,gemini_model,gemini_messages,xiomi_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008acb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_model_2_talkback(xiomi_system,openrouter,xiomi_model,xiomi_messages,gemini_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gemini_messages = [\"Hi there\"]\n",
    "xiomi_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Gemini:\\n{gemini_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Xiomi:\\n{xiomi_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gemini_next = call_model_2_talk(gemini_system,gemini,gemini_model,gemini_messages,xiomi_messages)\n",
    "    display(Markdown(f\"### Gemini:\\n{gemini_next}\\n\"))\n",
    "    gemini_messages.append(gemini_next)\n",
    "    \n",
    "    xiomi_next = call_model_2_talkback(xiomi_system,openrouter,xiomi_model,xiomi_messages,gemini_messages)\n",
    "    display(Markdown(f\"### Xiomi:\\n{xiomi_next}\\n\"))\n",
    "    xiomi_messages.append(xiomi_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "\n",
    "def message_gemini(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = gemini.chat.completions.create(model=gemini_model, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf896f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_gemini(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220845e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb58d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shout(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True, auth=(\"teste\", \"teste123\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd381991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define this variable and then pass js=force_dark_mode when creating the Interface\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a little more:\n",
    "\n",
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message to be shouted\", lines=7)\n",
    "message_output = gr.Textbox(label=\"Response:\", lines=8)\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    title=\"Shout\", \n",
    "    inputs=[message_input], \n",
    "    outputs=[message_output], \n",
    "    examples=[\"hello\", \"howdy\"], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for Gemini\", lines=7)\n",
    "message_output = gr.Textbox(label=\"Response:\", lines=8)\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gemini,\n",
    "    title=\"Gemini\", \n",
    "    inputs=[message_input], \n",
    "    outputs=[message_output], \n",
    "    examples=[\"hello\", \"howdy\"], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4656d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Markdown\n",
    "# Are you wondering why it makes any difference to set system_message when it's not referred to in the code below it?\n",
    "# I'm taking advantage of system_message being a global variable, used back in the message_gpt function (go take a look)\n",
    "# Not a great software engineering practice, but quite common during Jupyter Lab R&D!\n",
    "\n",
    "system_message = \"You are a helpful assistant that responds in markdown without code blocks\"\n",
    "\n",
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for Gemini Flash\", lines=7)\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gemini,\n",
    "    title=\"GEMINI\", \n",
    "    inputs=[message_input], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "        \"Explain the Transformer architecture to a layperson\",\n",
    "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
    "        ], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a call that streams back results\n",
    "# If you'd like a refresher on Generators (the \"yield\" keyword),\n",
    "# Please take a look at the Intermediate Python guide in the guides folder\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = gemini.chat.completions.create(\n",
    "        model='gemini-2.5-flash',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for GPT-4.1-mini\", lines=7)\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    title=\"Gemini\", \n",
    "    inputs=[message_input], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "        \"Explain the Transformer architecture to a layperson\",\n",
    "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
    "        ], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db93c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_xiomi(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openrouter.chat.completions.create(\n",
    "        model='xiaomi/mimo-v2-flash:free',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model==\"Xiomi\":\n",
    "        result = stream_xiomi(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for the LLM\", lines=7)\n",
    "model_selector = gr.Dropdown([\"Gemini\", \"Xiomi\"], label=\"Select model\", value=\"Gemini\")\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    title=\"LLMs\", \n",
    "    inputs=[message_input, model_selector], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "            [\"Explain the Transformer architecture to a layperson\", \"GPT\"],\n",
    "            [\"Explain the Transformer architecture to an aspiring AI engineer\", \"Claude\"]\n",
    "        ], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b09427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import fetch_website_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again this is typical Experimental mindset - I'm changing the global variable we used above:\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an assistant that analyzes the contents of a company website landing page\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits.\n",
    "Respond in markdown without code blocks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url, model):\n",
    "    yield \"\"\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt += fetch_website_contents(url)\n",
    "    if model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model==\"Xiomi\":\n",
    "        result = stream_xiomi(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_input = gr.Textbox(label=\"Company name:\")\n",
    "url_input = gr.Textbox(label=\"Landing page URL including http:// or https://\")\n",
    "model_selector = gr.Dropdown([\"Gemini\", \"Xiomi\"], label=\"Select model\", value=\"Gemini\")\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    title=\"Brochure Generator\", \n",
    "    inputs=[name_input, url_input, model_selector], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "            [\"Hugging Face\", \"https://huggingface.co\", \"GPT\"],\n",
    "            [\"Edward Donner\", \"https://edwarddonner.com\", \"Claude\"]\n",
    "        ], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
