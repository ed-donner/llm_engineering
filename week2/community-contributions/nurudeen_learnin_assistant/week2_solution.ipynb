{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "A python learning assistant with tool calling capability and streaming with model toggle between Openai and Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96066270",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants    \n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_GEMINI = 'gemini-2.5-flash'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51acf1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment\n",
        "load_dotenv(override=True)\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if openai_api_key and openai_api_key.startswith('sk-proj-'):\n",
        "    print(\"Openai API key is set and ready\")\n",
        "else:\n",
        "    print(\"Check env file to make sure that openai key is set!\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and ready\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15da5937",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Client Liberary\n",
        "openai = OpenAI()\n",
        "\n",
        "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
        "\n",
        "#system prompt\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful programming assistant. When given a technical question about Python, your task is to analyze the question and provide a detailed explanation, formatted in Markdown. Your responses should be clear, concise, and educational, aiming to help the user understand the concept or code in question. Use code blocks, bullet points, and examples as appropriate to illustrate your explanation.\n",
        "But When you call a tool, limit the response to the tool's response and do not provide any further explanation on the topic.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096bfd0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "DB = \"scores.db\"\n",
        "\n",
        "with sqlite3.connect(DB) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS scores (\n",
        "            topic TEXT COLLATE NOCASE PRIMARY KEY,\n",
        "            score INT NOT NULL\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd3fe99",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_score(topic, score):\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('INSERT INTO scores (topic, score) VALUES (?, ?) ON CONFLICT(topic) DO UPDATE SET score=excluded.score', (topic.lower(), score))\n",
        "        conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f20776",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_scores = {\n",
        "    \"Python Functions\": 95,\n",
        "    \"List Comprehensions\": 88,\n",
        "    \"SQLite Usage\": 92,\n",
        "    \"Decorators\": 85,\n",
        "    \"Exception Handling\": 90\n",
        "}\n",
        "\n",
        "for topic, score in sample_scores.items():\n",
        "    set_score(topic, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e60bdc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get note\n",
        "\n",
        "def get_score(topic):\n",
        "    print(f\"DATABASE TOOL CALLED: Getting score for {topic}\", flush=True)\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('SELECT score FROM scores WHERE topic = ?', (topic.lower(),))\n",
        "        result = cursor.fetchone()\n",
        "        # Return clear note content so the model can use it in the final answer\n",
        "        return result[0] if result else \"No note data available for this topic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81c1716",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool Description\n",
        "scores_function = {\n",
        "    \"name\": \"get_score\",\n",
        "    \"description\": \"Get the score for a provided topic\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"topic_name\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the topic the user wants a score for\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"topic_name\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "tools = [{\"type\": \"function\", \"function\": scores_function}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af528d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool calling handler\n",
        "\n",
        "def handle_tool_call(message):\n",
        "  \"\"\"message: dict with 'tool_calls' (list of {id, function: {name, arguments}}).\"\"\"\n",
        "  responses = []\n",
        "  for tool_call in message.get(\"tool_calls\", []):\n",
        "      fn = tool_call.get(\"function\", {})\n",
        "      if fn.get(\"name\") == \"get_score\":\n",
        "          arguments = json.loads(fn.get(\"arguments\", \"{}\"))\n",
        "          topic_name = arguments.get(\"topic_name\")\n",
        "          topic_score = get_score(topic_name)\n",
        "          responses.append({\n",
        "              \"role\": \"tool\",\n",
        "              \"content\": str(topic_score),\n",
        "              \"tool_call_id\": tool_call[\"id\"]\n",
        "          })\n",
        "          print(responses)\n",
        "  return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfeb6b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shared: streaming + tool-calling loop (OpenAI-compatible client: openai or gemini)\n",
        "def _history_to_messages(history):\n",
        "    \"\"\"Normalize Gradio history to list of {role, content}. Supports 'messages' format or 'tuples' format.\"\"\"\n",
        "    if not history:\n",
        "        return []\n",
        "    first = history[0]\n",
        "    if isinstance(first, dict) and \"role\" in first:\n",
        "        return [{\"role\": h[\"role\"], \"content\": h[\"content\"] if isinstance(h.get(\"content\"), str) else str(h.get(\"content\", \"\"))} for h in history]\n",
        "    out = []\n",
        "    for pair in history:\n",
        "        u, a = (pair[0], pair[1]) if len(pair) >= 2 else (pair[0], \"\")\n",
        "        out.append({\"role\": \"user\", \"content\": u if isinstance(u, str) else str(u)})\n",
        "        out.append({\"role\": \"assistant\", \"content\": a if isinstance(a, str) else str(a)})\n",
        "    return out\n",
        "\n",
        "def chat_stream_with_tools(client, model_name, message, history):\n",
        "    \"\"\"Stream chat with tool calling. Yields accumulated content; runs tools and re-requests until model returns text only.\"\"\"\n",
        "    history_msgs = _history_to_messages(history)\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history_msgs + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    while True:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            stream=True,\n",
        "        )\n",
        "        content = \"\"\n",
        "        tool_calls_accum = {}\n",
        "        finish_reason = None\n",
        "\n",
        "        for chunk in stream:\n",
        "            if not chunk.choices:\n",
        "                continue\n",
        "            d = chunk.choices[0].delta\n",
        "            if d.content:\n",
        "                content += d.content\n",
        "                yield content\n",
        "            for tc in d.tool_calls or []:\n",
        "                i = tc.index\n",
        "                if i not in tool_calls_accum:\n",
        "                    tool_calls_accum[i] = {\"id\": \"\", \"name\": \"\", \"arguments\": \"\"}\n",
        "                if tc.id:\n",
        "                    tool_calls_accum[i][\"id\"] = tc.id\n",
        "                if tc.function and tc.function.name:\n",
        "                    tool_calls_accum[i][\"name\"] = tc.function.name\n",
        "                if tc.function and tc.function.arguments:\n",
        "                    tool_calls_accum[i][\"arguments\"] += tc.function.arguments or \"\"\n",
        "            finish_reason = chunk.choices[0].finish_reason or finish_reason\n",
        "\n",
        "        if finish_reason != \"tool_calls\" or not tool_calls_accum:\n",
        "            break\n",
        "\n",
        "        sorted_tcs = [tool_calls_accum[i] for i in sorted(tool_calls_accum)]\n",
        "        assistant_msg = {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": content or None,\n",
        "            \"tool_calls\": [\n",
        "                {\"id\": t[\"id\"], \"type\": \"function\", \"function\": {\"name\": t[\"name\"], \"arguments\": t[\"arguments\"]}}\n",
        "                for t in sorted_tcs\n",
        "            ],\n",
        "        }\n",
        "        messages.append(assistant_msg)\n",
        "        responses = handle_tool_call(assistant_msg)\n",
        "        messages.extend(responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e338fb58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# declare the function for openai (streaming + tool calling)\n",
        "def chat_openai(message, history):\n",
        "    \"\"\"Stream response with tool calling via shared logic.\"\"\"\n",
        "    yield from chat_stream_with_tools(openai, MODEL_GPT, message, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a1e3b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# declare the function for gemini (streaming + tool calling, same as OpenAI)\n",
        "def chat_gemini(message, history):\n",
        "    \"\"\"Stream response with tool calling via shared logic.\"\"\"\n",
        "    yield from chat_stream_with_tools(gemini, MODEL_GEMINI, message, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "405259c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# model selector: receives message, history (from ChatInterface), and model (from dropdown)\n",
        "def model_selector(message, history, model):\n",
        "    if model == \"GPT\":\n",
        "        yield from chat_openai(message, history)\n",
        "    elif model == \"Gemini\":\n",
        "        yield from chat_gemini(message, history)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146c0f7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build chat interface using gr.ChatInterface â€” handles message, history, and model automatically\n",
        "model_dropdown = gr.Dropdown([\"GPT\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
        "\n",
        "view = gr.ChatInterface(\n",
        "    fn=model_selector,\n",
        "    type=\"messages\",\n",
        "    title=\"Personal Assistant\",\n",
        "    description=\"Ask me anything about Python programming. Try: 'What do your notes say about Decorators?'\",\n",
        "    additional_inputs=[model_dropdown],\n",
        "    examples=[\n",
        "        [\"Explain how enumerate() works in Python\", \"GPT\"],\n",
        "        [\"What is a list comprehension?\", \"Gemini\"],\n",
        "        [\"What is the score for Decorators?\", \"GPT\"],\n",
        "    ],\n",
        ")\n",
        "\n",
        "view.launch()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
