{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# The Entropy Archivist: Final Prototype\n",
    "A multi-modal system using OpenRouter, Gradio, and persistent persona-based logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import base64\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OR_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OR_API_KEY)\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"\n",
    "# Act as a brilliant polymath and storyteller. Explain entropy in thermodynamics through the lens of a crumbling ancient library. \n",
    "# Use the library as an analogy for order/disorder, fading ink for energy dissipation, and remain scientifically accurate.\n",
    "# \"\"\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Act as a brilliant polymath and storyteller. The user is about to ask you anything, answer scientifically accurately and clearly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_models(model_id):\n",
    "    # In a real implementation, I'd validate the model_id here to ensure it's an audio model\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    if not audio_path:\n",
    "        return \"Explain entropy.\"\n",
    "\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        base64_audio = base64.b64encode(audio_file.read()).decode(\"utf-8\")\n",
    "        response = client.chat.completions.create(\n",
    "            model='openai/gpt-4o-audio-preview',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Please transcribe this audio file.\"},\n",
    "                        {\n",
    "                            \"type\": \"input_audio\",\n",
    "                            \"input_audio\": {\n",
    "                                \"data\": base64_audio,\n",
    "                                \"format\": \"wav\", # Specify the format of your audio file\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    print('Transcribed: ' + response.choices[0].message.content)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import base64\n",
    "import os\n",
    "def generate_speech(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-audio-preview', # Ensure this matches your provider's requirement\n",
    "        messages=[{\"role\": \"user\", \"content\": 'Say the following: ' + text}],\n",
    "        modalities=['text', \"audio\"],\n",
    "        audio={\"format\": \"pcm16\", \"voice\": \"alloy\"},\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    output_path = \"output.wav\"\n",
    "    \n",
    "    with wave.open(output_path, 'wb') as wav_file:\n",
    "        wav_file.setnchannels(1)      # Mono\n",
    "        wav_file.setsampwidth(2)      # 16-bit\n",
    "        wav_file.setframerate(24000)  # Standard for gpt-4o-audio\n",
    "        \n",
    "        for chunk in response:\n",
    "            # Convert the Pydantic object to a dict to access nested fields safely\n",
    "            chunk_dict = chunk.model_dump()\n",
    "            \n",
    "            # Navigate the dictionary structure\n",
    "            choices = chunk_dict.get(\"choices\", [])\n",
    "            if choices:\n",
    "                delta = choices[0].get(\"delta\", {})\n",
    "                audio_data = delta.get(\"audio\", {}).get(\"data\")\n",
    "                \n",
    "                if audio_data:\n",
    "                    raw_bytes = base64.b64decode(audio_data)\n",
    "                    wav_file.writeframes(raw_bytes)\n",
    "                    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_agent_response(audio_input, model_choice):\n",
    "    print(f\"Received audio input: {audio_input}, model choice: {model_choice}\")\n",
    "    user_query = transcribe_audio(audio_input)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=get_or_models(model_choice),\n",
    "        messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_query}],\n",
    "        stream=True\n",
    "    )\n",
    "    full_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            full_text += chunk.choices[0].delta.content\n",
    "            yield full_text # , None\n",
    "    yield full_text # , generate_speech(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Entropy Archivist\") as demo:\n",
    "    gr.Markdown(\"# üèõÔ∏è The Entropy Archivist\")\n",
    "    with gr.Row():\n",
    "        mic_in = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "        model_sel = gr.Dropdown([\"openai/gpt-4o-mini\", \"meta-llama/llama-3.2-3b-instruct\"], value=\"openai/gpt-4o-mini\")\n",
    "    \n",
    "    text_out = gr.Markdown()\n",
    "    # audio_out = gr.Audio(autoplay=True, type=\"filepath\", streaming=True)\n",
    "    \n",
    "    btn = gr.Button(\"Seek Wisdom\")\n",
    "    btn.click(stream_agent_response, inputs=[mic_in, model_sel], outputs=text_out)\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
