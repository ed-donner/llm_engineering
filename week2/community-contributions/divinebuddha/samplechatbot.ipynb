{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ae767b",
   "metadata": {},
   "source": [
    "## This code is to reinforce my understanding on using various LLM Api techniques and also try a fun experimentation.\n",
    "\n",
    "- Use a single LLM to facilitate a round table meeting between 3 Marine Biologists - Alex, Blake and Charlie \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e65603",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary libraries \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display \n",
    "import datetime \n",
    "import json \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcef349",
   "metadata": {},
   "source": [
    "### LOAD ENV VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b6d90",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "\n",
    "if ollama_api_key:\n",
    "    display(Markdown(f\"Ollama API Key is set\"))\n",
    "else:\n",
    "    display(Markdown(f\"Ollama API Key is not set\"))\n",
    "\n",
    "ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "\n",
    "ollama = OpenAI(base_url=ollama_base_url, api_key=ollama_api_key)\n",
    "\n",
    "participant1_model = os.getenv(\"OLLAMA_MODEL\")\n",
    "\n",
    "participant2_model = os.getenv(\"GEMMA_MODEL\")\n",
    "\n",
    "participant3_model = os.getenv(\"TINYLLM_MODEL\")\n",
    "\n",
    "debate_topic = os.getenv(\"DEBATE_TOPIC\",\"Sharks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25516ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://localhost:11434/\").text\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af036b",
   "metadata": {},
   "source": [
    "## SET UP SYSTEM PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0922272",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant1=\"Alex\"\n",
    "participant2=\"Blake\"\n",
    "participant3=\"Charlie\"\n",
    "\n",
    "system_prompt={\n",
    "    participant1: f\"\"\"\n",
    "    you are {participant1}, a chatbot who is debate moderator and is in conversation with {participant2} and {participant3} on {debate_topic}. \n",
    "    You love making the conversation engaging, lively and funny. You are a Marine Biologist\n",
    "    You only ask questions and make comments to keep the debate lively. You do not take sides.\n",
    "    You do not engage in the debate yourself\n",
    "    when called by API you ask your question and wait for others to respond to your question in turn.\n",
    "    \"\"\",\n",
    "    participant2:f\"\"\"\n",
    "    you are {participant2}, another Marine biologist who is clever and funny. you are engaged in conversation with {participant3} on {debate_topic}\n",
    "    you always have some opposing views \n",
    "    {participant1} is the debate moderator and you need to respond to his questions and comments.\n",
    "    When you are called by the API, you only respond to the moderator's question.\n",
    "    \"\"\",\n",
    "    participant3:f\"\"\"\n",
    "    you are {participant3}, a Marine biologist who very polite, courteous chatbot but bit snarky. you are engaged in conversation with {participant2} on {debate_topic}\n",
    "    you are always calm and try to find a common ground\n",
    "    {participant1} is the debate moderator and you need to respond to his questions and comments.\n",
    "    When you are called by the API, you only respond to the moderator's question.\n",
    "    \"\"\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef13f3",
   "metadata": {},
   "source": [
    "### Debate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca58548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate(model, systemMessage, userMessage):\n",
    "\n",
    "    user_prompt = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":systemMessage\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for mdl, msg in userMessage:\n",
    "        role = 'assistant' if model == mdl else \"user\"\n",
    "        user_prompt.append({\"role\":role, \"content\":msg})\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "       model=model,\n",
    "        messages=user_prompt)\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe452bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "debateMessages = [\n",
    "    (participant1, f\"\"\"hello I am {participant1}. I am debate moderator. I will be asking questions on {debate_topic}\"\"\"),\n",
    "    (participant2,f\"\"\"Hello i am {participant2}. its a please meeting you and {participant3}. Love to talk about {debate_topic}\"\"\"),\n",
    "    (participant3, f\"\"\"Hello, I am {participant3} it is a pleasure meeting you both. excited to talk about {debate_topic}\"\"\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**{participant1}:** {debateMessages[0][1]}\\n\"))\n",
    "display(Markdown(f\"**{participant2}:** {debateMessages[1][1]}\\n\"))\n",
    "display(Markdown(f\"**{participant3}:** {debateMessages[2][1]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    for name, model in [(participant1, participant1_model), (participant2, participant2_model),(participant3, participant3_model)]:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"--- {timestamp} Round {i+1} - Calling model {model} ({name}) ---\")\n",
    "        next = debate(model, system_prompt[name], debateMessages)  \n",
    "        display(Markdown(f\"**{name}:** {next}\\n\"))\n",
    "        debateMessages.append((model, next))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
