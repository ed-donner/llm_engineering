{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup imports\n",
    "# imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9a6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3f8ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key and api_key.startswith(('sk-proj-', 'sk-or-v1-')) and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3895fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craft system prompt for AI technical asistance that explain technical questions concisely\n",
    "system_prompt = \"\"\"\n",
    " You are a customer technical support assistant at fintech compnay called XYZ. \n",
    " Your primary objective is to provide precise, and easy to understand explanation to a given technical questions about their tranasactions.\n",
    "\n",
    "Behavioural constraints\n",
    "- Prefer correctness over verbosity.\n",
    "- Ask clarification questions when requirements are underspecified.\n",
    "- Do not invent missing facts.\n",
    "- Explicitly state uncertainty.\n",
    "\n",
    "Reasoning Approach:\n",
    "- Analyze requirements before answering.\n",
    "- Break complex problems into steps.\n",
    "- State assumptions when needed.\n",
    "\n",
    "Output Requirements:\n",
    "- Use structured formatting.\n",
    "- Provide concrete recommendations.\n",
    "- Avoid speculation.\n",
    "- Be concise but complete.\n",
    "- Be Polite\n",
    "\n",
    "Interaction Guidelines:\n",
    "- Ask clarification questions when ambiguity exists.\n",
    "- Do not fabricate information.\n",
    "- Maintain consistency across conversation turns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64151a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets setup tool functions and mock data\n",
    "\n",
    "transaction_db = {\n",
    "    \"TXN-USD-0001\": {\"amount\": 7000, \"currency\": \"USD\", \"status\": \"processing\"},\n",
    "    \"TXN-USD-0002\": {\"amount\": 1250, \"currency\": \"USD\", \"status\": \"successful\"},\n",
    "    \"TXN-USD-0003\": {\"amount\": 980, \"currency\": \"USD\", \"status\": \"failed\"},\n",
    "    \"TXN-USD-0004\": {\"amount\": 4300, \"currency\": \"USD\", \"status\": \"pending\"},\n",
    "    \"TXN-USD-0005\": {\"amount\": 250, \"currency\": \"USD\", \"status\": \"successful\"},\n",
    "    \"TXN-USD-0006\": {\"amount\": 9999, \"currency\": \"USD\", \"status\": \"processing\"},\n",
    "    \"TXN-USD-0007\": {\"amount\": 3100, \"currency\": \"USD\", \"status\": \"failed\"},\n",
    "    \"TXN-USD-0008\": {\"amount\": 15000, \"currency\": \"USD\", \"status\": \"pending\"},\n",
    "    \"TXN-USD-0009\": {\"amount\": 875, \"currency\": \"USD\", \"status\": \"successful\"},\n",
    "    \"TXN-USD-0010\": {\"amount\": 6400, \"currency\": \"USD\", \"status\": \"processing\"},\n",
    "\n",
    "    \"TXN-NGN-0001\": {\"amount\": 500000, \"currency\": \"NGN\", \"status\": \"successful\"},\n",
    "    \"TXN-NGN-0002\": {\"amount\": 120000, \"currency\": \"NGN\", \"status\": \"processing\"},\n",
    "    \"TXN-NGN-0003\": {\"amount\": 75000, \"currency\": \"NGN\", \"status\": \"failed\"},\n",
    "    \"TXN-NGN-0004\": {\"amount\": 900000, \"currency\": \"NGN\", \"status\": \"pending\"},\n",
    "    \"TXN-NGN-0005\": {\"amount\": 30000, \"currency\": \"NGN\", \"status\": \"successful\"},\n",
    "    \"TXN-NGN-0006\": {\"amount\": 450000, \"currency\": \"NGN\", \"status\": \"processing\"},\n",
    "    \"TXN-NGN-0007\": {\"amount\": 210000, \"currency\": \"NGN\", \"status\": \"failed\"},\n",
    "    \"TXN-NGN-0008\": {\"amount\": 66000, \"currency\": \"NGN\", \"status\": \"pending\"},\n",
    "    \"TXN-NGN-0009\": {\"amount\": 880000, \"currency\": \"NGN\", \"status\": \"successful\"},\n",
    "    \"TXN-NGN-0010\": {\"amount\": 150000, \"currency\": \"NGN\", \"status\": \"processing\"},\n",
    "\n",
    "    \"TXN-GBP-0001\": {\"amount\": 450, \"currency\": \"GBP\", \"status\": \"successful\"},\n",
    "    \"TXN-GBP-0002\": {\"amount\": 1200, \"currency\": \"GBP\", \"status\": \"processing\"},\n",
    "    \"TXN-GBP-0003\": {\"amount\": 800, \"currency\": \"GBP\", \"status\": \"failed\"},\n",
    "    \"TXN-GBP-0004\": {\"amount\": 2500, \"currency\": \"GBP\", \"status\": \"pending\"},\n",
    "    \"TXN-GBP-0005\": {\"amount\": 150, \"currency\": \"GBP\", \"status\": \"successful\"},\n",
    "    \"TXN-GBP-0006\": {\"amount\": 990, \"currency\": \"GBP\", \"status\": \"processing\"},\n",
    "    \"TXN-GBP-0007\": {\"amount\": 1750, \"currency\": \"GBP\", \"status\": \"failed\"},\n",
    "    \"TXN-GBP-0008\": {\"amount\": 300, \"currency\": \"GBP\", \"status\": \"pending\"},\n",
    "    \"TXN-GBP-0009\": {\"amount\": 640, \"currency\": \"GBP\", \"status\": \"successful\"},\n",
    "    \"TXN-GBP-0010\": {\"amount\": 1100, \"currency\": \"GBP\", \"status\": \"processing\"}\n",
    "}\n",
    "\n",
    "def get_transaction(reference):\n",
    "    print(f\"Getting transaction for {reference}\")\n",
    "    transaction = transaction_db.get(reference.upper(), \"Transaction Not found\")\n",
    "    return str(transaction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0407dedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'get_transaction',\n",
       "   'description': 'Get a single transaction details',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'reference': {'type': 'string',\n",
       "      'description': 'transaction unique reference id'}},\n",
       "    'required': ['reference'],\n",
       "    'additionalProperties': False}}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup AI tools using the functions\n",
    "\n",
    "get_transaction_function = {\n",
    "    \"name\": \"get_transaction\",\n",
    "'description': \"Get a single transaction details\",\n",
    "\"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"reference\": {\n",
    "            \"type\": 'string',\n",
    "            \"description\": \"transaction unique reference id\"\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"reference\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "}\n",
    "\n",
    "fintech_tools = [{\"type\": \"function\",\"function\": get_transaction_function}]\n",
    "\n",
    "fintech_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e41e9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool call handler\n",
    "\n",
    "#handle multiple tool calls\n",
    "\n",
    "def handle_fintech_tool_calls(message):\n",
    "    responses = []\n",
    "\n",
    "    for tool_call in message['tool_calls']:\n",
    "        tool_function = globals().get(tool_call.function.name, lambda: None)\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        city = arguments.get('reference')\n",
    "        trx = tool_function(city)\n",
    "        responses.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": trx,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eb83357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer technical suppor chat function for gradio\n",
    "def customer_support_chat(message, history, model):\n",
    "\n",
    "    if not message:\n",
    "        return history\n",
    "\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    provider = None\n",
    "\n",
    "    if(model == MODEL_GPT):\n",
    "        openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "        provider = OpenAI(base_url=openrouter_url)\n",
    "    else:\n",
    "        ollama_url=\"http://localhost:11434/v1\"\n",
    "        provider = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "\n",
    "    response = provider.chat.completions.create(model= model, messages=messages, tools=fintech_tools)\n",
    "\n",
    "    responseChoice = response.choices\n",
    "    print(responseChoice)\n",
    "    while response.choices[0].finish_reason == 'tool_calls':\n",
    "        message = response.choices[0].message\n",
    "        toolsResponses = handle_fintech_tool_calls(message)\n",
    "        messages.append(message)\n",
    "        messages.extend(toolsResponses)\n",
    "        response = provider.chat.completions.create(model= model, messages=messages, tools=fintech_tools)\n",
    "\n",
    "        print(response, \"response now in a tool call\")\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fff8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable streaming\n",
    "# Customer technical suppor chat function for gradio\n",
    "def stream_customer_support_chat(message, history, model):\n",
    "\n",
    "    if not message:\n",
    "        yield \"\"\n",
    "        return \"\"\n",
    "\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    provider = None\n",
    "\n",
    "    if(model == MODEL_GPT):\n",
    "        openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "        provider = OpenAI(base_url=openrouter_url)\n",
    "    else:\n",
    "        ollama_url=\"http://localhost:11434/v1\"\n",
    "        provider = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "    stream = provider.chat.completions.create(model= model, messages=messages, tools=fintech_tools, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    tool_call_data = []\n",
    "    isToolCall = False\n",
    "    \n",
    "    for chunk in stream:\n",
    "        chunkChoice = chunk.choices[0]\n",
    "        chunkChoiceDelta = chunkChoice.delta\n",
    "\n",
    "        for tool_call in chunkChoiceDelta.tool_calls or []:\n",
    "            if tool_call:\n",
    "                idx = tool_call.index\n",
    "                if  not tool_call_data[idx: idx+1]:\n",
    "                    tool_call_data.append(tool_call)\n",
    "                \n",
    "                if  tool_call.id:\n",
    "                    tool_call_data[tool_call.index].id = tool_call.id or ''\n",
    "\n",
    "                if  tool_call.function.name:\n",
    "                    tool_call_data[tool_call.index].function.name = tool_call.function.name\n",
    "                \n",
    "                if  tool_call.function.arguments:\n",
    "                    tool_call_data[tool_call.index].function.arguments += tool_call.function.arguments\n",
    "\n",
    "        response += chunkChoiceDelta.content or ''\n",
    "        yield response\n",
    "\n",
    "        if chunkChoice.finish_reason == 'tool_calls':\n",
    "            isToolCall = True\n",
    "    \n",
    "    if  isToolCall:\n",
    "        message = {'role': 'assistant', 'content': response, 'tool_calls': tool_call_data }\n",
    "        toolsResponses = handle_fintech_tool_calls(message)\n",
    "        messages.append(message)\n",
    "        messages.extend(toolsResponses)\n",
    "        tool_call_stream = provider.chat.completions.create(model= model, messages=messages, tools=fintech_tools, stream=True)\n",
    "        for tool_chunk in tool_call_stream:\n",
    "            response += tool_chunk.choices[0].delta.content or ''\n",
    "            yield response\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "075aa31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844f828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c070a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True message\n"
     ]
    }
   ],
   "source": [
    "# additonal input to chatInterface to select model\n",
    "# 2. Setup the Additional Inputs\n",
    "model_selector = gr.Dropdown(\n",
    "    choices=[MODEL_GPT, MODEL_LLAMA], \n",
    "    value= MODEL_GPT,\n",
    "    label=\"Select Model\",\n",
    "    info=\"Pick from the available models\"\n",
    ")\n",
    "\n",
    "def validateInput(input):\n",
    "    return [\n",
    "        gr.validate(not input, \"Please provide input that I can help you with\"),\n",
    "    ]\n",
    "\n",
    "gr.ChatInterface(fn=stream_customer_support_chat, type=\"messages\", additional_inputs=[model_selector], validator=validateInput).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1c9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
