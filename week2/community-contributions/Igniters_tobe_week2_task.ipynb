{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3c6e59",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it`...`\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfd5f8",
   "metadata": {},
   "source": [
    "## Technical Question / Answerer â€” Chat App Prototype\n",
    "\n",
    "This notebook implements a **streaming technical Q&A assistant** with a Gradio web UI.\n",
    "\n",
    "### Features\n",
    "- **Streaming responses** â€” tokens appear in real-time as the model generates them\n",
    "- **Model switching** â€” dropdown to switch between OpenRouter, OpenAI, or an offline stub model\n",
    "- **Voice input (STT)** â€” record from your microphone; audio is transcribed to text automatically\n",
    "- **Voice output (TTS)** â€” the assistant's reply is converted to speech and played back\n",
    "- **Fallback chain** â€” audio features gracefully degrade depending on what's installed\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "**Required:**\n",
    "```\n",
    "pip install gradio openai python-dotenv\n",
    "```\n",
    "\n",
    "**For audio (pick what you need):**\n",
    "\n",
    "| Package | Purpose | Install |\n",
    "|---------|---------|---------|\n",
    "| `SpeechRecognition` | Voice input (free, uses Google Web API) | `pip install SpeechRecognition` |\n",
    "| `gTTS` | Voice output (free, uses Google Translate TTS) | `pip install gTTS` |\n",
    "| `openai-whisper` | Voice input (local, more accurate, needs ~1GB) | `pip install openai-whisper` |\n",
    "| `pyttsx3` | Voice output (fully offline) | `pip install pyttsx3` |\n",
    "\n",
    "> **macOS users:** The built-in `say` command is used as a TTS fallback automatically â€” no install needed.\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "Create a `.env` file in the project root with at least one API key:\n",
    "\n",
    "```\n",
    "OPENROUTER_API_KEY=sk-or-v1-...\n",
    "# and/or\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "If no keys are set, the app still runs using the **stub (offline)** model.\n",
    "\n",
    "### How to run\n",
    "\n",
    "Run all cells in order. The last cell launches the Gradio UI at `http://127.0.0.1:7860`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af00bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import atexit\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "import textwrap\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Generator, Optional\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Optional dependencies â€” set to None if not installed\n",
    "try:\n",
    "    import whisper\n",
    "except ImportError:\n",
    "    whisper = None\n",
    "\n",
    "try:\n",
    "    import speech_recognition as sr\n",
    "except ImportError:\n",
    "    sr = None\n",
    "\n",
    "try:\n",
    "    from gtts import gTTS\n",
    "except ImportError:\n",
    "    gTTS = None\n",
    "\n",
    "try:\n",
    "    import pyttsx3\n",
    "except ImportError:\n",
    "    pyttsx3 = None\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93834eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.19' requires the ipykernel package.\n",
      "\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Constants & system prompt\n",
    "\n",
    "MAX_TTS_CHARS = 5000\n",
    "MAX_TTS_CHARS_SAY = 3000\n",
    "MAX_TTS_CHARS_OPENAI = 4096\n",
    "STUB_ECHO_LIMIT = 200\n",
    "STUB_DELAY_SEC = 0.02\n",
    "SAY_TIMEOUT_SEC = 30\n",
    "CHATBOT_HEIGHT = 520\n",
    "\n",
    "SYSTEM_PROMPT = textwrap.dedent(\"\"\"\\\n",
    "    You are a rigorous technical assistant.\n",
    "\n",
    "    Rules:\n",
    "    1. Answer technical questions with step-by-step reasoning.\n",
    "    2. State your assumptions explicitly.\n",
    "    3. When asked about code, provide clear, runnable examples.\n",
    "    4. Keep answers concise but correct.\n",
    "    5. If you are unsure about something, say so rather than guessing.\n",
    "    6. Format code blocks with the appropriate language tag.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52860cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 23:13:01,909 [INFO] STT backend: speech_recognition | TTS backend: gtts\n"
     ]
    }
   ],
   "source": [
    "# Audio â€” Speech-to-Text (STT) and Text-to-Speech (TTS)\n",
    "\n",
    "_whisper_model = None\n",
    "_stt_backend: Optional[str] = None\n",
    "_tts_backend: Optional[str] = None\n",
    "_temp_audio_files: list[str] = []\n",
    "\n",
    "\n",
    "def _track_temp(path: str) -> str:\n",
    "    \"\"\"Register a temp file for cleanup on exit.\"\"\"\n",
    "    _temp_audio_files.append(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def _cleanup_temp_audio():\n",
    "    for path in _temp_audio_files:\n",
    "        try:\n",
    "            os.unlink(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "atexit.register(_cleanup_temp_audio)\n",
    "\n",
    "\n",
    "def _strip_markdown(text: str) -> str:\n",
    "    \"\"\"Remove markdown formatting so TTS reads clean prose.\"\"\"\n",
    "    text = re.sub(r\"```[\\s\\S]*?```\", \" code omitted \", text)\n",
    "    text = re.sub(r\"`[^`]+`\", lambda m: m.group(0).strip(\"`\"), text)\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\*(.+?)\\*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"#{1,6}\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \". \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _init_stt() -> str:\n",
    "    \"\"\"Detect best available STT backend.\"\"\"\n",
    "    global _whisper_model\n",
    "    if whisper is not None:\n",
    "        try:\n",
    "            _whisper_model = whisper.load_model(\"base\")\n",
    "            return \"whisper\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if sr is not None:\n",
    "        return \"speech_recognition\"\n",
    "    return \"none\"\n",
    "\n",
    "\n",
    "def _init_tts() -> str:\n",
    "    \"\"\"Detect best available TTS backend.\"\"\"\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"openai\"\n",
    "    if gTTS is not None:\n",
    "        return \"gtts\"\n",
    "    try:\n",
    "        result = subprocess.run([\"which\", \"say\"], capture_output=True)\n",
    "        if result.returncode == 0:\n",
    "            return \"macos_say\"\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    if pyttsx3 is not None:\n",
    "        return \"pyttsx3\"\n",
    "    return \"none\"\n",
    "\n",
    "\n",
    "_stt_backend = _init_stt()\n",
    "_tts_backend = _init_tts()\n",
    "log.info(\"STT backend: %s | TTS backend: %s\", _stt_backend, _tts_backend)\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path: Optional[str]) -> str:\n",
    "    \"\"\"Transcribe audio file to text using the best available backend.\"\"\"\n",
    "    if not audio_path:\n",
    "        return \"\"\n",
    "\n",
    "    if _stt_backend == \"whisper\" and _whisper_model is not None:\n",
    "        result = _whisper_model.transcribe(audio_path)\n",
    "        return result.get(\"text\", \"\").strip()\n",
    "\n",
    "    if _stt_backend == \"speech_recognition\" and sr is not None:\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            return recognizer.recognize_google(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            return \"[Could not understand audio]\"\n",
    "        except sr.RequestError as e:\n",
    "            return f\"[Speech recognition error: {e}]\"\n",
    "\n",
    "    return \"[Audio transcription not available â€” install `openai-whisper` or `SpeechRecognition`]\"\n",
    "\n",
    "\n",
    "def text_to_speech(text: str) -> Optional[str]:\n",
    "    \"\"\"Convert text to an audio file. Tries each backend with fallback.\"\"\"\n",
    "    clean = _strip_markdown(text)[:MAX_TTS_CHARS]\n",
    "    if not clean.strip():\n",
    "        return None\n",
    "\n",
    "    if _tts_backend == \"openai\":\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False)\n",
    "            response = client.audio.speech.create(\n",
    "                model=\"tts-1\", voice=\"nova\", input=clean[:MAX_TTS_CHARS_OPENAI],\n",
    "            )\n",
    "            response.stream_to_file(tmp.name)\n",
    "            return _track_temp(tmp.name)\n",
    "        except Exception:\n",
    "            log.warning(\"OpenAI TTS failed, trying next backend\", exc_info=True)\n",
    "\n",
    "    if _tts_backend == \"gtts\" and gTTS is not None:\n",
    "        try:\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False)\n",
    "            gTTS(text=clean, lang=\"en\").save(tmp.name)\n",
    "            return _track_temp(tmp.name)\n",
    "        except Exception:\n",
    "            log.warning(\"gTTS failed, falling back to macOS say\", exc_info=True)\n",
    "\n",
    "    try:\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".aiff\", delete=False)\n",
    "        tmp.close()\n",
    "        subprocess.run(\n",
    "            [\"say\", \"-o\", tmp.name, clean[:MAX_TTS_CHARS_SAY]],\n",
    "            timeout=SAY_TIMEOUT_SEC,\n",
    "            check=True,\n",
    "        )\n",
    "        return _track_temp(tmp.name)\n",
    "    except (FileNotFoundError, subprocess.SubprocessError):\n",
    "        log.warning(\"macOS say failed\", exc_info=True)\n",
    "\n",
    "    if _tts_backend == \"pyttsx3\" and pyttsx3 is not None:\n",
    "        try:\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "            engine = pyttsx3.init()\n",
    "            engine.save_to_file(clean, tmp.name)\n",
    "            engine.runAndWait()\n",
    "            return _track_temp(tmp.name)\n",
    "        except Exception:\n",
    "            log.warning(\"pyttsx3 TTS failed\", exc_info=True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 23:13:04,119 [INFO] Available models: ['stub (offline)', 'GPT-4o-mini (OpenRouter)', 'Gemini 2.5 Flash (OpenRouter)']\n"
     ]
    }
   ],
   "source": [
    "# Model abstraction â€” pluggable backends\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"Contract that every chat model must satisfy.\"\"\"\n",
    "    name: str\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, messages: list[dict[str, str]]) -> Generator[str, None, None]:\n",
    "        \"\"\"Yield streamed tokens for the given conversation.\"\"\"\n",
    "\n",
    "\n",
    "class StubModel(BaseModel):\n",
    "    \"\"\"Offline echo model â€” always available, no API key needed.\"\"\"\n",
    "    name = \"stub (offline)\"\n",
    "\n",
    "    def generate(self, messages: list[dict[str, str]]) -> Generator[str, None, None]:\n",
    "        last = messages[-1][\"content\"] if messages else \"\"\n",
    "        reply = (\n",
    "            f\"**[Stub model â€” no API key configured]**\\n\\n\"\n",
    "            f\"You said: {last[:STUB_ECHO_LIMIT]}\\n\\n\"\n",
    "            f\"To enable a real model, set `OPENROUTER_API_KEY` or `OPENAI_API_KEY` in `.env`.\"\n",
    "        )\n",
    "        for word in reply.split(\" \"):\n",
    "            yield word + \" \"\n",
    "            time.sleep(STUB_DELAY_SEC)\n",
    "\n",
    "\n",
    "class OpenAICompatibleModel(BaseModel):\n",
    "    \"\"\"Single class for any OpenAI-compatible endpoint (OpenAI, OpenRouter, Ollama, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str, display_name: str, api_key: Optional[str] = None, base_url: Optional[str] = None):\n",
    "        self.name = display_name\n",
    "        self._model = model_id\n",
    "        self._client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def generate(self, messages: list[dict[str, str]]) -> Generator[str, None, None]:\n",
    "        stream = self._client.chat.completions.create(\n",
    "            model=self._model, messages=messages, stream=True,\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            delta = chunk.choices[0].delta.content if chunk.choices else None\n",
    "            if delta:\n",
    "                yield delta\n",
    "\n",
    "\n",
    "def _build_models() -> dict[str, BaseModel]:\n",
    "    models: dict[str, BaseModel] = {}\n",
    "    models[\"stub (offline)\"] = StubModel()\n",
    "\n",
    "    if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "        or_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        or_url = \"https://openrouter.ai/api/v1\"\n",
    "        models[\"GPT-4o-mini (OpenRouter)\"] = OpenAICompatibleModel(\n",
    "            \"openai/gpt-4o-mini\", \"GPT-4o-mini (OpenRouter)\", api_key=or_key, base_url=or_url,\n",
    "        )\n",
    "        models[\"Gemini 2.5 Flash (OpenRouter)\"] = OpenAICompatibleModel(\n",
    "            \"google/gemini-2.5-flash\", \"Gemini 2.5 Flash (OpenRouter)\", api_key=or_key, base_url=or_url,\n",
    "        )\n",
    "\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        models[\"GPT-4o-mini (OpenAI)\"] = OpenAICompatibleModel(\"gpt-4o-mini\", \"GPT-4o-mini (OpenAI)\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "MODELS = _build_models()\n",
    "log.info(\"Available models: %s\", list(MODELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64631c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat logic â€” streaming with audio response\n",
    "\n",
    "def _build_messages(history: list[dict[str, str]], user_message: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Assemble the full message list from conversation history.\"\"\"\n",
    "    return (\n",
    "        [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        + [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "        + [{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "\n",
    "\n",
    "def chat_respond(\n",
    "    user_message: str, history: list[dict[str, str]], model_name: str,\n",
    ") -> Generator[tuple[list, Optional[str]], None, None]:\n",
    "    \"\"\"Stream assistant response and generate optional TTS audio.\"\"\"\n",
    "    if not user_message.strip():\n",
    "        yield history, None\n",
    "        return\n",
    "\n",
    "    model = MODELS.get(model_name) or MODELS[\"stub (offline)\"]\n",
    "    messages = _build_messages(history, user_message)\n",
    "\n",
    "    history = history + [\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    assistant_text = \"\"\n",
    "    try:\n",
    "        for token in model.generate(messages):\n",
    "            assistant_text += token\n",
    "            history[-1][\"content\"] = assistant_text\n",
    "            yield history, None\n",
    "    except Exception as e:\n",
    "        log.error(\"Model generation failed: %s\", e, exc_info=True)\n",
    "        error_msg = f\"\\n\\nâš ï¸ *Error: {type(e).__name__} â€” {e}*\"\n",
    "        history[-1][\"content\"] += error_msg\n",
    "        yield history, None\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio_path = text_to_speech(assistant_text)\n",
    "    except Exception:\n",
    "        log.warning(\"TTS failed after successful generation\", exc_info=True)\n",
    "        audio_path = None\n",
    "\n",
    "    yield history, audio_path\n",
    "\n",
    "\n",
    "def on_audio(audio_path: Optional[str], text_input: str) -> str:\n",
    "    \"\"\"Transcribe audio and append to the text input.\"\"\"\n",
    "    transcript = transcribe_audio(audio_path)\n",
    "    if transcript and not transcript.startswith(\"[\"):\n",
    "        return f\"{text_input} {transcript}\".strip() if text_input else transcript\n",
    "    return text_input or transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a601b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI â€” launch the app\n",
    "\n",
    "def build_ui() -> gr.Blocks:\n",
    "    \"\"\"Construct the Gradio interface. Separated from launch for testability.\"\"\"\n",
    "    model_names = list(MODELS.keys())\n",
    "    default_model = model_names[1] if len(model_names) > 1 else model_names[0]\n",
    "\n",
    "    stt_label = f\"ðŸŽ™ Voice input ({_stt_backend})\" if _stt_backend != \"none\" else \"ðŸŽ™ Voice input (not configured)\"\n",
    "    tts_note = f\"TTS: {_tts_backend}\" if _tts_backend != \"none\" else \"TTS: not configured\"\n",
    "\n",
    "    with gr.Blocks(title=\"Technical Q/A Assistant\", theme=gr.themes.Soft()) as ui:\n",
    "        gr.Markdown(\n",
    "            \"# Technical Question / Answerer\\n\"\n",
    "            \"Ask technical questions with text or voice. The assistant streams its response \"\n",
    "            \"and can reply with audio.\\n\\n\"\n",
    "            f\"*Audio status â€” STT: `{_stt_backend}` | TTS: `{_tts_backend}`*\"\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            model_dd = gr.Dropdown(choices=model_names, value=default_model, label=\"Model\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(height=CHATBOT_HEIGHT, type=\"messages\", label=\"Chat\")\n",
    "                with gr.Row():\n",
    "                    txt = gr.Textbox(placeholder=\"Ask a technical questionâ€¦\", show_label=False, scale=5, lines=1)\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                    clear_btn = gr.Button(\"Clear\", variant=\"secondary\", scale=1)\n",
    "                audio_in = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=stt_label)\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(f\"### Audio Response\\n*{tts_note}*\")\n",
    "                audio_out = gr.Audio(label=\"Assistant voice\", type=\"filepath\", interactive=False, autoplay=True)\n",
    "\n",
    "        def submit(msg: str, hist: list, model: str):\n",
    "            for updated_hist, audio in chat_respond(msg, hist, model):\n",
    "                yield updated_hist, audio, \"\"\n",
    "\n",
    "        send_btn.click(fn=submit, inputs=[txt, chatbot, model_dd], outputs=[chatbot, audio_out, txt])\n",
    "        txt.submit(fn=submit, inputs=[txt, chatbot, model_dd], outputs=[chatbot, audio_out, txt])\n",
    "        clear_btn.click(fn=lambda: ([], None, \"\"), outputs=[chatbot, audio_out, txt])\n",
    "        audio_in.stop_recording(fn=on_audio, inputs=[audio_in, txt], outputs=[txt])\n",
    "        \n",
    "    return ui\n",
    "\n",
    "\n",
    "app = build_ui()\n",
    "app.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
