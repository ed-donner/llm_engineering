{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frank Asket's Week 2 Exercise\n",
    "\n",
    "[Frank Asket](https://github.com/frank-asket) — *Founder & CTO building Human-Centered AI infrastructure.*\n",
    "\n",
    "Full prototype of the **technical Q&A** from Week 1: **Gradio UI**, **streaming**, **system prompt** for expertise, **model switching** (OpenRouter GPT vs Ollama Llama), and a **tool** (current time) so the assistant can answer “What time is it?”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenRouter.\n"
     ]
    }
   ],
   "source": [
    "# environment & API client (OpenRouter preferred, fallback OpenAI)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openrouter_api_key and openrouter_api_key.startswith(\"sk-or-\"):\n",
    "    openai = OpenAI(api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "    MODEL_GPT = \"openai/gpt-4o-mini\"\n",
    "    print(\"Using OpenRouter.\")\n",
    "elif openai_api_key:\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "    MODEL_GPT = \"gpt-4o-mini\"\n",
    "    print(\"Using OpenAI.\")\n",
    "else:\n",
    "    openai = OpenAI()\n",
    "    MODEL_GPT = \"gpt-4o-mini\"\n",
    "    print(\"Using default client (set OPENROUTER_API_KEY or OPENAI_API_KEY in .env).\")\n",
    "\n",
    "MODEL_OLLAMA = \"llama3.2:3b-instruct-q4_0\"\n",
    "OLLAMA_BASE = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompts (expertise personas)\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"Technical tutor\": (\n",
    "        \"You are a helpful technical tutor. Answer questions about Python, software engineering, \"\n",
    "        \"data science and LLMs. Use markdown and be clear. If the user asks for the current time, \"\n",
    "        \"use the get_current_time tool.\"\n",
    "    ),\n",
    "    \"Code reviewer\": (\n",
    "        \"You are a senior code reviewer. Explain code snippets, suggest improvements, and point out \"\n",
    "        \"pitfalls. Use markdown. If the user asks what time it is, use the get_current_time tool.\"\n",
    "    ),\n",
    "    \"LLM explainer\": (\n",
    "        \"You explain how LLMs, APIs and prompt engineering work. Be precise and educational. \"\n",
    "        \"Use markdown. If the user asks for the current time, use the get_current_time tool.\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool: get current time (demonstrates tool use)\n",
    "\n",
    "def get_current_time(timezone_name: str = \"UTC\") -> str:\n",
    "    \"\"\"Return the current date and time in the given timezone (e.g. UTC, Europe/Paris).\"\"\"\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo\n",
    "        tz = ZoneInfo(timezone_name)\n",
    "    except Exception:\n",
    "        tz = timezone.utc\n",
    "    now = datetime.now(tz)\n",
    "    return f\"Current time in {timezone_name}: {now.strftime('%Y-%m-%d %H:%M:%S %Z')}\"\n",
    "\n",
    "time_tool = {\n",
    "    \"name\": \"get_current_time\",\n",
    "    \"description\": \"Get the current date and time in a given timezone (e.g. UTC, Europe/Paris, America/New_York).\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timezone_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"IANA timezone name, e.g. UTC or Europe/Paris\",\n",
    "                \"default\": \"UTC\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": [],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "TOOLS = [{\"type\": \"function\", \"function\": time_tool}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_code_fence(text: str) -> str:\n",
    "    \"\"\"Remove code-fence wrappers so markdown displays cleanly.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    s = text\n",
    "    if s.startswith(\"```markdown\"):\n",
    "        i = s.find(\"\\n\")\n",
    "        s = s[i + 1:] if i != -1 else s[11:]\n",
    "    elif s.startswith(\"```\"):\n",
    "        i = s.find(\"\\n\")\n",
    "        s = s[i + 1:] if i != -1 else s[3:]\n",
    "    if s.rstrip().endswith(\"```\"):\n",
    "        s = s[:s.rstrip().rfind(\"```\")].rstrip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def handle_tool_calls(message):\n",
    "    \"\"\"Execute tool calls and return a list of tool results for the API.\"\"\"\n",
    "    results = []\n",
    "    for tc in message.tool_calls:\n",
    "        name = tc.function.name\n",
    "        args = json.loads(tc.function.arguments) if tc.function.arguments else {}\n",
    "        if name == \"get_current_time\":\n",
    "            out = get_current_time(**args)\n",
    "        else:\n",
    "            out = f\"Unknown tool: {name}\"\n",
    "        results.append({\"role\": \"tool\", \"content\": out, \"tool_call_id\": tc.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_stream(message, history, model_choice, persona_key):\n",
    "    \"\"\"Streaming chat: supports GPT (OpenRouter/OpenAI) with optional tool, or Ollama. Yields cumulative response for Gradio.\"\"\"\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    system = SYSTEM_PROMPTS.get(persona_key, list(SYSTEM_PROMPTS.values())[0])\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    use_ollama = \"Ollama\" in model_choice\n",
    "    client = openai\n",
    "    model = MODEL_GPT\n",
    "    use_tools = False\n",
    "    if use_ollama:\n",
    "        try:\n",
    "            client = OpenAI(base_url=OLLAMA_BASE, api_key=\"ollama\")\n",
    "            model = MODEL_OLLAMA\n",
    "        except Exception:\n",
    "            yield \"Ollama not available. Start with: `ollama serve` and `ollama pull llama3.2`\"\n",
    "            return\n",
    "    else:\n",
    "        use_tools = True\n",
    "\n",
    "    # Tool loop for GPT (one round of tool calls, then yield final answer)\n",
    "    if use_tools:\n",
    "        response = client.chat.completions.create(model=model, messages=messages, tools=TOOLS)\n",
    "        while response.choices[0].finish_reason == \"tool_calls\":\n",
    "            msg = response.choices[0].message\n",
    "            messages.append(msg)\n",
    "            messages.extend(handle_tool_calls(msg))\n",
    "            response = client.chat.completions.create(model=model, messages=messages, tools=TOOLS)\n",
    "        final_content = response.choices[0].message.content or \"\"\n",
    "        yield strip_code_fence(final_content)\n",
    "        return\n",
    "\n",
    "    # Ollama: stream chunks\n",
    "    stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "    acc = \"\"\n",
    "    for chunk in stream:\n",
    "        acc += chunk.choices[0].delta.content or \"\"\n",
    "        yield strip_code_fence(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI: model switch, persona (system prompt), streaming chat\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## Technical Q&A prototype (Week 2)\n",
    "        - **Model:** OpenRouter GPT or Ollama Llama\n",
    "        - **Persona:** system prompt sets expertise (tutor, code reviewer, LLM explainer)\n",
    "        - **Streaming** answers; **tool:** ask *What time is it?* to see the assistant use the clock.\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        model_choice = gr.Dropdown(\n",
    "            [\"OpenRouter GPT\", \"Ollama Llama\"],\n",
    "            value=\"OpenRouter GPT\",\n",
    "            label=\"Model\"\n",
    "        )\n",
    "        persona = gr.Dropdown(\n",
    "            list(SYSTEM_PROMPTS.keys()),\n",
    "            value=\"Technical tutor\",\n",
    "            label=\"Persona\"\n",
    "        )\n",
    "    chatbot = gr.Chatbot(height=400)\n",
    "    msg = gr.Textbox(placeholder=\"Ask a technical question or 'What time is it?'\", label=\"Message\")\n",
    "    send = gr.Button(\"Send\", variant=\"primary\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, history, model, persona_name):\n",
    "        if not message or not message.strip():\n",
    "            return \"\", history\n",
    "        history = history + [{\"role\": \"user\", \"content\": message}]\n",
    "        full = \"\"\n",
    "        for chunk in chat_stream(message, history[:-1], model, persona_name):\n",
    "            full = chunk\n",
    "        history = history + [{\"role\": \"assistant\", \"content\": full}]\n",
    "        return \"\", history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot, model_choice, persona], [msg, chatbot])\n",
    "    send.click(respond, [msg, chatbot, model_choice, persona], [msg, chatbot])\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(inbrowser=True, theme=gr.themes.Soft())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
