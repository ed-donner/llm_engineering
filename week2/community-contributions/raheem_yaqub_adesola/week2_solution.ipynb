{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "babd9426",
   "metadata": {},
   "source": [
    "# Week 2 Exercise Solution\n",
    "\n",
    "## Technical Question Assistant with Gradio UI, Streaming, Model Switching, and System Prompt\n",
    "\n",
    "This notebook demonstrates a tool that takes a technical question and responds with an explanation using both OpenAI (gpt-4o-mini) and Ollama (llama3:latest) models, with a Gradio UI for interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af92e6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We need the following libraries:\n",
    "- `os`, `dotenv` for environment variables\n",
    "- `openai` for OpenAI API\n",
    "- `ollama` for local LLM\n",
    "- `gradio` for UI\n",
    "\n",
    "> If not installed, run `pip install openai python-dotenv ollama gradio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a565c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8463d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3:latest'\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an expert technical explainer. Provide clear, concise, and accurate answers to technical questions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c624b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Streaming Answer Functions\n",
    "\n",
    "These functions stream answers from OpenAI and Ollama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69179064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming OpenAI response\n",
    "def stream_openai_response(question, system_prompt=SYSTEM_PROMPT):\n",
    "    if not openrouter_api_key:\n",
    "        return \"No OpenRouter API key found.\"\n",
    "    openai = OpenAI(api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32ac6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Ollama response\n",
    "def stream_ollama_response(question, system_prompt=SYSTEM_PROMPT):\n",
    "    stream = ollama.chat(model=MODEL_LLAMA, messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}], stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if 'message' in chunk and 'content' in chunk['message']:\n",
    "            response += chunk['message']['content']\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d997542",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Gradio UI\n",
    "\n",
    "This cell creates a Gradio UI for entering questions, selecting models, and streaming answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd94b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI function\n",
    "\n",
    "def answer_question(question, model):\n",
    "    if model == \"OpenAI (gpt-4o-mini)\":\n",
    "        for partial in stream_openai_response(question):\n",
    "            yield partial\n",
    "    else:\n",
    "        for partial in stream_ollama_response(question):\n",
    "            yield partial\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Week 2 Technical Question Answerer\n",
    "    Enter your technical question below, select a model, and get a streaming expert answer!\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        question = gr.Textbox(label=\"Technical Question\", placeholder=\"Ask your technical question here...\")\n",
    "        model = gr.Radio([\"OpenAI (gpt-4o-mini)\", \"Ollama (llama3:latest)\"], value=\"OpenAI (gpt-4o-mini)\", label=\"Model\")\n",
    "    answer = gr.Textbox(label=\"Streaming Answer\", lines=8)\n",
    "    submit = gr.Button(\"Get Answer\")\n",
    "\n",
    "    submit.click(answer_question, inputs=[question, model], outputs=answer, queue=True)\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    *Bonus: Try adding audio input/output or tool use!*\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
