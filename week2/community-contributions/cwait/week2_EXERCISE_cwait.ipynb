{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
        "\n",
        "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
        "\n",
        "I will publish a full solution here soon - unless someone beats me to it...\n",
        "\n",
        "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import ollama\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2fba78aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants and environment\n",
        "MODEL_GPT = \"gpt-4o-mini\"\n",
        "MODEL_LLAMA = \"llama3.2\"\n",
        "\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not set. Add it to .env or your environment.\")\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fed120c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# system prompt: technical tutor expertise\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert technical tutor. You explain programming, computer science, and technology clearly and concisely. Use examples when helpful. If the user asks for the current date or time, use the get_current_time tool. Format code and key terms clearly.\n",
        "\n",
        "For mathematical or chemical equations, use standard LaTeX in markdown so they render properly:\n",
        "- Display equations: wrap in double dollar signs on their own line, e.g. $$x^2 + y^2 = z^2$$\n",
        "- Inline math: use single dollar signs, e.g. $E = mc^2$\n",
        "- Do not wrap equations in square brackets like [ \\\\text{...} ]; use $$ ... $$ for display equations instead.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "859aee8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# tool: get_current_time (demonstrates tool use with GPT)\n",
        "get_current_time_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_current_time\",\n",
        "        \"description\": \"Get the current date and time. Use when the user asks for the time, date, or 'what time is it'.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"additionalProperties\": False,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "TOOLS = [get_current_time_tool]\n",
        "\n",
        "\n",
        "def handle_tool_calls(message):\n",
        "    \"\"\"Execute tool calls and return tool response messages.\"\"\"\n",
        "    responses = []\n",
        "    for tc in message.tool_calls:\n",
        "        if tc.function.name == \"get_current_time\":\n",
        "            print(\"Tool call received....:\", tc.function.name, flush=True)\n",
        "            now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
        "            responses.append({\"role\": \"tool\", \"content\": now, \"tool_call_id\": tc.id})\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b0d62ddb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_streaming(history, model_name):\n",
        "    \"\"\"Stream chat response. Supports GPT (with tools) and Llama. Yields (history_with_partial_assistant_message).\"\"\"\n",
        "    history = list(history) if history else []\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + [\n",
        "        {\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history\n",
        "    ]\n",
        "\n",
        "    if model_name == MODEL_GPT:\n",
        "        # GPT: use tools, then stream final reply (or stream directly if no tool call)\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_GPT, messages=messages, tools=TOOLS, stream=False\n",
        "        )\n",
        "        while response.choices[0].finish_reason == \"tool_calls\":\n",
        "            msg = response.choices[0].message\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content or \"\", \"tool_calls\": [{\"id\": tc.id, \"type\": \"function\", \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}} for tc in msg.tool_calls]})\n",
        "            for r in handle_tool_calls(msg):\n",
        "                messages.append(r)\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_GPT, messages=messages, tools=TOOLS, stream=False\n",
        "            )\n",
        "        final_content = response.choices[0].message.content or \"\"\n",
        "        # Simulate streaming by yielding progressively\n",
        "        accumulated = \"\"\n",
        "        for char in final_content:\n",
        "            accumulated += char\n",
        "            yield history + [{\"role\": \"assistant\", \"content\": accumulated}]\n",
        "        return\n",
        "\n",
        "    # Llama (Ollama): stream directly\n",
        "    stream = ollama.chat(\n",
        "        model=MODEL_LLAMA,\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "    )\n",
        "    accumulated = \"\"\n",
        "    for chunk in stream:\n",
        "        if \"message\" in chunk and chunk[\"message\"].get(\"content\"):\n",
        "            accumulated += chunk[\"message\"][\"content\"]\n",
        "            yield history + [{\"role\": \"assistant\", \"content\": accumulated}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "11fee6fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# audio: speech-to-text (Whisper) and text-to-speech (OpenAI TTS)\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Convert speech to text using OpenAI Whisper.\"\"\"\n",
        "    if audio_path is None:\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(audio_path, \"rb\") as f:\n",
        "            transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=f)\n",
        "        return transcript.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"(Transcription error: {e})\"\n",
        "\n",
        "\n",
        "def text_to_speech(text):\n",
        "    \"\"\"Convert assistant reply to speech; returns path to temp MP3 for Gradio.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    try:\n",
        "        fd, path = tempfile.mkstemp(suffix=\".mp3\")\n",
        "        os.close(fd)\n",
        "        with client.audio.speech.with_streaming_response.create(\n",
        "            model=\"tts-1\", voice=\"nova\", input=text[:4096]\n",
        "        ) as response:\n",
        "            response.stream_to_file(path)\n",
        "        return path\n",
        "    except Exception as e:\n",
        "        print(f\"TTS error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6e7560c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio UI: technical Q&A with streaming, model switch, audio in/out\n",
        "def add_user_and_chat(message, history, model_name, respond_with_audio):\n",
        "    \"\"\"Add user message to history, stream assistant reply, optionally generate TTS.\"\"\"\n",
        "    if not message or not message.strip():\n",
        "        return history, None\n",
        "    is_first_message = not (history or [])\n",
        "    history = list(history or []) + [{\"role\": \"user\", \"content\": message.strip()}]\n",
        "    # Only show placeholder on first message so the loader appears on subsequent ones\n",
        "    if is_first_message:\n",
        "        yield history + [{\"role\": \"assistant\", \"content\": \"...\"}], None\n",
        "    last_assistant_content = \"\"\n",
        "    for updated in chat_streaming(history, model_name):\n",
        "        last_assistant_content = updated[-1][\"content\"] if updated else \"\"\n",
        "        yield updated, None\n",
        "    if respond_with_audio and last_assistant_content:\n",
        "        audio_path = text_to_speech(last_assistant_content)\n",
        "        yield history + [{\"role\": \"assistant\", \"content\": last_assistant_content}], audio_path\n",
        "    else:\n",
        "        yield history + [{\"role\": \"assistant\", \"content\": last_assistant_content}], None\n",
        "\n",
        "\n",
        "def on_audio_upload(audio_path, history, model_name, respond_with_audio):\n",
        "    \"\"\"Transcribe audio to text, then run add_user_and_chat.\"\"\"\n",
        "    text = transcribe_audio(audio_path)\n",
        "    if not text or text.startswith(\"(\"):\n",
        "        return history, None\n",
        "    for hist, aud in add_user_and_chat(text, history, model_name, respond_with_audio):\n",
        "        yield hist, aud\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Technical Q&A Assistant\") as demo:\n",
        "    gr.Markdown(\"## Technical Q&A Assistant\\nAsk a technical question; switch models; use voice in/out.\")\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=[MODEL_GPT, MODEL_LLAMA],\n",
        "            value=MODEL_GPT,\n",
        "            label=\"Model\",\n",
        "        )\n",
        "        respond_with_audio = gr.Checkbox(value=False, label=\"Respond with audio (TTS)\")\n",
        "    chatbot = gr.Chatbot(height=400, type=\"messages\", label=\"Chat\")\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(placeholder=\"Ask a technical question...\", label=\"Message\", scale=4)\n",
        "        send_btn = gr.Button(\"Send\", scale=1)\n",
        "    audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\", label=\"Or speak your question\")\n",
        "    audio_output = gr.Audio(label=\"Assistant reply (audio)\", visible=False)\n",
        "    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "    def clear():\n",
        "        return [], None, None\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=add_user_and_chat,\n",
        "        inputs=[msg, chatbot, model_dropdown, respond_with_audio],\n",
        "        outputs=[chatbot, audio_output],\n",
        "    ).then(lambda: \"\", None, msg)\n",
        "\n",
        "    msg.submit(\n",
        "        fn=add_user_and_chat,\n",
        "        inputs=[msg, chatbot, model_dropdown, respond_with_audio],\n",
        "        outputs=[chatbot, audio_output],\n",
        "    ).then(lambda: \"\", None, msg)\n",
        "\n",
        "    audio_input.stop_recording(\n",
        "        fn=on_audio_upload,\n",
        "        inputs=[audio_input, chatbot, model_dropdown, respond_with_audio],\n",
        "        outputs=[chatbot, audio_output],\n",
        "    )\n",
        "    audio_input.change(\n",
        "        fn=on_audio_upload,\n",
        "        inputs=[audio_input, chatbot, model_dropdown, respond_with_audio],\n",
        "        outputs=[chatbot, audio_output],\n",
        "    )\n",
        "\n",
        "    respond_with_audio.change(\n",
        "        fn=lambda x: gr.Audio(visible=x),\n",
        "        inputs=respond_with_audio,\n",
        "        outputs=audio_output,\n",
        "    )\n",
        "    clear_btn.click(fn=clear, inputs=None, outputs=[chatbot, audio_output, audio_input])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "088d20f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo.queue()\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
