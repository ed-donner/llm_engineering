{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a469b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "cloud_model = os.getenv('CLOUD_MODEL')\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = \"\"\n",
    "\n",
    "glm = \"glm-4.6:cloud\"\n",
    "oss = \"gpt-oss:120b-cloud\"\n",
    "llama = \"llama3.2\"\n",
    "\n",
    "glm_system_prompt = ''' \n",
    "You are Alex, a customer of a business,  who is very angry and unhappy with the quality of service that you have been getting.\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "'''\n",
    "\n",
    "oss_system_prompt = ''' \n",
    "You are Blake, a customer support agent who is very confident in his knowledge concerning all domain space;\n",
    "you defend anything  you say and you believe no one knows more than you.\n",
    "You are not happy with your job, hence whenever you feel insulted or disrespected or even the slightest form of aggression from a customer, \n",
    "you always get triggered, and reciprocate the same energy\n",
    "'''\n",
    "\n",
    "llama_system_prompt = ''' \n",
    "You're Charlie, a customer support chatbot who intercedes conversations between angry customers, and very confident agents. \n",
    "You always try to make peace whenever you notice that a conversation is getting heated up, by finding common ground for both parties.\n",
    "'''\n",
    "\n",
    "def generate_glm_user_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation_history}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\" \n",
    "    return user_prompt\n",
    "\n",
    "def generate_oss_user_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Blake, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation_history}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\" \n",
    "    return user_prompt\n",
    "\n",
    "def generate_llama_user_prompt():\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Charlie, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation_history}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\" \n",
    "    return user_prompt\n",
    "\n",
    "def create_glms_complaint():\n",
    "    global conversation_history\n",
    "    message = [{\"role\": \"user\", \"content\":\"Come up with any complaint that you can think about on a product\"}]\n",
    "    response = ollama.chat.completions.create(model=glm, messages=message)\n",
    "    content = response.choices[0].message.content\n",
    "    conversation_history += f\"\\n {content}\"\n",
    "    return content\n",
    "\n",
    "def call_glm():\n",
    "    global conversation_history\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\" : glm_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{generate_glm_user_prompt()}\"}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(model=glm, messages=messages)\n",
    "    content = response.choices[0].message.content\n",
    "    conversation_history += f\"\\n {content}\"\n",
    "    return content\n",
    "\n",
    "\n",
    "def call_oss():\n",
    "    global conversation_history\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\" : oss_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{generate_oss_user_prompt()}\"}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(model=oss, messages=messages)\n",
    "    content = response.choices[0].message.content\n",
    "    conversation_history += f\"\\n {content}\"\n",
    "    return content\n",
    "\n",
    "def call_llama():\n",
    "    global conversation_history\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\" : llama_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{generate_llama_user_prompt()}\"}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(model=llama, messages=messages)\n",
    "    content = response.choices[0].message.content\n",
    "    conversation_history += f\"\\n {content}\"\n",
    "    return content\n",
    "\n",
    "def start_trouble():\n",
    "    glms_complaint = create_glms_complaint()\n",
    "\n",
    "    display(Markdown(f\"### GLM Complaint: \\n {glms_complaint} \\n\"))\n",
    "\n",
    "    for i in range(5):\n",
    "        oss_response = call_oss()\n",
    "\n",
    "        display(Markdown(f\"### OSS: \\n {oss_response} \\n\"))\n",
    "\n",
    "        llama_response = call_llama()\n",
    "\n",
    "        display(Markdown(f\"### LLAMA: \\n {llama_response} \\n\"))\n",
    "\n",
    "        glm_response = call_glm()\n",
    "\n",
    "        display(Markdown(f\"### GLM: \\n {glm_response} \\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_trouble()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
