{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
        "\n",
        "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
        "\n",
        "I will publish a full solution here soon - unless someone beats me to it...\n",
        "\n",
        "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48288316",
      "metadata": {},
      "source": [
        "   ## NOTE\n",
        "   \n",
        "   **Run the following in your system terminal (you will be prompted for your password):**\n",
        "\n",
        "   _sudo apt-get update && sudo apt-get install -y libportaudio2 portaudio19-dev_\n",
        "\n",
        "   **For chat:** Set `OPENROUTER_API_KEY` in `.env`. **For TTS:** Set `GROQ_API_KEY` in `.env` (Groq TTS for first-level audio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2937eb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install STT dependencies\n",
        "\n",
        "! uv pip install moonshine-voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "from moonshine_voice import (\n",
        "    Transcriber,\n",
        "    TranscriptEventListener,\n",
        "    get_model_for_language,\n",
        "    load_wav_file,\n",
        "    ModelArch,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd9a6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "MODEL = \"openai/gpt-4o-mini\"\n",
        "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=openrouter_key) if openrouter_key else None\n",
        "groq_client = Groq(api_key=groq_key) if groq_key else None\n",
        "\n",
        "DB = \"technical_qa.db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed174b82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt and follow-up (3 levels of explanation)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert technical educator. Keep every answer to ONE concise paragraph (no bullet points, no headings, no code blocks).\n",
        "Answer directly without preamble. Explain in plain text only; if code is essential, describe it verbally.\n",
        "When asked to go deeper, name the single hardest concept from your previous answer and explain only that concept in one new paragraph. Reference but do not repeat earlier explanations.\n",
        "Use the lookup_technical_term tool when a precise definition would help.\"\"\"\n",
        "\n",
        "FOLLOWUP = \"What is the single hardest concept in your last answer? Explain it in one concise paragraph. You may reference what you said before but do not repeat it.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5bf043",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool: technical term lookup (demonstrates tool use)\n",
        "\n",
        "TECH_TERMS = {\n",
        "    \"decorator\": \"A decorator is a function that modifies another function. In Python, @decorator wraps a function to add behavior.\",\n",
        "    \"closure\": \"A closure is a function that captures variables from its enclosing scope.\",\n",
        "    \"generator\": \"A generator yields values one at a time using yield, supporting lazy evaluation.\",\n",
        "    \"recursion\": \"Recursion is when a function calls itself; it requires a base case to terminate.\",\n",
        "    \"api\": \"An API defines how software components communicate. REST APIs use HTTP methods on URLs.\",\n",
        "}\n",
        "\n",
        "def lookup_technical_term(term: str) -> str:\n",
        "    key = term.strip().lower()\n",
        "    return TECH_TERMS.get(key, f\"No definition for '{term}'.\")\n",
        "\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"lookup_technical_term\",\n",
        "        \"description\": \"Look up a technical term (decorator, closure, generator, recursion, api).\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"term\": {\"type\": \"string\", \"description\": \"Term to look up\"}},\n",
        "            \"required\": [\"term\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e12822",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQLite conversation storage and TTS\n",
        "\n",
        "def setup_database():\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        conn.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS conversations (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                model TEXT,\n",
        "                user_msg TEXT,\n",
        "                assistant_msg TEXT,\n",
        "                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "            )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "setup_database()\n",
        "\n",
        "def save_conversation(model, user_msg, assistant_msg):\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        conn.execute(\n",
        "            \"INSERT INTO conversations (model, user_msg, assistant_msg) VALUES (?, ?, ?)\",\n",
        "            (model, user_msg, assistant_msg)\n",
        "        )\n",
        "        conn.commit()\n",
        "\n",
        "def talker(text):\n",
        "    \"\"\"Convert text to speech via Groq. Returns WAV bytes or None.\"\"\"\n",
        "    if not groq_client or not text:\n",
        "        return None\n",
        "    try:\n",
        "        r = groq_client.audio.speech.create(\n",
        "            model=\"canopylabs/orpheus-v1-english\",\n",
        "            voice=\"troy\",\n",
        "            input=text[:4000],\n",
        "            response_format=\"wav\"\n",
        "        )\n",
        "        return r.read()\n",
        "    except Exception as e:\n",
        "        if \"429\" in str(e) or \"rate\" in str(e).lower():\n",
        "            print(\"TTS rate limit or error:\", str(e)[:300])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22324ea8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3-level iterative explanation with tool use (list comp where compact)\n",
        "\n",
        "def _get_reply_with_tools(messages):\n",
        "    \"\"\"One API round with tool handling. Returns (full_reply, messages, tools_used).\"\"\"\n",
        "    full_reply = \"\"\n",
        "    tools_used = []\n",
        "    for _ in range(3):\n",
        "        resp = client.chat.completions.create(model=MODEL, messages=messages, tools=tools, stream=False)\n",
        "        msg = resp.choices[0].message\n",
        "        full_reply = msg.content or \"\"\n",
        "        if not getattr(msg, \"tool_calls\", None):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": full_reply})\n",
        "            return full_reply, messages, tools_used\n",
        "        messages.append({\"role\": \"assistant\", \"content\": full_reply, \"tool_calls\": msg.tool_calls})\n",
        "        for tc in msg.tool_calls or []:\n",
        "            if getattr(tc.function, \"name\", None) == \"lookup_technical_term\":\n",
        "                term = json.loads(tc.function.arguments or \"{}\").get(\"term\", \"\")\n",
        "                result = lookup_technical_term(term)\n",
        "                tools_used.append(f\"lookup_technical_term(term={term!r})\")\n",
        "                messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": result})\n",
        "    return full_reply, messages, tools_used\n",
        "\n",
        "def chat_stream(history, message):\n",
        "    \"\"\"Three levels of explanation (What #1, #2, #3). Yields (history, audio) after each level (full output, no chunking).\"\"\"\n",
        "    if not client:\n",
        "        yield history + [{\"role\": \"assistant\", \"content\": \"OpenRouter not configured. Set OPENROUTER_API_KEY in .env\"}], None\n",
        "        return\n",
        "    api_messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": message}]\n",
        "    current_history = list(history)\n",
        "    last_audio = None\n",
        "    all_tools_used = []\n",
        "    for level in range(3):\n",
        "        user_msg = message if level == 0 else FOLLOWUP\n",
        "        if level > 0:\n",
        "            api_messages.append({\"role\": \"user\", \"content\": FOLLOWUP})\n",
        "        full_reply, api_messages, tools_used = _get_reply_with_tools(api_messages)\n",
        "        all_tools_used.extend(tools_used)\n",
        "        save_conversation(MODEL, user_msg, full_reply)\n",
        "        content = f\"**What #{level + 1}**\\n\\n{full_reply}\"\n",
        "        level_audio = talker(full_reply) if level == 0 else None\n",
        "        if level == 0 and level_audio is not None:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
        "                f.write(level_audio)\n",
        "                last_audio = f.name\n",
        "        if level == 0 and level_audio is None and groq_client:\n",
        "            content += \"\\n\\n_(Audio unavailable: TTS rate limit or error. Try again later or check Groq billing.)_\"\n",
        "        current_history = current_history + [{\"role\": \"assistant\", \"content\": content}]\n",
        "        out_audio = last_audio\n",
        "        if level < 2:\n",
        "            current_history = current_history + [{\"role\": \"user\", \"content\": \"Go deeper (hardest concept from above)\"}]\n",
        "            yield current_history, out_audio\n",
        "        else:\n",
        "            yield current_history, out_audio\n",
        "    if all_tools_used:\n",
        "        print(\"Tools used this conversation:\", all_tools_used)\n",
        "    with sqlite3.connect(DB) as conn:\n",
        "        rows = conn.execute(\"SELECT id, model, user_msg, assistant_msg, timestamp FROM conversations ORDER BY id\").fetchall()\n",
        "    print(\"Conversation history from DB:\")\n",
        "    for row in rows:\n",
        "        print(f\"  --- id={row[0]} model={row[1]} {row[4]}\")\n",
        "        print(f\"    user: {(row[2][:200] + '...') if len(row[2]) > 200 else row[2]}\")\n",
        "        print(f\"    assistant: {(row[3][:200] + '...') if len(row[3]) > 200 else row[3]}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "683ab95f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# STT: Moonshine file-based transcription (first voice use may download the model)\n",
        "\n",
        "_cached_transcriber = None\n",
        "\n",
        "def _get_transcriber(language: str = \"en\"):\n",
        "    global _cached_transcriber\n",
        "    if _cached_transcriber is None:\n",
        "        model_path, model_arch = get_model_for_language(language, ModelArch.BASE)\n",
        "        _cached_transcriber = Transcriber(model_path=model_path, model_arch=model_arch)\n",
        "    return _cached_transcriber\n",
        "\n",
        "class _CollectingListener(TranscriptEventListener):\n",
        "    def __init__(self):\n",
        "        self.lines = []\n",
        "    def on_line_completed(self, event):\n",
        "        if event.line.text.strip():\n",
        "            self.lines.append(event.line.text.strip())\n",
        "\n",
        "def _wav_chunk_generator(wav_path: str, chunk_duration: float = 0.1):\n",
        "    audio_data, sample_rate = load_wav_file(wav_path)\n",
        "    chunk_size = int(chunk_duration * sample_rate)\n",
        "    for i in range(0, len(audio_data), chunk_size):\n",
        "        yield (audio_data[i : i + chunk_size], sample_rate)\n",
        "\n",
        "def transcribe_audio_file(audio_path: str | None, language: str = \"en\") -> str:\n",
        "    \"\"\"Transcribe a WAV file to text using Moonshine. Returns '' if path is invalid or transcription fails.\"\"\"\n",
        "    transcriber = _get_transcriber(language)\n",
        "    listener = _CollectingListener()\n",
        "    stream = transcriber.create_stream(update_interval=0.5)\n",
        "    stream.add_listener(listener)\n",
        "    stream.start()\n",
        "    for chunk, sample_rate in _wav_chunk_generator(audio_path):\n",
        "        stream.add_audio(chunk, sample_rate)\n",
        "    stream.stop()\n",
        "    stream.close()\n",
        "    return \" \".join(listener.lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caa9bdc",
      "metadata": {},
      "source": [
        "## Gradio UI\n",
        "\n",
        "Technical Q&A with **3 levels of explanation** (What #1 → What #2 → What #3). Each level picks the hardest concept from the previous answer and explains it.\n",
        "- **Full response** per level (no chunk streaming)\n",
        "- **Tool use**: `lookup_technical_term` for decorator, closure, generator, recursion, api\n",
        "- **Audio output**: TTS via Groq for the first level only (if GROQ_API_KEY set)\n",
        "- **SQLite**: Conversation history saved to `technical_qa.db`\n",
        "- **Voice input**: Record with the microphone; Moonshine transcribes to text, then the same 3-level chat runs (first use may download the model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63dd485e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio UI\n",
        "\n",
        "def chat_from_history(history):\n",
        "    \"\"\"Get last user message from history and run chat_stream.\"\"\"\n",
        "    if not history or history[-1].get(\"role\") != \"user\":\n",
        "        yield history, None\n",
        "        return\n",
        "    message = history[-1].get(\"content\", \"\").strip()\n",
        "    for h, a in chat_stream(history, message):\n",
        "        yield h, a\n",
        "\n",
        "def voice_submit(audio_path, history):\n",
        "    \"\"\"Transcribe audio and append as user message, or show error if empty.\"\"\"\n",
        "    transcript = transcribe_audio_file(audio_path)\n",
        "    if not transcript:\n",
        "        return history + [{\"role\": \"assistant\", \"content\": \"Could not transcribe audio.\"}]\n",
        "    return history + [{\"role\": \"user\", \"content\": transcript}]\n",
        "\n",
        "with gr.Blocks(title=\"Technical Q&A Assistant\") as ui:\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(height=450, type=\"messages\")\n",
        "    with gr.Row():\n",
        "        audio_output = gr.Audio(autoplay=True)\n",
        "    with gr.Row():\n",
        "        audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Ask by voice (first use may download model)\")\n",
        "    with gr.Row():\n",
        "        submit_voice_btn = gr.Button(\"Submit voice\")\n",
        "\n",
        "    submit_voice_btn.click(voice_submit, inputs=[audio_input, chatbot], outputs=[chatbot]).then(\n",
        "        chat_from_history, inputs=[chatbot], outputs=[chatbot, audio_output]\n",
        "    )\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
