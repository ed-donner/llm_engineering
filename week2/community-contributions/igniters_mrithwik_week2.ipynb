{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5f3562",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import sys\n",
    "import io\n",
    "import traceback\n",
    "from openai import OpenAI\n",
    "\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#Load the API key from the environment variable\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Check if we have a key\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"API key found!\")\n",
    "else:\n",
    "    print(\"No API key found! Please set OPENAI_API_KEY\")\n",
    "    \n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31340ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tool- Execute Python code to compute results\n",
    "def execute_python(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Safely execute Python code and return the output.\n",
    "    This is our TOOL that the AI can call!\n",
    "    \"\"\"\n",
    "    # Capture stdout so we can return it\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        # Execute the code\n",
    "        exec(code, {\"__builtins__\": __builtins__})\n",
    "        output = buffer.getvalue()\n",
    "        if not output:\n",
    "            output = \"Code ran successfully with no output.\"\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Error: {traceback.format_exc()}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "# Define the tool schema\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"execute_python\",\n",
    "            \"description\": \"Execute Python code and return the output. Use this to demonstrate code examples, run calculations, or test solutions to technical questions.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Python code to execute\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"code\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Quick test of the tool\n",
    "test_result = execute_python(\"print('Hello from the tool!')\\nprint(2 + 2)\")\n",
    "print(\"Tool test:\", test_result)\n",
    "print(\"Tool is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting the context for the LLM\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly experienced software engineer. You specialize in Python, JavaScript, algorithms, system design, \n",
    "and general programming concepts.\n",
    "\n",
    "Your personality:\n",
    "- You explain things clearly, like a patient teacher\n",
    "- You use analogies and examples to make complex topics accessible  \n",
    "- You're enthusiastic about technology but not condescending\n",
    "- When relevant, you DEMONSTRATE concepts by running actual code using your execute_python tool\n",
    "- You always mention if there are gotchas, edge cases, or common mistakes to avoid\n",
    "\n",
    "When someone asks a technical question:\n",
    "1. Give a clear explanation first\n",
    "2. If it involves code or calculations, USE the execute_python tool to show a working example\n",
    "3. Explain what the code does and why\n",
    "4. Mention any important caveats\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Chat function with streaming and tools\n",
    "def chat_with_streaming(message: str, history: list, model: str):\n",
    "    \"\"\"\n",
    "    Send a message and get a streaming response.\n",
    "    Handles tool calls too!\n",
    "    \n",
    "    history format: list of [user_message, assistant_message] pairs (Gradio format)\n",
    "    \"\"\"\n",
    "    # Convert Gradio history format to OpenAI messages format\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    \n",
    "    for human_msg, ai_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human_msg})\n",
    "        if ai_msg:  # ai_msg might be None if still streaming\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    \n",
    "    # Add the new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    response_text = \"\"\n",
    "    \n",
    "    # First, try with streaming. When tools might be needed, \n",
    "    # we do a non-streaming first pass to check for tool calls.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"  # Let the AI decide when to use tools\n",
    "    )\n",
    "    \n",
    "    # Check if the LLM infers tool usage\n",
    "    if response.choices[0].finish_reason == \"tool_calls\":\n",
    "        # \n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        tool_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # Show user we're executing code\n",
    "        yield f\" *Running code...*\\n```python\\n{tool_args['code']}\\n```\\n\\n\"\n",
    "        \n",
    "        # Run the code\n",
    "        tool_result = execute_python(tool_args[\"code\"])\n",
    "        \n",
    "        # Add tool results to messages and get final response\n",
    "        messages.append(response.choices[0].message)\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": tool_result\n",
    "        })\n",
    "        \n",
    "        # Get the final streaming response with the tool results incorporated\n",
    "        prefix = f\" *Running code...*\\n```python\\n{tool_args['code']}\\n```\\n\\n**Output:**\\n```\\n{tool_result}\\n```\\n\\n\"\n",
    "        yield prefix\n",
    "        \n",
    "        # Stream the final explanation\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = prefix\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "                yield full_response\n",
    "    \n",
    "    else:\n",
    "        # No tool needed. Streaming the regular response\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                response_text += chunk.choices[0].delta.content\n",
    "                yield response_text\n",
    "\n",
    "print(\"Chat function working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio functionality\n",
    "def transcribe_audio(audio_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert speech to text using Whisper.\n",
    "    audio_file_path: path to the audio file from Gradio\n",
    "    \"\"\"\n",
    "    if audio_file_path is None:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return f\"[Audio transcription failed: {str(e)}]\"\n",
    "\n",
    "def text_to_speech(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Convert text to speech using OpenAI TTS.\n",
    "    Returns path to audio file, or None if it fails.\n",
    "    \"\"\"\n",
    "    # Cleaning up the text - removing markdown formatting for audio\n",
    "    clean_text = text\n",
    "    for char in [\"**\", \"*\", \"```python\", \"```\", \"#\", \"`\"]:\n",
    "        clean_text = clean_text.replace(char, \"\")\n",
    "    \n",
    "    # Limit length for TTS\n",
    "    if len(clean_text) > 1000:\n",
    "        clean_text = clean_text[:1000] + \"... [response truncated for audio]\"\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as tmp_file:\n",
    "            tmp_path = tmp_file.name\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"alloy\",  # Options: alloy, echo, fable, onyx, nova, shimmer\n",
    "            input=clean_text\n",
    "        ) as response:\n",
    "            response.stream_to_file(tmp_path)\n",
    "        return tmp_path\n",
    "    except Exception as e:\n",
    "        print(f\"TTS error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Audio functions working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradio UI\n",
    "# Handler functions that connect UI to logic\n",
    "\n",
    "def handle_text_message(message: str, history: list, model: str):\n",
    "    \"\"\"\n",
    "    Handle a text message - streams response back to chat.\n",
    "    \"\"\"\n",
    "    if not message.strip():\n",
    "        yield history, None\n",
    "        return\n",
    "    \n",
    "    # Add user message to history\n",
    "    history = history + [[message, None]]\n",
    "    \n",
    "    # Stream the response\n",
    "    for partial_response in chat_with_streaming(message, history[:-1], model):\n",
    "        history[-1][1] = partial_response\n",
    "        yield history, None  # No audio yet during streaming\n",
    "    \n",
    "    # Generate audio for the final response\n",
    "    final_response = history[-1][1]\n",
    "    if final_response:\n",
    "        audio_path = text_to_speech(final_response)\n",
    "        yield history, audio_path\n",
    "\n",
    "\n",
    "def handle_audio_message(audio_path: str, history: list, model: str):\n",
    "    \"\"\"\n",
    "    Handle an audio message - transcribe it, then chat.\n",
    "    \"\"\"\n",
    "    if audio_path is None:\n",
    "        yield history, None\n",
    "        return\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    transcribed_text = transcribe_audio(audio_path)\n",
    "    \n",
    "    if not transcribed_text:\n",
    "        yield history, None\n",
    "        return\n",
    "    \n",
    "    # Handling the audio message with text\n",
    "    yield from handle_text_message(f\" {transcribed_text}\", history, model)\n",
    "\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Reset everything\"\"\"\n",
    "    return [], None, None\n",
    "\n",
    "\n",
    "# Building the UI\n",
    "with gr.Blocks(\n",
    "    title=\"Technical Q&A Assistant\",\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\"\"\"\n",
    "        .gradio-container { max-width: 900px; margin: auto; }\n",
    "        .title-text { text-align: center; }\n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Technical Q&A Assistant\n",
    "    ### Your expert coding companion - ask me anything about programming!\n",
    "    \n",
    "    *I can explain concepts, write and run code examples, and even respond with audio!*\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Model selector\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=[\n",
    "                \"gpt-4o-mini\",  # Faster and cheaper - good for most questions\n",
    "                \"gpt-4o\",       # More powerful - for complex stuff\n",
    "            ],\n",
    "            value=\"gpt-4o-mini\",\n",
    "            label=\"Model\",\n",
    "            info=\"gpt-4o-mini is faster & cheaper; gpt-4o is smarter\",\n",
    "            scale=2\n",
    "        )\n",
    "    \n",
    "    # Main chat area\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Chat\",\n",
    "        height=350,\n",
    "        render_markdown=True,\n",
    "        bubble_full_width=False,\n",
    "        show_copy_button=True,\n",
    "        avatar_images=(None, \" \")  # User gets default, AI gets robot emoji\n",
    "    )\n",
    "    \n",
    "    # Audio output (plays the AI's response)\n",
    "    audio_output = gr.Audio(\n",
    "        label=\"AI Voice Response\",\n",
    "        autoplay=True,\n",
    "        visible=True\n",
    "    )\n",
    "    \n",
    "    # Text input row\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(\n",
    "            placeholder=\"Ask a technical question... e.g. 'How does recursion work?' or 'Show me how to sort a list in Python'\",\n",
    "            label=\"Your Question\",\n",
    "            lines=2,\n",
    "            scale=5\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send \", variant=\"primary\", scale=1)\n",
    "    \n",
    "    # Audio input row\n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(\n",
    "            sources=[\"microphone\"],\n",
    "            type=\"filepath\",\n",
    "            label=\"ðŸŽ¤ Or speak your question!\",\n",
    "            scale=4\n",
    "        )\n",
    "        audio_send_btn = gr.Button(\"Send Audio ðŸŽ¤\", variant=\"secondary\", scale=1)\n",
    "    \n",
    "    # Clear button\n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"stop\")\n",
    "    \n",
    "    # Example questions to get people started\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"What is a Python decorator and how do I use one?\",\n",
    "            \"What is the significance of Transformers in AI\",\n",
    "            \"What are Vector Embeddings?\",\n",
    "            \"What's the difference between async and sync code?\",\n",
    "            \"Write and run a fibonacci sequence in Python\",\n",
    "            \"What is backpropagation in neural networks?\",\n",
    "        ],\n",
    "        inputs=text_input,\n",
    "        label=\"Example Questions (click to try!)\"\n",
    "    )\n",
    "    \n",
    "    # Wire up the events\n",
    "    \n",
    "    # Text message: submit on Enter or button click\n",
    "    text_input.submit(\n",
    "        fn=handle_text_message,\n",
    "        inputs=[text_input, chatbot, model_dropdown],\n",
    "        outputs=[chatbot, audio_output]\n",
    "    ).then(lambda: \"\", outputs=text_input)  # Clear input after sending\n",
    "    \n",
    "    send_btn.click(\n",
    "        fn=handle_text_message,\n",
    "        inputs=[text_input, chatbot, model_dropdown],\n",
    "        outputs=[chatbot, audio_output]\n",
    "    ).then(lambda: \"\", outputs=text_input)\n",
    "    \n",
    "    # Audio message\n",
    "    audio_send_btn.click(\n",
    "        fn=handle_audio_message,\n",
    "        inputs=[audio_input, chatbot, model_dropdown],\n",
    "        outputs=[chatbot, audio_output]\n",
    "    )\n",
    "    \n",
    "    # Clear chat\n",
    "    clear_btn.click(\n",
    "        fn=clear_chat,\n",
    "        outputs=[chatbot, audio_output, audio_input]\n",
    "    )\n",
    "\n",
    "print(\"Gradio app working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8422b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the app!\n",
    "demo.launch(\n",
    "    debug=False,       # Set to True to see error messages in notebook\n",
    "    show_error=True,   # Show errors in the UI\n",
    "    inbrowser=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
