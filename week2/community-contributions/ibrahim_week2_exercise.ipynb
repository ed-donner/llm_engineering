{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0751a167",
   "metadata": {},
   "source": [
    "Technical Question Explainer with Voice functionality\n",
    "\n",
    "A tool that takes a technical question (or code snippet) and returns a structured explanation.  \n",
    "Supports **OpenAI** and **Ollama** (local models) ‚Äî switchable from the Gradio UI.\n",
    "\n",
    "---\n",
    "### Setup\n",
    "```bash\n",
    "uv add openai gradio\n",
    "```\n",
    "For **Ollama**, make sure it's running locally and you've pulled a model:\n",
    "```bash\n",
    "ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4f5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import gradio as gr\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b350f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL   = \"gpt-4o-mini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb518cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "OLLAMA_MODEL    = \"llama3.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9545634",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL = \"whisper-1\"\n",
    "TTS_VOICE     = \"alloy\"\n",
    "TTS_MODEL     = \"tts-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eaf2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert technical educator.\n",
    "When given a technical question or code snippet, provide:\n",
    "1. A clear, concise explanation (2-3 sentences)\n",
    "2. A simple analogy to make it relatable\n",
    "3. A line-by-line or concept breakdown (especially for code)\n",
    "4. Key takeaways as bullet points\n",
    "\n",
    "Keep explanations accessible but accurate. Format your response in clean markdown.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cadc0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25324701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_client() -> OpenAI:\n",
    "    \"\"\"Return an OpenAI client (always needed for Whisper & TTS).\"\"\"\n",
    "    if not OPENAI_API_KEY :\n",
    "        raise ValueError(\"Please set OPENAI_API_KEY in the Configuration cell.\")\n",
    "    return OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "def get_client(provider: str) -> tuple[OpenAI, str]:\n",
    "    \"\"\"Return (OpenAI client, model name) for the chosen provider.\"\"\"\n",
    "    if provider == \"Ollama (Local)\":\n",
    "        client = OpenAI(\n",
    "            base_url=OLLAMA_BASE_URL,\n",
    "            api_key=\"ollama\",   \n",
    "        )\n",
    "        return client, OLLAMA_MODEL\n",
    "    else:\n",
    "        return get_openai_client(), OPENAI_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a4cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_streaming(question: str, provider: str):\n",
    "    \"\"\"Gradio streaming generator ‚Äî yields progressively longer markdown strings.\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"-*- Please enter a question or code snippet.\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        client, model = get_client(provider)\n",
    "    except ValueError as e:\n",
    "        yield f\" **Error:** {e}\"\n",
    "        return\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "\n",
    "    accumulated = f\"*Using **{model}** via {provider}‚Ä¶*\\n\\n---\\n\\n\"\n",
    "    try:\n",
    "        with client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        ) as stream:\n",
    "            for chunk in stream:\n",
    "                delta = chunk.choices[0].delta.content or \"\"\n",
    "                accumulated += delta\n",
    "                yield accumulated\n",
    "    except Exception as e:\n",
    "        yield f\"-*- **API Error:** {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a723e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"Transcribe a recorded audio file to text using OpenAI Whisper.\"\"\"\n",
    "    if audio_path is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        client = get_openai_client()\n",
    "        with open(audio_path, \"rb\") as f:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=WHISPER_MODEL,\n",
    "                file=f,\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        return f\"[Transcription error: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33be9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text: str) -> str | None:\n",
    "    \"\"\"Convert text to speech and return a path to the audio file.\"\"\"\n",
    "    if not text or text.startswith(\"-*-\"):\n",
    "        return None\n",
    "\n",
    "    import re\n",
    "    clean = re.sub(r\"[#*`_>\\-]+\", \" \", text)   # remove markdown symbols\n",
    "    clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "    # Limit length to avoid very long TTS calls\n",
    "    clean = clean[:3000]\n",
    "\n",
    "    try:\n",
    "        client = get_openai_client()\n",
    "        response = client.audio.speech.create(\n",
    "            model=TTS_MODEL,\n",
    "            voice=TTS_VOICE,\n",
    "            input=clean,\n",
    "        )\n",
    "        # Save to a temp file and return the path for Gradio\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "        tmp.write(response.content)\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "    except Exception as e:\n",
    "        print(f\"TTS error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7036d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_audio_input(audio_path: str, provider: str):\n",
    "    \"\"\"Transcribe audio, run explanation, return (question_text, markdown, audio_path).\"\"\"\n",
    "    question = transcribe_audio(audio_path)\n",
    "    if not question or question.startswith(\"[\"):\n",
    "        yield question, \"-*- Could not transcribe audio.\", None\n",
    "        return\n",
    "\n",
    "    full_markdown = \"\"\n",
    "    for chunk in explain_streaming(question, provider):\n",
    "        full_markdown = chunk\n",
    "        yield question, full_markdown, None   # stream text; audio comes at the end\n",
    "\n",
    "    audio_out = text_to_speech(full_markdown)\n",
    "    yield question, full_markdown, audio_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16cfe452",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_QUESTION = question.strip()\n",
    "\n",
    "with gr.Blocks(\n",
    "    title=\"Tech Explainer\",\n",
    "    theme=gr.themes.Base(\n",
    "        primary_hue=\"teal\",\n",
    "        neutral_hue=\"zinc\",\n",
    "        font=[gr.themes.GoogleFont(\"IBM Plex Mono\"), \"monospace\"],\n",
    "    ),\n",
    "    css=\"\"\"\n",
    "        #header { text-align:center; padding:1rem 0 0.25rem; }\n",
    "        #sub    { text-align:center; color:#71717a; margin-bottom:1.5rem; }\n",
    "        footer  { display:none !important; }\n",
    "    \"\"\",\n",
    ") as demo:\n",
    "\n",
    "    gr.Markdown(\"# üî¨ Technical Question Explainer\", elem_id=\"header\")\n",
    "    gr.Markdown(\n",
    "        \"Type or speak your question ‚Äî get a structured explanation with audio playback.\",\n",
    "        elem_id=\"sub\",\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        provider_radio = gr.Radio(\n",
    "            choices=[\"OpenAI (GPT-4o-mini)\", \"Ollama (Local)\"],\n",
    "            value=\"OpenAI (GPT-4o-mini)\",\n",
    "            label=\"LLM Provider\",\n",
    "        )\n",
    "\n",
    "    with gr.Tabs():\n",
    "\n",
    "        with gr.TabItem(\"‚å®Ô∏è  Text\"):\n",
    "            question_box = gr.Textbox(\n",
    "                label=\"Question or Code Snippet\",\n",
    "                placeholder=\"Paste code or ask a technical question‚Ä¶\",\n",
    "                lines=6,\n",
    "                value=EXAMPLE_QUESTION,\n",
    "            )\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"Explain\", variant=\"primary\", scale=4)\n",
    "                clear_btn  = gr.Button(\"Clear\",  variant=\"secondary\", scale=1)\n",
    "\n",
    "    \n",
    "        with gr.TabItem(\"üéôÔ∏è  Voice\"):\n",
    "            gr.Markdown(\n",
    "                \"Record your question. It will be transcribed by **Whisper**, \"\n",
    "                \"explained by the selected LLM, and read back to you via **TTS**.  \\n\"\n",
    "                \"*(Whisper & TTS always use OpenAI regardless of the LLM provider chosen above.)*\"\n",
    "            )\n",
    "            audio_input = gr.Audio(\n",
    "                sources=[\"microphone\"],\n",
    "                type=\"filepath\",\n",
    "                label=\"Record your question\",\n",
    "            )\n",
    "            voice_btn = gr.Button(\"Transcribe & Explain\", variant=\"primary\")\n",
    "\n",
    "    answer_box = gr.Markdown(\n",
    "        label=\"Explanation\",\n",
    "        value=\"*Your explanation will appear here‚Ä¶*\",\n",
    "    )\n",
    "    audio_output = gr.Audio(\n",
    "        label=\"Listen to the Explanation\",\n",
    "        type=\"filepath\",\n",
    "        autoplay=True,\n",
    "        visible=True,\n",
    "    )\n",
    "\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [EXAMPLE_QUESTION],\n",
    "            [\"What is the difference between a process and a thread?\"],\n",
    "            [\"Explain Python's GIL and when it actually matters.\"],\n",
    "            [\"What is gradient descent and how does backpropagation use it?\"],\n",
    "            [\"What does async/await do under the hood in Python?\"],\n",
    "            [\"Explain the CAP theorem in distributed systems.\"],\n",
    "        ],\n",
    "        inputs=question_box,\n",
    "        label=\"Example Questions ‚Äî click to load\",\n",
    "    )\n",
    "\n",
    "    def explain_then_speak(question, provider):\n",
    "        \"\"\"Stream text explanation, then generate audio at the end.\"\"\"\n",
    "        full_markdown = \"\"\n",
    "        for chunk in explain_streaming(question, provider):\n",
    "            full_markdown = chunk\n",
    "            yield full_markdown, None\n",
    "        audio_path = text_to_speech(full_markdown)\n",
    "        yield full_markdown, audio_path\n",
    "\n",
    "    submit_btn.click(\n",
    "        explain_then_speak,\n",
    "        inputs=[question_box, provider_radio],\n",
    "        outputs=[answer_box, audio_output],\n",
    "    )\n",
    "    question_box.submit(\n",
    "        explain_then_speak,\n",
    "        inputs=[question_box, provider_radio],\n",
    "        outputs=[answer_box, audio_output],\n",
    "    )\n",
    "    clear_btn.click(\n",
    "        lambda: (EXAMPLE_QUESTION, \"*Your explanation will appear here‚Ä¶*\", None),\n",
    "        outputs=[question_box, answer_box, audio_output],\n",
    "    )\n",
    "\n",
    "\n",
    "    voice_btn.click(\n",
    "        handle_audio_input,\n",
    "        inputs=[audio_input, provider_radio],\n",
    "        outputs=[question_box, answer_box, audio_output],\n",
    "    )       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e817c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a9c6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
