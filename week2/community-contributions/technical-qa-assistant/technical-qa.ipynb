{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f06e80",
   "metadata": {},
   "source": [
    "Technical Q&A Assistant\n",
    "\n",
    "Multi-model chatbot with streaming responses, voice I/O, and function calling.\n",
    "Supports GPT-4o-mini and Gemini Flash with adaptive expertise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f24965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_client = OpenAI()\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if google_api_key:\n",
    "    genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expertise-based system prompts that shape response style\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"Beginner\": \"\"\"You are a patient and clear technical instructor. \n",
    "Explain concepts as if teaching someone new to programming. Use simple analogies, \n",
    "avoid jargon when possible, and provide step-by-step explanations.\"\"\",\n",
    "    \n",
    "    \"Intermediate\": \"\"\"You are an experienced software engineer and technical mentor.\n",
    "Provide clear, practical explanations with code examples. Assume the user has \n",
    "basic programming knowledge but may need clarification on advanced concepts.\"\"\",\n",
    "    \n",
    "    \"Expert\": \"\"\"You are a senior software architect and technical expert.\n",
    "Provide deep, comprehensive explanations with advanced insights. Include \n",
    "performance considerations, best practices, and architectural patterns. \n",
    "Assume strong technical background.\"\"\"\n",
    "}\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = SYSTEM_PROMPTS[\"Intermediate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool implementations - extend these for richer functionality\n",
    "def explain_code(code_snippet):\n",
    "    return f\"Code analysis requested for: {code_snippet[:50]}...\"\n",
    "\n",
    "def lookup_documentation(topic):\n",
    "    docs = {\n",
    "        \"python\": \"Python is a high-level programming language...\",\n",
    "        \"async\": \"Async/await allows concurrent execution...\",\n",
    "        \"gradio\": \"Gradio is a Python library for building UIs...\",\n",
    "    }\n",
    "    return docs.get(topic.lower(), f\"Documentation for {topic} not found in cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a860ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI function calling schema\n",
    "code_explanation_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"explain_code\",\n",
    "        \"description\": \"Analyze and explain a code snippet in detail. Use this when the user asks about specific code.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"code_snippet\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The code snippet to analyze and explain\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"code_snippet\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "documentation_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"lookup_documentation\",\n",
    "        \"description\": \"Look up technical documentation for a topic. Use this when the user asks about documentation or references.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The technical topic to look up\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "TOOLS = [code_explanation_tool, documentation_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aba86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_explain_code(arguments, tool_call_id):\n",
    "    code = arguments.get('code_snippet')\n",
    "    result = explain_code(code)\n",
    "    return {\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call_id}\n",
    "\n",
    "def handle_lookup_documentation(arguments, tool_call_id):\n",
    "    topic = arguments.get('topic')\n",
    "    result = lookup_documentation(topic)\n",
    "    return {\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call_id}\n",
    "\n",
    "TOOL_HANDLERS = {\n",
    "    \"explain_code\": handle_explain_code,\n",
    "    \"lookup_documentation\": handle_lookup_documentation,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Convert speech to text using Whisper.\"\"\"\n",
    "    if audio_file is None:\n",
    "        return None\n",
    "    try:\n",
    "        with open(audio_file, \"rb\") as audio:\n",
    "            transcript = openai_client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        return f\"Error transcribing audio: {str(e)}\"\n",
    "\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Generate audio from text using OpenAI TTS.\"\"\"\n",
    "    try:\n",
    "        response = openai_client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"onyx\",\n",
    "            input=text\n",
    "        )\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "        temp_file.write(response.content)\n",
    "        temp_file.close()\n",
    "        return temp_file.name\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_openai(message, history, system_prompt, use_tools=True):\n",
    "    \"\"\"Stream responses from GPT-4o-mini with optional tool calling.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    for msg in history:\n",
    "        messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    tools = TOOLS if use_tools else None\n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    tool_calls = []\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_response += content\n",
    "            yield content\n",
    "        \n",
    "        # Accumulate tool call fragments from streaming response\n",
    "        if chunk.choices[0].delta.tool_calls:\n",
    "            for tool_call in chunk.choices[0].delta.tool_calls:\n",
    "                if tool_call.index is not None:\n",
    "                    while len(tool_calls) <= tool_call.index:\n",
    "                        tool_calls.append({\"id\": \"\", \"function\": {\"name\": \"\", \"arguments\": \"\"}})\n",
    "                    \n",
    "                    if tool_call.id:\n",
    "                        tool_calls[tool_call.index][\"id\"] = tool_call.id\n",
    "                    if tool_call.function.name:\n",
    "                        tool_calls[tool_call.index][\"function\"][\"name\"] = tool_call.function.name\n",
    "                    if tool_call.function.arguments:\n",
    "                        tool_calls[tool_call.index][\"function\"][\"arguments\"] += tool_call.function.arguments\n",
    "    \n",
    "    # Execute tools and continue conversation with results\n",
    "    if tool_calls and use_tools:\n",
    "        tool_messages = []\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call[\"function\"][\"name\"]:\n",
    "                handler = TOOL_HANDLERS.get(tool_call[\"function\"][\"name\"])\n",
    "                if handler:\n",
    "                    arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "                    tool_response = handler(arguments, tool_call[\"id\"])\n",
    "                    tool_messages.append(tool_response)\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_response,\n",
    "            \"tool_calls\": [{\"id\": tc[\"id\"], \"function\": {\"name\": tc[\"function\"][\"name\"], \"arguments\": tc[\"function\"][\"arguments\"]}} for tc in tool_calls]\n",
    "        })\n",
    "        messages.extend(tool_messages)\n",
    "        \n",
    "        final_stream = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for chunk in final_stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                yield content\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gemini(message, history, system_prompt):\n",
    "    \"\"\"Stream responses from Gemini Flash.\"\"\"\n",
    "    if not google_api_key:\n",
    "        yield \"Error: GOOGLE_API_KEY not configured. Please set it in your .env file.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-2.0-flash-exp\",\n",
    "            system_instruction=system_prompt\n",
    "        )\n",
    "        \n",
    "        # Convert history format for Gemini API\n",
    "        gemini_history = []\n",
    "        for msg in history:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                gemini_history.append({\"role\": \"user\", \"parts\": [msg[\"content\"]]})\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                gemini_history.append({\"role\": \"model\", \"parts\": [msg[\"content\"]]})\n",
    "        \n",
    "        chat_session = model.start_chat(history=gemini_history)\n",
    "        response = chat_session.send_message(message, stream=True)\n",
    "        \n",
    "        for chunk in response:\n",
    "            if hasattr(chunk, 'text') and chunk.text:\n",
    "                yield chunk.text\n",
    "    except Exception as e:\n",
    "        yield f\"Error with Gemini: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, model_choice, expertise_level, use_tools, audio_enabled):\n",
    "    \"\"\"Main chat handler - routes to appropriate model(s).\"\"\"\n",
    "    if not message:\n",
    "        return history, None\n",
    "    \n",
    "    history = history or []\n",
    "    system_prompt = SYSTEM_PROMPTS.get(expertise_level, DEFAULT_SYSTEM_PROMPT)\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    assistant_response = \"\"\n",
    "    \n",
    "    if model_choice == \"GPT-4o-mini\" or model_choice == \"Both\":\n",
    "        response_text = \"\"\n",
    "        for chunk in chat_with_openai(message, history[:-1], system_prompt, use_tools):\n",
    "            response_text += chunk\n",
    "            temp_history = history + [{\"role\": \"assistant\", \"content\": response_text}]\n",
    "            yield temp_history, None\n",
    "        \n",
    "        assistant_response = response_text\n",
    "        \n",
    "        if model_choice == \"Both\":\n",
    "            assistant_response += \"\\n\\n--- Gemini Response ---\\n\\n\"\n",
    "            gemini_response = \"\"\n",
    "            for chunk in chat_with_gemini(message, history[:-1], system_prompt):\n",
    "                gemini_response += chunk\n",
    "                combined_response = assistant_response + gemini_response\n",
    "                temp_history = history + [{\"role\": \"assistant\", \"content\": combined_response}]\n",
    "                yield temp_history, None\n",
    "            assistant_response += gemini_response\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "            yield history, None\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "            yield history, None\n",
    "    \n",
    "    elif model_choice == \"Gemini Flash\":\n",
    "        response_text = \"\"\n",
    "        for chunk in chat_with_gemini(message, history[:-1], system_prompt):\n",
    "            response_text += chunk\n",
    "            temp_history = history + [{\"role\": \"assistant\", \"content\": response_text}]\n",
    "            yield temp_history, None\n",
    "        \n",
    "        assistant_response = response_text\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        yield history, None\n",
    "    \n",
    "    audio_file = None\n",
    "    if audio_enabled and assistant_response:\n",
    "        audio_file = text_to_speech(assistant_response)\n",
    "    \n",
    "    yield history, audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad976717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_input(audio_file, history, model_choice, expertise_level, use_tools, audio_enabled):\n",
    "    \"\"\"Handle voice input - transcribe and route to chat.\"\"\"\n",
    "    if audio_file is None:\n",
    "        return history, None, None\n",
    "    \n",
    "    transcribed_text = transcribe_audio(audio_file)\n",
    "    \n",
    "    if not transcribed_text or transcribed_text.startswith(\"Error\"):\n",
    "        return history, None, None\n",
    "    \n",
    "    # Voice input automatically enables audio response\n",
    "    for hist, audio in chat(transcribed_text, history, model_choice, expertise_level, use_tools, True):\n",
    "        yield hist, audio, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Technical Question Answerer\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸš€ Technical Q&A Assistant\n",
    "    \n",
    "    Ask programming questions and get explanations tailored to your level.\n",
    "    \n",
    "    **Features:** Multi-model support â€¢ Real-time streaming â€¢ Voice I/O â€¢ Tool calling\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Technical Q&A Chat\",\n",
    "                type=\"messages\",\n",
    "                height=500,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    label=\"Ask a technical question\",\n",
    "                    placeholder=\"e.g., Explain async/await in Python...\",\n",
    "                    scale=4\n",
    "                )\n",
    "                submit_btn = gr.Button(\"Send ðŸ“¤\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                audio_input = gr.Audio(\n",
    "                    sources=[\"microphone\"],\n",
    "                    type=\"filepath\",\n",
    "                    label=\"ðŸŽ¤ Speak your question\",\n",
    "                    show_label=True\n",
    "                )\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### âš™ï¸ Settings\")\n",
    "            \n",
    "            model_choice = gr.Radio(\n",
    "                choices=[\"GPT-4o-mini\", \"Gemini Flash\", \"Both\"],\n",
    "                value=\"GPT-4o-mini\",\n",
    "                label=\"ðŸ¤– Model Selection\",\n",
    "                info=\"Choose which model(s) to use\"\n",
    "            )\n",
    "            \n",
    "            expertise_level = gr.Radio(\n",
    "                choices=[\"Beginner\", \"Intermediate\", \"Expert\"],\n",
    "                value=\"Intermediate\",\n",
    "                label=\"ðŸ“š Expertise Level\",\n",
    "                info=\"Adjust explanation depth\"\n",
    "            )\n",
    "            \n",
    "            use_tools = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"ðŸ”§ Enable Tools\",\n",
    "                info=\"Use code explanation and documentation tools\"\n",
    "            )\n",
    "            \n",
    "            audio_enabled = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"ðŸ”Š Audio Output\",\n",
    "                info=\"Generate speech responses\"\n",
    "            )\n",
    "            \n",
    "            audio_output = gr.Audio(\n",
    "                label=\"ðŸ”Š Assistant's Voice Response\",\n",
    "                autoplay=True,\n",
    "                visible=True\n",
    "            )\n",
    "            \n",
    "            clear_btn = gr.Button(\"ðŸ—‘ï¸ Clear Chat\", variant=\"stop\")\n",
    "    \n",
    "    def toggle_audio_visibility(audio_enabled_val):\n",
    "        return gr.Audio(visible=audio_enabled_val)\n",
    "    \n",
    "    audio_enabled.change(\n",
    "        fn=toggle_audio_visibility,\n",
    "        inputs=audio_enabled,\n",
    "        outputs=audio_output\n",
    "    )\n",
    "    \n",
    "    msg.submit(\n",
    "        fn=chat,\n",
    "        inputs=[msg, chatbot, model_choice, expertise_level, use_tools, audio_enabled],\n",
    "        outputs=[chatbot, audio_output]\n",
    "    ).then(lambda: \"\", None, msg)\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=chat,\n",
    "        inputs=[msg, chatbot, model_choice, expertise_level, use_tools, audio_enabled],\n",
    "        outputs=[chatbot, audio_output]\n",
    "    ).then(lambda: \"\", None, msg)\n",
    "    \n",
    "    audio_input.stop_recording(\n",
    "        fn=process_audio_input,\n",
    "        inputs=[audio_input, chatbot, model_choice, expertise_level, use_tools, audio_enabled],\n",
    "        outputs=[chatbot, audio_output, audio_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], None, None),\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, audio_output, audio_input]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    **Tips:** Use the microphone for voice queries â€¢ Adjust expertise for simpler/deeper explanations â€¢ Enable tools for code analysis\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333543a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=False, inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
