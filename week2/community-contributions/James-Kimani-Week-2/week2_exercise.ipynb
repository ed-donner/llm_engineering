{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Additional End of week Exercise - week 2\n",
        "\n",
        "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
        "\n",
        "This version includes a Gradio UI, streaming, system prompt expertise modes, and model switching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8bc22a94",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cfd41255",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "OPENAI_MODELS = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-5\"]\n",
        "MODEL_LLAMA = \"llama3.2\"\n",
        "ALL_MODELS = OPENAI_MODELS + [MODEL_LLAMA]\n",
        "DEFAULT_MODEL = OPENAI_MODELS[0]\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6ddfda0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(\".env\", override=True)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY is missing. Add it to a .env file.\")\n",
        "\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d6239288",
      "metadata": {},
      "outputs": [],
      "source": [
        "expertise_modes = {\n",
        "    \"General Tutor\": \"You are a helpful technical tutor. Explain clearly with simple examples.\",\n",
        "    \"AI Tools Mentor\": \"You are an AI tools mentor. Explain practical steps, trade-offs, and one beginner mistake to avoid.\",\n",
        "    \"Interview Coach\": \"You are a technical interview coach. Explain clearly, then end with one short follow-up practice question.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "30680139",
      "metadata": {},
      "outputs": [],
      "source": [
        "def respond(message, history, model, expertise):\n",
        "    history = history or []\n",
        "\n",
        "    if not message.strip():\n",
        "        yield \"\", history\n",
        "        return\n",
        "\n",
        "    client = openai_client if model in OPENAI_MODELS else ollama_client\n",
        "    system_prompt = expertise_modes[expertise]\n",
        "\n",
        "    chat_history = []\n",
        "    for turn in history:\n",
        "        if isinstance(turn, dict):\n",
        "            role = turn.get(\"role\")\n",
        "            content = turn.get(\"content\")\n",
        "            if role in (\"user\", \"assistant\") and content:\n",
        "                chat_history.append({\"role\": role, \"content\": content})\n",
        "        elif isinstance(turn, (list, tuple)) and len(turn) == 2:\n",
        "            user_msg, assistant_msg = turn\n",
        "            if user_msg:\n",
        "                chat_history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            if assistant_msg:\n",
        "                chat_history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    api_messages = [{\"role\": \"system\", \"content\": system_prompt}] + chat_history\n",
        "    api_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    partial = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=api_messages,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    base_history = chat_history + [{\"role\": \"user\", \"content\": message}]\n",
        "    for chunk in stream:\n",
        "        token = chunk.choices[0].delta.content or \"\"\n",
        "        if token:\n",
        "            partial += token\n",
        "            yield \"\", base_history + [{\"role\": \"assistant\", \"content\": partial}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4382b97e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Week 2 Technical Q&A Assistant\")\n",
        "    gr.Markdown(\"Ask a technical question, switch model and expertise mode, then chat.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4, min_width=780, elem_id=\"main-pane\"):\n",
        "            chatbot = gr.Chatbot(label=\"Assistant\", height=520)\n",
        "\n",
        "            with gr.Row(elem_id=\"input-row\"):\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Ask a technical question...\",\n",
        "                    label=\"Message\",\n",
        "                    lines=2,\n",
        "                    max_lines=5,\n",
        "                    scale=8,\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=2, min_width=140)\n",
        "\n",
        "        with gr.Column(scale=2, min_width=300, elem_id=\"left-sidebar\"):\n",
        "            gr.Markdown(\"### Settings\")\n",
        "            model_dropdown = gr.Dropdown(\n",
        "                choices=ALL_MODELS,\n",
        "                value=DEFAULT_MODEL,\n",
        "                label=\"AI Model\",\n",
        "                interactive=True,\n",
        "            )\n",
        "            mode_dropdown = gr.Dropdown(\n",
        "                choices=list(expertise_modes.keys()),\n",
        "                value=\"AI Tools Mentor\",\n",
        "                label=\"Mode\",\n",
        "                interactive=True,\n",
        "            )\n",
        "            clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[msg, chatbot, model_dropdown, mode_dropdown],\n",
        "        outputs=[msg, chatbot],\n",
        "    )\n",
        "    msg.submit(\n",
        "        fn=respond,\n",
        "        inputs=[msg, chatbot, model_dropdown, mode_dropdown],\n",
        "        outputs=[msg, chatbot],\n",
        "    )\n",
        "    clear_btn.click(lambda: (\"\", []), outputs=[msg, chatbot], queue=False)\n",
        "\n",
        "demo.launch(\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\".gradio-container {max-width: 1400px !important; width: 96vw !important; margin: 0 auto;} #right-sidebar {border-left: 1px solid rgba(255,255,255,0.10); padding-left: 14px;} #input-row {align-items: end;}\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
