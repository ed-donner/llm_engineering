{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b12f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise OpenAI client using Ollama\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='hohoho')\n",
    "OL_MODEL1 = \"llama3.2:latest\" # text only, multilingual, 128k context window, 3b parameters\n",
    "OL_MODEL2 = \"phi3:latest\" # text only, 128k context window, 3.8b parameters\n",
    "OL_MODEL3 = \"gemma3:270m\" # text only, 32k context window, 270m parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857eae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for our chatbots Alfie, Bill, and Charlie\n",
    "llama_system = \"\"\"\n",
    "You are Alfie, a socratic chatbot.\n",
    "You ask questions to help explore topics and issues.\n",
    "You are in a conversation with Bill and Charlie.\n",
    "Please limit your response to a single paragraph.\n",
    "\"\"\"\n",
    "phi_system = \"\"\"\n",
    "You are Bill, a chatbot who is a prolific speaker.\n",
    "You are good at using metaphors, for example food and sports.\n",
    "You are in a conversation with Alfie and Charlie.\n",
    "Please limit your response to a single paragraph.\n",
    "\"\"\"\n",
    "gemma_system = \"\"\"\n",
    "You are Charlie, a chatbot.\n",
    "You are in a conversation with Alfie and Bill.\n",
    "Please limit your response to a single paragraph.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the conversation and randomly starts with a chatbot\n",
    "first_user_message = \"Hi there\"\n",
    "choice = random.randint(1, 3)\n",
    "\n",
    "if choice == 1:\n",
    "    active_bot = \"Alfie\"\n",
    "elif choice == 2:\n",
    "    active_bot = \"Bill\"\n",
    "else:\n",
    "    active_bot = \"Charlie\"\n",
    "\n",
    "conversation = f\"{active_bot}: {first_user_message}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompts for our chatbots Alfie, Bill, and Charlie\n",
    "llama_user = f\"\"\"\n",
    "You are Alfie, in conversation with Bill and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next.\n",
    "\"\"\"\n",
    "\n",
    "phi_user = f\"\"\"\n",
    "You are Bill, in conversation with Alfie and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next.\n",
    "\"\"\"\n",
    "\n",
    "gemma_user = f\"\"\"\n",
    "You are Charlie, in conversation with Alfie and Bill.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define call chatbot function\n",
    "def call_llama():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": llama_system},\n",
    "        {\"role\": \"user\", \"content\": llama_user}\n",
    "    ]\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=OL_MODEL1,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_phi():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": phi_system},\n",
    "        {\"role\": \"user\", \"content\": phi_user}\n",
    "    ]\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=OL_MODEL2,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_gemma():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": gemma_system},\n",
    "        {\"role\": \"user\", \"content\": gemma_user}\n",
    "    ]\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=OL_MODEL3,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi chatbots conversations for a defined number of turns\n",
    "# Display first message by the first chosen chatbot to speak\n",
    "display(Markdown(f\"### {active_bot}:\\n{first_user_message}\\n\"))\n",
    "last_bot = choice\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Make sure the same chatbot doesn't speak two turns in a row\n",
    "    while last_bot == choice:\n",
    "        last_bot = random.randint(1, 3)\n",
    "    \n",
    "    choice = last_bot\n",
    "\n",
    "    if choice == 1:\n",
    "        llama_next = call_llama()\n",
    "        active_bot = \"Alfie\"\n",
    "        display(Markdown(f\"### {active_bot}:\\n{llama_next}\\n\"))\n",
    "        conversation += f\"{active_bot}: {llama_next}\\n\"\n",
    "    elif choice == 2:\n",
    "        phi_next = call_phi()\n",
    "        active_bot = \"Bill\"\n",
    "        display(Markdown(f\"### {active_bot}:\\n{phi_next}\\n\"))\n",
    "        conversation += f\"{active_bot}: {phi_next}\\n\"\n",
    "    else:\n",
    "        gemma_next = call_gemma()\n",
    "        active_bot = \"Charlie\"\n",
    "        display(Markdown(f\"### {active_bot}:\\n{gemma_next}\\n\"))\n",
    "        conversation += f\"{active_bot}: {gemma_next}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9390e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (conversation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
