{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Ensemble Learning</u>\n",
    "[Link to original article on Kaggle](https://www.kaggle.com/code/vipulgandhi/a-comprehensive-guide-to-ensemble-learning)\n",
    "\n",
    "If we aggregate the predictions of a group of predictors (classifiers / regressors), we will <u> often</u> get better predictions than with the best individual predictor. A group of predictor is called an ensemble, the technique of aggregation is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.\n",
    "\n",
    "<b>Ensemble methods work best when the predictors are as independent from one another as possible. Training the dataset using very different classifiers increases the chance of them making very different types of errors, improving ensemble’s accuracy.</b>\n",
    "\n",
    "<u>Ensemble methods can be divided into two groups: </u>\n",
    "\n",
    "- <b> Sequential ensemble methods </b> where the base learners are generated sequentially (e.g. AdaBoost).\n",
    "  - The basic motivation of sequential methods is to <u> exploit the dependence between the base learners </u>. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
    "\n",
    "- <b> Parallel ensemble methods </b> where the base learners are generated in parallel (e.g. Random Forest).\n",
    "  - The basic motivation of parallel methods is to <u> exploit independence between the base learners </u> since the error can be reduced dramatically by averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Voting Classifiers</u> :\n",
    "Train a few classifiers (Logistic Regression classifier, SVM classifier, Random Forest classifier, K-Nearest Neighbors classifier etc.) on the training set.\n",
    "\n",
    "![1.png](static/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Aggregate the predictions of each classifier and predict the class that gets the most votes. This majority vote classifier is called a hard voting classifier.\n",
    " \n",
    "![2.png](static/2.png)\n",
    "\n",
    "This voting classifier often achieves a higher accuracy than the best classifier in the ensemble. Even if each classifier is a weak learner (it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist.target = mnist.target.astype(np.int64)\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 50,000 instances for training, 10,000 for validation, and 10,000 for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifier, Extra-Trees classifier, SVM and MLP\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .score() method directly calls sklearn.metrics.accuracy_score method.\n",
    "\n",
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the classifiers into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier.\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "named_estimators = [ (\"random_forest_clf\", random_forest_clf), (\"extra_trees_clf\", extra_trees_clf), (\"svm_clf\", svm_clf), (\"mlp_clf\", mlp_clf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(named_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM is far outperformed by the other classifiers. Remove the SVM to see if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove an estimator by setting it to None using set_params()\n",
    "\n",
    "voting_clf.set_params(svm_clf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated list of estimators\n",
    "\n",
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated list of trained estimators\n",
    "\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators:\n",
    "\n",
    "del voting_clf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck\n",
    "\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the VotingClassifier again:\n",
    "\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better! SVM was hurting performance. Let's try using a soft voting classifier.\n",
    "\n",
    "If all classifiers in the ensemble can estimate class probabilities (predict_proba()), we can set \"voting = 'soft'\" to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set voting to \"soft\"\n",
    "\n",
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Bagging and Pasting</u>\n",
    "In Bagging/ Pasting,  same training algorithm is used for every predictor(Classifier/ Regressor), but we train them on different random subsets of the training set. When sampling is performed with replacement, the method is called <b> bagging / bootstrap aggregating </b>. When sampling is performed without replacement, it is called <b> pasting </b>.\n",
    "\n",
    "![3.png](static/3.png)\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by aggregating the predictions of all predictors. The aggregation function is generally the statistical mode (most frequent prediction) for classification, or the statistical mean for regression.\n",
    "\n",
    "Each individual predictor has a higher bias(underfit) because only a subset of the trining set is trained on it, aggregation reduces both bias and variance. The ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "Bagging and Pasting scale very well on different CPU cores and servers, all predictors can be trained in parallel, predictions can also be made in parallel.\n",
    "\n",
    "### <u>Out-of-Bag Evaluation </u>\n",
    "\n",
    "With bagging, at each iteration, the remaining training instances that are not sampled are called out-of-bag (oob) instances. These instances are not same for all predictors.\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation. We can evaluate the ensemble itself by averaging out the oob evaluations of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Base Classifier for Bagging Method\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train an ensemble of 500 Decision Tree classifiers.\n",
    "# BaggingClassifier automatically performs soft voting if the base classifier has a predict_proba() method.\n",
    "# To train each predictor on a random subset of the input features, use max_features(0-1). Useful if training set has high dimentional features.\n",
    "# \"n_estimators = 500\" was taking lot of CPU time, I used \"n_estimators = 10\" for demonstration purpose.\n",
    "# bootstrap = True (Bagging)\n",
    "clf_bagging = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 0.8, bootstrap = True, oob_score=True, n_jobs = -1)\n",
    "# bootstrap = False (Pasting)\n",
    "clf_pasting = BaggingClassifier( DecisionTreeClassifier(), n_estimators = 10, max_samples = 0.8, bootstrap = False, n_jobs = -1)\n",
    "\n",
    "clf_bagging.fit(X_train, y_train)\n",
    "clf_pasting.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagging = clf_bagging.predict(X_val)\n",
    "y_pred_pasting = clf_bagging.predict(X_val)\n",
    "\n",
    "\n",
    "clf_bagging.oob_score_, accuracy_score(y_val, y_pred_bagging), accuracy_score(y_val, y_pred_pasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision function returns the class probabilities (if base estimator has a predict_proba() method) for each training instance. \n",
    "clf_bagging.oob_decision_function_[112]\n",
    "#y_train[112]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Random Forests </u>\n",
    "Random Forest is an ensemble of Decision Trees.\n",
    "\n",
    "Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, we can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees.\n",
    "\n",
    "RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an <b> extremely randomized trees </b> algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows reduction of the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_randomforest = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "clf_randomforest.fit(X_train, y_train)\n",
    "y_pred_rf = clf_randomforest.predict(X_val)\n",
    "\n",
    "accuracy_score(y_val, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Feature Importance</u>\n",
    "\n",
    "If we look at a single Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves (or not at all). It is possible to get an estimate of a feature’s importance by computing the average depth at which it appears across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Boosting</u>\n",
    "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better than random guessing, such as small decision trees to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds.\n",
    "\n",
    "The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. \n",
    "\n",
    "Boosting technique cannot be parallelized (or only partially) because each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging / pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> AdaBoost </u>\n",
    "\n",
    "The predictors(classifier/ regressor) fit the training set in sequence. The next predictor corrects its predecessor by paying more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases.\n",
    "\n",
    "To build an AdaBoost classifier, each instance's weight is set to an initial value. A base classifier (eg. Decision Tree) is trained and makes predictions on the training set. The relative weight of misclassified training instances is then increased. The second classifier is trained on the training set using the updated weights and again it makes predictions on the training set and update the weights. The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "![4.png](static/4.png)\n",
    "\n",
    "This sequential learning technique is similar to Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds more predictors to the ensemble, gradually making it better.\n",
    "\n",
    "<b><u> Algorithm </u></b>:\n",
    "\n",
    "- Each instance weight $w^{(i)}$ is initially set to $\\frac{1}{m}$ (m = number of instances) . The first predictor is trained and its weighted error rate $r_1$ is computed on the training set:\n",
    "\n",
    "$$\\text{ Weighted error rate for $j^{th}$ predictor:   } r_j = \\frac{\\hat{y_j}^i \\neq y^i}{\\sum_{i=1}^{m} w^i} $$\n",
    "\n",
    "where ${y_j}^i$ is the $j^{th}$ predictor’s prediction for the $i^{th}$ instance.\n",
    "\n",
    "- The predictor’s weight $\\alpha_j$ is then computed using its weighted error rate , where $\\eta$ is the learning rate hyperparameter (defaults to 1).\n",
    "  - Predictor guessing accurately - Higher weight\n",
    "  - Predictor guessing randomly - Weight close to 0\n",
    "  - Predictor guessing mostly wrong - Negative weights\n",
    "\n",
    "$$\\text{ Predictor weight:   } \\alpha_j = \\eta \\log\\frac{1 - r_j}{r_j} $$\n",
    "\n",
    "- The instance weights are updated using above equation and the misclassified instance' weights are boosted.\n",
    "\n",
    "$$\\text{ Weight update rule:   } w^{i}  \\leftarrow \n",
    "  \\begin{cases}\n",
    "    w^i       & \\quad \\text{if }  \\hat{y_j}^i = y^i\\\\\n",
    "    w^i \\exp(\\alpha_j)  & \\quad \\text{if } \\hat{y_j}^i \\neq y^i\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "- All the instance weights are normalized (divided by $\\sum_{i=1}^{m} w^i$).\n",
    "\n",
    "A new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained. \n",
    "\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using their predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses a multiclass version of AdaBoost - SAMME. When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities (if they have a predict_proba() method), Scikit-Learn can use a variant of SAMME called SAMME.R, which relies on class probabilities rather than predictions and generally performs better.\n",
    "\n",
    "The following code using Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1—in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier based on 200 Decision Trees \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_val)\n",
    "accuracy_score(y_val, y_pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost, Gradient Boosting tries to fit the new predictor to the residual errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a second DecisionTreeRegressor on the residual errors made by the first\n",
    "predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a third regressor on the residual errors made by the second predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the same ensemble as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If we set it to a low value(0.1) we will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called shrinkage.\n",
    "\n",
    "If Gradient Boosting ensemble overfits the training set, we should try decreasing the learning rate. We could also use early stopping to find the right number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "\n",
    "plt.figure(figsize=(11,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
    "\n",
    "#save_fig(\"gbrt_learning_rate_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping (see Chap‐ ter 4). A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of train‐ ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the opti‐ mal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set.\n",
    "\n",
    "![5.png](static/5.png)\n",
    "\n",
    "First, the training set is split in two subsets. The first subset is used to train the predictors in the first layer. Next, the predictors in the first layer are used to make predictions on the second(hold-out) set. Now (in example above) for each instance in the hold-out set there are four predicted values. A new training set is created using these predicted values as input features and keeping the target values. The blender is trained on this new training set, it learns to predict the target value where inputs are the the first layer’s predictions.\n",
    "\n",
    "It is possible to train several different blenders on the top of one another (e.g., one using Linear Regression, another using Random Forest Regression etc). The training set should be divided equal to the number of layers(see image above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [random_forest_clf, extra_trees_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune this blender or try other types of blenders (e.g., an MLPClassifier), then select the best one using cross-validation, as always.\n",
    "\n",
    "We have trained a blender, and together with the classifiers they form a stacking ensemble.\n",
    "\n",
    "Let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all our classifiers, then feed the predictions to the blender to get the ensemble's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End\n",
    "If you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
