{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Week 5 Exercise - Personal Knowledge Worker with RAG\n",
    "### Author: Samuel Kalu, Team Euclid, Week 5\n",
    "\n",
    "This notebook implements a RAG (Retrieval Augmented Generation) based Personal Knowledge Worker that can answer questions about your personal data.\n",
    "\n",
    "Features:\n",
    "- Document loading from multiple sources (Markdown files)\n",
    "- Intelligent text chunking with overlap\n",
    "- Vector embeddings using OpenAI/HuggingFace\n",
    "- Chroma vector store for efficient retrieval\n",
    "- t-SNE visualization (2D and 3D)\n",
    "- Conversational RAG with memory\n",
    "- Gradio chat interface\n",
    "- Model switching support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API Keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "# Configuration\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "DB_NAME = \"personal_knowledge_db\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K_RESULTS = 5\n",
    "\n",
    "# Knowledge base path\n",
    "KNOWLEDGE_BASE_PATH = Path(\"knowledge_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-setup-header",
   "metadata": {},
   "source": [
    "## 2. Sample Knowledge Base Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample knowledge base if it doesn't exist\n",
    "\n",
    "def create_sample_knowledge_base():\n",
    "    \"\"\"Create a sample knowledge base with personal, projects, and learning data.\"\"\"\n",
    "    \n",
    "    folders = [\"personal\", \"projects\", \"learning\"]\n",
    "    \n",
    "    for folder in folders:\n",
    "        (KNOWLEDGE_BASE_PATH / folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Personal info\n",
    "    personal_text = \"\"\"\n",
    "# Personal Profile\n",
    "\n",
    "## About Me\n",
    "Name: Samuel Kalu\n",
    "Role: Software Engineer & AI Enthusiast\n",
    "Location: Tech Hub City\n",
    "\n",
    "## Background\n",
    "I am a passionate software engineer with over 5 years of experience building scalable applications.\n",
    "My journey started with web development and has evolved into specializing in AI and machine learning.\n",
    "\n",
    "## Skills\n",
    "- Programming Languages: Python, JavaScript, TypeScript, Go\n",
    "- Frameworks: React, Node.js, FastAPI, LangChain\n",
    "- AI/ML: LLMs, RAG systems, Vector Databases, Prompt Engineering\n",
    "- Databases: PostgreSQL, MongoDB, Chroma, Pinecone\n",
    "\n",
    "## Interests\n",
    "I love exploring new technologies, contributing to open-source projects, and mentoring aspiring developers.\n",
    "In my free time, I enjoy hiking, reading tech blogs, and experimenting with new AI tools.\n",
    "\"\"\"\n",
    "    \n",
    "    # Projects\n",
    "    projects_text = \"\"\"\n",
    "# Projects Portfolio\n",
    "\n",
    "## AI-Powered Document Assistant\n",
    "A RAG-based system that helps users query large document collections efficiently.\n",
    "Tech Stack: Python, LangChain, Chroma, OpenAI API\n",
    "Key Features: Semantic search, multi-document support, conversation history\n",
    "\n",
    "## Real-time Analytics Dashboard\n",
    "Built a scalable dashboard for visualizing business metrics in real-time.\n",
    "Tech Stack: React, Node.js, PostgreSQL, Redis\n",
    "Impact: Reduced reporting time by 80% for the operations team\n",
    "\n",
    "## Code Review Automation Tool\n",
    "An AI assistant that provides automated code reviews and suggestions.\n",
    "Tech Stack: Python, GitHub API, LLM integration\n",
    "Features: Pattern detection, best practices recommendations, security checks\n",
    "\n",
    "## E-commerce Platform\n",
    "Full-stack e-commerce solution with payment integration and inventory management.\n",
    "Tech Stack: Next.js, Stripe, PostgreSQL, Docker\n",
    "Scale: Handles 10,000+ daily active users\n",
    "\"\"\"\n",
    "    \n",
    "    # Learning\n",
    "    learning_text = \"\"\"\n",
    "# Learning Journey\n",
    "\n",
    "## Current Focus Areas\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "Studying transformer architectures, attention mechanisms, and fine-tuning techniques.\n",
    "Key Resources: HuggingFace courses, research papers, hands-on experimentation\n",
    "\n",
    "### Retrieval Augmented Generation (RAG)\n",
    "Learning about vector embeddings, semantic search, and retrieval strategies.\n",
    "Practical Applications: Building knowledge bases, question-answering systems\n",
    "\n",
    "### Vector Databases\n",
    "Exploring Chroma, Pinecone, and Weaviate for efficient similarity search.\n",
    "Use Cases: Recommendation systems, semantic caching, document retrieval\n",
    "\n",
    "## Recent Learnings\n",
    "- Advanced prompt engineering techniques (CoT, Few-shot learning)\n",
    "- LangChain abstractions and chains\n",
    "- Evaluation metrics for RAG systems\n",
    "- Agentic AI and autonomous agents\n",
    "\n",
    "## Goals\n",
    "- Master advanced RAG architectures\n",
    "- Contribute to open-source AI projects\n",
    "- Build production-ready AI applications\n",
    "\"\"\"\n",
    "    \n",
    "    # Write files\n",
    "    (KNOWLEDGE_BASE_PATH / \"personal\" / \"info.md\").write_text(personal_text, encoding=\"utf-8\")\n",
    "    (KNOWLEDGE_BASE_PATH / \"projects\" / \"portfolio.md\").write_text(projects_text, encoding=\"utf-8\")\n",
    "    (KNOWLEDGE_BASE_PATH / \"learning\" / \"journey.md\").write_text(learning_text, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"Sample knowledge base created at {KNOWLEDGE_BASE_PATH}\")\n",
    "\n",
    "# Create the sample data\n",
    "create_sample_knowledge_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingestion-header",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-documents-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents using LangChain loaders\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Load all markdown documents from the knowledge base.\"\"\"\n",
    "    \n",
    "    folders = glob.glob(str(KNOWLEDGE_BASE_PATH / \"*\"))\n",
    "    \n",
    "    def add_metadata(doc, doc_type):\n",
    "        \"\"\"Add document type metadata.\"\"\"\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        doc.metadata[\"source\"] = doc.metadata.get(\"source\", \"unknown\")\n",
    "        return doc\n",
    "    \n",
    "    text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "    documents = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        doc_type = Path(folder).name\n",
    "        loader = DirectoryLoader(\n",
    "            folder, \n",
    "            glob=\"**/*.md\", \n",
    "            loader_cls=TextLoader, \n",
    "            loader_kwargs=text_loader_kwargs\n",
    "        )\n",
    "        folder_docs = loader.load()\n",
    "        documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load the documents\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Document types: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-documents-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vectorstore-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with Chroma\n",
    "\n",
    "print(\"Initializing embeddings...\")\n",
    "\n",
    "# Use OpenAI embeddings (or switch to HuggingFace for free alternative)\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "if Path(DB_NAME).exists():\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "    print(f\"Deleted existing vector store: {DB_NAME}\")\n",
    "\n",
    "# Create new vector store\n",
    "print(\"Creating vector store...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=DB_NAME\n",
    ")\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "# Get sample embedding to determine dimensions\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "\n",
    "print(f\"Vectorstore created with {count:,} documents\")\n",
    "print(f\"Embedding dimensions: {dimensions:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## 4. Vector Store Visualization (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-2d-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D t-SNE Visualization\n",
    "\n",
    "def visualize_2d():\n",
    "    \"\"\"Create a 2D t-SNE visualization of the vector store.\"\"\"\n",
    "    \n",
    "    result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "    vectors = np.array(result['embeddings'])\n",
    "    documents = result['documents']\n",
    "    metadatas = result['metadatas']\n",
    "    doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "    \n",
    "    # Color mapping for document types\n",
    "    unique_types = list(set(doc_types))\n",
    "    colors_map = {'personal': 'blue', 'projects': 'green', 'learning': 'red'}\n",
    "    colors = [colors_map.get(t, 'gray') for t in doc_types]\n",
    "    \n",
    "    # t-SNE requires at least 3 samples\n",
    "    n = vectors.shape[0]\n",
    "    if n < 3:\n",
    "        print(f\"t-SNE needs at least 3 samples, got {n}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate perplexity (should be less than number of samples)\n",
    "    perplexity = max(5.0, min(30.0, (n - 1) / 3.0))\n",
    "    \n",
    "    print(f\"Running t-SNE with perplexity={perplexity:.1f}...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000)\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "    \n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=colors, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        text=[f\"Type: {t}<br>Text: {d[:150]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text',\n",
    "        name='Chunks'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='2D Vector Store Visualization (t-SNE)',\n",
    "        xaxis_title='t-SNE Dimension 1',\n",
    "        yaxis_title='t-SNE Dimension 2',\n",
    "        width=900,\n",
    "        height=700,\n",
    "        hovermode='closest',\n",
    "        template='plotly_white',\n",
    "        legend=dict(x=0, y=1)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Show 2D visualization\n",
    "fig_2d = visualize_2d()\n",
    "if fig_2d:\n",
    "    fig_2d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-3d-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D t-SNE Visualization\n",
    "\n",
    "def visualize_3d():\n",
    "    \"\"\"Create a 3D t-SNE visualization of the vector store.\"\"\"\n",
    "    \n",
    "    result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "    vectors = np.array(result['embeddings'])\n",
    "    documents = result['documents']\n",
    "    metadatas = result['metadatas']\n",
    "    doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "    \n",
    "    unique_types = list(set(doc_types))\n",
    "    colors_map = {'personal': 'blue', 'projects': 'green', 'learning': 'red'}\n",
    "    colors = [colors_map.get(t, 'gray') for t in doc_types]\n",
    "    \n",
    "    n = vectors.shape[0]\n",
    "    if n < 3:\n",
    "        print(f\"t-SNE needs at least 3 samples, got {n}\")\n",
    "        return None\n",
    "    \n",
    "    perplexity = max(5.0, min(30.0, (n - 1) / 3.0))\n",
    "    \n",
    "    print(f\"Running 3D t-SNE with perplexity={perplexity:.1f}...\")\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=perplexity, n_iter=1000)\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "    \n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        z=reduced_vectors[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=colors, opacity=0.8),\n",
    "        text=[f\"Type: {t}<br>Text: {d[:150]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='3D Vector Store Visualization (t-SNE)',\n",
    "        scene=dict(\n",
    "            xaxis_title='t-SNE Dimension 1',\n",
    "            yaxis_title='t-SNE Dimension 2',\n",
    "            zaxis_title='t-SNE Dimension 3'\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        margin=dict(r=10, b=10, l=10, t=50)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Show 3D visualization\n",
    "fig_3d = visualize_3d()\n",
    "if fig_3d:\n",
    "    fig_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain-header",
   "metadata": {},
   "source": [
    "## 5. RAG Chain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-rag-chain-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the RAG chain with conversation memory\n",
    "\n",
    "print(\"Setting up RAG chain...\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=MODEL,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Create retriever with custom search parameters\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": TOP_K_RESULTS}\n",
    ")\n",
    "\n",
    "# Setup conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Create the conversational retrieval chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG chain initialized successfully!\")\n",
    "\n",
    "# Test the chain\n",
    "def chat(question: str, history: list) -> str:\n",
    "    \"\"\"Process a question and return an answer.\"\"\"\n",
    "    try:\n",
    "        result = conversation_chain.invoke({\"question\": question})\n",
    "        answer = result[\"answer\"]\n",
    "        \n",
    "        # Add source information\n",
    "        sources = set()\n",
    "        for doc in result.get(\"source_documents\", []):\n",
    "            doc_type = doc.metadata.get(\"doc_type\", \"unknown\")\n",
    "            sources.add(doc_type)\n",
    "        \n",
    "        if sources:\n",
    "            answer += f\"\\n\\n_Sources: {', '.join(sources)}_\"\n",
    "        \n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Quick test\n",
    "test_question = \"What is my background?\"\n",
    "print(f\"\\nTest question: {test_question}\")\n",
    "test_answer = chat(test_question, [])\n",
    "print(f\"Test answer: {test_answer[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-header",
   "metadata": {},
   "source": [
    "## 6. Gradio Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-ui-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio UI\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create the Gradio chat interface.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Soft(\n",
    "            primary_hue=gr.themes.colors.blue,\n",
    "            spacing_size=gr.themes.sizes.spacing_md\n",
    "        ),\n",
    "        title=\"Personal Knowledge Worker - Samuel Kalu, Team Euclid\"\n",
    "    ) as ui:\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # Personal Knowledge Worker\n",
    "            ### By Samuel Kalu, Team Euclid - Week 5\n",
    "            \n",
    "            Chat with your personal knowledge base powered by RAG (Retrieval Augmented Generation).\n",
    "            Ask questions about your profile, projects, and learning journey.\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Chat interface\n",
    "        chat_interface = gr.ChatInterface(\n",
    "            fn=chat,\n",
    "            type=\"messages\",\n",
    "            title=\"Knowledge Base Chat\",\n",
    "            description=\"Ask anything about your personal data\",\n",
    "            examples=[\n",
    "                [\"What is my background?\"],\n",
    "                [\"Tell me about my projects\"],\n",
    "                [\"What am I currently learning?\"],\n",
    "                [\"What are my main skills?\"]\n",
    "            ],\n",
    "            retry_btn=None,\n",
    "            undo_btn=None,\n",
    "            clear_btn=\"Clear Chat\"\n",
    "        )\n",
    "        \n",
    "        # Additional info\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ---\n",
    "            **Features:**\n",
    "            - Semantic search across all your documents\n",
    "            - Conversation history for context-aware responses\n",
    "            - Source attribution for answers\n",
    "            - Powered by OpenAI embeddings and GPT-4.1-nano\n",
    "            \n",
    "            **Tips:**\n",
    "            - Ask specific questions for better answers\n",
    "            - Follow-up questions work thanks to conversation memory\n",
    "            - Check the source documents for more details\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    return ui\n",
    "\n",
    "print(\"Gradio interface configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-header",
   "metadata": {},
   "source": [
    "## 7. Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio UI\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = create_gradio_interface()\n",
    "    print(\"\\nLaunching Personal Knowledge Worker...\")\n",
    "    print(\"Open the URL shown below in your browser to start chatting.\")\n",
    "    print(\"Press Ctrl+C to stop the server.\\n\")\n",
    "    ui.launch(share=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Week 5 exercise solution demonstrates a complete RAG pipeline:\n",
    "\n",
    "### Key Components:\n",
    "1. **Document Loading**: Uses LangChain DirectoryLoader to load markdown files\n",
    "2. **Text Chunking**: CharacterTextSplitter with configurable chunk size and overlap\n",
    "3. **Vector Embeddings**: OpenAI embeddings (text-embedding-3-small)\n",
    "4. **Vector Store**: Chroma for persistent storage and retrieval\n",
    "5. **Visualization**: t-SNE for 2D/3D visualization of document clusters\n",
    "6. **RAG Chain**: ConversationalRetrievalChain with memory\n",
    "7. **Chat Interface**: Gradio with conversation history\n",
    "\n",
    "### Features Implemented:\n",
    "- Multi-document type support (personal, projects, learning)\n",
    "- Conversation memory for context-aware responses\n",
    "- Source attribution in answers\n",
    "- Interactive 2D and 3D visualizations\n",
    "- Clean, modern Gradio UI\n",
    "- Error handling and graceful degradation\n",
    "\n",
    "### Business Applications:\n",
    "- Employee knowledge bases\n",
    "- Company documentation Q&A\n",
    "- Personal note-taking assistants\n",
    "- Customer support automation\n",
    "- Research paper search engines\n",
    "\n",
    "### Author: Samuel Kalu, Team Euclid, Week 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
