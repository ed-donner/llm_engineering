{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9be818c2",
      "metadata": {},
      "source": [
        "## Mood Based Library Assistant\n",
        "\n",
        "RAG pipeline with OpenAI + Chroma. Conversational: history is used; a **query rewriter** (GPT-4.1-nano) rewrites the current question using the **last 3 user questions** for better retrieval (handles \"something like that\", \"more like the last one\", etc.). Retrieval uses a **score threshold** so only high-quality contexts are returned; fallback to standard retrieval if none pass. Book availability and user-rating info are appended from the tool.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dae0485",
      "metadata": {},
      "source": [
        "Run all cells in order. Ensure you are in the notebook's directory (so `knowledge_base/` and this notebook are in the same folder) and that `OPENAI_API_KEY` is set in `.env`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75f9884",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and paths\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Knowledge base folder is next to this notebook (same parent directory)\n",
        "BASE_DIR = Path.cwd()\n",
        "KB_DIR = BASE_DIR / \"knowledge_base\"\n",
        "CHROMA_DIR = BASE_DIR / \"vector_db\"\n",
        "DB_PATH = BASE_DIR / \"library.db\"\n",
        "\n",
        "print(\"KB_DIR:\", KB_DIR)\n",
        "print(\"CHROMA_DIR:\", CHROMA_DIR)\n",
        "print(\"DB_PATH:\", DB_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397ab645",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all CSVs from knowledge_base; each row = one Document with full row text + metadata\n",
        "def load_csv_documents():\n",
        "    documents = []\n",
        "    if not KB_DIR.exists():\n",
        "        raise FileNotFoundError(f\"Knowledge base folder not found: {KB_DIR}\")\n",
        "    for csv_path in sorted(KB_DIR.glob(\"*.csv\")):\n",
        "        df = pd.read_csv(csv_path, on_bad_lines='skip')\n",
        "        for _, row in df.iterrows():\n",
        "            # Full row as readable text for embedding/search\n",
        "            parts = [f\"{col}: {row[col]}\" for col in df.columns if pd.notna(row.get(col))]\n",
        "            page_content = \"\\n\".join(parts)\n",
        "            if not page_content.strip():\n",
        "                continue\n",
        "            metadata = {\"source\": csv_path.name}\n",
        "            for col in df.columns:\n",
        "                val = row.get(col)\n",
        "                if pd.notna(val) and isinstance(val, str) and len(str(val)) < 500:\n",
        "                    metadata[col] = str(val)\n",
        "            documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "    return documents\n",
        "\n",
        "documents = load_csv_documents()\n",
        "print(f\"Loaded {len(documents)} document chunks (one per CSV row).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22be69a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector store: Chroma with OpenAI embeddings (chunks = full CSV rows with metadata)\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "LLM_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
        "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=str(CHROMA_DIR),\n",
        ")\n",
        "# Retriever with score threshold so only high-quality contexts are returned (Chroma uses similarity, higher = better)\n",
        "SCORE_THRESHOLD = 0.8\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 5, \"score_threshold\": SCORE_THRESHOLD},\n",
        ")\n",
        "print(\"Chroma vector store ready (with score threshold).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f031e82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQLite: books table (title, author, availability, rating_count, rating_summary)\n",
        "RATING_SUMMARIES = [\n",
        "    \"readers rated it a good read\",\n",
        "    \"people found it worth the time\",\n",
        "    \"readers would recommend it\",\n",
        "    \"of readers enjoyed it\",\n",
        "    \"readers said it was a page-turner\",\n",
        "    \"of readers found the book boring\"\n",
        "]\n",
        "\n",
        "def init_availability_db():\n",
        "    conn = sqlite3.connect(str(DB_PATH))\n",
        "    conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS books (\n",
        "            title TEXT NOT NULL,\n",
        "            author TEXT NOT NULL,\n",
        "            availability TEXT NOT NULL,\n",
        "            rating_count INTEGER DEFAULT 0,\n",
        "            rating_summary TEXT,\n",
        "            PRIMARY KEY (title, author)\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    books_seen = set()\n",
        "    for doc in documents:\n",
        "        t, a = doc.metadata.get(\"title\"), doc.metadata.get(\"author\")\n",
        "        if t and a and (t, a) not in books_seen:\n",
        "            books_seen.add((t, a))\n",
        "    book_list = list(books_seen)\n",
        "    if not book_list:\n",
        "        conn.close()\n",
        "        return\n",
        "    n = min(random.randint(5, max(5, len(book_list))), len(book_list))\n",
        "    chosen = random.sample(book_list, n)\n",
        "    statuses = [\"available\", \"not available\", \"checked out\", \"on hold\"]\n",
        "    for (title, author) in chosen:\n",
        "        status = random.choice(statuses)\n",
        "        count = random.randint(20, 400)\n",
        "        summary = f\"{count} {random.choice(RATING_SUMMARIES)}\"\n",
        "        conn.execute(\n",
        "            \"\"\"INSERT OR REPLACE INTO books (title, author, availability, rating_count, rating_summary)\n",
        "               VALUES (?, ?, ?, ?, ?)\"\"\",\n",
        "            (title, author, status, count, summary),\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_availability_db()\n",
        "print(\"SQLite library.db initialized with availability and user-rating data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e39f390",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool: get book info (availability + user ratings). No result = no record = clear (no extra note).\n",
        "@tool\n",
        "def get_book_info(title: str, author: str = \"\") -> str:\n",
        "    \"\"\"Get library info for a book: availability and user ratings. Pass title; author is optional (use title-only to avoid matching books when only the author is mentioned).\"\"\"\n",
        "    conn = sqlite3.connect(str(DB_PATH))\n",
        "    if author:\n",
        "        row = conn.execute(\n",
        "            \"SELECT availability, rating_count, rating_summary FROM books WHERE title = ? AND author = ?\",\n",
        "            (title.strip(), author.strip()),\n",
        "        ).fetchone()\n",
        "    else:\n",
        "        row = conn.execute(\n",
        "            \"SELECT availability, rating_count, rating_summary FROM books WHERE title = ? LIMIT 1\",\n",
        "            (title.strip(),),\n",
        "        ).fetchone()\n",
        "    conn.close()\n",
        "    if row:\n",
        "        availability, count, summary = row[0], row[1], row[2] or \"\"\n",
        "        parts = [f\"Availability: {availability}\"]\n",
        "        if summary:\n",
        "            parts.append(summary)\n",
        "        elif count:\n",
        "            parts.append(f\"{count} readers gave feedback.\")\n",
        "        return \". \".join(parts)\n",
        "    return \"No record in library system (assume clear/available).\"\n",
        "\n",
        "def get_book_info_for_response(response_text: str, docs) -> str:\n",
        "    \"\"\"Call the tool only for books whose *title* appears in the final response (title-only lookup; no author to avoid irrelevant matches).\"\"\"\n",
        "    if not response_text or not docs:\n",
        "        return \"\"\n",
        "    # Unique titles from retrieved context (candidates)\n",
        "    candidate_titles = set()\n",
        "    for doc in docs:\n",
        "        t = doc.metadata.get(\"title\")\n",
        "        if t and t.strip():\n",
        "            candidate_titles.add(t.strip())\n",
        "    # Only call tool for titles that appear in the assistant's final response\n",
        "    lines = []\n",
        "    for title in sorted(candidate_titles):\n",
        "        if title not in response_text:\n",
        "            continue\n",
        "        result = get_book_info.invoke({\"title\": title, \"author\": \"\"})\n",
        "        lines.append(f\"- **{title}**: {result}\")\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    return \"\\n\\n**Book info (availability & ratings):**\\n\" + \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82ae32b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query rewriter: use full conversation (user + assistant) so \"they\" / \"that author\" can be resolved\n",
        "REWRITER_MODEL = \"gpt-4.1-nano\"  # or \"gpt-4o-mini\" if 4.1-nano is not available\n",
        "rewriter_llm = ChatOpenAI(model=REWRITER_MODEL, temperature=0)\n",
        "\n",
        "REWRITER_PROMPT = \"\"\"You are a query rewriter for a library book search. Turn the current user question into a single, self-contained search query that will find the right books.\n",
        "\n",
        "Use the recent conversation (both user and assistant) to resolve references:\n",
        "- \"they\" / \"that author\" / \"the same writer\" → use the author from the assistant's previous answer (e.g. User: \"Who is the author of Dune?\" Assistant: \"Frank Herbert.\" User: \"What other books do they have?\" → rewrite as \"other books by Frank Herbert\").\n",
        "- \"that book\" / \"like the last one\" / \"something similar\" → use the book or theme from the previous turn.\n",
        "Including the assistant's answers is essential so you know what \"they\" or \"that book\" refers to. Output only the rewritten query, no explanation.\n",
        "\n",
        "Recent conversation (oldest first):\n",
        "{conversation}\n",
        "\n",
        "Current user question: {current_question}\n",
        "\n",
        "Rewritten search query:\"\"\"\n",
        "\n",
        "def _format_conversation_for_rewriter(history, max_turns=2):\n",
        "    \"\"\"Format last N turns as 'User: ... Assistant: ...' so the rewriter can resolve pronouns.\"\"\"\n",
        "    if not history:\n",
        "        return \"(none)\"\n",
        "    turns = []\n",
        "    for turn in history[-max_turns:]:\n",
        "        if hasattr(turn, \"content\") and hasattr(turn, \"role\"):\n",
        "            role = getattr(turn, \"role\", \"\")\n",
        "            content = (getattr(turn, \"content\", \"\") or \"\").strip()\n",
        "            if role == \"user\":\n",
        "                turns.append(f\"User: {content}\")\n",
        "            elif role == \"assistant\":\n",
        "                turns.append(f\"Assistant: {content}\")\n",
        "        elif isinstance(turn, (list, tuple)) and len(turn) >= 2:\n",
        "            if turn[0]:\n",
        "                turns.append(f\"User: {turn[0]}\")\n",
        "            if turn[1]:\n",
        "                turns.append(f\"Assistant: {turn[1]}\")\n",
        "    return \"\\n\".join(turns) if turns else \"(none)\"\n",
        "\n",
        "def rewrite_query(current_question: str, history: list) -> str:\n",
        "    \"\"\"Rewrite the current question using full conversation (user + assistant) for retrieval.\"\"\"\n",
        "    conversation = _format_conversation_for_rewriter(history, max_turns=3)\n",
        "    prompt = REWRITER_PROMPT.format(conversation=conversation, current_question=current_question)\n",
        "    response = rewriter_llm.invoke([HumanMessage(content=prompt)])\n",
        "    rewritten = (response.content or current_question).strip()\n",
        "    return rewritten if rewritten else current_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c84815",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main LLM and RAG prompt (conversational: history is used in messages)\n",
        "llm = ChatOpenAI(model=LLM_MODEL, temperature=0, streaming=True)\n",
        "\n",
        "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a friendly library assistant. Use the following context from the library catalog to answer the user. Recommend books when relevant (by title and author). If you don't know, say so. Be concise and helpful. You have access to the conversation history.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef0444b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve with scores; dedupe once so LLM context and displayed chunks always match. Chroma returns distance (lower=better)\n",
        "def _dedupe_docs(docs, scores):\n",
        "    \"\"\"Remove duplicate docs by (title, author) so same book from multiple CSVs counts once. Keep first (best score).\"\"\"\n",
        "    seen = set()\n",
        "    out_docs, out_scores = [], []\n",
        "    for doc, score in zip(docs, scores):\n",
        "        key = (str(doc.metadata.get(\"title\") or \"\").strip(), str(doc.metadata.get(\"author\") or \"\").strip())\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out_docs.append(doc)\n",
        "        out_scores.append(score)\n",
        "    return out_docs, out_scores\n",
        "\n",
        "def _retrieve_with_scores(query: str):\n",
        "    \"\"\"Returns (deduplicated docs, scores). Single source for LLM context, display, and book-info tool.\"\"\"\n",
        "    try:\n",
        "        pairs = vectorstore.similarity_search_with_relevance_scores(query, k=10)\n",
        "    except Exception:\n",
        "        pairs = [(d, 0.0) for d in vectorstore.similarity_search(query, k=10)]\n",
        "    if not pairs:\n",
        "        return [], []\n",
        "    docs, scores = zip(*pairs)\n",
        "    docs, scores = _dedupe_docs(list(docs), list(scores))\n",
        "    docs, scores = docs[:5], scores[:5]\n",
        "    return docs, scores\n",
        "\n",
        "def format_context_with_scores(docs, scores) -> str:\n",
        "    \"\"\"Format retrieved chunks as markdown. Chroma returns distance (lower=better); show as similarity (higher=better).\"\"\"\n",
        "    if not docs:\n",
        "        return \"_No retrieved context._\"\n",
        "    lines = []\n",
        "    for i, (doc, raw) in enumerate(zip(docs, scores), 1):\n",
        "        if isinstance(raw, (int, float)):\n",
        "            similarity = 1.0 - raw if raw <= 1.0 else 1.0 / (1.0 + raw)\n",
        "            score_str = f\"similarity: `{similarity:.3f}` (distance: {raw:.3f})\"\n",
        "        else:\n",
        "            score_str = str(raw)\n",
        "        lines.append(f\"**Chunk {i}** | {score_str}\")\n",
        "        lines.append(\"\")\n",
        "        lines.append(doc.page_content.replace(\"\\n\", \"  \\n\"))\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"---\")\n",
        "        lines.append(\"\")\n",
        "    return \"\\n\".join(lines).rstrip()\n",
        "\n",
        "# Chat: one deduplicated docs list for LLM context, display, and book-info (keeps counts consistent)\n",
        "def chat_with_availability(message, history):\n",
        "    text = getattr(message, \"content\", message) or message\n",
        "    text = str(text or \"\").strip()\n",
        "    if not text:\n",
        "        yield \"\", \"\"\n",
        "        return\n",
        "    rewritten = rewrite_query(text, history)\n",
        "    docs, scores = _retrieve_with_scores(rewritten)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs) if docs else \"(No closely matching books found.)\"\n",
        "    context_md = format_context_with_scores(docs, scores)\n",
        "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
        "    messages = [SystemMessage(content=system_prompt)]\n",
        "    for turn in history:\n",
        "        if hasattr(turn, \"content\") and hasattr(turn, \"role\"):\n",
        "            role, content = getattr(turn, \"role\", \"\"), getattr(turn, \"content\", \"\") or \"\"\n",
        "            if role == \"user\":\n",
        "                messages.append(HumanMessage(content=content))\n",
        "            elif role == \"assistant\":\n",
        "                messages.append(AIMessage(content=content))\n",
        "        elif isinstance(turn, (list, tuple)) and len(turn) >= 2:\n",
        "            if turn[0]:\n",
        "                messages.append(HumanMessage(content=str(turn[0])))\n",
        "            if turn[1]:\n",
        "                messages.append(AIMessage(content=str(turn[1])))\n",
        "    messages.append(HumanMessage(content=text))\n",
        "    full = \"\"\n",
        "    for chunk in llm.stream(messages):\n",
        "        if chunk.content:\n",
        "            full += chunk.content\n",
        "            yield full, \"\"\n",
        "    extra = get_book_info_for_response(full, docs)\n",
        "    if extra:\n",
        "        full += extra\n",
        "    yield full, context_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec8aead",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio: chat + second output (retrieved context with scores) in a toggleable Accordion\n",
        "with gr.Blocks(title=\"Mood Based Library Assistant\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Mood Based Library Assistant\")\n",
        "    gr.Markdown(\"Ask for book recommendations by mood, theme, or topic. You'll get availability and user-rating info for recommended books.\")\n",
        "    with gr.Accordion(\"Retrieved context (toggle to view)\", open=False):\n",
        "        context_out = gr.Markdown(value=\"\", label=\"Context with scores\")\n",
        "    chat_if = gr.ChatInterface(\n",
        "        fn=chat_with_availability,\n",
        "        type=\"messages\",\n",
        "        additional_outputs=[context_out],\n",
        "    )\n",
        "demo.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
