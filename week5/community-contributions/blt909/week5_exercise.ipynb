{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f989236c",
      "metadata": {},
      "source": [
        "# Week 5 — RST documentation mentor (chatbot)\n",
        "\n",
        "RAG pipeline: download repo `.rst` docs → chunk & embed → Chroma → answer questions with context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "408ed304",
      "metadata": {},
      "source": [
        "**Models**\n",
        "- Chunking: Gemini 2.5 Flash  \n",
        "- Chat: Gemini 3 Flash  \n",
        "- Eval: Gemini 2.5 Flash \n",
        "- Embeddings: EmbeddingGemma-300M"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629261dc",
      "metadata": {},
      "source": [
        "## Three scripts\n",
        "\n",
        "| Script | Role |\n",
        "|--------|------|\n",
        "| **git_rst_extractor.py** | Download repo zip (tag/branch) → unzip → keep only `.rst` files. |\n",
        "| **ingest.py** | Load `.rst` → chunk with LLM (headline + summary + text) → embed (SentenceTransformer) → Chroma. Collection name = last path segment (e.g. `owner_repo_tag_v1.0`). |\n",
        "| **answer.py** | Retrieve top‑k chunks from Chroma → build system prompt with context → chat LLM → return answer + sources. |\n",
        "\n",
        "**Flow:** extractor → path → `ingest(path)` → run `app.py` with same collection name for Q&A.\n",
        "\n",
        "---\n",
        "\n",
        "## git_rst_extractor.py\n",
        "\n",
        "- **CLI:** `repo_url`, `ref`; optional `--ref-type {tag|branch}`, `-o/--output-dir`. Output under `output_dir/<owner>_<repo>_<ref_type>_<ref>/`.\n",
        "- **From code:** `download_github_repo_rst(repo_url, ref, ref_type=..., output_dir=...)` → returns `Path` to folder with only `.rst` files.\n",
        "\n",
        "---\n",
        "\n",
        "## ingest.py\n",
        "\n",
        "- **Pipeline:** `fetch_documents(path)` (subdirs → `.rst` → `{type, source, text}`) → `create_chunks(docs, repo)` (LLM returns headline/summary/original per chunk; optional `Pool`) → `create_embeddings(chunks, collection_name)` (SentenceTransformer + Chroma, batched).\n",
        "- **Entry:** `ingest(repo)` or CLI: `python ingest.py <path>`.\n",
        "\n",
        "---\n",
        "\n",
        "## answer.py\n",
        "\n",
        "- **Retrieval:** Chroma with same `DB_NAME` and embedding model; `collection_name=repo`; retriever `k=10`.\n",
        "- **Generation:** System prompt = “technical mentor for {repo}, use context; if empty say so.” Context = concatenated retrieved chunks. Returns `(answer_text, list_of_docs)`.\n",
        "\n",
        "---\n",
        "\n",
        "## Usage\n",
        "\n",
        "**CLI**\n",
        "```bash\n",
        "python git_rst_extractor.py https://github.com/owner/repo v1.0 --ref-type tag -o ./download\n",
        "python ingest.py ./download/owner_repo_tag_v1.0\n",
        "# Then run app.py; pass repo=owner_repo_tag_v1.0 for answers.\n",
        "```\n",
        "\n",
        "**From Python**\n",
        "```python\n",
        "from pathlib import Path\n",
        "from git_rst_extractor import download_github_repo_rst\n",
        "from ingest import ingest\n",
        "from answer import answer_question\n",
        "\n",
        "doc_path = download_github_repo_rst(\"https://github.com/owner/repo\", \"main\", ref_type=\"branch\", output_dir=Path(\"./download\"))\n",
        "ingest(str(doc_path))\n",
        "answer, docs = answer_question(\"How do I configure X?\", repo=doc_path.name, history=[])\n",
        "```\n",
        "Use the same `DB_NAME` and embedding model in ingest and answer.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
