{"question":"How do I use asynchronous embeddings in LiteLLM?","keywords":["aembedding","asyncio","text-embedding-ada-002"],"reference_answer":"Use the async API `aembedding(...)` inside an `async def`, then run it with `asyncio.run(...)`. The docs show model `text-embedding-ada-002` in the example.","category":"how_to"}
{"question":"What is Helicone in LiteLLM and what provider route does it use?","keywords":["Helicone","helicone/","AI gateway","observability"],"reference_answer":"Helicone is an AI gateway/observability integration in LiteLLM, and its provider route is `helicone/`.","category":"direct_fact"}
{"question":"What are the two main Helicone integration methods in LiteLLM?","keywords":["As a Provider","Callbacks","Integration Methods"],"reference_answer":"The docs list two methods: use Helicone as a provider, or use callbacks to log requests while using any provider.","category":"comparison"}
{"question":"What does LiteLLM AI Hub do?","keywords":["AI Hub","Share models and agents","organization"],"reference_answer":"AI Hub lets teams share models and agents in an organization so developers can see and use available assets without rebuilding.","category":"feature"}
{"question":"What does Humanloop integration focus on in LiteLLM?","keywords":["Humanloop","Prompt Management","Observability","Evaluation"],"reference_answer":"Humanloop integration in LiteLLM focuses on managing prompts and observability/evaluation workflows for LLM features.","category":"overview"}
