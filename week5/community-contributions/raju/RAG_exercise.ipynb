{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajuDasa/llm_engineering/blob/week5_branch/week5/community-contributions/raju/RAG_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwktaoI_uko"
      },
      "source": [
        "##RAG using Gemini models, Huggingface dataset, Chromadb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdT9xA2eMwq_"
      },
      "outputs": [],
      "source": [
        "!pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDoU9IeqHX7H"
      },
      "outputs": [],
      "source": [
        "#sample datasets - https://huggingface.co/datasets/rag-datasets/rag-mini-wikipedia\n",
        "#ref code - \"https://github.com/\" + \"google-gemini/cookbook/blob/main/examples/\" + \"chromadb/Vectordb_with_chroma.ipynb\"\n",
        "#Colab notebook - make sure HF_TOKEN and GEMINI_API_KEY are present in secrets\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "from chromadb import EmbeddingFunction, Client\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "from huggingface_hub import login\n",
        "from sentence_transformers import SentenceTransformer  #direct version\n",
        "\n",
        "#hf_token = userdata.get('HF_TOKEN')\n",
        "#login(hf_token, add_to_git_credential=True) #HF\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "gemini = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "EMBEDDING_MODEL_ID = \"gemini-embedding-001\"\n",
        "LLM_MODEL_ID = \"gemini-2.5-flash\"  #\"gemini-2.5-flash-lite\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsT-tBhzZ5C"
      },
      "source": [
        "**Load Datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzKh-gJOzOVf"
      },
      "outputs": [],
      "source": [
        "#ds_builder = load_dataset_builder(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
        "#ds_builder.info.features  # {'passage': Value('string'), 'id': Value('int64')}\n",
        "#ds_builder.info.description # empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8YpY9jIIYOT"
      },
      "outputs": [],
      "source": [
        "#HF dataset has total 3200 records, lets take first 500\n",
        "\n",
        "ds = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", split=\"passages\", streaming=True) #features: ['passage', 'id']\n",
        "ds_delta = list(ds.take(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBtlbrEPIxhY"
      },
      "outputs": [],
      "source": [
        "# Download 100 Q&A to test and evaluate - total: 918 records\n",
        "\n",
        "ds_qa = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", split=\"test\", streaming=True) #features: ['question', 'answer', 'id']\n",
        "dsqa_delta = list(ds_qa.take(100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dsqa_delta[-5:]"
      ],
      "metadata": {
        "id": "p4R1Hzf0Ocd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtered_ds_delta = [record for record in ds_delta if 'Coolidge' in record.get('passage', '')]\n",
        "#filtered_ds_delta[:5]"
      ],
      "metadata": {
        "id": "ODRTE6FpO30R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb_TBgCMzfmG"
      },
      "source": [
        "**Prepare vectorstore:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzxSvAJLNqBJ"
      },
      "outputs": [],
      "source": [
        "#using heavy model via API will limit/cost\n",
        "class CustomEmbeddingFunction(EmbeddingFunction):\n",
        "  def __init__(self, embed_model_id):\n",
        "    self.model_id = embed_model_id\n",
        "    self.title = \"wiki query\"\n",
        "    self.task_type = \"RETRIEVAL_DOCUMENT\"\n",
        "\n",
        "  def __call__(self, docs):\n",
        "    response = gemini.models.embed_content(\n",
        "        model=self.model_id,\n",
        "        contents=docs,\n",
        "        config=types.EmbedContentConfig(\n",
        "          task_type=self.task_type,\n",
        "          title=self.title\n",
        "        )\n",
        "    )\n",
        "    return response.embeddings[0].values\n",
        "\n",
        "#error: 429 RESOURCE_EXHAUSTED. limit: 100. retry in 5.666227241s => with free account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8L0Zd8wJmAj"
      },
      "outputs": [],
      "source": [
        "#using small model in local (free)\n",
        "class AllMiniEmbeddingFunction(EmbeddingFunction):\n",
        "  def __init__(self):\n",
        "    self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "  def __call__(self, docs):\n",
        "    embeddings = self.model.encode(docs, normalize_embeddings=True)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt8nXk2ALwDR"
      },
      "outputs": [],
      "source": [
        "def get_chroma_db(documents, name):\n",
        "  client = Client()\n",
        "  db = client.get_or_create_collection(\n",
        "      name = name,\n",
        "      embedding_function = AllMiniEmbeddingFunction()  #CustomEmbeddingFunction(EMBEDDING_MODEL_ID)\n",
        "  )\n",
        "\n",
        "  for docs in documents:\n",
        "    db.add(\n",
        "      documents=docs['passage'],\n",
        "      ids= str(docs['id'])\n",
        "    )\n",
        "  return db, client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db, client = get_chroma_db(ds_delta, \"Wiki_DB\")"
      ],
      "metadata": {
        "id": "1m960a0mQhqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF_WcxiCRm3Y"
      },
      "outputs": [],
      "source": [
        "#clear coll to start fresh\n",
        "#Client().delete_collection(name=\"Wiki_DB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcjYfGcEDVwJ"
      },
      "outputs": [],
      "source": [
        "def get_context(query):\n",
        "  docs = db.query(query_texts=[query], n_results=8)['documents'][0]\n",
        "  return docs or []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If topic not found in context, return irrevant content\n",
        "get_context('Do beetles antennae function primarily as organs of smell')"
      ],
      "metadata": {
        "id": "QpfkWMiCWXwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect with Model:**"
      ],
      "metadata": {
        "id": "i0rPsX-ZoV3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjWfcN7XEhE-"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(**kwargs):  #query, context\n",
        "  prompt = (\"\"\"\n",
        "    You are a helpful assistant who answers questions using the context provided below.\n",
        "    Respond with short answer, e.g: 'yes', 'no', '18 months'.\n",
        "    Only use the provided context as grounding true source.\n",
        "    If no context is provided or it is irrelevant, respond with - <I don't know>\n",
        "\n",
        "    QUESTION: '{query}'\n",
        "    CONTEXT: '{context}'\n",
        "\n",
        "    ANSWER:\n",
        "  \"\"\").format(**kwargs)  #query=query, context=context\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtvSweIs_-8Z"
      },
      "outputs": [],
      "source": [
        "def get_answer_for(question):\n",
        "  context = get_context(question)\n",
        "  context = \"\\n\".join(context)\n",
        "  prompt = generate_prompt(query=question, context=context)\n",
        "  #print(prompt)\n",
        "  answer = gemini.models.generate_content(\n",
        "      model = LLM_MODEL_ID,\n",
        "      contents = prompt\n",
        "  )\n",
        "  return answer.text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test and Evaluate RAG:**\n"
      ],
      "metadata": {
        "id": "6HDhDIswnVJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Manual testing:\n",
        "get_answer_for(\"Who or what vary greatly in form within the coleoptera?\")"
      ],
      "metadata": {
        "id": "nb21sIDYZL5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "#Auto test - 5 tests per topic\n",
        "def evaluate(qa_data):\n",
        "  if not qa_data:\n",
        "    display(\"Empty Q&A set\")\n",
        "    return\n",
        "\n",
        "  limit = min(5, len(qa_data))\n",
        "  for i in range(0, limit):\n",
        "    idx = i  #random.randint(0, len(qa_data)-1)\n",
        "    qa = qa_data[idx]\n",
        "    qa_data\n",
        "    gen_answer = get_answer_for(qa[\"question\"])\n",
        "    display(qa[\"id\"], f\"Question: {qa[\"question\"]}\\n Actual: {qa[\"answer\"]}\\n Generated: {gen_answer}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "YB_g_Q3-mPI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dsqa_delta has questions whose topics are not present in ds_delta (context)\n",
        "#choose questions based on topics in ds_delta\n",
        "\n",
        "TOPIC = \"Coolidge\"\n",
        "qa_data = [record for record in dsqa_delta if TOPIC in record.get('question', '')]\n",
        "evaluate(qa_data)"
      ],
      "metadata": {
        "id": "v2CoSlSrbaL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4dLQOpOwtvv3VPyoSdNTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}