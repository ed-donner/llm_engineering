{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644753e7-17f3-4999-a37a-b6aebf1e4579",
   "metadata": {},
   "source": [
    "# The Huberman Lab Chatbot\n",
    "\n",
    "Created with a selection of episodes from The Hubermand Lab podcasts taken from YouTube. We pull the transcripts from them and let OpenAI create a structured report in markdown. \n",
    "\n",
    "Tried Ollama too, but instead of generating comprehensive reports, it created  brief summaries. Too much knowledge loss.\n",
    "\n",
    "I was also wondering if it would be better to use full transcripts instead of structurized versions of the transcripts but chose the structured approach after reading about the pros and cons for both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, VideoUnavailable\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\"\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "OLLAMA_MODEL=\"llama3.2\"\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['YOUTUBE_API_KEY'] = os.getenv('YOUTUBE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894e742-82ba-4f5c-af59-9a6d22af0022",
   "metadata": {},
   "source": [
    "## You need to pip install some stuff\n",
    "\n",
    "The google api python client and youtube transcript api.\n",
    "\n",
    "You also need to a Youtube API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ac42c-0649-40c0-8dac-82de95cde140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-api-python-client youtube-transcript-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c03fe-85bb-4b40-9e51-9017c5dc49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade google-api-python-client pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a1e94-cd73-4f1b-9875-791b14f58c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the API key from an environment variable\n",
    "YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "if YOUTUBE_API_KEY is None:\n",
    "    print(\"Error: YOUTUBE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set the environment variable or ensure it's correctly loaded.\")\n",
    "    # Exiting here as the Google API part won't work without the key\n",
    "    exit()\n",
    "\n",
    "def get_video_id(youtube_url):\n",
    "    \"\"\"\n",
    "    Extracts the video ID from various YouTube URL formats.\n",
    "    Handles standard, shortened (youtu.be), and googleusercontent.com proxy URLs.\n",
    "    \"\"\"\n",
    "    # Regex to capture the 11-character video ID\n",
    "    # Handles:\n",
    "    # - https://www.youtube.com/watch?v=VIDEO_ID\n",
    "    # - https://youtu.be/VIDEO_ID\n",
    "    # - https://www.youtube.com/embed/VIDEO_ID\n",
    "    # - youtu.be//VIDEO_ID (common proxy format)\n",
    "    video_id_match = re.search(r\"(?:v=|youtu\\.be\\/|embed\\/|youtube\\.com\\/)([a-zA-Z0-9_-]{11})\", youtube_url)\n",
    "    if video_id_match:\n",
    "        return video_id_match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_youtube_info_with_api(youtube_url, api_key):\n",
    "    \"\"\"\n",
    "    Extracts video title and transcript using YouTube Data API and youtube-transcript-api.\n",
    "\n",
    "    Args:\n",
    "        youtube_url (str): The full YouTube video URL.\n",
    "        api_key (str): Your YouTube Data API v3 key.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (video_title, transcript_text) or (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    video_id = get_video_id(youtube_url)\n",
    "    if not video_id:\n",
    "        print(\"Error: Could not extract video ID from the URL. Please ensure it's a valid YouTube video URL format.\")\n",
    "        return None, None\n",
    "\n",
    "    video_title = None\n",
    "    transcript_text = None\n",
    "\n",
    "    # --- Step 1: Get video title using YouTube Data API ---\n",
    "    try:\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if response and response.get('items'):\n",
    "            video_title = response['items'][0]['snippet']['title']\n",
    "        else:\n",
    "            print(f\"Could not find video details for ID: {video_id} using YouTube Data API. It might be private, deleted, or geo-restricted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching video title with YouTube Data API: {e}\")\n",
    "\n",
    "    # --- Step 2: Get transcript using youtube-transcript-api ---\n",
    "    try:\n",
    "        # Attempt to get transcript in English or US English first.\n",
    "        # If you know the specific language, you can specify it, e.g., languages=['es'] for Spanish.\n",
    "        # If you want to try all available transcripts, you can remove the 'languages' parameter,\n",
    "        # but then you'd need to iterate through the transcript list to find the one you want.\n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'en-US'])\n",
    "        transcript_text = \" \".join([entry['text'] for entry in transcript_list])\n",
    "    except NoTranscriptFound:\n",
    "        print(f\"No transcript found for video ID: {video_id} in the specified languages (en, en-US).\")\n",
    "        print(\"This video might not have an auto-generated or uploaded transcript, or it's in a different language.\")\n",
    "        transcript_text = None\n",
    "    except VideoUnavailable:\n",
    "        print(f\"Video with ID: {video_id} is unavailable for transcript retrieval (e.g., private, deleted, or geo-restricted).\")\n",
    "        transcript_text = None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching transcript: {e}\")\n",
    "        transcript_text = None\n",
    "\n",
    "    return video_title, transcript_text\n",
    "\n",
    "# Example Usage for testing:\n",
    "\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=Z3OpxT65fKw\" \n",
    "\n",
    "title, transcript = get_youtube_info_with_api(youtube_url, YOUTUBE_API_KEY)\n",
    "\n",
    "if title:\n",
    "    print(f\"Video Title: {title}\\n\")\n",
    "\n",
    "if transcript:\n",
    "    print(\"--- Transcript ---\")\n",
    "    print(transcript[:500] + \"...\" if len(transcript) > 500 else transcript) # Print first 500 chars\n",
    "elif title and not transcript:\n",
    "    print(\"Transcript not available for this video (as indicated above).\")\n",
    "else:\n",
    "    print(\"Failed to retrieve any video information. Please double-check the URL, your API key, and internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e89c5f-d557-4e14-afa1-7debf1f7dd06",
   "metadata": {},
   "source": [
    "## Creating the structured markdown versions of the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863410e9-c899-4ee7-94c5-e79d63a0bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You're an assistants that converts raw transcripts to markdown files.\"\n",
    "system_message += \"You will rewrite the entire transcript and add structure using headings, paragraphs, lists etc.\"\n",
    "system_message += \"You will make sure not to leave out any important information. This is not a summary, \\\n",
    "but an comprehenisve report of the conversation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d450b24-9220-4c99-93c0-072626f4cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Convert this transcript to structured markdown. Use the video title as h1 in resulting markdown: \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349341dc-01f6-47fc-9610-fec9777a5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown(youtube_url):\n",
    "    video_title, transcript_text = get_youtube_info_with_api(youtube_url, YOUTUBE_API_KEY)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message + video_title + transcript_text}\n",
    "          ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd2a79-88d2-403d-a44d-51ce9c4b98cb",
   "metadata": {},
   "source": [
    "## Creating the markdown files from a list of Youtube urls\n",
    "\n",
    "The list of urls in the list includes: \n",
    "- the guest series about exercise with Dr. Andy Galpin\n",
    "- the guest series about sleep with Dr. Matthew Walker\n",
    "- two episodes with Dr. Layne Norton about exercise and nutrition\n",
    "- the episode with Dr. Gabrielle Lyon about exercise and nutrition\n",
    "- the episode about the vagus nerve\n",
    "- the episode about how to grow by doing hard things with Michael Easter\n",
    "- the episode about intermittent fasting with Dr. Satchin Panda\n",
    "- the episode about building strength, endurance with Pavel Tsatsouline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03674de-a1d0-4ef3-abd5-0f68dccc60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('youtube-video-list.txt', 'r', encoding='utf-8') as youtube_list:\n",
    "    raw_lines = youtube_list.readlines()\n",
    "\n",
    "    # To remove the newline characters and any leading/trailing whitespace\n",
    "    clean_list = [line.strip() for line in raw_lines]\n",
    "\n",
    "for youtube_url in clean_list:\n",
    "    transcript_in_markdown = create_markdown(youtube_url)\n",
    "    \n",
    "    with open(f\"Huberman/{transcript_in_markdown[2:25]}.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write(transcript_in_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb306e-d804-4fc2-a130-7f1e68ff4e1c",
   "metadata": {},
   "source": [
    "## Adding a structured report from a single Youtube url to expand the RAG database\n",
    "\n",
    "I've added categorized folders as in the example. Not sure if this helps if you're not creating visualisations. You need to add newly created .md files to an appropriate folder or create a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a0cf2-46bf-497c-8782-15565f83ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = 'https://www.youtube.com/watch?v=zqANjUGarAw&t=16s'\n",
    "\n",
    "transcript_in_markdown = create_markdown(youtube_url)\n",
    "    \n",
    "with open(f\"Huberman/{transcript_in_markdown[2:25]}.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(transcript_in_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4745a-0a6c-4544-b78b-c827cfec1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our Huberman folder\n",
    "\n",
    "folders = glob.glob(\"Huberman/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d51bf-2b11-4703-89e9-93fd2ab4850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95effd99-523a-4f26-9725-ca3bea6359e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f220724-b2e1-40d0-92c0-345b764871bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdde646-16be-4113-bfd3-a3fbc2307734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20193b34-b51c-4f18-a426-669724884a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4e9e9-6471-4647-89c0-ff28a577a126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
