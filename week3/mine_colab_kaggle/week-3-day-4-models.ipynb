{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Models\n\nLooking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n\nThis notebook can run on a low-cost or free T4 runtime.\n","metadata":{"id":"aKs1PM-O-VQa"}},{"cell_type":"markdown","source":"## One more reminder\n\n**Pro-tip:**\n\nIn the middle of running a Colab, you might get an error like this:\n\n> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n\nThis is a super-misleading error message! Please don't try changing versions of packages...\n\nThis actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n\n1. Kernel menu >> Disconnect and delete runtime\n2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n3. Connect to a new T4 using the button at the top right\n4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n5. Rerun the cells in the colab, from the top down, starting with the pip installs\n\nAnd all should work great - otherwise, ask me!","metadata":{"id":"WTDQBZpH25QB"}},{"cell_type":"markdown","source":"# # Run P100 kaggle","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade bitsandbytes accelerate","metadata":{"id":"QJ1qpySJTtGy","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:05.555761Z","iopub.execute_input":"2026-02-06T04:43:05.556487Z","iopub.status.idle":"2026-02-06T04:43:08.943755Z","shell.execute_reply.started":"2026-02-06T04:43:05.556459Z","shell.execute_reply":"2026-02-06T04:43:08.943012Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Let's check the GPU \n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)\n  if gpu_info.find('P100') >= 0:\n    print(\"Success - Connected to P100\")\n  else:\n    print(\"NOT CONNECTED TO A P100\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:08.945566Z","iopub.execute_input":"2026-02-06T04:43:08.945842Z","iopub.status.idle":"2026-02-06T04:43:09.003983Z","shell.execute_reply.started":"2026-02-06T04:43:08.945814Z","shell.execute_reply":"2026-02-06T04:43:09.003432Z"}},"outputs":[{"name":"stdout","text":"Fri Feb  6 04:43:08 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             34W /  250W |    2583MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nSuccess - Connected to P100\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"#For colab\n```python\nfrom google.colab import userdata\nfrom huggingface_hub import login\n```\n","metadata":{}},{"cell_type":"code","source":"#IMPORTS\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\nimport gc\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.004769Z","iopub.execute_input":"2026-02-06T04:43:09.005003Z","iopub.status.idle":"2026-02-06T04:43:09.010298Z","shell.execute_reply.started":"2026-02-06T04:43:09.004982Z","shell.execute_reply":"2026-02-06T04:43:09.009568Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Sign in to Hugging Face\n\n1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions by clicking on the WRITE tab\n\n2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n`HF_TOKEN = your_token`\n\n3. Execute the cell below to log in.","metadata":{"id":"xyKWKWSw7Iqp"}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nif hf_token and hf_token.startswith(\"hf_\"):\n    print(\"HF key looks good so far\")\n    login(hf_token)\nelse:\n    print(\"HF key is not set correctly\")","metadata":{"id":"xd7cEDUC6Lkq","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.011882Z","iopub.execute_input":"2026-02-06T04:43:09.012123Z","iopub.status.idle":"2026-02-06T04:43:09.492775Z","shell.execute_reply.started":"2026-02-06T04:43:09.012102Z","shell.execute_reply":"2026-02-06T04:43:09.492122Z"}},"outputs":[{"name":"stdout","text":"HF key looks good so far\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"### Accessing Llama\n\nYesterday you should have received approval to use this model:\n\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n\nYou can either use that today, or it's faster if you get approval for this model too.\n\nhttps://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n\nSelect this link to see if you need to request approval too. Pick the version of Llama that you want below by commenting out one of these! Or skip Llama altogether.","metadata":{"id":"5yQI6rGLkLrK"}},{"cell_type":"code","source":"# instruct models and 1 reasoning model\n\n# Llama 3.1 is larger and you should already be approved\n# see here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n\n# LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Llama 3.2 is smaller but you might need to request access again\n# see here: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n\nLLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n\nPHI = \"microsoft/Phi-4-mini-instruct\"\nGEMMA = \"google/gemma-3-270m-it\"\nQWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\nDEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"","metadata":{"id":"UtN7OKILQato","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.493579Z","iopub.execute_input":"2026-02-06T04:43:09.493888Z","iopub.status.idle":"2026-02-06T04:43:09.498198Z","shell.execute_reply.started":"2026-02-06T04:43:09.493859Z","shell.execute_reply":"2026-02-06T04:43:09.497458Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a knowledgeable professor and a great Researcher in AI field\"},\n    \n    {\"role\": \"user\", \"content\": \"What is Physics Informed Neural Network\"}\n  ]","metadata":{"id":"KgxCLBJIT5Hx","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.499075Z","iopub.execute_input":"2026-02-06T04:43:09.499306Z","iopub.status.idle":"2026-02-06T04:43:09.512727Z","shell.execute_reply.started":"2026-02-06T04:43:09.499277Z","shell.execute_reply":"2026-02-06T04:43:09.512091Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"# Accessing Llama 3.1 from Meta\n\nIn order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n\nVisit their model instructions page in Hugging Face:\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n\nAt the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n\nIn my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models.\n\nIf you have any problems accessing Llama, please see this colab, including some suggestions if you don't get approved by Meta for any reason.\n\nhttps://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8","metadata":{"id":"ZSiYqPn87msu"}},{"cell_type":"code","source":"# Quantization Config - this allows us to load the model into memory and use less memory\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\"\n)","metadata":{"id":"hhOgL1p_T6-b","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.513726Z","iopub.execute_input":"2026-02-06T04:43:09.514086Z","iopub.status.idle":"2026-02-06T04:43:09.526001Z","shell.execute_reply.started":"2026-02-06T04:43:09.514053Z","shell.execute_reply":"2026-02-06T04:43:09.525370Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"If the next cell gives you a 403 permissions error, then please check:\n1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n2. Did you set up your API key with full read and write permissions?\n3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n\nAnd work through my Llama troubleshooting colab:\n\nhttps://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8\n","metadata":{"id":"7MtWZYZG920F"}},{"cell_type":"code","source":"# Tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(LLAMA)\ntokenizer.pad_token = tokenizer.eos_token\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")","metadata":{"id":"Zi8YXiwJHF59","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:09.526733Z","iopub.execute_input":"2026-02-06T04:43:09.526983Z","iopub.status.idle":"2026-02-06T04:43:10.387239Z","shell.execute_reply.started":"2026-02-06T04:43:09.526952Z","shell.execute_reply":"2026-02-06T04:43:10.386677Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"inputs","metadata":{"id":"J_HHBEjF8ion","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:10.388099Z","iopub.execute_input":"2026-02-06T04:43:10.388296Z","iopub.status.idle":"2026-02-06T04:43:10.394930Z","shell.execute_reply.started":"2026-02-06T04:43:10.388277Z","shell.execute_reply":"2026-02-06T04:43:10.394178Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n            220,   2705,  13806,    220,   2366,     21,    271,   2675,    527,\n            264,  42066,  14561,    323,    264,   2294,   8483,    261,    304,\n          15592,   2115, 128009, 128006,    882, 128007,    271,   3923,    374,\n          28415,    763,  10365,  61577,   8304, 128009]], device='cuda:0')"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# The model\n\nmodel = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)","metadata":{"id":"S5jly421tno3","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:10.397816Z","iopub.execute_input":"2026-02-06T04:43:10.398100Z","iopub.status.idle":"2026-02-06T04:43:13.948773Z","shell.execute_reply.started":"2026-02-06T04:43:10.398069Z","shell.execute_reply":"2026-02-06T04:43:13.948107Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"memory = model.get_memory_footprint() / 1e6\nprint(f\"Memory footprint: {memory:,.1f} MB\")","metadata":{"id":"bdbYaT8hWXWE","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:13.949581Z","iopub.execute_input":"2026-02-06T04:43:13.949857Z","iopub.status.idle":"2026-02-06T04:43:13.955042Z","shell.execute_reply.started":"2026-02-06T04:43:13.949826Z","shell.execute_reply":"2026-02-06T04:43:13.954351Z"}},"outputs":[{"name":"stdout","text":"Memory footprint: 1,012.0 MB\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"## Looking under the hood at the Transformer model\n\nThe next cell prints the HuggingFace `model` object for Llama.\n\nThis model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n\nWhile we're not going to go deep into the theory, this is an opportunity to get some intuition for what the Transformer actually is.\n\nIf you're completely new to Neural Networks, check out my [YouTube intro playlist](https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs) for the foundations.\n\nNow take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n\n- It consists of layers\n- There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors. We'll learn more about this in Week 5.\n- There are then 16 sets of groups of layers (32 for Llama 3.1) called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n- There is an LM Head layer at the end; this produces the output\n\nNotice the mention that the model has been quantized to 4 bits.\n\nIt's not required to go any deeper into the theory at this point, but if you'd like to, I've asked our mutual friend to take this printout and make a tutorial to walk through each layer. This also looks at the dimensions at each point. If you're interested, work through this tutorial after running the next cell:\n\nhttps://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b","metadata":{"id":"w5mcojpzrD_y"}},{"cell_type":"code","source":"# Execute this cell and look at what gets printed; investigate the layers\n\nmodel","metadata":{"id":"P0qmAD5ZtqWA","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:13.955788Z","iopub.execute_input":"2026-02-06T04:43:13.956484Z","iopub.status.idle":"2026-02-06T04:43:13.968724Z","shell.execute_reply.started":"2026-02-06T04:43:13.956451Z","shell.execute_reply":"2026-02-06T04:43:13.967987Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":52},{"cell_type":"markdown","source":"### And if you want to go even deeper into Transformers\n\nIn addition to looking at each of the layers in the model, you can actually look at the HuggingFace code that implements Llama using PyTorch.\n\nHere is the HuggingFace Transformers repo:  \nhttps://github.com/huggingface/transformers\n\nAnd within this, here is the code for Llama 4:  \nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n\nObviously it's not neceesary at all to get into this detail - the job of an AI engineer is to select, optimize, fine-tune and apply LLMs rather than to code a transformer in PyTorch. OpenAI, Meta and other frontier labs spent millions building and training these models. But it's a fascinating rabbit hole if you're interested!","metadata":{"id":"Kx_0SygM_nmA"}},{"cell_type":"code","source":"# OK, with that, now let's run the model!\n\noutputs = model.generate(inputs, max_new_tokens=80)\noutputs[0]","metadata":{"id":"SkYEXzbotcud","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:13.969588Z","iopub.execute_input":"2026-02-06T04:43:13.969848Z","iopub.status.idle":"2026-02-06T04:43:17.335233Z","shell.execute_reply.started":"2026-02-06T04:43:13.969828Z","shell.execute_reply":"2026-02-06T04:43:17.334521Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n           220,   2705,  13806,    220,   2366,     21,    271,   2675,    527,\n           264,  42066,  14561,    323,    264,   2294,   8483,    261,    304,\n         15592,   2115, 128009, 128006,    882, 128007,    271,   3923,    374,\n         28415,    763,  10365,  61577,   8304, 128009, 128006,  78191, 128007,\n           271,  34999,  32600,  10365,  61577,  39810,    320,  57161,  48460,\n             8,    374,    264,    955,    315,   5655,   6975,   1646,    430,\n         33511,    279,   2410,    315,  30828,  14488,    449,    279,  37072,\n         78477,    315,   7276,  41264,  39006,    320,     47,   1170,     82,\n           570,   1102,    574,  11784,    555,  18989,    383,    273,     75,\n           323,    426,   3102,   6672,    304,    220,    679,     24,    439,\n           264,   1648,    311,  11886,   7276,  41264,  39006,    320,     47,\n          1170,     82,      8,    304,    264,  22027,  53161,  11827,    382,\n           791,   6913,   4623,   4920,  28228], device='cuda:0')"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# Well that doesn't make much sense!\n# How about this..\n\ntokenizer.decode(outputs[0])","metadata":{"id":"ZXXnANipSUel","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:17.336205Z","iopub.execute_input":"2026-02-06T04:43:17.336535Z","iopub.status.idle":"2026-02-06T04:43:17.341760Z","shell.execute_reply.started":"2026-02-06T04:43:17.336501Z","shell.execute_reply":"2026-02-06T04:43:17.341050Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 Feb 2026\\n\\nYou are a knowledgeable professor and a great Researcher in AI field<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is Physics Informed Neural Network<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nPhysics-Informed Neural Networks (PINNs) is a type of deep learning model that combines the power of neural networks with the mathematical rigor of partial differential equations (PDEs). It was introduced by Rahelel and Bouchette in 2019 as a way to solve partial differential equations (PDEs) in a physics-inspired manner.\\n\\nThe basic idea behind PIN'"},"metadata":{}}],"execution_count":54},{"cell_type":"markdown","source":"# to clear all memory\n```python\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n```","metadata":{}},{"cell_type":"code","source":"# Clean up memory\n# Thank you Kuan L. for helping me get this to properly free up memory!\n# If you select \"Show Resources\" on the top right to see GPU memory, it might not drop down right away\n# But it does seem that the memory is available for use by new models in the later code.\n\n\ndel model, inputs, tokenizer, outputs\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"id":"2oL0RWU2ttZf","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:17.342659Z","iopub.execute_input":"2026-02-06T04:43:17.343367Z","iopub.status.idle":"2026-02-06T04:43:17.864331Z","shell.execute_reply.started":"2026-02-06T04:43:17.343334Z","shell.execute_reply":"2026-02-06T04:43:17.863612Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"## A couple of quick notes on the next block of code:\n\nI'm using a HuggingFace utility called TextStreamer so that results stream back.\nTo stream results, we simply replace:  \n`outputs = model.generate(inputs, max_new_tokens=80)`  \nWith:  \n`streamer = TextStreamer(tokenizer)`  \n`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n\nAlso I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n\nhttps://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n\nThank you to student Piotr B for raising the issue!","metadata":{"id":"iDCeJ20e4Hxx"}},{"cell_type":"code","source":"# Wrapping everything in a function - and adding Streaming and generation prompts\n\ndef generate(model, messages, quant=True, max_new_tokens=80):\n  tokenizer = AutoTokenizer.from_pretrained(model)\n  tokenizer.pad_token = tokenizer.eos_token\n  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n  streamer = TextStreamer(tokenizer)\n  if quant:\n    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n  else:\n    model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n\n","metadata":{"id":"RO_VYZ3DZ7cs","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:17.865313Z","iopub.execute_input":"2026-02-06T04:43:17.865642Z","iopub.status.idle":"2026-02-06T04:43:17.871293Z","shell.execute_reply.started":"2026-02-06T04:43:17.865610Z","shell.execute_reply":"2026-02-06T04:43:17.870623Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"generate(PHI, messages)","metadata":{"id":"RFjaY4Pdvbfy","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:43:17.872107Z","iopub.execute_input":"2026-02-06T04:43:17.872295Z","iopub.status.idle":"2026-02-06T04:44:04.862434Z","shell.execute_reply.started":"2026-02-06T04:43:17.872276Z","shell.execute_reply":"2026-02-06T04:44:04.861511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a621ca539f4871a8e7e274e347f5fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba78edf003634c4f852ff2f4060f02ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e6ded9b2664a06b1e21cd1069f5e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43751a7c4cd149f6b924d102352f6213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ded0bc3bd84e09868ef65934098927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfcbb5f582c4eacbef9d5ae022c7362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb752661cc4f4e3b81bc9df6c9cd8fd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce41e76ee444e9588fc0e107f4d1d8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aab1af33892468e86c1ce496e383faf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e7b23230e44e51be51b190f3324875"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c68b795eaea4eb386cfe3d9f68ec22b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c763b02878f84900b4e3f6808fe1a30d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e835c09a6e624a1a91ee389745acbe45"}},"metadata":{}},{"name":"stdout","text":"<|system|>You are a knowledgeable professor and a great Researcher in AI field<|end|><|user|>What is Physics Informed Neural Network<|end|><|assistant|>Physics-Informed Neural Networks (PINNs) are a novel approach to solving differential equations by leveraging the power of neural networks. The core idea behind PINNs is to incorporate the underlying physical laws, expressed as differential equations, directly into the neural network training process. This integration helps ensure that the solutions generated by the neural network are consistent with the known physics of the problem.\n\nHere's a more detailed breakdown of\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## Accessing Gemma from Google\n\nA student let me know (thank you, Alex K!) that Google also now requires you to accept their terms in HuggingFace before you use Gemma.\n\nPlease visit their model page at this link and confirm you're OK with their terms, so that you're granted access.\n\nhttps://huggingface.co/google/gemma-3-270m-it","metadata":{"id":"hxZQmZDCe4Jf"}},{"cell_type":"code","source":"# gemma don't accept system prompt only user\nmessages = [\n    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n  ]\ngenerate(GEMMA, messages, quant=False)","metadata":{"id":"q1JW41D-viGy","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:45:20.944970Z","iopub.execute_input":"2026-02-06T04:45:20.945659Z","iopub.status.idle":"2026-02-06T04:45:34.408227Z","shell.execute_reply.started":"2026-02-06T04:45:20.945625Z","shell.execute_reply":"2026-02-06T04:45:34.407341Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152a64bc727f443b9cbeec6a19ec22c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51dca5f752ac4ebc9b8c5b17722e3b08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b98c42f22a4c338c588941445b414d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf830e00d4c747748ece5d4a3ff6d64c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5744cd75a70a480387495a04a5f45997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c875d209614da2942a209e58bab6cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f767dfc540c4c08a03c64e42b10bdac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a007e8fbe04fdd9834f09fbe532232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40641f569e4477e8eec33ce08b0ea52"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<bos><start_of_turn>user\nTell a light-hearted joke for a room of Data Scientists<end_of_turn>\n<start_of_turn>model\nWhy don't scientists ever get a snack? \n\nBecause they're always too busy staring at the data!\n<end_of_turn>\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"messages1 = [\n    {\"role\": \"system\", \"content\": \"You are a knowledgeable professor and a great Researcher in AI field\"},\n    \n    {\"role\": \"user\", \"content\": \"What is Physics Informed Neural Network\"}\n  ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:47:17.753254Z","iopub.execute_input":"2026-02-06T04:47:17.753855Z","iopub.status.idle":"2026-02-06T04:47:17.757670Z","shell.execute_reply.started":"2026-02-06T04:47:17.753823Z","shell.execute_reply":"2026-02-06T04:47:17.756855Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"generate(QWEN, messages1)","metadata":{"id":"0m8yjMB3ZTp3","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:47:18.532483Z","iopub.execute_input":"2026-02-06T04:47:18.532792Z","iopub.status.idle":"2026-02-06T04:47:38.107105Z","shell.execute_reply.started":"2026-02-06T04:47:18.532764Z","shell.execute_reply":"2026-02-06T04:47:38.106464Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1af4161c274429486eab5f09e6744c6"}},"metadata":{}},{"name":"stdout","text":"<|im_start|>system\nYou are a knowledgeable professor and a great Researcher in AI field<|im_end|>\n<|im_start|>user\nWhat is Physics Informed Neural Network<|im_end|>\n<|im_start|>assistant\nGreat question!\n\n**Physics-Informed Neural Networks (PINNs)** are a class of deep learning models that integrate physical laws—typically expressed as partial differential equations (PDEs)—directly into the training process of neural networks. Instead of relying solely on data-driven learning (like traditional machine learning), PINNs enforce the underlying physics of a system as constraints during the neural network training, making them especially\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"generate(DEEPSEEK, messages1, quant=False, max_new_tokens=500)","metadata":{"id":"SyTvb4ESS3u_","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:47:38.108303Z","iopub.execute_input":"2026-02-06T04:47:38.108568Z","iopub.status.idle":"2026-02-06T04:48:19.868351Z","shell.execute_reply.started":"2026-02-06T04:47:38.108544Z","shell.execute_reply":"2026-02-06T04:48:19.867510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc312c11e7a40f68ae3c4b6c12bbf52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72aec3d5d3a04f7bbf84016975e5303a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81750dcf40e442aa8078a703add7f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b952a7ead94c9a9269a7160e881c57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27509c390e974895a01c23fc8f434210"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<｜begin▁of▁sentence｜>You are a knowledgeable professor and a great Researcher in AI field<｜User｜>What is Physics Informed Neural Network<｜Assistant｜><think>\nAlright, so I need to understand what a Physics Informed Neural Network (PINN) is. I've heard the term before, especially in the context of machine learning and scientific computing. From what I gather, it's a type of neural network that incorporates physical laws into its training process. But I'm not entirely sure how that works or why it's useful.\n\nFirst, I know neural networks are a broad class of machine learning models inspired by the structure and function of the human brain. They're used for a variety of tasks like image recognition, natural language processing, and more. But they don't inherently consider the laws of physics. So, how can we make them more aligned with real-world phenomena?\n\nI think the idea is to use the known equations of physics, like Newton's laws, Maxwell's equations, or the laws of thermodynamics, to inform the neural network. By doing so, the network can learn solutions to these equations rather than just fit the data. That makes sense because real-world systems often obey these laws, and incorporating them can lead to more accurate predictions.\n\nSo, how exactly does this work? Maybe the neural network is trained on data, but during the training process, it also enforces these physical laws. For example, if we're modeling heat diffusion, the network would learn the partial differential equations that describe this process, including the diffusion coefficient and other parameters.\n\nI'm also curious about the advantages of using PINNs. They seem promising for solving complex problems where traditional numerical methods are difficult to apply. They can handle high-dimensional problems and provide solutions in real-time, which is great for applications in fields like climate modeling or drug discovery.\n\nBut I'm not sure about the implementation details. How do you integrate physical laws into the neural network architecture? Are there specific layers or techniques used for this? Also, what kind of data do you need to train a PINN effectively?\n\nI remember reading that PINNs can be used for solving partial differential equations. So, perhaps the network is structured to approximate the solution of these equations. The loss function in neural networks is typically the difference between the predicted output and the actual data. In PINNs, this loss function is augmented with additional terms that represent the physical laws. This way, the network not only fits the training data but also adheres to the underlying physics.\n\nAnother thing I'm thinking about is the role of boundary and initial conditions. These are essential in solving differential equations. PINNs probably enforce these conditions\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"id":"ZWCHrOD-1xhp","trusted":true},"outputs":[],"execution_count":null}]}