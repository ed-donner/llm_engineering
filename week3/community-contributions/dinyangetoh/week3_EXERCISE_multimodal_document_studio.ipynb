{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1af1c278",
      "metadata": {},
      "source": [
        "# üóÇÔ∏è Multi-Modal Document Studio\n",
        "**Week 3 Exercise ‚Äî End-to-End Open-Source Document Analysis**\n",
        "\n",
        "Automates first-pass review of any document (contract, email, article, policy) using only open-source models ‚Äî no API keys required.\n",
        "\n",
        "**Produces:**\n",
        "- Token structure & chat template preview\n",
        "- Structured LLM brief auto-adapted to document type\n",
        "- Named entity extraction (people, orgs, locations)\n",
        "- Per-clause risk scoring\n",
        "- Overall tone / sentiment analysis\n",
        "\n",
        "---\n",
        "**Hardware:** Set `DEVICE = \"mps\"` for Apple Silicon (default) or `DEVICE = \"cuda\"` for NVIDIA GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71cad78",
      "metadata": {},
      "source": [
        "## Cell 1 ‚Äî Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "02490e23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages installed. Restart the kernel, then run from Cell 2 onwards.\n"
          ]
        }
      ],
      "source": [
        "# Run once ‚Äî restart kernel after installation\n",
        "!uv pip install -q transformers torch accelerate pdfplumber gradio sentencepiece optimum-quanto huggingface_hub\n",
        "\n",
        "print(\"‚úÖ All packages installed. Restart the kernel, then run from Cell 2 onwards.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9cc510",
      "metadata": {},
      "source": [
        "## Cell 2 ‚Äî HuggingFace Authentication\n",
        "\n",
        "Models are downloaded from the [HuggingFace Hub](https://huggingface.co). You need a free account and an access token.\n",
        "\n",
        "1. Go to https://huggingface.co/settings/tokens\n",
        "2. Create a token with **Read** permissions\n",
        "3. Set it as an environment variable before launching Jupyter: `export HF_TOKEN=hf_...`  \n",
        "   ‚Äî or paste it directly into the cell below (avoid committing it to git)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5ab84e45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF key looks good so far\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Logged in to HuggingFace Hub.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = os.environ.get('HF_TOKEN')\n",
        "if hf_token and hf_token.startswith(\"hf_\"):\n",
        "  print(\"HF key looks good so far\")\n",
        "else:\n",
        "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "print(\"‚úÖ Logged in to HuggingFace Hub.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34067df5",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Constants & Device Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7d9f0915",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device      : mps\n",
            "Quant backend: optimum-quanto\n",
            "\n",
            "‚úÖ Constants set.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ‚îÄ‚îÄ Device ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if torch.backends.mps.is_available(): # Macbook Silicon\n",
        "    DEVICE = \"mps\"\n",
        "    QUANT_BACKEND = \"optimum-quanto\"   # MPS path\n",
        "elif torch.cuda.is_available(): # NVIDIA GPU\n",
        "    DEVICE = \"cuda\"\n",
        "    QUANT_BACKEND = \"bitsandbytes\"     # CUDA path\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    QUANT_BACKEND = \"none\"\n",
        "\n",
        "print(f\"Device      : {DEVICE}\")\n",
        "print(f\"Quant backend: {QUANT_BACKEND}\")\n",
        "\n",
        "# ‚îÄ‚îÄ Model IDs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "NER_MODEL         = \"dslim/bert-base-NER\"\n",
        "ZERO_SHOT_MODEL    = \"facebook/bart-large-mnli\"      # Zero-shot classification\n",
        "SENTIMENT_MODEL    = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "MODEL          = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
        "\n",
        "# ‚îÄ‚îÄ Risk labels for zero-shot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "RISK_LABELS = [\n",
        "    \"low risk\", \"medium risk\", \"high risk\",\n",
        "    \"financial obligation\", \"liability\", \"termination clause\",\n",
        "    \"data privacy\", \"intellectual property\", \"indemnification\"\n",
        "]\n",
        "\n",
        "print(\"\\n‚úÖ Constants set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93c7d57",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Utility: Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d492dc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text extractor ready.\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber, pathlib\n",
        "\n",
        "def extract_text(source) -> str:\n",
        "    \"\"\"Extract text from a PDF path, TXT path, or raw string.\"\"\"\n",
        "    if source is None:\n",
        "        return \"\"\n",
        "    if hasattr(source, \"name\"):          # Gradio UploadedFile\n",
        "        source = source.name\n",
        "    # Raw text strings (multi-line or long) are not valid file paths\n",
        "    if isinstance(source, str) and ('\\n' in source or len(source) > 260):\n",
        "        return source\n",
        "    p = pathlib.Path(str(source))\n",
        "    try:\n",
        "        if p.exists():\n",
        "            if p.suffix.lower() == \".pdf\":\n",
        "                with pdfplumber.open(p) as pdf:\n",
        "                    return \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
        "            else:\n",
        "                return p.read_text(errors=\"ignore\")\n",
        "    except OSError:\n",
        "        pass\n",
        "    return str(source)\n",
        "\n",
        "# Quick smoke-test\n",
        "sample = \"This Agreement is entered into between Acme Corp and John Smith on 1 Jan 2025.\"\n",
        "# print(extract_text(sample)[:200])\n",
        "print(\"‚úÖ Text extractor ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78640359",
      "metadata": {},
      "source": [
        "## Cell 4 ‚Äî Week 3 / Day 2: NER Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "782824ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NER model onto mps ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This Agreement is entered into between Acme Corp and John Smith on 1 Jan 2025.']\n",
            "| Entity | Group | Score |\n",
            "|--------|-------|-------|\n",
            "| Agreement | MISC | 0.54 |\n",
            "| A | ORG | 1.00 |\n",
            "| ##cme Corp | ORG | 0.94 |\n",
            "| John Smith | ORG | 0.88 |\n",
            "\n",
            "‚úÖ NER pipeline ready.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "print(f\"Loading NER model onto {DEVICE} ‚Ä¶\")\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=NER_MODEL,\n",
        "    aggregation_strategy=\"simple\",\n",
        "    device=0 if DEVICE == \"cuda\" else -1  # pipeline uses int device index\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "def run_ner(text: str) -> str:\n",
        "    \"\"\"Return a formatted string of named entities.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    chunk_size = 400\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    all_entities = []\n",
        "    print(chunks)\n",
        "    for chunk in chunks:\n",
        "        ids = tokenizer.encode(chunk, truncation=True, max_length=512, add_special_tokens=False)\n",
        "        safe_chunk = tokenizer.decode(ids)\n",
        "        all_entities.extend(ner_pipeline(safe_chunk))\n",
        "    \n",
        "    if not all_entities:\n",
        "        return \"No named entities found.\"\n",
        "    \n",
        "    header = [\"| Entity | Group | Score |\", \"|--------|-------|-------|\"]\n",
        "    rows = [f\"| {e['word']} | {e['entity_group']} | {e['score']:.2f} |\" for e in all_entities]\n",
        "    return \"\\n\".join(header + rows)\n",
        "\n",
        "# Smoke-test\n",
        "print(run_ner(sample))\n",
        "print(\"\\n‚úÖ NER pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0b7cef",
      "metadata": {},
      "source": [
        "## Cell 5 ‚Äî Week 3 / Day 2: Sentiment Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8b0594fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading sentiment model ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sentiment pipeline ready.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading sentiment model ‚Ä¶\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def run_sentiment(text: str) -> str:\n",
        "    \"\"\"Return overall document sentiment.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    # Sentiment models also cap at 512 tokens ‚Äî use first 400 words as proxy\n",
        "    snippet = \" \".join(text.split()[:400])\n",
        "    result = sentiment_pipeline(snippet, truncation=True, max_length=512)[0]\n",
        "    \n",
        "    emoji = \"üü¢\" if result[\"label\"] == \"POSITIVE\" else \"üî¥\"\n",
        "    return f\"{emoji} {result['label']}  (confidence: {result['score']:.2f})\"\n",
        "\n",
        "# print(run_sentiment(sample))\n",
        "print(\"‚úÖ Sentiment pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b75a878",
      "metadata": {},
      "source": [
        "## Cell 6 ‚Äî Week 3 / Day 2: Zero-Shot Risk Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c37be135",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading zero-shot classification model ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document type: legal contract\n",
            "| # | Risk | Label | Score | Clause |\n",
            "|---|------|-------|-------|--------|\n",
            "| 01 | üî¥ | indemnification | 0.52 | The Licensor shall not be liable for any indirect damages arising from the use of this‚Ä¶ |\n",
            "| 02 | üü° | termination clause | 0.84 | Either party may terminate this agreement with 30 days written notice. |\n",
            "\n",
            "‚úÖ Risk scorer ready.\n"
          ]
        }
      ],
      "source": [
        "import re, textwrap\n",
        "\n",
        "print(\"Loading zero-shot classification model ‚Ä¶\")\n",
        "zsc_pipeline = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=ZERO_SHOT_MODEL,\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "SUPPORTED_DOC_TYPES = [\"legal contract\", \"email\", \"privacy policy\", \"research article\", \"general document\"]\n",
        "\n",
        "DOC_TYPE = \"general document\"\n",
        "\n",
        "def run_risk_scoring(text: str) -> str:\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\", \"No text provided.\"\n",
        "    \n",
        "    snippet = text[:512]\n",
        "    DOC_TYPE = zsc_pipeline(snippet, candidate_labels=SUPPORTED_DOC_TYPES, truncation=True, max_length=512)[\"labels\"][0]\n",
        "    \n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())[:10]\n",
        "    lines = [\"| # | Risk | Label | Score | Clause |\", \"|---|------|-------|-------|--------|\"]\n",
        "    for i, clause in enumerate(sentences, 1):\n",
        "        result = zsc_pipeline(clause, candidate_labels=RISK_LABELS, truncation=True, max_length=512)\n",
        "        top_label = result[\"labels\"][0]\n",
        "        top_score = result[\"scores\"][0]\n",
        "        risk_icon = (\n",
        "            \"üî¥\" if \"high\" in top_label or top_label in (\"liability\", \"indemnification\") else\n",
        "            \"üü°\" if \"medium\" in top_label or top_label in (\"termination clause\", \"financial obligation\") else\n",
        "            \"üü¢\"\n",
        "        )\n",
        "        snippet = textwrap.shorten(clause, width=90, placeholder=\"‚Ä¶\")\n",
        "        lines.append(f\"| {i:02d} | {risk_icon} | {top_label} | {top_score:.2f} | {snippet} |\")\n",
        "\n",
        "    \n",
        "    print(f\"Document type: {DOC_TYPE}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# Smoke-test on a two-sentence doc\n",
        "test_doc = (\n",
        "    \"The Licensor shall not be liable for any indirect damages arising from the use of this software. \"\n",
        "    \"Either party may terminate this agreement with 30 days written notice.\"\n",
        ")\n",
        "print(run_risk_scoring(test_doc))\n",
        "print(\"\\n‚úÖ Risk scorer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a9bfff",
      "metadata": {},
      "source": [
        "## Cell 7 ‚Äî Week 3 / Day 3: Tokenizer + Chat Template Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "957f6451",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer ‚Ä¶\n",
            "üìä Token Stats\n",
            "  Raw token count    : 21\n",
            "  First 10 token IDs : [128000, 2028, 23314, 374, 10862, 1139, 1990, 6515, 2727, 22621]\n",
            "  Chat prompt tokens : 69\n",
            "\n",
            "üìù Chat-Template Prompt (first 600 chars):\n",
            "------------------------------------------------------------\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 27 Feb 2026\n",
            "\n",
            "You are a professional document analyst.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Briefly summarise this document:\n",
            "\n",
            "This Agreement is entered into between Acme Corp and John Smith on 1 Jan 2025.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "‚úÖ Tokenizer ready.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Loading tokenizer ‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "def run_token_preview(text: str) -> str:\n",
        "    \"\"\"Show token count and the chat-template-formatted prompt.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    \n",
        "    # Raw tokenization\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_count = len(tokens)\n",
        "    first_10 = tokens[:10]\n",
        "    \n",
        "    # Chat template ‚Äî the Day 3 \"aha moment\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional document analyst.\"},\n",
        "        {\"role\": \"user\",   \"content\": f\"Briefly summarise this document:\\n\\n{' '.join(text.split()[:300])}\"}\n",
        "    ]\n",
        "    chat_prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    chat_tokens = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    return (\n",
        "        f\"üìä Token Stats\\n\"\n",
        "        f\"  Raw token count    : {token_count}\\n\"\n",
        "        f\"  First 10 token IDs : {first_10}\\n\"\n",
        "        f\"  Chat prompt tokens : {len(chat_tokens)}\\n\\n\"\n",
        "        f\"üìù Chat-Template Prompt (first 600 chars):\\n\"\n",
        "        f\"{'-'*60}\\n\"\n",
        "        f\"{chat_prompt[:600]}\"\n",
        "    )\n",
        "\n",
        "print(run_token_preview(sample))\n",
        "print(\"\\n‚úÖ Tokenizer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6dc7d2",
      "metadata": {},
      "source": [
        "## Cell 8 ‚Äî Week 3 / Days 4‚Äì5: LLM Brief Generator (Streaming)\n",
        "\n",
        "> **Note:** First run will download ~600 MB for TinyLlama. Subsequent runs use the local cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c0913cb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LLM on mps ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM loaded.\n",
            "=== LLM Brief (streaming) ===\n"
          ]
        }
      ],
      "source": [
        "import gc, threading\n",
        "from transformers import AutoModelForCausalLM, TextIteratorStreamer\n",
        "\n",
        "print(f\"Loading LLM on {DEVICE} ‚Ä¶\")\n",
        "\n",
        "# Quantization ‚Äî MPS path uses optimum-quanto; CUDA path uses bitsandbytes\n",
        "load_kwargs = dict(device_map=\"auto\" if DEVICE == \"cuda\" else None)\n",
        "\n",
        "if QUANT_BACKEND == \"bitsandbytes\":\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n",
        "elif QUANT_BACKEND == \"optimum-quanto\":\n",
        "    from optimum.quanto import quantize, qint8\n",
        "    # We quantize after loading for MPS\n",
        "    pass\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(MODEL, **load_kwargs)\n",
        "\n",
        "if QUANT_BACKEND == \"optimum-quanto\":\n",
        "    quantize(llm_model, weights=qint8)\n",
        "\n",
        "if DEVICE == \"mps\":\n",
        "    llm_model = llm_model.to(\"mps\")\n",
        "\n",
        "llm_model.eval()\n",
        "print(\"‚úÖ LLM loaded.\")\n",
        "\n",
        "# ‚îÄ‚îÄ Document-type detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "DOC_TYPES = [\"legal contract\", \"email\", \"privacy policy\", \"research article\", \"general document\"]\n",
        "\n",
        "def detect_doc_type(text: str) -> str:\n",
        "    snippet = text[:512]  # only need a small sample\n",
        "    result = zsc_pipeline(snippet, candidate_labels=DOC_TYPES, truncation=True, max_length=512)\n",
        "    return result[\"labels\"][0]\n",
        "\n",
        "def build_brief_prompt(text: str) -> list[dict]:\n",
        "    doc_type = detect_doc_type(text)\n",
        "    print(f\"Document type: {doc_type}\")\n",
        "    SYSTEM_PROMPT = (\n",
        "        f\"You are a professional document analyst. The following is a {doc_type}.\\n\"\n",
        "        \"Provide a structured brief with:\\n\"\n",
        "        \"1. One-sentence summary\\n\"\n",
        "        \"2. Key parties or stakeholders\\n\"\n",
        "        \"3. Main obligations or key points (up to 5 bullet points)\\n\"\n",
        "        \"4. Notable risks or red flags\\n\"\n",
        "        \"5. Recommended next action\\n\"\n",
        "        \"Be concise. Use bullet points.\"\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\",   \"content\": text[:1500]}  # cap context for speed\n",
        "    ]\n",
        "\n",
        "def run_llm_brief(text: str, max_new_tokens: int = 400):\n",
        "    \"\"\"Generate a structured LLM brief. Yields accumulated text as each token arrives.\"\"\"\n",
        "    if not text.strip():\n",
        "        yield \"No text provided.\"\n",
        "        return\n",
        "\n",
        "    messages = build_brief_prompt(text)\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    attention_mask = torch.ones_like(input_ids).to(DEVICE)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        streamer=streamer,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    thread = threading.Thread(target=llm_model.generate, kwargs=gen_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    accumulated = \"\"\n",
        "    for token_text in streamer:\n",
        "        accumulated += token_text\n",
        "        yield accumulated\n",
        "\n",
        "    thread.join()\n",
        "\n",
        "    del input_ids\n",
        "    gc.collect()\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    elif DEVICE == \"mps\":\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "# Smoke-test\n",
        "print(\"=== LLM Brief (streaming) ===\")\n",
        "# _ = run_llm_brief(test_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a6ad7c",
      "metadata": {},
      "source": [
        "## Cell 9 ‚Äî Full Analysis Pipeline (no UI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fa9981a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document analysis ready\n"
          ]
        }
      ],
      "source": [
        "def analyse_document(source, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Run the full 5-stage analysis on any document source.\n",
        "    source: file path (PDF/TXT) or raw text string.\n",
        "    Returns a dict with keys: text, token_preview, entities, risk_scores, sentiment, llm_brief\n",
        "    \"\"\"\n",
        "    sep = \"=\" * 70\n",
        "    \n",
        "    text = extract_text(source)\n",
        "    if not text.strip():\n",
        "        print(\"‚ö†Ô∏è  No text found in document.\")\n",
        "        return {}\n",
        "    \n",
        "    results = {\"text\": text}\n",
        "    \n",
        "    # Stage 1 ‚Äî Token preview (Day 3)\n",
        "    print(f\"{sep}\\n[1/5] TOKENIZER PREVIEW\\n{sep}\")\n",
        "    results[\"token_preview\"] = run_token_preview(text)\n",
        "    if verbose: print(results[\"token_preview\"])\n",
        "    \n",
        "    # Stage 2 ‚Äî Named entities (Day 2)\n",
        "    print(f\"\\n{sep}\\n[2/5] NAMED ENTITY EXTRACTION\\n{sep}\")\n",
        "    results[\"entities\"] = run_ner(text)\n",
        "    if verbose: print(results[\"entities\"])\n",
        "    \n",
        "    # Stage 3 ‚Äî Risk scoring (Day 2)\n",
        "    print(f\"\\n{sep}\\n[3/5] PER-CLAUSE RISK SCORING\\n{sep}\")\n",
        "    results[\"risk_scores\"] = run_risk_scoring(text)\n",
        "    if verbose: print(results[\"risk_scores\"])\n",
        "    \n",
        "    # Stage 4 ‚Äî Sentiment (Day 2)\n",
        "    print(f\"\\n{sep}\\n[4/5] OVERALL SENTIMENT\\n{sep}\")\n",
        "    results[\"sentiment\"] = run_sentiment(text)\n",
        "    if verbose: print(results[\"sentiment\"])\n",
        "    \n",
        "    # Stage 5 ‚Äî LLM brief (Days 4-5)\n",
        "    print(f\"\\n{sep}\\n[5/5] LLM DOCUMENT BRIEF (streaming)\\n{sep}\")\n",
        "    results[\"llm_brief\"] = \"\".join(run_llm_brief(text))\n",
        "    \n",
        "    print(f\"\\n{sep}\\n‚úÖ Analysis complete.\\n{sep}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ Run on a sample contract excerpt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CONTRACT_SAMPLE = \"\"\"\n",
        "SERVICE AGREEMENT\n",
        "\n",
        "This Agreement is entered into as of January 1, 2025 between Acme Corporation\n",
        "(\"Client\") and TechSolutions Ltd (\"Service Provider\").\n",
        "\n",
        "1. SERVICES. Service Provider agrees to develop and deliver a custom analytics\n",
        "dashboard by March 31, 2025. Deliverables are specified in Exhibit A.\n",
        "\n",
        "2. PAYMENT. Client shall pay $50,000 USD within 30 days of invoice. Late payments\n",
        "shall incur a penalty of 1.5% per month.\n",
        "\n",
        "3. TERMINATION. Either party may terminate this Agreement with 30 days written\n",
        "notice. Client may terminate immediately for material breach.\n",
        "\n",
        "4. LIMITATION OF LIABILITY. In no event shall either party be liable for indirect,\n",
        "incidental, or consequential damages.\n",
        "\n",
        "5. GOVERNING LAW. This Agreement shall be governed by the laws of the State of\n",
        "California. Any disputes shall be resolved by arbitration in San Francisco.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Document analysis ready\")\n",
        "\n",
        "# results = analyse_document(CONTRACT_SAMPLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f19215c",
      "metadata": {},
      "source": [
        "## Cell 10 ‚Äî Gradio UI (Optional)\n",
        "\n",
        "Launches a single-page web UI with file upload and streaming LLM output.\n",
        "\n",
        "> Run this cell to start the app. A local URL (e.g. `http://127.0.0.1:7860`) will appear below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e45ab09b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 125, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 111, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 391, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/Users/davidinyang-etoh/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/davidinyang-etoh/.local/share/uv/python/cpython-3.12.12-macos-aarch64-none/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x13abb54c0 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['EMPLOYMENT AGREEMENT This Employment Agreement (\"Agreement\") is made and entered into as of August 20, 2025, by and between Prompt Computers IO LLC (\"Company\"), and David Inyang-Etoh (\"Employee\"). 1. Position, Duties, and Work Location The Company hereby employs the Employee as a Full Stack Developer . The Employee agrees to devote full working time, attention, and best efforts to the employment and to comply with all lawful instructions, policies, rules, and regulations of the Company, including mandatory attendance at all departmental and company-wide meetings. This role is fully remote, and the Employee agrees to maintain a suitable and secure work environment to fulfill their job responsibilities effectively. 2. Term of Employment Employment shall commence on August 20, 2025. The Employee shall undergo a probationary period of one (1) month, which may be extended at the sole discretion of the Company. During this probationary period, the Company may terminate the Employee‚Äôs employment if performance is deemed below satisfactory. 3. Compensation (a) Salary: The Company agrees to pay the Employee a monthly salary of ‚Ç¶1,000,000, payable at the end of each month by direct deposit or other agreed method. (b) Bonuses: The Employee may be eligible for performance-based bonuses, which are discretionary and shall be determined solely by the Company. 4. Duties and Responsibilities The Employee agrees to faithfully and diligently perform the duties of the position, including but not limited to: ‚óè Develop, maintain, and optimize backend systems and APIs. ‚óè Design, build, and enhance front-end applications. ‚óè Utilize various AWS services (Cognito, Lambda, S3, RDS, DynamoDB, ECS, SNS, SQS) for cloud-native application development. ‚óè Architect and support scalable, serverless, and microservices-based solutions with CI/CD pipelines. ‚óè Proactively identify and resolve performance, scalability, and reliability issues across the stack. ‚óè Collaborate closely with cross-functional teams, including frontend and backend developers, to ensure seamless integration and delivery. ‚óè Monitor application health and performance. ‚óè Participate actively in project planning, code reviews, and team collaboration sessions. 5. Work Hours The Employee is expected to work a minimum of 40 hours per week, Monday through Friday. Additional hours may be required as necessary to fulfill responsibilities or meet project deadlines. 6 . Non-Disclosure and Confidentiality The Employee acknowledges that during the course of employment, they will have access to confidential and proprietary information belonging to the Company. The Employee agrees: ‚óè Not to disclose or use any confidential information for any purpose other than', \"to perform their duties. ‚óè To maintain strict confidentiality both during and after employment. ‚óè To return all Company property and materials containing confidential information upon termination of employment. 7. Intellectual Property All inventions, designs, developments, improvements, trade secrets, works of authorship, and other intellectual property created by the Employee during and within the scope of employment related to the Company‚Äôs business shall be the exclusive property of the Company. 8. Compliance with Company Policies The Employee agrees to abide by all current and future Company policies, rules, and regulations, including mandatory attendance at scheduled meetings and participation in Company initiatives. 9. Term and Termination (a) Term: This Agreement shall continue indefinitely until terminated by either party in accordance with the terms herein. (b) Restriction on Employee Resignation: The Employee shall not voluntarily terminate employment while actively engaged in any ongoing project assigned by the Company. Termination shall only be effective after completion of the project or with express written consent of the Company. (c) Termination by Company: The Company may terminate this Agreement by providing thirty (30) days' written notice or immediately for cause. Cause includes but is not limited to gross misconduct, breach of confidentiality, violation of Company policies, or other serious violations. (d) Termination by Employee: Other than when not actively working on a project, the Employee may terminate employment by providing thirty (30) days‚Äô prior written notice to the Company. (e) Return of Company Property: Upon termination of employment by either party, the Employee agrees to return all Company property, including but not limited to laptops, mobile devices, access cards, documents, and any other materials belonging to the Company. 10. Governing Law This Agreement shall be governed by and construed in accordance with the laws of the State of Texas. 11. Entire Agreement This Agreement contains the entire understanding between the parties and supersedes all prior agreements or understandings, whether written or oral, concerning the subject matter herein. Any amendments must be made in writing and signed by both parties. 12. Severability If any provision of this Agreement is found to be invalid or unenforceable, the remaining provisions shall continue in full force and effect. 13. Acknowledgment The Employee acknowledges having read and understood this Agreement and agrees to be bound by its terms. IN WITNESS WHEREOF, the parties hereto have executed this Agreement as of the date first above written. David Inyang-Etoh Employee 20th July,\", '2025 Date: _______________________ Kadisi Mitee Prompt Computers IO LLC Date: _______________________']\n",
            "Document type: legal contract\n",
            "Document type: legal contract\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_analyse(file_obj, raw_text: str):\n",
        "    \"\"\"Gradio handler ‚Äî streams results stage-by-stage as each completes.\"\"\"\n",
        "    source = file_obj if file_obj is not None else raw_text\n",
        "    if not source:\n",
        "        yield \"\", \"Please upload a file or paste text.\", \"\", \"\", \"\", \"\"\n",
        "        return\n",
        "\n",
        "    text = extract_text(source)\n",
        "    yield text, \"[1/5] Running token preview‚Ä¶\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    tok = run_token_preview(text)\n",
        "    yield text, tok, \"[2/5] Running NER‚Ä¶\", \"\", \"\", \"\"\n",
        "\n",
        "    ents = run_ner(text)\n",
        "    yield text, tok, ents, \"[3/5] Running risk scoring‚Ä¶\", \"\", \"\"\n",
        "\n",
        "    risk = run_risk_scoring(text)\n",
        "    yield text, tok, ents, risk, \"[4/5] Running sentiment‚Ä¶\", \"\"\n",
        "\n",
        "    sent = run_sentiment(text)\n",
        "    yield text, tok, ents, risk, sent, \"[5/5] Generating LLM brief‚Ä¶\"\n",
        "\n",
        "    for partial_brief in run_llm_brief(text):\n",
        "        yield text, tok, ents, risk, sent, partial_brief\n",
        "\n",
        "with gr.Blocks(title=\"Multi-Modal Document Studio\") as demo:\n",
        "    gr.Markdown(\"# üóÇÔ∏è Multi-Modal Document Studio\\nUpload a PDF/TXT or paste text below.\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\".pdf\", \".txt\"])\n",
        "        text_input = gr.Textbox(label=\"Or paste text here\", lines=8, placeholder=\"Paste document text‚Ä¶\")\n",
        "    \n",
        "    run_btn = gr.Button(\"üîç Analyse Document\", variant=\"primary\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üìä Token Preview\"):  tok_out  = gr.Markdown()\n",
        "        with gr.Tab(\"üè∑Ô∏è Named Entities\"): ent_out  = gr.Markdown()\n",
        "        with gr.Tab(\"‚ö†Ô∏è Risk Scores\"):    risk_out = gr.Markdown()\n",
        "        with gr.Tab(\"üí¨ Final Brief\"): \n",
        "            sent_out = gr.Markdown(label=\"üòê Sentiment\")     \n",
        "            llm_out  = gr.Markdown(label=\"üí¨ Brief\")\n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "    run_btn.click(\n",
        "        gradio_analyse,\n",
        "        inputs=[file_input, text_input],\n",
        "        outputs=[text_input, tok_out, ent_out, risk_out, sent_out, llm_out]\n",
        "    )\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d078b4d5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Concepts Demonstrated\n",
        "\n",
        "| Cell | Week 3 Concept | What it shows |\n",
        "|------|---------------|---------------|\n",
        "| 4 | Day 2 ‚Äî `pipeline('ner')` | Extract parties, dates, money |\n",
        "| 5 | Day 2 ‚Äî `pipeline('sentiment-analysis')` | Overall document tone |\n",
        "| 6 | Day 2 ‚Äî `pipeline('zero-shot-classification')` | Per-clause risk without labelled data |\n",
        "| 7 | Day 3 ‚Äî `AutoTokenizer` + `apply_chat_template` | Token IDs & prompt format |\n",
        "| 8 | Day 4 ‚Äî `AutoModelForCausalLM` + `TextIteratorStreamer` + quantization | Local LLM + streaming |\n",
        "| 8 | Day 4 ‚Äî `gc.collect()` + `empty_cache()` | MPS/CUDA memory management |\n",
        "| 9 | Day 5 ‚Äî End-to-end chaining | All components wired together |\n",
        "| 10 | ‚Äî | Gradio UI with file upload |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
