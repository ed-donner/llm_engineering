{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1af1c278",
      "metadata": {},
      "source": [
        "# üóÇÔ∏è Multi-Modal Document Studio\n",
        "**End-to-End Open-Source Document Analysis ‚Äî No API Keys Required**\n",
        "\n",
        "Built an interactive document analysis pipeline that accepts any PDF, TXT, or pasted text (contracts, emails, articles, policies) and runs a full automated review using only open-source models from HuggingFace.\n",
        "\n",
        "**What it does:**\n",
        "- Extracts and previews token structure and the chat-template formatted prompt\n",
        "- Identifies named entities (people, organisations, locations) across the full document\n",
        "- Scores each clause for risk categories (liability, termination, indemnification, etc.) using zero-shot classification ‚Äî no labelled training data needed\n",
        "- Detects overall document tone / sentiment\n",
        "- Generates a structured analyst brief (summary, key parties, obligations, risks, next action) via a locally-running quantized LLM with token-by-token streaming\n",
        "- Synthesises the brief as audio using a text-to-speech model\n",
        "- Exposes everything through a Gradio Blocks UI with file upload and tabbed outputs\n",
        "\n",
        "**Concepts applied:**\n",
        "- `pipeline()` API ‚Äî NER, sentiment, zero-shot classification, text-to-speech\n",
        "- `AutoTokenizer` + `apply_chat_template` ‚Äî understanding token IDs and chat prompt formatting\n",
        "- `AutoModelForCausalLM` ‚Äî loading and running a local instruction-tuned LLM\n",
        "- INT8 quantization via `optimum-quanto` (Apple MPS) and `bitsandbytes` (CUDA) to fit the model in memory\n",
        "- `TextIteratorStreamer` + background thread ‚Äî real-time streaming token generation\n",
        "- Device-aware loading and memory management (`gc.collect()`, `empty_cache()`)\n",
        "\n",
        "---\n",
        "**Hardware:** Runs on Apple Silicon (`mps`), NVIDIA GPU (`cuda`), or CPU. Set `DEVICE` in Cell 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71cad78",
      "metadata": {},
      "source": [
        "## Cell 1 ‚Äî Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02490e23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run once ‚Äî restart kernel after installation\n",
        "!uv pip install -q transformers torch accelerate pdfplumber gradio sentencepiece optimum-quanto huggingface_hub\n",
        "\n",
        "print(\"‚úÖ All packages installed. Restart the kernel, then run from Cell 2 onwards.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9cc510",
      "metadata": {},
      "source": [
        "## Cell 2 ‚Äî HuggingFace Authentication\n",
        "\n",
        "Models are downloaded from the [HuggingFace Hub](https://huggingface.co). You need a free account and an access token.\n",
        "\n",
        "1. Go to https://huggingface.co/settings/tokens\n",
        "2. Create a token with **Read** permissions\n",
        "3. Set it as an environment variable before launching Jupyter: `export HF_TOKEN=hf_...`  \n",
        "   ‚Äî or paste it directly into the cell below (avoid committing it to git)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab84e45",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = os.environ.get('HF_TOKEN')\n",
        "if hf_token and hf_token.startswith(\"hf_\"):\n",
        "  print(\"HF key looks good so far\")\n",
        "else:\n",
        "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "print(\"‚úÖ Logged in to HuggingFace Hub.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34067df5",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Constants & Device Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9f0915",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ‚îÄ‚îÄ Device ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if torch.backends.mps.is_available(): # Macbook Silicon\n",
        "    DEVICE = \"mps\"\n",
        "    QUANT_BACKEND = \"optimum-quanto\"   # MPS path\n",
        "elif torch.cuda.is_available(): # NVIDIA GPU\n",
        "    DEVICE = \"cuda\"\n",
        "    QUANT_BACKEND = \"bitsandbytes\"     # CUDA path\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    QUANT_BACKEND = \"none\"\n",
        "\n",
        "print(f\"Device      : {DEVICE}\")\n",
        "print(f\"Quant backend: {QUANT_BACKEND}\")\n",
        "\n",
        "# ‚îÄ‚îÄ Model IDs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "NER_MODEL         = \"dslim/bert-base-NER\"\n",
        "ZERO_SHOT_MODEL    = \"facebook/bart-large-mnli\"      # Zero-shot classification\n",
        "SENTIMENT_MODEL    = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "MODEL          = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
        "\n",
        "# ‚îÄ‚îÄ Risk labels for zero-shot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "RISK_LABELS = [\n",
        "    \"low risk\", \"medium risk\", \"high risk\",\n",
        "    \"financial obligation\", \"liability\", \"termination clause\",\n",
        "    \"data privacy\", \"intellectual property\", \"indemnification\"\n",
        "]\n",
        "\n",
        "print(\"\\n‚úÖ Constants set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93c7d57",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Utility: Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d492dc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pdfplumber, pathlib\n",
        "\n",
        "def extract_text(source) -> str:\n",
        "    \"\"\"Extract text from a PDF path, TXT path, or raw string.\"\"\"\n",
        "    if source is None:\n",
        "        return \"\"\n",
        "    if hasattr(source, \"name\"):          # Gradio UploadedFile\n",
        "        source = source.name\n",
        "    # Raw text strings (multi-line or long) are not valid file paths\n",
        "    if isinstance(source, str) and ('\\n' in source or len(source) > 260):\n",
        "        return source\n",
        "    p = pathlib.Path(str(source))\n",
        "    try:\n",
        "        if p.exists() and not p.is_dir():\n",
        "            if p.suffix.lower() == \".pdf\":\n",
        "                with pdfplumber.open(p) as pdf:\n",
        "                    return \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
        "            else:\n",
        "                return p.read_text(errors=\"ignore\")\n",
        "    except OSError:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "# Quick smoke-test\n",
        "# sample = \"This Agreement is entered into between Acme Corp and John Smith on 1 Jan 2025.\"\n",
        "# print(extract_text(sample)[:200])\n",
        "print(\"‚úÖ Text extractor ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78640359",
      "metadata": {},
      "source": [
        "## Cell 4 ‚Äî NER Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782824ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "print(f\"Loading NER model onto {DEVICE} ‚Ä¶\")\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=NER_MODEL,\n",
        "    aggregation_strategy=\"simple\",\n",
        "    device=0 if DEVICE == \"cuda\" else -1  # pipeline uses int device index\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "def run_ner(text: str) -> str:\n",
        "    \"\"\"Return a formatted string of named entities.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    chunk_size = 400\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    all_entities = []\n",
        "    print(chunks)\n",
        "    for chunk in chunks:\n",
        "        ids = tokenizer.encode(chunk, truncation=True, max_length=512, add_special_tokens=False)\n",
        "        safe_chunk = tokenizer.decode(ids)\n",
        "        all_entities.extend(ner_pipeline(safe_chunk))\n",
        "    \n",
        "    if not all_entities:\n",
        "        return \"No named entities found.\"\n",
        "    \n",
        "    header = [\"| Entity | Group | Score |\", \"|--------|-------|-------|\"]\n",
        "    rows = [f\"| {e['word']} | {e['entity_group']} | {e['score']:.2f} |\" for e in all_entities]\n",
        "    return \"\\n\".join(header + rows)\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ NER pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0b7cef",
      "metadata": {},
      "source": [
        "## Cell 5 ‚Äî Sentiment Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0594fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading sentiment model ‚Ä¶\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def run_sentiment(text: str) -> str:\n",
        "    \"\"\"Return overall document sentiment.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    # Sentiment models also cap at 512 tokens ‚Äî use first 400 words as proxy\n",
        "    snippet = \" \".join(text.split()[:400])\n",
        "    result = sentiment_pipeline(snippet, truncation=True, max_length=512)[0]\n",
        "    \n",
        "    emoji = \"üü¢\" if result[\"label\"] == \"POSITIVE\" else \"üî¥\"\n",
        "    return f\"{emoji} {result['label']}  (confidence: {result['score']:.2f})\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Sentiment pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b75a878",
      "metadata": {},
      "source": [
        "## Cell 6 ‚Äî Zero-Shot Risk Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37be135",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, textwrap\n",
        "\n",
        "print(\"Loading zero-shot classification model ‚Ä¶\")\n",
        "zsc_pipeline = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=ZERO_SHOT_MODEL,\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "SUPPORTED_DOC_TYPES = [\"legal contract\", \"email\", \"privacy policy\", \"research article\", \"general document\"]\n",
        "\n",
        "DOC_TYPE = \"general document\"\n",
        "\n",
        "def run_risk_scoring(text: str) -> str:\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\", \"No text provided.\"\n",
        "    \n",
        "    snippet = text[:512]\n",
        "    DOC_TYPE = zsc_pipeline(snippet, candidate_labels=SUPPORTED_DOC_TYPES, truncation=True, max_length=512)[\"labels\"][0]\n",
        "    \n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())[:10]\n",
        "    lines = [\"| # | Risk | Label | Score | Clause |\", \"|---|------|-------|-------|--------|\"]\n",
        "    for i, clause in enumerate(sentences, 1):\n",
        "        result = zsc_pipeline(clause, candidate_labels=RISK_LABELS, truncation=True, max_length=512)\n",
        "        top_label = result[\"labels\"][0]\n",
        "        top_score = result[\"scores\"][0]\n",
        "        risk_icon = (\n",
        "            \"üî¥\" if \"high\" in top_label or top_label in (\"liability\", \"indemnification\") else\n",
        "            \"üü°\" if \"medium\" in top_label or top_label in (\"termination clause\", \"financial obligation\") else\n",
        "            \"üü¢\"\n",
        "        )\n",
        "        snippet = textwrap.shorten(clause, width=90, placeholder=\"‚Ä¶\")\n",
        "        lines.append(f\"| {i:02d} | {risk_icon} | {top_label} | {top_score:.2f} | {snippet} |\")\n",
        "\n",
        "    \n",
        "    print(f\"Document type: {DOC_TYPE}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Risk scorer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a9bfff",
      "metadata": {},
      "source": [
        "## Cell 7 ‚Äî Tokenizer + Chat Template Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "957f6451",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Loading tokenizer ‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "def run_token_preview(text: str) -> str:\n",
        "    \"\"\"Show token count and the chat-template-formatted prompt.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    \n",
        "    # Raw tokenization\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_count = len(tokens)\n",
        "    first_10 = tokens[:10]\n",
        "    \n",
        "    # Apply chat template to format the prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional document analyst.\"},\n",
        "        {\"role\": \"user\",   \"content\": f\"Briefly summarise this document:\\n\\n{' '.join(text.split()[:300])}\"}\n",
        "    ]\n",
        "    chat_prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    chat_tokens = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    return (\n",
        "        f\"üìä Token Stats\\n\"\n",
        "        f\"  Raw token count    : {token_count}\\n\"\n",
        "        f\"  First 10 token IDs : {first_10}\\n\"\n",
        "        f\"  Chat prompt tokens : {len(chat_tokens)}\\n\\n\"\n",
        "        f\"üìù Chat-Template Prompt (first 600 chars):\\n\"\n",
        "        f\"{'-'*60}\\n\"\n",
        "        f\"{chat_prompt[:600]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Tokenizer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6dc7d2",
      "metadata": {},
      "source": [
        "## Cell 8 ‚Äî LLM Brief Generator (Streaming)\n",
        "\n",
        "> **Note:** First run will download ~600 MB for TinyLlama. Subsequent runs use the local cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0913cb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc, threading\n",
        "from transformers import AutoModelForCausalLM, TextIteratorStreamer\n",
        "\n",
        "print(f\"Loading LLM on {DEVICE} ‚Ä¶\")\n",
        "\n",
        "# Quantization ‚Äî MPS path uses optimum-quanto; CUDA path uses bitsandbytes\n",
        "load_kwargs = dict(device_map=\"auto\" if DEVICE == \"cuda\" else None)\n",
        "\n",
        "if QUANT_BACKEND == \"bitsandbytes\":\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n",
        "elif QUANT_BACKEND == \"optimum-quanto\":\n",
        "    from optimum.quanto import quantize, qint8\n",
        "    # We quantize after loading for MPS\n",
        "    pass\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "\n",
        "if QUANT_BACKEND == \"optimum-quanto\":\n",
        "    quantize(llm_model, weights=qint8)\n",
        "\n",
        "if DEVICE == \"mps\":\n",
        "    llm_model = llm_model.to(\"mps\")\n",
        "\n",
        "llm_model.eval()\n",
        "print(\"‚úÖ LLM loaded.\")\n",
        "\n",
        "\n",
        "def build_brief_prompt(text: str) -> list[dict]:\n",
        "    \n",
        "    SYSTEM_PROMPT = (\n",
        "        f\"You are a professional document analyst. The following is a {DOC_TYPE}.\\n\"\n",
        "        \"Provide a structured brief with:\\n\"\n",
        "        \"1. One-sentence summary\\n\"\n",
        "        \"2. Key parties or stakeholders\\n\"\n",
        "        \"3. Main obligations or key points (up to 5 bullet points)\\n\"\n",
        "        \"4. Notable risks or red flags\\n\"\n",
        "        \"5. Recommended next action\\n\"\n",
        "        \"Be concise. Use bullet points.\"\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\",   \"content\": text[:1500]}  # cap context for speed\n",
        "    ]\n",
        "\n",
        "def run_llm_brief(text: str, max_new_tokens: int = 400):\n",
        "    \"\"\"Generate a structured LLM brief. Yields accumulated text as each token arrives.\"\"\"\n",
        "    if not text.strip():\n",
        "        yield \"No text provided.\"\n",
        "        return\n",
        "\n",
        "    messages = build_brief_prompt(text)\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    attention_mask = torch.ones_like(input_ids).to(DEVICE)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        streamer=streamer,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    thread = threading.Thread(target=llm_model.generate, kwargs=gen_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    accumulated = \"\"\n",
        "    for token_text in streamer:\n",
        "        accumulated += token_text\n",
        "        yield accumulated\n",
        "\n",
        "    thread.join()\n",
        "\n",
        "    del input_ids\n",
        "    gc.collect()\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    elif DEVICE == \"mps\":\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "\n",
        "print(\"=== LLM Brief (streaming) ===\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a6ad7c",
      "metadata": {},
      "source": [
        "## Cell 9 ‚Äî Full Analysis Pipeline (no UI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1596d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "def run_speech(text: str):\n",
        "    synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device=DEVICE)\n",
        "    embeddings_dataset = load_dataset(\"matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
        "    speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "    speech = synthesiser(text, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "    return Audio(speech[\"audio\"], rate=speech[\"sampling_rate\"])\n",
        "\n",
        "print(\"Text to speech ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f19215c",
      "metadata": {},
      "source": [
        "## Cell 10 ‚Äî Gradio UI (Optional)\n",
        "\n",
        "Launches a single-page web UI with file upload and streaming LLM output.\n",
        "\n",
        "> Run this cell to start the app. A local URL (e.g. `http://127.0.0.1:7860`) will appear below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45ab09b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from pydantic.v1.types import NoneBytes\n",
        "\n",
        "def gradio_analyse(file_obj, raw_text: str):\n",
        "    \"\"\"Gradio handler ‚Äî streams results stage-by-stage as each completes.\"\"\"\n",
        "    source = file_obj if file_obj is not None else raw_text\n",
        "    if not source:\n",
        "        yield \"\", \"Please upload a file or paste text.\", \"\", \"\", \"\", None\n",
        "        return\n",
        "\n",
        "    text = extract_text(source)\n",
        "    yield text, \"[1/6] Running token preview‚Ä¶\", \"\", \"\", \"\", \"\", None\n",
        "\n",
        "    tok = run_token_preview(text)\n",
        "    yield text, tok, \"[2/6] Running NER‚Ä¶\", \"\", \"\", \"\", None\n",
        "\n",
        "    ents = run_ner(text)\n",
        "    yield text, tok, ents, \"[3/6] Running risk scoring‚Ä¶\", \"\",\"\", None\n",
        "\n",
        "    risk = run_risk_scoring(text)\n",
        "    yield text, tok, ents, risk, \"[4/6] Running sentiment‚Ä¶\", \"\", None\n",
        "\n",
        "    sent = run_sentiment(text)\n",
        "    yield text, tok, ents, risk, sent, \"[5/6] Generating brief‚Ä¶\", None\n",
        "\n",
        "    for partial_brief in run_llm_brief(text):\n",
        "        yield text, tok, ents, risk, sent, partial_brief, None\n",
        "\n",
        "    speech = run_speech(partial_brief)\n",
        "    yield text, tok, ents, risk, sent, partial_brief, speech\n",
        "\n",
        "with gr.Blocks(title=\"Multi-Modal Document Studio\") as demo:\n",
        "    gr.Markdown(\"# üóÇÔ∏è Multi-Modal Document Studio\\nUpload a PDF/TXT or paste text below.\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\".pdf\", \".txt\"])\n",
        "        text_input = gr.Textbox(label=\"Or paste text here\", lines=8, placeholder=\"Paste document text‚Ä¶\")\n",
        "    \n",
        "    run_btn = gr.Button(\"üîç Analyse Document\", variant=\"primary\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üìä Token Preview\"):  tok_out  = gr.Markdown()\n",
        "        with gr.Tab(\"üè∑Ô∏è Named Entities\"): ent_out  = gr.Markdown()\n",
        "        with gr.Tab(\"‚ö†Ô∏è Risk Scores\"):    risk_out = gr.Markdown()\n",
        "        with gr.Tab(\"üí¨ Final Brief\"): \n",
        "            sent_out = gr.Markdown(label=\"üòê Sentiment\")     \n",
        "            llm_out  = gr.Markdown(label=\"üí¨ Brief\")\n",
        "        \n",
        "        with gr.Tab(\"üé§ Speech\"):\n",
        "            speech_out = gr.Audio(label=\"üé§ Speech\")\n",
        "        \n",
        "        \n",
        "    \n",
        "    run_btn.click(\n",
        "        gradio_analyse,\n",
        "        inputs=[file_input, text_input],\n",
        "        outputs=[text_input, tok_out, ent_out, risk_out, sent_out, llm_out, speech_out]\n",
        "    )\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d078b4d5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Concepts Demonstrated\n",
        "\n",
        "| Cell | Pipeline / API | What it shows |\n",
        "|------|---------------|---------------|\n",
        "| 4 | `pipeline('ner')` | Extract parties, dates, money |\n",
        "| 5 | `pipeline('sentiment-analysis')` | Overall document tone |\n",
        "| 6 | `pipeline('zero-shot-classification')` | Per-clause risk without labelled data |\n",
        "| 7 | `AutoTokenizer` + `apply_chat_template` | Token IDs & prompt format |\n",
        "| 8 | `AutoModelForCausalLM` + `TextIteratorStreamer` + quantization | Local LLM + streaming |\n",
        "| 8 | `gc.collect()` + `empty_cache()` | MPS/CUDA memory management |\n",
        "| 9 | End-to-end chaining | All components wired together |\n",
        "| 10 | Gradio Blocks UI | File upload, tabbed outputs, streaming |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
