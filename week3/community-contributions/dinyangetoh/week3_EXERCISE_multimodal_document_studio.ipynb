{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1af1c278",
      "metadata": {},
      "source": [
        "# üóÇÔ∏è Multi-Modal Document Studio\n",
        "**Week 3 Exercise ‚Äî End-to-End Open-Source Document Analysis**\n",
        "\n",
        "Automates first-pass review of any document (contract, email, article, policy) using only open-source models ‚Äî no API keys required.\n",
        "\n",
        "**Produces:**\n",
        "- Token structure & chat template preview\n",
        "- Structured LLM brief auto-adapted to document type\n",
        "- Named entity extraction (people, orgs, locations)\n",
        "- Per-clause risk scoring\n",
        "- Overall tone / sentiment analysis\n",
        "\n",
        "---\n",
        "**Hardware:** Set `DEVICE = \"mps\"` for Apple Silicon (default) or `DEVICE = \"cuda\"` for NVIDIA GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71cad78",
      "metadata": {},
      "source": [
        "## Cell 1 ‚Äî Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02490e23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages installed. Restart the kernel, then run from Cell 2 onwards.\n"
          ]
        }
      ],
      "source": [
        "# Run once ‚Äî restart kernel after installation\n",
        "!uv pip install -q transformers torch accelerate pdfplumber gradio sentencepiece optimum-quanto huggingface_hub\n",
        "\n",
        "print(\"‚úÖ All packages installed. Restart the kernel, then run from Cell 2 onwards.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9cc510",
      "metadata": {},
      "source": [
        "## Cell 2 ‚Äî HuggingFace Authentication\n",
        "\n",
        "Models are downloaded from the [HuggingFace Hub](https://huggingface.co). You need a free account and an access token.\n",
        "\n",
        "1. Go to https://huggingface.co/settings/tokens\n",
        "2. Create a token with **Read** permissions\n",
        "3. Set it as an environment variable before launching Jupyter: `export HF_TOKEN=hf_...`  \n",
        "   ‚Äî or paste it directly into the cell below (avoid committing it to git)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ab84e45",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Logged in to HuggingFace Hub.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option A ‚Äî read from environment variable (recommended)\n",
        "hf_token = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "\n",
        "# Option B ‚Äî paste token directly (remove before sharing this notebook)\n",
        "# hf_token = \"hf_YOUR_TOKEN_HERE\"\n",
        "\n",
        "if not hf_token:\n",
        "    raise EnvironmentError(\n",
        "        \"HuggingFace token not found.\\n\"\n",
        "        \"Set it with: export HF_TOKEN=hf_... (in your terminal before launching Jupyter)\\n\"\n",
        "        \"Or paste it into the hf_token variable above.\"\n",
        "    )\n",
        "\n",
        "login(token=hf_token, add_to_git_credential=False)\n",
        "print(\"‚úÖ Logged in to HuggingFace Hub.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34067df5",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Constants & Device Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7d9f0915",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device      : mps\n",
            "Quant backend: optimum-quanto\n",
            "\n",
            "‚úÖ Constants set.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ‚îÄ‚îÄ Device ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "    QUANT_BACKEND = \"optimum-quanto\"   # MPS path\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    QUANT_BACKEND = \"bitsandbytes\"     # CUDA path\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    QUANT_BACKEND = \"none\"\n",
        "\n",
        "print(f\"Device      : {DEVICE}\")\n",
        "print(f\"Quant backend: {QUANT_BACKEND}\")\n",
        "\n",
        "# ‚îÄ‚îÄ Model IDs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "NER_MODEL          = \"dslim/bert-base-NER\"           # Fast NER\n",
        "ZERO_SHOT_MODEL    = \"facebook/bart-large-mnli\"      # Zero-shot classification\n",
        "SENTIMENT_MODEL    = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "LLM_MODEL          = \"meta-llama/Llama-3.2-1B-Instruct\"  # Lightweight LLM, runs on CPU/MPS\n",
        "\n",
        "# ‚îÄ‚îÄ Risk labels for zero-shot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "RISK_LABELS = [\n",
        "    \"low risk\", \"medium risk\", \"high risk\",\n",
        "    \"financial obligation\", \"liability\", \"termination clause\",\n",
        "    \"data privacy\", \"intellectual property\", \"indemnification\"\n",
        "]\n",
        "\n",
        "print(\"\\n‚úÖ Constants set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93c7d57",
      "metadata": {},
      "source": [
        "## Cell 3 ‚Äî Utility: Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d492dc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text extractor ready.\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber, pathlib\n",
        "\n",
        "def extract_text(source) -> str:\n",
        "    \"\"\"Extract text from a PDF path, TXT path, or raw string.\"\"\"\n",
        "    if source is None:\n",
        "        return \"\"\n",
        "    if hasattr(source, \"name\"):          # Gradio UploadedFile\n",
        "        source = source.name\n",
        "    # Raw text strings (multi-line or long) are not valid file paths\n",
        "    if isinstance(source, str) and ('\\n' in source or len(source) > 260):\n",
        "        return source\n",
        "    p = pathlib.Path(str(source))\n",
        "    try:\n",
        "        if p.exists():\n",
        "            if p.suffix.lower() == \".pdf\":\n",
        "                with pdfplumber.open(p) as pdf:\n",
        "                    return \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
        "            else:\n",
        "                return p.read_text(errors=\"ignore\")\n",
        "    except OSError:\n",
        "        pass\n",
        "    return str(source)\n",
        "\n",
        "# Quick smoke-test\n",
        "sample = \"This Agreement is entered into between Acme Corp and John Smith on 1 Jan 2025.\"\n",
        "# print(extract_text(sample)[:200])\n",
        "print(\"‚úÖ Text extractor ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78640359",
      "metadata": {},
      "source": [
        "## Cell 4 ‚Äî Week 3 / Day 2: NER Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "782824ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NER model onto mps ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ NER pipeline ready.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "print(f\"Loading NER model onto {DEVICE} ‚Ä¶\")\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=NER_MODEL,\n",
        "    aggregation_strategy=\"simple\",\n",
        "    device=0 if DEVICE == \"cuda\" else -1  # pipeline uses int device index\n",
        ")\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)\n",
        "\n",
        "def run_ner(text: str) -> str:\n",
        "    \"\"\"Return a formatted string of named entities.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    chunk_size = 400\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    all_entities = []\n",
        "    for chunk in chunks:\n",
        "        ids = ner_tokenizer.encode(chunk, truncation=True, max_length=512, add_special_tokens=False)\n",
        "        safe_chunk = ner_tokenizer.decode(ids)\n",
        "        all_entities.extend(ner_pipeline(safe_chunk))\n",
        "    \n",
        "    if not all_entities:\n",
        "        return \"No named entities found.\"\n",
        "    \n",
        "    lines = [f\"  [{e['entity_group']}] {e['word']}  (score: {e['score']:.2f})\" for e in all_entities]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# Smoke-test\n",
        "# print(run_ner(sample))\n",
        "print(\"\\n‚úÖ NER pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0b7cef",
      "metadata": {},
      "source": [
        "## Cell 5 ‚Äî Week 3 / Day 2: Sentiment Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8b0594fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading sentiment model ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sentiment pipeline ready.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading sentiment model ‚Ä¶\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=SENTIMENT_MODEL,\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def run_sentiment(text: str) -> str:\n",
        "    \"\"\"Return overall document sentiment.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    # Sentiment models also cap at 512 tokens ‚Äî use first 400 words as proxy\n",
        "    snippet = \" \".join(text.split()[:400])\n",
        "    result = sentiment_pipeline(snippet, truncation=True, max_length=512)[0]\n",
        "    emoji = \"üü¢\" if result[\"label\"] == \"POSITIVE\" else \"üî¥\"\n",
        "    return f\"{emoji} {result['label']}  (confidence: {result['score']:.2f})\"\n",
        "\n",
        "# print(run_sentiment(sample))\n",
        "print(\"‚úÖ Sentiment pipeline ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b75a878",
      "metadata": {},
      "source": [
        "## Cell 6 ‚Äî Week 3 / Day 2: Zero-Shot Risk Scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c37be135",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading zero-shot classification model ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Risk scorer ready.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "print(\"Loading zero-shot classification model ‚Ä¶\")\n",
        "zsc_pipeline = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=ZERO_SHOT_MODEL,\n",
        "    device=0 if DEVICE == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "def split_into_clauses(text: str, max_words: int = 80) -> list[str]:\n",
        "    \"\"\"Split text into sentence-level chunks suitable for per-clause scoring.\"\"\"\n",
        "    import re\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    clauses, current = [], []\n",
        "    for sent in sentences:\n",
        "        current.append(sent)\n",
        "        if len(\" \".join(current).split()) >= max_words:\n",
        "            clauses.append(\" \".join(current))\n",
        "            current = []\n",
        "    if current:\n",
        "        clauses.append(\" \".join(current))\n",
        "    return clauses[:10]  # Cap at 10 clauses for speed\n",
        "\n",
        "def run_risk_scoring(text: str) -> str:\n",
        "    \"\"\"Score each clause and return a formatted risk report.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    clauses = split_into_clauses(text)\n",
        "    lines = []\n",
        "    for i, clause in enumerate(clauses, 1):\n",
        "        result = zsc_pipeline(clause, candidate_labels=RISK_LABELS, truncation=True, max_length=512)\n",
        "        top_label = result[\"labels\"][0]\n",
        "        top_score = result[\"scores\"][0]\n",
        "        risk_icon = \"üî¥\" if \"high\" in top_label or top_label in (\"liability\",\"indemnification\") else \\\n",
        "                    \"üü°\" if \"medium\" in top_label or top_label in (\"termination clause\",\"financial obligation\") else \"üü¢\"\n",
        "        snippet = textwrap.shorten(clause, width=90, placeholder=\"‚Ä¶\")\n",
        "        lines.append(f\"Clause {i:02d}: {risk_icon} {top_label} ({top_score:.2f})\\n          \\\"{snippet}\\\"\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# Smoke-test on a two-sentence doc\n",
        "test_doc = (\n",
        "    \"The Licensor shall not be liable for any indirect damages arising from the use of this software. \"\n",
        "    \"Either party may terminate this agreement with 30 days written notice.\"\n",
        ")\n",
        "# print(run_risk_scoring(test_doc))\n",
        "print(\"\\n‚úÖ Risk scorer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a9bfff",
      "metadata": {},
      "source": [
        "## Cell 7 ‚Äî Week 3 / Day 3: Tokenizer + Chat Template Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "957f6451",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer ‚Ä¶\n",
            "üìä Token Stats\n",
            "  Raw token count    : 21\n",
            "  First 10 token IDs : [128000, 2028, 23314, 374, 10862, 1139, 1990, 6515, 2727, 22621]\n",
            "  Chat prompt tokens : 69\n",
            "\n",
            "\n",
            "\n",
            "‚úÖ Tokenizer ready.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Loading tokenizer ‚Ä¶\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "\n",
        "def run_token_preview(text: str) -> str:\n",
        "    \"\"\"Show token count and the chat-template-formatted prompt.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided.\"\n",
        "    \n",
        "    # Raw tokenization\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_count = len(tokens)\n",
        "    first_10 = tokens[:10]\n",
        "    \n",
        "    # Chat template ‚Äî the Day 3 \"aha moment\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional document analyst.\"},\n",
        "        {\"role\": \"user\",   \"content\": f\"Briefly summarise this document:\\n\\n{' '.join(text.split()[:300])}\"}\n",
        "    ]\n",
        "    # chat_prompt = tokenizer.apply_chat_template(\n",
        "    #     messages, tokenize=False, add_generation_prompt=True\n",
        "    # )\n",
        "    chat_tokens = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    return (\n",
        "        f\"üìä Token Stats\\n\"\n",
        "        f\"  Raw token count    : {token_count}\\n\"\n",
        "        f\"  First 10 token IDs : {first_10}\\n\"\n",
        "        f\"  Chat prompt tokens : {len(chat_tokens)}\\n\\n\"\n",
        "        # f\"üìù Chat-Template Prompt (first 600 chars):\\n\"\n",
        "        # f\"{'-'*60}\\n\"\n",
        "        # f\"{chat_prompt[:600]}\"\n",
        "    )\n",
        "\n",
        "print(run_token_preview(sample))\n",
        "print(\"\\n‚úÖ Tokenizer ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6dc7d2",
      "metadata": {},
      "source": [
        "## Cell 8 ‚Äî Week 3 / Days 4‚Äì5: LLM Brief Generator (Streaming)\n",
        "\n",
        "> **Note:** First run will download ~600 MB for TinyLlama. Subsequent runs use the local cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0913cb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LLM on mps ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/davidinyang-etoh/Projects/ai-projects/llm_engineering/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM loaded.\n",
            "=== LLM Brief (streaming) ===\n"
          ]
        }
      ],
      "source": [
        "import gc, threading\n",
        "from transformers import AutoModelForCausalLM, TextIteratorStreamer\n",
        "\n",
        "print(f\"Loading LLM on {DEVICE} ‚Ä¶\")\n",
        "\n",
        "# Quantization ‚Äî MPS path uses optimum-quanto; CUDA path uses bitsandbytes\n",
        "load_kwargs = dict(device_map=\"auto\" if DEVICE == \"cuda\" else None)\n",
        "\n",
        "if QUANT_BACKEND == \"bitsandbytes\":\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n",
        "elif QUANT_BACKEND == \"optimum-quanto\":\n",
        "    from optimum.quanto import quantize, qint8\n",
        "    # We quantize after loading for MPS\n",
        "    pass\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL, **load_kwargs)\n",
        "\n",
        "if QUANT_BACKEND == \"optimum-quanto\":\n",
        "    quantize(llm_model, weights=qint8)\n",
        "\n",
        "if DEVICE == \"mps\":\n",
        "    llm_model = llm_model.to(\"mps\")\n",
        "\n",
        "llm_model.eval()\n",
        "print(\"‚úÖ LLM loaded.\")\n",
        "\n",
        "# ‚îÄ‚îÄ Document-type detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def detect_doc_type(text: str) -> str:\n",
        "    text_lower = text.lower()\n",
        "    if any(w in text_lower for w in [\"whereas\", \"licensor\", \"indemnif\", \"herein\", \"party\"]):\n",
        "        return \"legal contract\"\n",
        "    elif any(w in text_lower for w in [\"dear\", \"regards\", \"sincerely\", \"subject:\"]):\n",
        "        return \"email\"\n",
        "    elif any(w in text_lower for w in [\"privacy\", \"data controller\", \"gdpr\", \"personal data\"]):\n",
        "        return \"privacy policy\"\n",
        "    elif any(w in text_lower for w in [\"abstract\", \"methodology\", \"conclusion\", \"references\"]):\n",
        "        return \"research article\"\n",
        "    else:\n",
        "        return \"general document\"\n",
        "\n",
        "def build_brief_prompt(text: str) -> list[dict]:\n",
        "    doc_type = detect_doc_type(text)\n",
        "    instruction = (\n",
        "        f\"You are a professional document analyst. The following is a {doc_type}.\\n\"\n",
        "        \"Provide a structured brief with:\\n\"\n",
        "        \"1. One-sentence summary\\n\"\n",
        "        \"2. Key parties or stakeholders\\n\"\n",
        "        \"3. Main obligations or key points (up to 5 bullet points)\\n\"\n",
        "        \"4. Notable risks or red flags\\n\"\n",
        "        \"5. Recommended next action\\n\"\n",
        "        \"Be concise. Use bullet points.\"\n",
        "    )\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\",   \"content\": text[:1500]}  # cap context for speed\n",
        "    ]\n",
        "\n",
        "def run_llm_brief(text: str, max_new_tokens: int = 400):\n",
        "    \"\"\"Generate a structured LLM brief. Yields accumulated text as each token arrives.\"\"\"\n",
        "    if not text.strip():\n",
        "        yield \"No text provided.\"\n",
        "        return\n",
        "\n",
        "    messages = build_brief_prompt(text)\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    attention_mask = torch.ones_like(input_ids).to(DEVICE)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        streamer=streamer,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    thread = threading.Thread(target=llm_model.generate, kwargs=gen_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    accumulated = \"\"\n",
        "    for token_text in streamer:\n",
        "        accumulated += token_text\n",
        "        yield accumulated\n",
        "\n",
        "    thread.join()\n",
        "\n",
        "    del input_ids\n",
        "    gc.collect()\n",
        "    if DEVICE == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    elif DEVICE == \"mps\":\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "# Smoke-test\n",
        "print(\"=== LLM Brief (streaming) ===\")\n",
        "# _ = run_llm_brief(test_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a6ad7c",
      "metadata": {},
      "source": [
        "## Cell 9 ‚Äî Full Analysis Pipeline (no UI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fa9981a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document analysis ready\n"
          ]
        }
      ],
      "source": [
        "def analyse_document(source, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Run the full 5-stage analysis on any document source.\n",
        "    source: file path (PDF/TXT) or raw text string.\n",
        "    Returns a dict with keys: text, token_preview, entities, risk_scores, sentiment, llm_brief\n",
        "    \"\"\"\n",
        "    sep = \"=\" * 70\n",
        "    \n",
        "    text = extract_text(source)\n",
        "    if not text.strip():\n",
        "        print(\"‚ö†Ô∏è  No text found in document.\")\n",
        "        return {}\n",
        "    \n",
        "    results = {\"text\": text}\n",
        "    \n",
        "    # Stage 1 ‚Äî Token preview (Day 3)\n",
        "    print(f\"{sep}\\n[1/5] TOKENIZER PREVIEW\\n{sep}\")\n",
        "    results[\"token_preview\"] = run_token_preview(text)\n",
        "    if verbose: print(results[\"token_preview\"])\n",
        "    \n",
        "    # Stage 2 ‚Äî Named entities (Day 2)\n",
        "    print(f\"\\n{sep}\\n[2/5] NAMED ENTITY EXTRACTION\\n{sep}\")\n",
        "    results[\"entities\"] = run_ner(text)\n",
        "    if verbose: print(results[\"entities\"])\n",
        "    \n",
        "    # Stage 3 ‚Äî Risk scoring (Day 2)\n",
        "    print(f\"\\n{sep}\\n[3/5] PER-CLAUSE RISK SCORING\\n{sep}\")\n",
        "    results[\"risk_scores\"] = run_risk_scoring(text)\n",
        "    if verbose: print(results[\"risk_scores\"])\n",
        "    \n",
        "    # Stage 4 ‚Äî Sentiment (Day 2)\n",
        "    print(f\"\\n{sep}\\n[4/5] OVERALL SENTIMENT\\n{sep}\")\n",
        "    results[\"sentiment\"] = run_sentiment(text)\n",
        "    if verbose: print(results[\"sentiment\"])\n",
        "    \n",
        "    # Stage 5 ‚Äî LLM brief (Days 4-5)\n",
        "    print(f\"\\n{sep}\\n[5/5] LLM DOCUMENT BRIEF (streaming)\\n{sep}\")\n",
        "    results[\"llm_brief\"] = \"\".join(run_llm_brief(text))\n",
        "    \n",
        "    print(f\"\\n{sep}\\n‚úÖ Analysis complete.\\n{sep}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ Run on a sample contract excerpt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CONTRACT_SAMPLE = \"\"\"\n",
        "SERVICE AGREEMENT\n",
        "\n",
        "This Agreement is entered into as of January 1, 2025 between Acme Corporation\n",
        "(\"Client\") and TechSolutions Ltd (\"Service Provider\").\n",
        "\n",
        "1. SERVICES. Service Provider agrees to develop and deliver a custom analytics\n",
        "dashboard by March 31, 2025. Deliverables are specified in Exhibit A.\n",
        "\n",
        "2. PAYMENT. Client shall pay $50,000 USD within 30 days of invoice. Late payments\n",
        "shall incur a penalty of 1.5% per month.\n",
        "\n",
        "3. TERMINATION. Either party may terminate this Agreement with 30 days written\n",
        "notice. Client may terminate immediately for material breach.\n",
        "\n",
        "4. LIMITATION OF LIABILITY. In no event shall either party be liable for indirect,\n",
        "incidental, or consequential damages.\n",
        "\n",
        "5. GOVERNING LAW. This Agreement shall be governed by the laws of the State of\n",
        "California. Any disputes shall be resolved by arbitration in San Francisco.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Document analysis ready\")\n",
        "\n",
        "# results = analyse_document(CONTRACT_SAMPLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f19215c",
      "metadata": {},
      "source": [
        "## Cell 10 ‚Äî Gradio UI (Optional)\n",
        "\n",
        "Launches a single-page web UI with file upload and streaming LLM output.\n",
        "\n",
        "> Run this cell to start the app. A local URL (e.g. `http://127.0.0.1:7860`) will appear below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45ab09b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_analyse(file_obj, raw_text: str):\n",
        "    \"\"\"Gradio handler ‚Äî streams results stage-by-stage as each completes.\"\"\"\n",
        "    source = file_obj if file_obj is not None else raw_text\n",
        "    if not source:\n",
        "        yield \"\", \"Please upload a file or paste text.\", \"\", \"\", \"\", \"\"\n",
        "        return\n",
        "\n",
        "    text = extract_text(source)\n",
        "    yield text, \"[1/5] Running token preview‚Ä¶\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    tok = run_token_preview(text)\n",
        "    yield text, tok, \"[2/5] Running NER‚Ä¶\", \"\", \"\", \"\"\n",
        "\n",
        "    ents = run_ner(text)\n",
        "    yield text, tok, ents, \"[3/5] Running risk scoring‚Ä¶\", \"\", \"\"\n",
        "\n",
        "    risk = run_risk_scoring(text)\n",
        "    yield text, tok, ents, risk, \"[4/5] Running sentiment‚Ä¶\", \"\"\n",
        "\n",
        "    sent = run_sentiment(text)\n",
        "    yield text, tok, ents, risk, sent, \"[5/5] Generating LLM brief‚Ä¶\"\n",
        "\n",
        "    for partial_brief in run_llm_brief(text):\n",
        "        yield text, tok, ents, risk, sent, partial_brief\n",
        "\n",
        "with gr.Blocks(title=\"Multi-Modal Document Studio\") as demo:\n",
        "    gr.Markdown(\"# üóÇÔ∏è Multi-Modal Document Studio\\nUpload a PDF/TXT or paste text below.\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\".pdf\", \".txt\"])\n",
        "        text_input = gr.Textbox(label=\"Or paste text here\", lines=8, placeholder=\"Paste document text‚Ä¶\")\n",
        "    \n",
        "    run_btn = gr.Button(\"üîç Analyse Document\", variant=\"primary\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üìä Token Preview\"):  tok_out  = gr.Textbox(lines=15, show_label=False)\n",
        "        with gr.Tab(\"üè∑Ô∏è Named Entities\"): ent_out  = gr.Markdown()\n",
        "        with gr.Tab(\"‚ö†Ô∏è Risk Scores\"):    risk_out = gr.Markdown()\n",
        "        with gr.Tab(\"üòê Sentiment\"):      sent_out = gr.Markdown()\n",
        "        with gr.Tab(\"ü§ñ LLM Brief\"):      llm_out  = gr.Markdown()\n",
        "    \n",
        "    run_btn.click(\n",
        "        gradio_analyse,\n",
        "        inputs=[file_input, text_input],\n",
        "        outputs=[text_input, tok_out, ent_out, risk_out, sent_out, llm_out]\n",
        "    )\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d078b4d5",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Concepts Demonstrated\n",
        "\n",
        "| Cell | Week 3 Concept | What it shows |\n",
        "|------|---------------|---------------|\n",
        "| 4 | Day 2 ‚Äî `pipeline('ner')` | Extract parties, dates, money |\n",
        "| 5 | Day 2 ‚Äî `pipeline('sentiment-analysis')` | Overall document tone |\n",
        "| 6 | Day 2 ‚Äî `pipeline('zero-shot-classification')` | Per-clause risk without labelled data |\n",
        "| 7 | Day 3 ‚Äî `AutoTokenizer` + `apply_chat_template` | Token IDs & prompt format |\n",
        "| 8 | Day 4 ‚Äî `AutoModelForCausalLM` + `TextIteratorStreamer` + quantization | Local LLM + streaming |\n",
        "| 8 | Day 4 ‚Äî `gc.collect()` + `empty_cache()` | MPS/CUDA memory management |\n",
        "| 9 | Day 5 ‚Äî End-to-end chaining | All components wired together |\n",
        "| 10 | ‚Äî | Gradio UI with file upload |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
