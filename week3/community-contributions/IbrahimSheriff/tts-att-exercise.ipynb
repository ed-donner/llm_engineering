{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Meeting Audio Processing Pipeline\n",
        "### This notebook walks you through an easy workflow for processing meeting audio:\n",
        "1. Upload a meeting recording.\n",
        "2. Transcribe the audio to text automatically.\n",
        "3. Get a summarized version of the meeting notes.\n",
        "4. Listen to a spoken version of the summary (text-to-speech).\n",
        "###With this, you‚Äôll quickly generate readable and audible meeting summaries‚Äîperfect for documentation and accessibility."
      ],
      "id": "d9f3d0d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Load Environment Variables\n",
        "\n",
        "This cell imports all the dependencies used throughout the notebook:\n",
        "\n",
        "- **`base64`** ‚Äî for encoding audio files before sending them to cloud APIs.\n",
        "- **`numpy` / `scipy.io.wavfile`** ‚Äî for audio array manipulation and WAV file I/O.\n",
        "- **`tempfile` / `os`** ‚Äî for creating temporary files and reading environment variables.\n",
        "- **`gradio`** ‚Äî for building the interactive web UI.\n",
        "- **`dotenv`** ‚Äî for loading API keys from a `.env` file.\n",
        "- **`huggingface_hub`** ‚Äî for authenticating with the Hugging Face model hub.\n",
        "- **`openai`** ‚Äî the OpenAI Python client, also used to talk to OpenRouter.\n",
        "- **`transformers`** ‚Äî for loading the local Whisper speech-recognition model.\n",
        "\n",
        "`load_dotenv(override=True)` reads the `.env` file and makes its variables available via `os.getenv`."
      ],
      "id": "84abff70"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import base64\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "import tempfile\n",
        "import os\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from openai import OpenAI\n",
        "from transformers import pipeline\n",
        "\n",
        "load_dotenv(override=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          }
        }
      ],
      "id": "756df119"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Verify Hugging Face Token\n",
        "\n",
        "Load the `HF_TOKEN` environment variable and confirm it is set. This token is needed to authenticate with the Hugging Face Hub for downloading the Whisper model in a later step."
      ],
      "id": "05c07226"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "  print(\"HF_TOKEN is not set\")\n",
        "else:\n",
        "  print(\"HF_TOKEN is set\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HF_TOKEN is set\n"
          ]
        }
      ],
      "id": "a1457aa4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Authenticate with Hugging Face\n",
        "\n",
        "Log in to the Hugging Face Hub using the token retrieved in the previous step. This authentication is required to download gated or private models (like Whisper) from the Hub."
      ],
      "id": "47191ab9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Login to Hugging Face\n",
        "login(token=HF_TOKEN)\n",
        "print (\"Logged in to Hugging Face\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Logged in to Hugging Face\n"
          ]
        }
      ],
      "id": "354a7cb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load the Whisper Speech Recognition Model\n",
        "\n",
        "The Hugging Face `pipeline` API is used to load **OpenAI's Whisper Medium (English)** model locally. On the first run this downloads ~3 GB of model weights. The `transcribe_audio_hf` wrapper function accepts an audio file path and returns the transcribed text. This provides an offline/local alternative to the cloud-based Gemini transcription used later."
      ],
      "id": "c1001948"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load Whisper pipeline once (downloads ~3GB on first run)\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-medium.en\")\n",
        "\n",
        "def transcribe_audio_hf(audio_path: str) -> str:\n",
        "    \"\"\"Transcribe audio using HuggingFace Whisper (local model).\"\"\"\n",
        "    result = pipe(audio_path)\n",
        "    return result[\"text\"]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d83cbf9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify OpenRouter API Key\n",
        "\n",
        "Similar to the Hugging Face token check, this cell loads the `OPENROUTER_API_KEY` from the environment and confirms it is available. OpenRouter is used as a unified gateway to access multiple LLM providers (GPT-4o-mini for summarization, Gemini for transcription)."
      ],
      "id": "215b04da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "  print(\"OPENROUTER_API_KEY is not set\")\n",
        "else:\n",
        "  print(\"OPENROUTER_API_KEY is set\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPENROUTER_API_KEY is set\n"
          ]
        }
      ],
      "id": "67f539f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize API Clients\n",
        "\n",
        "Two OpenAI-compatible clients are created:\n",
        "\n",
        "- **`openrouter`** ‚Äî points to the OpenRouter API (`openrouter.ai/api/v1`). This is used for transcription (Gemini) and summarization (GPT-4o-mini).\n",
        "- **`openai_client`** ‚Äî points to the standard OpenAI API. This is used for text-to-speech generation, which requires a direct OpenAI connection."
      ],
      "id": "6045f3a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "openrouter = OpenAI(api_key=OPENROUTER_API_KEY, base_url=\"https://openrouter.ai/api/v1\")\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "execution_count": 31,
      "outputs": [],
      "id": "8d4bef1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Define the Summarization System Prompt\n",
        "\n",
        "This system prompt tells the LLM how to behave when summarizing a meeting transcript. It instructs the model to extract key discussion points, decisions, and action items with owners, and to keep the output concise enough to be read aloud (since the summary will later be converted to speech)."
      ],
      "id": "f9687eb9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an expert at summarizing meeting transcriptions.\n",
        "Given a raw meeting transcript, produce a concise, well-structured summary that covers:\n",
        "- Key discussion points\n",
        "- Decisions made\n",
        "- Action items with owners (if mentioned)\n",
        "Keep the summary clear and suitable for being read aloud.\"\"\""
      ],
      "execution_count": 32,
      "outputs": [],
      "id": "49acedb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Create the Transcript Summarization Function\n",
        "\n",
        "`summarize_transcript` sends the raw transcript to **GPT-4o-mini** via OpenRouter with streaming enabled. As each chunk of the summary arrives, the function yields the accumulated text so the UI can display it progressively ‚Äî giving the user real-time feedback instead of waiting for the full response."
      ],
      "id": "747e1ebe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def summarize_transcript(transcript: str):\n",
        "    \"\"\"Summarize a meeting transcript using OpenRouter with streaming.\"\"\"\n",
        "    stream = openrouter.chat.completions.create(\n",
        "        model=\"openai/gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"Please summarize the following meeting transcript:\\n\\n{transcript}\"},\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "    collected = \"\"\n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta\n",
        "        if delta.content:\n",
        "            collected += delta.content\n",
        "            yield collected"
      ],
      "execution_count": 33,
      "outputs": [],
      "id": "2e668677"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Define Audio Transcription (Gemini) and Text-to-Speech Functions\n",
        "\n",
        "Two utility functions are defined here:\n",
        "\n",
        "- **`transcribe_audio_openrouter`** ‚Äî reads the audio file, base64-encodes it, and sends it to **Google Gemini 2.0 Flash** via OpenRouter's chat completions endpoint. The audio is embedded as a data URL inside a multimodal message, and Gemini returns a verbatim transcription.\n",
        "- **`text_to_speech`** ‚Äî sends the summary text to the **OpenAI TTS API** (`gpt-4o-mini-tts`, voice \"alloy\") and streams the response directly to a WAV file on disk."
      ],
      "id": "d9f274ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transcribe_audio_openrouter(audio_path: str) -> str:\n",
        "    print(\"Transcribe audio by sending it to Gemini via OpenRouter as a base64 data URL.\")\n",
        "    with open(audio_path, \"rb\") as f:\n",
        "        audio_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    ext = audio_path.rsplit(\".\", 1)[-1].lower()\n",
        "    mime = {\"mp3\": \"audio/mpeg\", \"wav\": \"audio/wav\", \"m4a\": \"audio/mp4\"}.get(ext, \"audio/mpeg\")\n",
        "    print(\"ext\", ext)\n",
        "    response = openrouter.chat.completions.create(\n",
        "        model=\"google/gemini-2.0-flash-001\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio recording verbatim.\"},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{audio_b64}\"}},\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def text_to_speech(text: str, output_path: str = \"summary_audio.wav\") -> str:\n",
        "    \"\"\"Convert text to speech using OpenAI TTS API directly.\"\"\"\n",
        "    response = openai_client.audio.speech.create(\n",
        "        model=\"gpt-4o-mini-tts\",\n",
        "        voice=\"alloy\",\n",
        "        input=text,\n",
        "        response_format=\"wav\",\n",
        "    )\n",
        "    response.stream_to_file(output_path)\n",
        "    return output_path"
      ],
      "execution_count": 34,
      "outputs": [],
      "id": "3f21f74c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Build the Gradio Web Interface\n",
        "\n",
        "This is the main orchestration layer. Two core functions tie everything together:\n",
        "\n",
        "- **`summarize_meeting_audio`** ‚Äî accepts an uploaded audio file and the chosen transcription engine, runs transcription followed by summarization, and streams progress updates to the UI.\n",
        "- **`generate_audio_from_summary`** ‚Äî takes the finished summary text and converts it to a WAV file via the OpenAI TTS API.\n",
        "\n",
        "A **Gradio Blocks** UI is then assembled with:\n",
        "- An audio upload widget and a radio selector for the transcription engine (Gemini via OpenRouter *or* local Whisper).\n",
        "- A \"Summarize\" button that triggers transcription + summarization.\n",
        "- A \"Generate Audio Summary\" button that produces a spoken version of the summary.\n",
        "- Live status indicators and a markdown display for the generated summary."
      ],
      "id": "9c5a4c24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def summarize_meeting_audio(audio_file_path: str, transcription_method: str):\n",
        "    \"\"\"Transcribe and summarize the meeting audio with streaming.\"\"\"\n",
        "    if audio_file_path is None:\n",
        "        yield \"\", \"Please upload an audio file.\", \"Please upload an audio file.\"\n",
        "        return\n",
        "\n",
        "    yield \"‚è≥ Transcribing audio...\", \"\", \"\"\n",
        "\n",
        "    if transcription_method == \"OpenRouter / Gemini\":\n",
        "        transcript = transcribe_audio_openrouter(audio_file_path)\n",
        "    else:\n",
        "        transcript = transcribe_audio_hf(audio_file_path)\n",
        "\n",
        "    yield \"‚úÖ Transcribed audio\\n\\n‚è≥ Summarizing transcript...\", \"\", \"\"\n",
        "\n",
        "    final = \"\"\n",
        "    for partial in summarize_transcript(transcript):\n",
        "        final = partial\n",
        "        yield \"‚úÖ Transcribed audio\\n\\n‚è≥ Summarizing transcript...\", final, final\n",
        "\n",
        "    yield \"‚úÖ Transcribed audio\\n\\n‚úÖ Summarized transcript\", final, final\n",
        "\n",
        "\n",
        "def generate_audio_from_summary(summary: str):\n",
        "    \"\"\"Convert an existing summary text to speech.\"\"\"\n",
        "    if not summary or summary.strip() == \"\":\n",
        "        yield \"‚ö†Ô∏è No summary to convert to audio.\", None\n",
        "        return\n",
        "    try:\n",
        "        yield \"‚è≥ Generating audio from summary...\", None\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "            tts_path = tmp.name\n",
        "        audio_path = text_to_speech(summary, tts_path)\n",
        "        yield \"‚úÖ Audio summary generated\", audio_path\n",
        "    except Exception as e:\n",
        "        yield f\"‚ùå TTS failed: {e}\", None\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Meeting Audio Summarizer\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üéôÔ∏è Meeting Audio Summarizer\n",
        "        Upload a meeting recording (MP3 or WAV) and get an AI-generated summary ‚Äî both as text and audio.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            audio_input = gr.Audio(\n",
        "                label=\"Upload Meeting Recording (MP3 / WAV / M4A)\",\n",
        "                type=\"filepath\",\n",
        "                sources=[\"upload\"],\n",
        "                waveform_options=gr.WaveformOptions(show_recording_waveform=True),\n",
        "            )\n",
        "            transcription_choice = gr.Radio(\n",
        "                choices=[\n",
        "                    \"OpenRouter / Gemini\",\n",
        "                    \"HuggingFace Whisper (local, ~3 GB)\",\n",
        "                ],\n",
        "                value=\"OpenRouter / Gemini\",\n",
        "                label=\"Transcription Engine\",\n",
        "            )\n",
        "            summarize_btn = gr.Button(\"Summarize\", variant=\"primary\")\n",
        "            speak_btn = gr.Button(\"Generate Audio Summary\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            status_display = gr.Markdown(value=\"\", elem_id=\"status-log\")\n",
        "            summary_state = gr.State(\"\")\n",
        "            summary_display = gr.Markdown(\n",
        "                value=\"*Your meeting summary will appear here...*\",\n",
        "            )\n",
        "            summary_audio = gr.Audio(\n",
        "                label=\"Listen to Summary\",\n",
        "                type=\"filepath\",\n",
        "                interactive=False,\n",
        "                waveform_options=gr.WaveformOptions(show_recording_waveform=True),\n",
        "            )\n",
        "\n",
        "    summarize_btn.click(\n",
        "        fn=summarize_meeting_audio,\n",
        "        inputs=[audio_input, transcription_choice],\n",
        "        outputs=[status_display, summary_state, summary_display],\n",
        "    )\n",
        "\n",
        "    speak_btn.click(\n",
        "        fn=generate_audio_from_summary,\n",
        "        inputs=[summary_state],\n",
        "        outputs=[status_display, summary_audio],\n",
        "    )\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7866\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 125, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 111, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 391, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/Users/sheriffibrahim/.local/share/uv/python/cpython-3.12.12-macos-x86_64-none/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/projects/llm_engineering/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/.local/share/uv/python/cpython-3.12.12-macos-x86_64-none/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/sheriffibrahim/.local/share/uv/python/cpython-3.12.12-macos-x86_64-none/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x1385b35c0 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Transcribe audio by sending it to Gemini via OpenRouter as a base64 data URL.\n",
            "ext mp3\n",
            "Transcribe audio by sending it to Gemini via OpenRouter as a base64 data URL.\n",
            "ext mp3\n",
            "Transcribe audio by sending it to Gemini via OpenRouter as a base64 data URL.\n",
            "ext mp3\n"
          ]
        }
      ],
      "id": "4d3e8ef2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}