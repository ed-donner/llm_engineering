{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acc85bcf",
      "metadata": {},
      "source": [
        "# Synthetic Churn Dataset Generator\n",
        "\n",
        "Generate synthetic customer churn data via **voice** or **text**. Audio is transcribed with **Whisper** (Hugging Face); **MODEL** generates a markdown table streamed to the UI with **TextIteratorStreamer**.\n",
        "\n",
        "Tested on PC (RTX 2050)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ceb6fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "! uv pip install -q torch transformers bitsandbytes accelerate sentencepiece gradio python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a674419",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import torch\n",
        "import gradio as gr\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextIteratorStreamer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "# log into HF using HF_TOKEN from .env file for gated models\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d466bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    print(\"Logged in to Hugging Face.\")\n",
        "else:\n",
        "    print(\"HF_TOKEN not set in .env; skip if using only public models (e.g. MODEL, Whisper).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c027c698",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "MODEL = \"Qwen/Qwen1.5-0.5B-Chat\"                            # not a gated model\n",
        "WHISPER_MODEL = \"openai/whisper-base\"                       # not a gated model\n",
        "MAX_TOKENS = 2048\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5ad182",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if getattr(tokenizer, \"pad_token\", None) is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c65eb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper for speech-to-text (Hugging Face)\n",
        "whisper_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=WHISPER_MODEL,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727cdca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt: elaborate synthetic churn data generation for subscription services (markdown table only, no tools)\n",
        "CHURN_SYSTEM_PROMPT = \"\"\"You are an expert synthetic data generator specializing in customer churn datasets for subscription-based businesses (SaaS, streaming, membership, or recurring-revenue services). Your task is to produce realistic, analysis-ready tabular data suitable for training churn prediction models or exploratory analysis.\n",
        "\n",
        "## Schema (use exactly these columns in this order)\n",
        "- subscriber_id: unique identifier (e.g. SUB-001, or numeric id). No PII.\n",
        "- tenure_months: integer, months as a paying subscriber (0–120). New subscribers can be 0–3; long-term often 12+.\n",
        "- plan_tier: exactly one of: Basic, Standard, Premium, Enterprise.\n",
        "- billing_cycle: exactly one of: Monthly, Quarterly, Annual.\n",
        "- monthly_revenue: decimal (e.g. 9.99, 49.00). Must be positive. Vary by plan_tier (Basic lowest, Enterprise highest).\n",
        "- total_revenue: decimal, cumulative revenue (tenure_months * monthly_revenue, with some variation). Must be >= monthly_revenue.\n",
        "- num_logins_90d: integer, logins in last 90 days (0–500). Lower engagement often correlates with churn.\n",
        "- support_tickets: integer, tickets in last 12 months (0–30). Very high tickets can correlate with churn.\n",
        "- churned: exactly \"Yes\" or \"No\". Interpret the user's requested row count; produce a realistic mix (e.g. 15–40% churned unless asked otherwise).\n",
        "- cancel_reason: when churned is Yes, use exactly one of: Price, Competitor, Not using, Missing features, Support issues, Other. When churned is No, use \"-\" or leave blank.\n",
        "\n",
        "## Realism and correlations\n",
        "- Make data internally consistent: e.g. longer tenure usually implies higher total_revenue; Annual billing often has lower monthly_revenue per unit; Enterprise plans have higher revenue.\n",
        "- Churned subscribers tend to have lower num_logins_90d, and sometimes higher support_tickets or shorter tenure. Do not make it deterministic; add variety so the dataset is useful for ML.\n",
        "- Vary numeric values (revenue, logins, tickets) so distributions look plausible—include some outliers and edge cases (e.g. one or two high-engagement churners, or low-engagement non-churners).\n",
        "\n",
        "## Output format\n",
        "- Your entire reply must be ONLY a markdown table: optionally one short header line (e.g. \"Subscription churn dataset (N rows)\") immediately followed by the table.\n",
        "- Use standard markdown table syntax with a header row and pipe separators. No code blocks, no explanations, no extra text before or after the table.\n",
        "- Respect the user's requested number of rows when they specify it; otherwise default to a small table (e.g. 10–15 rows).\n",
        "\n",
        "Output nothing else.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c17376",
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcribe audio file with Whisper (Hugging Face). Returns text or empty string.\"\"\"\n",
        "    if not audio_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        out = whisper_pipeline(audio_path)\n",
        "        return (out.get(\"text\") or \"\").strip()\n",
        "    except Exception as e:\n",
        "        return f\"(Transcription error: {e})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9558712",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_churn_stream(user_text):\n",
        "    \"\"\"Generate synthetic churn data with MODEL; stream via TextIteratorStreamer to Gradio Markdown.\"\"\"\n",
        "    if not (user_text or \"\").strip():\n",
        "        user_text = \" \"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": CHURN_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_text.strip()},\n",
        "    ]\n",
        "\n",
        "    # for instruct-type models\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    # # for base-type models\n",
        "    # prompt = f\"{CHURN_SYSTEM_PROMPT}\\n\\nUser: {user_text.strip()}\\nAssistant:\"\n",
        "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,\n",
        "        skip_prompt=True,\n",
        "        decode_kwargs={\"skip_special_tokens\": True},\n",
        "    )\n",
        "    thread = threading.Thread(\n",
        "        target=model.generate,\n",
        "        kwargs={\"inputs\": inputs, \"max_new_tokens\": MAX_TOKENS, \"streamer\": streamer},\n",
        "    )\n",
        "    thread.start()\n",
        "    accumulated = \"\"\n",
        "    for text_chunk in streamer:\n",
        "        filtered = text_chunk.replace(\"<|eot_id|>\", \"\").replace(\"<|endoftext|>\", \"\")\n",
        "        accumulated += filtered\n",
        "        yield accumulated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0629a225",
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_from_voice(audio_path):\n",
        "    \"\"\"Transcribe audio then stream churn table into Markdown.\"\"\"\n",
        "    text = transcribe_audio(audio_path)\n",
        "    if not text.strip():\n",
        "        text = \"\"\n",
        "    yield from generate_churn_stream(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4587d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Synthetic Churn Dataset Generator\") as demo:\n",
        "    gr.Markdown(\"## Synthetic Churn Dataset Generator\")\n",
        "    gr.Markdown(\"Use **voice** or **text** to request a markdown table. Output streams in real time.\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            audio_input = gr.Audio(\n",
        "                sources=[\"microphone\"],\n",
        "                type=\"filepath\",\n",
        "                label=\"Speak your request (e.g. 'Generate 20 rows of churn data')\",\n",
        "            )\n",
        "            btn_voice = gr.Button(\"Generate from voice\")\n",
        "    markdown_output = gr.Markdown(label=\"Generated churn data\", min_height=200)\n",
        "\n",
        "    btn_voice.click(fn=stream_from_voice, inputs=[audio_input], outputs=[markdown_output])\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a003fb3f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-engineering (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
