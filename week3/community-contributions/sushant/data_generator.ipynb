{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaa5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ac5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "openr = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "MODEL = {\n",
    "    \"nvidia\":\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "    \"upstage\":\"upstage/solar-pro-3:free\",\n",
    "    \"liquidAI\":\"liquid/lfm-2.5-1.2b-instruct:free\",\n",
    "    \"gemini\":\"gemini-2.5-flash\"\n",
    "}\n",
    "\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f8425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt to generate Data Generation Prompt\n",
    "system_prompt_to_generate_data_generation_prompt = \"\"\"\n",
    "# You are a Senior Data Architect and Synthetic Data Specialist. Your goal is to write a highly optimized \"Data Generation Prompt\" that I can use to generate realistic datasets.\n",
    "\n",
    "## Process: Do NOT generate any data or the final prompt yet. Instead, guide me through a requirements gathering phase to understand the statistical nuances of the data I need.\n",
    "\n",
    "### Phase 1: The Interview Ask me the following questions. You may ask them all at once or step-by-step, but you must get clear answers for:\n",
    "\n",
    "- The Domain & Use Case: (What is this data for? e.g., Testing a fraud detection model, populating a demo dashboard, load testing?)\n",
    "\n",
    "- The Schema: (What columns are strictly required? What data types?)\n",
    "\n",
    "- The Business Logic & Correlations: (This is critical for realism. e.g., \"If Status is 'Shipped', then Delivery_Date must be populated,\" or \"High Salary correlates with Senior job titles.\")\n",
    "\n",
    "- Edge Cases & Noise: (Do you want perfect data, or should I introduce realistic errors like NULL values, typos, or outliers?)\n",
    "\n",
    "- Volume & Format: (CSV, JSON, SQL Insert statements? How many rows?)\n",
    "\n",
    "### Phase 2: The Construction Once I provide these details, you will analyze my requirements and write a comprehensive System Prompt that I can run in a fresh chat instance to generate the actual data.\n",
    "\n",
    "- Constraint for the Final Prompt: The prompt you write must use \"Chain of Thought\" reasoning to enforce the statistical correlations I described. It must explicitly forbid \"lazy\" generation (repetitive names, round numbers, or impossible dates).\n",
    "\n",
    "Start by asking me the Phase 1 questions.\n",
    "\n",
    "Return the final prompt in strict JSON Format, so that output can be directly parsed using Python json.loads(prompt) function. Follow following format strictly. Don't add json or anything in beginning.\n",
    "\n",
    "{\n",
    "    \"prompt\":\"json\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\", \"content\":system_prompt_to_generate_data_generation_prompt}, \n",
    "]\n",
    "\n",
    "def llm_call(messages, user_message):\n",
    "    messages = messages + [{\"role\":\"user\", \"content\":user_message}]\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=MODEL[\"gemini\"],\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    messages = messages + [{\"role\":\"assistant\", \"content\":assistant_message}]\n",
    "    return messages, assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, am = llm_call(messages, \"I want to generate a synthetic dataset to forecast revenue of a SaaS B2B company using Monte Carlo Simulations.\")\n",
    "display(Markdown(am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c074635",
   "metadata": {},
   "outputs": [],
   "source": [
    "um = \"\"\"\n",
    " 1. Purpose : I want to do a SaaS B2B sales forecast for current and next 4 quarters using Monte-Carlo Simulations. Today's date is 2026-01-29, generate the data according to this date.\n",
    "\n",
    " 2. Schema : I need following columns:\n",
    "    i. opportunity_id: TEXT (PRIMARY KEY)\n",
    "    ii. opportunity_name: TEXT\n",
    "    iii. geography: Any one value from [\"NoAM\", \"EMEA\", \"APAC\", \"GLOBAL\"]\n",
    "    iv. stage: Any one of the ordinal stages [\"Stage 1: Create\", \"Stage 2: Qualify\", \"Stage 3: Meet\", \"Stage 4: Demo\", \"Stage 5: POV\" , \"Stage 6: Verbal Confirmation\", \"Stage 7: Confirmed\", \"Stage 8: Closed Won\", \"Stage 9: Closed Lost\"]\n",
    "    v. probabilty: Chance of deal closure based on stage value. This should be a float value between 0 and 1.\n",
    "    vi. amount: Dollar value associated with the opportunity. \n",
    "    vii. createddate: Opporuntiy createdate in format YYYY-MM-DD\n",
    "    viii. closedate: Expected closedate for the opportunity in format YYYY-MM-DD. This should be always greater than createddate. It can be any date after 2025-01-29.\n",
    "    ix. status: Status of the opporunity which could be [\"Open\", \"Won\", \"Lost\"], this is based on stage and also the closed opps have past closedate.\n",
    "    x. type: Type of the opportunity from [\"New Logo\", \"Cross-Sell\", \"Upgrade\"]\n",
    "\n",
    " 3. Business Logic & Correlations:\n",
    "    i. The ratio of closed opportunities to open opportunities should be 2:1.\n",
    "    ii. Use the normal distribution for the amount field.\n",
    "    iii. 'EMEA' and 'NoAM' should have slightly higher average ARR than 'APAC' and 'GLOBAL'.\n",
    "    iv. SaaS sales data is rarely perfectly distributed. It follows a Pareto principle (80% of revenue comes from 20% of deals).\n",
    "\n",
    " 4. Edge Cases & Noise: Do not include noise, just create a noise free dataset.\n",
    "\n",
    " 5. Volume & Output Format:\n",
    "    i. Generate 10 rows.\n",
    "    ii. Format should be CSV with a header row.\n",
    "\"\"\"\n",
    "\n",
    "messages, am = llm_call(messages, um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = json.loads(messages[-1][\"content\"][8:][:-3])['prompt']\n",
    "display(Markdown(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857478e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"}]\n",
    "\n",
    "def llm_call(messages, user_message):\n",
    "    messages = messages + [{\"role\":\"user\", \"content\":user_message}]\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=MODEL[\"gemini\"],\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    messages = messages + [{\"role\":\"assistant\", \"content\":assistant_message}]\n",
    "    return messages, assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ea5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, am = llm_call(messages, prompt)\n",
    "display(Markdown(am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73178cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "data_io = io.StringIO(am)\n",
    "df = pd.read_csv(data_io)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8464f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Run if More Rows are required.\n",
    "\n",
    "messages, am = llm_call(messages, \"Continue Generating 100 more rows with header.\")\n",
    "\n",
    "data_io = io.StringIO(am)\n",
    "df1 = pd.read_csv(data_io)\n",
    "df = pd.concat([df, df1], ignore_index=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38686148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18f65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
