{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLKNU7uN6tF2FsFmkD7J2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajuDasa/llm_engineering/blob/week3_branch/week3/community-contributions/raju/week3_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Synthetic Code Generator:"
      ],
      "metadata": {
        "id": "LGOn35rJ-Pq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate  #for quantization"
      ],
      "metadata": {
        "id": "RDfxhnxH0Ver"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import re\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "LQSK2-iMu6dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NEW_TOKENS = 2000\n",
        "\n",
        "Models = {\n",
        "    \"SmolLM3\": \"HuggingFaceTB/SmolLM3-3B\",    #6GB\n",
        "    \"Phi-3-mini\": \"microsoft/Phi-3-mini-4k-instruct\",  #7GB (very slow with T4)\n",
        "    \"Phi-3-mini (quantized)\": \"microsoft/Phi-3-mini-4k-instruct\"  #2GB\n",
        "    }\n",
        "\n",
        "# 2 ways to call a model: pipe, tokenizer\n",
        "def strategy_method(chat_history, selected_model):\n",
        "  if(selected_model == \"SmolLM3\"):\n",
        "    return use_pipe(chat_history, Models[selected_model])\n",
        "  elif(\"quantized\" in selected_model):\n",
        "    return use_tokenizer(chat_history, Models[selected_model], quantize=True)\n",
        "  else:\n",
        "    return use_tokenizer(chat_history, Models[selected_model], quantize=False)\n",
        "\n",
        "pipe=None\n",
        "def use_pipe(messages, model):\n",
        "  global pipe\n",
        "  if not pipe:\n",
        "    pipe = pipeline(\"text-generation\", model=model, max_new_tokens=MAX_NEW_TOKENS)\n",
        "  response = pipe(messages)\n",
        "  msg = response[0]['generated_text']\n",
        "  # Extract the content of the last assistant message\n",
        "  assistant_response = msg[-1]['content'] if msg and msg[-1]['role'] == 'assistant' else \"Sorry, I couldn't generate data.\"\n",
        "  assistant_response = re.sub(r\"<think>.*?</think>\", \"\", assistant_response, flags=re.DOTALL).strip()  #remove reasoning tags for Smol\n",
        "  return assistant_response\n"
      ],
      "metadata": {
        "id": "9MKXXfAKMkBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "cqoQF3R5k022"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preload phi3 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(Models[\"Phi-3-mini\"])\n",
        "#many LLM tokenizers do not define a padding token, and is required for batch processing.\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def use_tokenizer(messages, model, quantize=False):\n",
        "  tensor = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\").to(\"cuda\")\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config = quant_config if quantize else None )\n",
        "\n",
        "  # if quantize:    # 7GB -> 2GB\n",
        "  #   memory = model.get_memory_footprint() / (1024*1024)\n",
        "  #   print(f\"Memory footprint: {memory:,.1f} MB\")\n",
        "\n",
        "  outputs = model.generate(tensor, max_new_tokens=MAX_NEW_TOKENS)\n",
        "  #we need only final model response but not complete conversation\n",
        "  generated_ids = outputs[0][tensor.shape[-1]:]\n",
        "  assistant_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "  return assistant_text.strip()\n"
      ],
      "metadata": {
        "id": "jiPcdE7iAfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "ROLE:\n",
        "You are a synthetic dataset generator.\n",
        "\n",
        "PRIMARY FUNCTION:\n",
        "Generate ONLY structured semantic data.\n",
        "\n",
        "ALLOWED OUTPUT FORMATS:\n",
        "- CSV (default)\n",
        "- JSON (only if explicitly requested)\n",
        "\n",
        "STRICT CONSTRAINTS:\n",
        "- Output must be structured data ONLY.\n",
        "- Do NOT provide explanations, reasoning, commentary, or natural language responses.\n",
        "- Do NOT answer questions, provide analysis, or generate instructional content.\n",
        "- Do NOT use Markdown or any formatting wrappers.\n",
        "- Output must be directly usable as raw CSV or JSON.\n",
        "\n",
        "FORMAT RULES:\n",
        "- If the user does not specify a format, output CSV.\n",
        "- If the user explicitly requests JSON, output JSON.\n",
        "- If the user requests any other format (e.g., YAML, XML, Markdown, text, tables, prose), you MUST refuse.\n",
        "\n",
        "REFUSAL BEHAVIOR:\n",
        "- If the request violates format or scope rules, respond with a single-line refusal message.\n",
        "- Do NOT include explanations or additional text.\n",
        "\n",
        "STANDARD REFUSAL MESSAGES:\n",
        "- For unsupported formats:\n",
        "  \"Unsupported format. Allowed formats: CSV or JSON.\"\n",
        "- For unstructured output or question answering:\n",
        "  \"Unsupported request. Only structured dataset generation is allowed.\"\n",
        "\n",
        "EXAMPLES:\n",
        "1. User: Generate synthetic user data in YAML.\n",
        "   Response: Unsupported format. Allowed formats: CSV or JSON.\n",
        "\n",
        "2. User: Answer this question in Markdown.\n",
        "   Response: Unsupported request. Only structured dataset generation is allowed.\n",
        "\n",
        "3. User: Generate 100 rows of product data.\n",
        "   Response: <CSV data only>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R2lHoLUAwLWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def message_builder(role, msg):\n",
        "  return [{\"role\": role, \"content\": msg}]\n",
        "\n",
        "# chat function\n",
        "def respond(message, chat_history, selected_model):\n",
        "  if not selected_model:\n",
        "    return message, message_builder(\"assistant\", \"please select a model\")\n",
        "\n",
        "  chat_message = message_builder(\"system\", system_prompt) + message_builder(\"user\", message)\n",
        "    #[{\"role\": dic['role'], \"content\": dic['content']} for dic in chat_history] + \\\n",
        "    #message_builder(\"user\", message)\n",
        "\n",
        "  assistant_response = \"\"\n",
        "\n",
        "  try:\n",
        "    assistant_response = strategy_method(chat_message, selected_model)\n",
        "  except Exception as e:\n",
        "    assistant_response = \"An error occurred during text generation\"\n",
        "    print(f\"Error during text generation: {e}\")\n",
        "\n",
        "  display(\"result: \", assistant_response)\n",
        "  return_message = message_builder(\"user\", message) + message_builder(\"assistant\", assistant_response)\n",
        "  return \"\", return_message  # clear textbox, and send result\n",
        "\n",
        "# options for dropdown\n",
        "model_options = list(Models.keys())\n"
      ],
      "metadata": {
        "id": "9cT5dz6I5fv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade gradio"
      ],
      "metadata": {
        "id": "qoMM9vqouoju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=300, allow_tags=True)\n",
        "    msg = gr.Textbox(label=\"Ask your dataset query directly (in CSV/JSON format):\")\n",
        "    with gr.Row():\n",
        "        dropdown = gr.Dropdown(\n",
        "            choices=model_options,\n",
        "            value=model_options[0], # Default\n",
        "            label=\"Select a Model\"\n",
        "        )\n",
        "        with gr.Column():\n",
        "            submit = gr.Button(\"submit\", [msg, chatbot, dropdown])\n",
        "            clear = gr.ClearButton([msg, chatbot, dropdown])\n",
        "\n",
        "    msg.submit(\n",
        "    fn=respond,\n",
        "    inputs=[msg, chatbot, dropdown],\n",
        "    outputs=[msg, chatbot]\n",
        "    )\n",
        "    submit.click(\n",
        "    fn=respond,\n",
        "    inputs=[msg, chatbot, dropdown],\n",
        "    outputs=[msg, chatbot]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "kOVfgItWa8_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}