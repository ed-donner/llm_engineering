{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Meetings Minutes transcriber and summarizer with Gradio\n",
        "<p> Using Huggingface Models </p>"
      ],
      "metadata": {
        "id": "uPmtBs33qhvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to Colab notebook https://colab.research.google.com/drive/1kdDGAcAyfUa4l59zudTtHJKqORVv081p#scrollTo=ec5b97ce\n",
        "\n",
        "Link to download a smaller meeting audio https://www.kaggle.com/datasets/znevzz/speech-recognition-and-speaker-diarization\n",
        "\n",
        "OR\n",
        "\n",
        "https://huggingface.co/datasets/snorbyte/world-audio-natural-conversations-sample\n",
        "https://huggingface.co/datasets/danielrosehill/Small-STT-Eval-Audio-Dataset\n",
        "\n",
        "with smaller datasets"
      ],
      "metadata": {
        "id": "P5ohNC8lE0nI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGM7xv5Ekr7A"
      },
      "outputs": [],
      "source": [
        "# package updates\n",
        "!pip install -q --upgrade bitsandbytes accelerate transformers==4.57.6 gradio soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import os\n",
        "import requests\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
        "import torch\n",
        "import gradio as gr\n",
        "import threading"
      ],
      "metadata": {
        "id": "x3_JZCdpra4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "TRANSCRIBER_MODEL = \"openai/whisper-medium.en\""
      ],
      "metadata": {
        "id": "kpu1trrZszgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "Y43VCCEBungE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: Transcribe Audio with Huggingface models"
      ],
      "metadata": {
        "id": "2GLpveo8cgzq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "737be57c"
      },
      "source": [
        "# Initialize Transcriber Pipeline\n",
        "transcriber_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=TRANSCRIBER_MODEL,\n",
        "    dtype=torch.float16,\n",
        "    device='cuda',\n",
        "    return_timestamps=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quant config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "ntVqkR-Yw3yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20477d0e"
      },
      "source": [
        "# Initialize LLM\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_file):\n",
        "  print(\"starting audio transcription\")\n",
        "  result = transcriber_pipeline(audio_file)\n",
        "  print(\"audio transcription complete\")\n",
        "  return result[\"text\"]"
      ],
      "metadata": {
        "id": "Ru_HqAQNudq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 2: Analyze & Report"
      ],
      "metadata": {
        "id": "EnbVr20pwNcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(transcription):\n",
        "  system_message = \"\"\"\n",
        "  You produce minutes of meetings from transcripts, with summary, key discussion points,\n",
        "  takeaways and action items with owners, in markdown format without code blocks.\n",
        "  \"\"\"\n",
        "  user_prompt = f\"\"\"\n",
        "  Below is an extract transcript of a Denver council meeting.\n",
        "  Please write minutes in markdown without code blocks, including:\n",
        "  - a summary with attendees, location and date\n",
        "  - discussion points\n",
        "  - takeaways\n",
        "  - action items with owners\n",
        "\n",
        "  Transcription:\n",
        "  {transcription}\n",
        "  \"\"\"\n",
        "\n",
        "  return [\n",
        "      {\"role\": \"system\", \"content\": system_message},\n",
        "      {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]"
      ],
      "metadata": {
        "id": "kbhYcTg-2iH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_minutes(transcription):\n",
        "  print(\"summarizing transcript\")\n",
        "  messages = build_prompt(transcription)\n",
        "\n",
        "  inputs = llm_tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  streamer = TextIteratorStreamer(llm_tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
        "\n",
        "  generation_kwargs = dict(inputs=inputs, streamer=streamer, max_new_tokens=2000, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "  thread = threading.Thread(target=llm_model.generate, kwargs=generation_kwargs)\n",
        "  thread.start()\n",
        "\n",
        "  current_response = \"\"\n",
        "  for new_text in streamer:\n",
        "      current_response += new_text\n",
        "      yield current_response"
      ],
      "metadata": {
        "id": "5mgrrapbwvbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_and_summarize(audio_file):\n",
        "  if audio_file is None:\n",
        "    return \"Please upload an audio file to generate meeting minutes.\"\n",
        "\n",
        "  # Initial message to indicate processing has started\n",
        "  yield \"Processing audio and generating minutes... Please wait.\"\n",
        "\n",
        "  transcription = transcribe_audio(audio_file)\n",
        "\n",
        "  # generate_minutes is a generator, so we need to iterate to get the full output\n",
        "  full_minutes = \"\"\n",
        "  for chunk in generate_minutes(transcription):\n",
        "      full_minutes = chunk\n",
        "      yield full_minutes # Yield chunks for streaming updates\n",
        "  return full_minutes"
      ],
      "metadata": {
        "id": "taGyuqoXxF5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec5b97ce"
      },
      "source": [
        "# Build the gradio interface\n",
        "gr.Interface(\n",
        "    fn=process_audio_and_summarize,\n",
        "    inputs=gr.Audio(sources=[\"upload\"], type=\"filepath\", label=\"Upload Audio File\"),\n",
        "    outputs=gr.Markdown(label=\"Meeting Minutes\"),\n",
        "    title=\"Meetings Minutes Transcriber and Summarizer\",\n",
        "    description=\"Upload an audio file to get a transcribed and summarized meeting minutes.\"\n",
        ").launch(debug=True, share=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}