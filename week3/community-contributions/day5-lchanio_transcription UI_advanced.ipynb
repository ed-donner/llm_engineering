{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e80cff-2e9d-4fc9-b1ea-11d3599a8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc6a06d-4ea2-451c-910c-26c097ff42d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from google import genai\n",
    "# from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "#from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5731f1-983b-43b4-939b-1fbca98bc7c1",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "I prefer to put my API Keys setups together in the beginning of the notebook, easier to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec869137-836b-4168-9ddf-1b54fb8ae4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB5\n",
      "Weather API Key exists and begins 51c3669\n",
      "HuggingFace API Key exists and begins 51c3669\n"
     ]
    }
   ],
   "source": [
    "# Set up API keys and sign it to services if they exist\n",
    "# Comment out the ones you're not using.\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "hf_api_key = os.getenv('HF_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "    claude = anthropic.Anthropic()\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "    gemini =  genai.Client(api_key=google_api_key)\n",
    "    #ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key=\"ollama\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if weather_api_key:\n",
    "    print(f\"Weather API Key exists and begins {weather_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Weather API Key not set\")\n",
    "\n",
    "if hf_api_key:\n",
    "    print(f\"HuggingFace API Key exists and begins {weather_api_key[:7]}\")\n",
    "    login(hf_api_key, add_to_git_credential=True)\n",
    "else:\n",
    "    print(\"HuggingFace API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e83e852-6bc4-4a03-9d6d-f9600f9f6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55a8f6-99c1-4ef3-bde4-4c5a6c44cdb9",
   "metadata": {},
   "source": [
    "## Audio Summarizer with Gradio UI and real time status updates\n",
    "1. **Status Updates with Timers**\n",
    "- Uses gr.State implicitly through the class instance\n",
    "- Yielding from the processing function to update status in real-time\n",
    "- Timer shows elapsed seconds for each processing step\n",
    "\n",
    "2. **Real-time Status Display**\n",
    "- Status box shows current operation and elapsed time\n",
    "- Different status messages for transcription vs summarization\n",
    "- Error handling with appropriate status messages\n",
    "\n",
    "3. **File Validation**\n",
    "- Checks if file exists and isn't too large\n",
    "- Prevents multiple simultaneous processing attempts\n",
    "- Shows file information when uploaded\n",
    "\n",
    "4. **Streaming Summary Output**\n",
    "- Summary appears in Markdown format after processing\n",
    "- Can be updated progressively if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40cc7ed0-fb94-4cd5-930c-f0516427a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingAudioSummarizer:\n",
    "    def __init__(self):\n",
    "        self.processing = False\n",
    "        self.start_time = None\n",
    "        self.status_queue = queue.Queue()\n",
    "        self.current_step = \"\"\n",
    "\n",
    "    def speech_to_text(self, audio_file_path):\n",
    "        \"\"\" Use an AI speech-to-text model AUDIO_MODEL to transcribe the audio. \"\"\"\n",
    "        self.current_step = \"transcription\"\n",
    "        audio_file = open(audio_file_path, \"rb\")\n",
    "        transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
    "        return transcription\n",
    "\n",
    "    def summarize_text_streaming(self, text):\n",
    "        \"\"\" Generator that yields streaming summary results. \n",
    "        We use a HuggingFace model here stored in LLAMA\"\"\"\n",
    "        self.current_step = \"summarization\"\n",
    "       \n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "        user_prompt = f\"\"\"Below is an extract transcript of a meeting. Please write minutes in markdown, including a summary with attendees, \n",
    "        location and date; discussion points; takeaways; and action items with owners.\\n{text}\"\"\"\n",
    "        messages = [\n",
    "           {\"role\": \"system\", \"content\": system_message},\n",
    "           {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        # Tokenize the input, pass it to the model and  and stream the model response.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "        streamer = TextIteratorStreamer(\n",
    "            tokenizer, \n",
    "            skip_prompt=True, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        # Use threading for the generator\n",
    "        def generate():\n",
    "            model.generate(\n",
    "                inputs, \n",
    "                max_new_tokens=2000, \n",
    "                streamer=streamer,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )  \n",
    "        thread = threading.Thread(target=generate)\n",
    "        thread.start()\n",
    "        result=\"\"\n",
    "        for new_text in streamer:\n",
    "            result += new_text or \"\"\n",
    "            yield result\n",
    "        thread.join()\n",
    "\n",
    "    def process_audio_with_streaming(self, audio_file):\n",
    "        \"\"\"Process audio with streaming status and summary\"\"\"\n",
    "        if audio_file is None:\n",
    "            yield \"‚ùå No audio file selected\", \"\"\n",
    "            return\n",
    "        \n",
    "        if self.processing:\n",
    "            yield \"‚ö†Ô∏è Already processing a file. Please wait.\", \"\"\n",
    "            return\n",
    "        \n",
    "        # Validate file\n",
    "        if not os.path.exists(audio_file):\n",
    "            yield f\"‚ùå File not found: {audio_file}\", \"\"\n",
    "            return\n",
    "        \n",
    "        file_size = os.path.getsize(audio_file)\n",
    "        if file_size > 100 * 1024 * 1024:  # 100MB limit\n",
    "            yield f\"‚ùå File too large: {file_size/(1024*1024):.1f}MB (max 100MB)\", \"\"\n",
    "            return\n",
    "        \n",
    "        self.processing = True\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Step 1: Speech-to-text\n",
    "            yield \"üéØ Starting transcription...\", \"*Preparing to transcribe audio...*\"\n",
    "            \n",
    "            # Define progress callback to update the UI during transcription\n",
    "            def transcription_progress(step):\n",
    "                elapsed = int(time.time() - self.start_time)\n",
    "                yield f\"üéôÔ∏è Transcribing audio - {elapsed} seconds (step {step}/8)\", \"*Transcribing audio content...*\"\n",
    "            \n",
    "            # Use threading to run transcription while updating progress\n",
    "            transcription_result = None\n",
    "            transcription_error = None\n",
    "            \n",
    "            def run_transcription():\n",
    "                nonlocal transcription_result, transcription_error\n",
    "                try:\n",
    "                    transcription_result = self.speech_to_text(audio_file)\n",
    "                except Exception as e:\n",
    "                    transcription_error = e\n",
    "            \n",
    "            # Start transcription in a separate thread\n",
    "            transcription_thread = threading.Thread(target=run_transcription)\n",
    "            transcription_thread.start()\n",
    "            \n",
    "            # Update progress while transcription is running\n",
    "            while transcription_thread.is_alive():\n",
    "                elapsed = int(time.time() - self.start_time)\n",
    "                yield f\"üéôÔ∏è Transcribing audio - {elapsed} seconds\", \"*Transcribing audio content...*\"\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Wait for thread to complete\n",
    "            transcription_thread.join()\n",
    "            \n",
    "            # Check for errors\n",
    "            if transcription_error:\n",
    "                raise transcription_error\n",
    "\n",
    "            transcription_time = int(time.time() - self.start_time)\n",
    "            \n",
    "            yield f\"‚úÖ Transcription complete - {transcription_time} seconds\", \"*Transcription finished. Starting summarization...*\"\n",
    "            \n",
    "            # Step 2: Streaming Summarization\n",
    "            yield \"ü§ñ Starting AI summarization...\", \"*Connecting to AI model...*\"\n",
    "            \n",
    "            # Stream the summary as it's generated\n",
    "            for partial_summary in self.summarize_text_streaming(transcription_result):\n",
    "                elapsed = int(time.time() - self.start_time)\n",
    "                status = f\"üìù Generating summary - {elapsed} seconds\"\n",
    "                yield status, partial_summary\n",
    "            \n",
    "            # Final results\n",
    "            total_time = int(time.time() - self.start_time)\n",
    "            final_status = f\"‚úÖ Processing complete! Total time: {total_time} seconds\"\n",
    "            \n",
    "            yield final_status, partial_summary\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error processing audio: {str(e)}\"\n",
    "            yield error_msg, f\"**Error occurred during processing:**\\n\\n{str(e)}\"\n",
    "        \n",
    "        finally:\n",
    "            self.processing = False\n",
    "            self.current_step = \"\"\n",
    "    \n",
    "    def cancel_processing(self):\n",
    "        \"\"\"Cancel current processing\"\"\"\n",
    "        if self.processing:\n",
    "            self.processing = False\n",
    "            return \"üõë Processing cancelled\"\n",
    "        return \"No processing to cancel\"\n",
    "\n",
    "\n",
    "# Instantiate the class\n",
    "summarizer = StreamingAudioSummarizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14d52-d236-448a-a0b1-4086a0cf86c1",
   "metadata": {},
   "source": [
    "Define CSS Stylesheet for streaming interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f42ca8-843d-4b01-a9b4-e3ed0c60e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_css = \"\"\"\n",
    ".status-box {\n",
    "    font-family: 'Courier New', monospace;\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "    color: white;\n",
    "    border-radius: 10px;\n",
    "    padding: 15px;\n",
    "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "\n",
    ".summary-box {\n",
    "    background: #f8f9fa;\n",
    "    border-radius: 10px;\n",
    "    padding: 20px;\n",
    "    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
    "    min-height: 400px;\n",
    "}\n",
    "\n",
    ".process-button {\n",
    "    background: linear-gradient(45deg, #FF6B6B, #4ECDC4);\n",
    "    border: none;\n",
    "    color: white;\n",
    "    font-weight: bold;\n",
    "    transition: all 0.3s ease;\n",
    "}\n",
    "\n",
    ".cancel-button {\n",
    "    background: linear-gradient(45deg, #ff7675, #fd79a8);\n",
    "    border: none;\n",
    "    color: white;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".streaming-indicator {\n",
    "    animation: pulse 2s infinite;\n",
    "}\n",
    "\n",
    "@keyframes pulse {\n",
    "    0% { opacity: 1; }\n",
    "    50% { opacity: 0.5; }\n",
    "    100% { opacity: 1; }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651fe7db-2059-4a56-befa-f4b784beb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the streaming interface\n",
    "with gr.Blocks(title=\"üéµ Streaming Audio Summarizer\", css=streaming_css) as interface:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üéµ Streaming Audio Summarizer\n",
    "    \n",
    "    Upload an audio file and watch the AI-generated summary appear in real-time as it's being created!\n",
    "    \n",
    "    ‚ú® **Features:**\n",
    "    - üéôÔ∏è Real-time transcription progress\n",
    "    - üìù Streaming AI summarization\n",
    "    - ‚è±Ô∏è Live processing timers\n",
    "    - üõë Cancel processing anytime\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # File upload\n",
    "            audio_input = gr.File(\n",
    "                label=\"üìÅ Upload Audio File\",\n",
    "                file_types=[\".mp3\", \".wav\", \".m4a\", \".flac\", \".ogg\", \".aac\"],\n",
    "                type=\"filepath\"\n",
    "            )\n",
    "            \n",
    "            # Control buttons\n",
    "            with gr.Row():\n",
    "                process_btn = gr.Button(\n",
    "                    \"üöÄ Start Processing\", \n",
    "                    variant=\"primary\",\n",
    "                    size=\"lg\",\n",
    "                    elem_classes=\"process-button\"\n",
    "                )\n",
    "                cancel_btn = gr.Button(\n",
    "                    \"üõë Cancel\",\n",
    "                    variant=\"secondary\",\n",
    "                    elem_classes=\"cancel-button\"\n",
    "                )\n",
    "            \n",
    "            # Status display\n",
    "            status_box = gr.Textbox(\n",
    "                label=\"üìä Processing Status\",\n",
    "                value=\"Ready to process audio file\",\n",
    "                interactive=False,\n",
    "                lines=4,\n",
    "                elem_classes=\"status-box\"\n",
    "            )\n",
    "            \n",
    "            # Progress indicator\n",
    "            progress_bar = gr.HTML(\n",
    "                value=\"<div style='text-align: center; color: #666;'>Upload a file to begin</div>\"\n",
    "            )\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            # Streaming summary output\n",
    "            summary_output = gr.Markdown(\n",
    "                value=\"*ü§ñ AI-generated summary will stream here in real-time...*\",\n",
    "                elem_classes=\"summary-box\"\n",
    "            )\n",
    "    \n",
    "    # File info section\n",
    "    with gr.Accordion(\"üìã Processing Details\", open=False):\n",
    "        file_details = gr.Markdown(\"*Upload a file to see processing details*\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def show_file_details(file_path):\n",
    "        if file_path:\n",
    "            try:\n",
    "                size = os.path.getsize(file_path)\n",
    "                name = os.path.basename(file_path)\n",
    "                ext = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                return f\"\"\"\n",
    "                ### File Information\n",
    "                - **Name**: {name}\n",
    "                - **Size**: {size/(1024*1024):.2f} MB\n",
    "                - **Format**: {ext.upper()}\n",
    "                - **Path**: `{file_path}`\n",
    "                \n",
    "                ### Processing Pipeline\n",
    "                1. **Speech-to-Text**: Convert audio to text using AI transcription\n",
    "                2. **AI Summarization**: Generate intelligent summary using language models\n",
    "                3. **Streaming Output**: Results appear in real-time as they're generated\n",
    "                \"\"\"\n",
    "            except:\n",
    "                return \"‚ùå Could not read file information\"\n",
    "        return \"*Upload a file to see processing details*\"\n",
    "    \n",
    "    def update_progress(processing_status):\n",
    "        if \"complete\" in processing_status.lower():\n",
    "            return \"<div style='color: #27ae60; font-weight: bold;'>‚úÖ Processing Complete!</div>\"\n",
    "        elif \"error\" in processing_status.lower():\n",
    "            return \"<div style='color: #e74c3c; font-weight: bold;'>‚ùå Processing Error</div>\"\n",
    "        elif \"processing\" in processing_status.lower() or \"transcribing\" in processing_status.lower() or \"generating\" in processing_status.lower():\n",
    "            return \"<div style='color: #f39c12; font-weight: bold;' class='streaming-indicator'>üîÑ Processing in Progress...</div>\"\n",
    "        else:\n",
    "            return \"<div style='color: #666;'>Ready to process</div>\"\n",
    "    \n",
    "    # Wire up events\n",
    "    audio_input.change(\n",
    "        show_file_details,\n",
    "        inputs=audio_input,\n",
    "        outputs=file_details\n",
    "    )\n",
    "    \n",
    "    process_btn.click(\n",
    "        summarizer.process_audio_with_streaming,\n",
    "        inputs=audio_input, \n",
    "        outputs=[status_box, summary_output]\n",
    "    )\n",
    "    \n",
    "    cancel_btn.click(\n",
    "        summarizer.cancel_processing,\n",
    "        outputs=status_box\n",
    "    )\n",
    "    \n",
    "    # Update progress indicator based on status\n",
    "    status_box.change(\n",
    "        update_progress,\n",
    "        inputs=status_box,\n",
    "        outputs=progress_bar\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd6a5b-2d18-42be-857f-bf560413746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5bff23c07d4915be5a7514be6d305a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# Launch the interface\n",
    "interface.launch(\n",
    "    debug=True,\n",
    "    share=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ba934-f87a-44a6-9ba1-8b216d658a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
