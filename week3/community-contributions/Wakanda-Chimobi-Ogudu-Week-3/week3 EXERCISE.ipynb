{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3: AI Tutor with Synthetic Data Generator\n",
        "\n",
        "Uses Hugging Face Hub (pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig) to generate synthetic teaching scenarios. The tutor inserts these into responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio python-dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "    load_dotenv(override=True)\n",
        "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"HF login OK\")\n",
        "else:\n",
        "    print(\"Set HF_TOKEN in Colab Secrets or .env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data generator model (Colab-friendly: small + 4-bit)\n",
        "GEN_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "MAX_GEN_TOKENS = 256\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEN_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        ")\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=MAX_GEN_TOKENS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_examples(topic: str) -> str:\n",
        "    \"\"\"Generate 1â€“2 short teaching scenarios/examples for the given topic.\"\"\"\n",
        "    prompt = f\"Generate two very short teaching scenarios or concrete code examples for explaining: {topic}. Each in 1-2 sentences. No preamble.\"\n",
        "    out = gen_pipe(prompt, do_sample=True, temperature=0.7)\n",
        "    text = out[0][\"generated_text\"] if out else \"\"\n",
        "    return text.replace(prompt, \"\").strip()[:500] if text else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenRouter: in Colab use Secrets (ðŸ”‘ sidebar); locally use .env or env var\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "    api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "    load_dotenv(override=True)\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "openrouter = OpenAI(api_key=api_key, base_url=BASE_URL) if api_key else None\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
        "\n",
        "# In Colab only OpenRouter is available (Ollama runs locally)\n",
        "MODEL_CHOICES = [\n",
        "    (\"gpt-4o-mini (OpenRouter)\", \"gpt-4o-mini\", \"openrouter\"),\n",
        "    (\"llama3.2 (Ollama)\", \"llama3.2\", \"ollama\"),\n",
        "] if not IN_COLAB else [\n",
        "    (\"gpt-4o-mini (OpenRouter)\", \"gpt-4o-mini\", \"openrouter\"),\n",
        "]\n",
        "\n",
        "BASE_SYSTEM = \"\"\"You are a professional AI coding tutor. Give clear, step-by-step explanations with code examples. Use the following synthetic teaching scenarios or examples to enrich your answer when relevant. Weave them into your explanation.\n",
        "\n",
        "Synthetic scenarios/examples to use when helpful:\n",
        "{synthetic}\n",
        "\n",
        "Keep a friendly, expert tone. Respond in markdown (e.g. code blocks).\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_reply(history, user_message, model_label, use_synthetic):\n",
        "    label_to = {l: (mid, back) for l, mid, back in MODEL_CHOICES}\n",
        "    model_id, backend = label_to.get(model_label, MODEL_CHOICES[0][1:])\n",
        "    client = openrouter if backend == \"openrouter\" else ollama\n",
        "    if not client:\n",
        "        if backend == \"openrouter\":\n",
        "            if IN_COLAB:\n",
        "                yield (\n",
        "                    \"**OpenRouter** key not found. In Colab: open the **Secrets** panel (key icon in left sidebar)\", \n",
        "                )\n",
        "            else:\n",
        "                yield (\n",
        "                    \"**OpenRouter** is not set. Add `OPENROUTER_API_KEY` to .env, \"\n",
        "                )\n",
        "        else:\n",
        "            yield \"Ollama is for local runs only. In Colab use OpenRouter.\"\n",
        "        return\n",
        "    synthetic = generate_synthetic_examples(user_message[:200]) if use_synthetic else \"(none)\"\n",
        "    system = BASE_SYSTEM.format(synthetic=synthetic)\n",
        "    messages = [{\"role\": \"system\", \"content\": system}]\n",
        "    for u, a in history:\n",
        "        messages += [{\"role\": \"user\", \"content\": u}, {\"role\": \"assistant\", \"content\": a or \"\"}]\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "    stream = client.chat.completions.create(model=model_id, messages=messages, stream=True)\n",
        "    for chunk in stream:\n",
        "        part = (chunk.choices[0].delta.content or \"\")\n",
        "        if part:\n",
        "            yield part\n",
        "\n",
        "def chat(message, history, model_choice, use_synthetic):\n",
        "    if not (message or message.strip()):\n",
        "        return\n",
        "    full = \"\"\n",
        "    for chunk in stream_reply(history, message, model_choice, use_synthetic):\n",
        "        full += chunk\n",
        "        yield full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dropdown = gr.Dropdown(choices=[l for l, _, _ in MODEL_CHOICES], value=MODEL_CHOICES[0][0], label=\"Model\")\n",
        "use_synthetic = gr.Checkbox(value=True, label=\"Inject synthetic teaching scenarios\")\n",
        "demo = gr.ChatInterface(\n",
        "    chat,\n",
        "    additional_inputs=[model_dropdown, use_synthetic],\n",
        "    title=\"Technical Q&A Tutor + Synthetic Data\",\n",
        "    description=\"Ask a coding question. Toggle to inject HF-generated scenarios into the answer.\",\n",
        ")\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
