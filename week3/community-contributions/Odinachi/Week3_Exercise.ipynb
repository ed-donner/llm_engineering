{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Dataset Generator and Gradio Chat\n",
    "\n",
    "\n",
    "A **Gradio-powered chat interface** for generating high-quality synthetic training data using open-source LLMs â€” no API keys required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a082e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\"LLAMA\" : \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "\n",
    "\"PHI\" : \"microsoft/Phi-4-mini-instruct\",\n",
    "\"GEMMA\" : \"google/gemma-3-270m-it\",\n",
    "\"QWEN\" : \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "\"DEEPSEEK\" : \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6649fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model_name, messages, quant=True, max_new_tokens=512):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "  input_ids = inputs[\"input_ids\"]\n",
    "  attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "  streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "  if quant:\n",
    "    llm = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config).to(\"cuda\")\n",
    "  else:\n",
    "    llm = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "  outputs = llm.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
    "  # Decode only the new tokens, not the prompt\n",
    "  new_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "  return tokenizer.decode(new_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55455a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = generate(MODELS[\"QWEN\"], [\n",
    "    {\"role\": \"user\", \"content\": \"generate json of synthetic data for training a football match model\"}\n",
    "  ])\n",
    "\n",
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_message_in_chatbot(message, history):\n",
    "    return \"\", history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "def chat(history, model_name):\n",
    "    messages = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    model_id = MODELS[model_name]\n",
    "    res = generate(model_id, messages)\n",
    "    history.append({\"role\": \"assistant\", \"content\": res})\n",
    "    return history\n",
    "\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "    with gr.Row():\n",
    "        message = gr.Textbox(label=\"What do you need data for?:\")\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=list(MODELS.keys()),\n",
    "            value=\"PHI\",\n",
    "            label=\"Select Model\",\n",
    "        )\n",
    "\n",
    "    message.submit(\n",
    "        put_message_in_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]\n",
    "    ).then(\n",
    "        chat, inputs=[chatbot, model_selector], outputs=chatbot\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
