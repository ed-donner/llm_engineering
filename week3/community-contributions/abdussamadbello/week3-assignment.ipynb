{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Exercise – Llama 3.2 Instruct (Hugging Face) quizzes you and scores you (abdussamadbello)\n",
        "\n",
        "You **select topics** → **Llama 3.2 Instruct** (Hugging Face) **generates a quiz** → **you** answer → **Llama scores** your answers and gives feedback. No OpenAI; runs locally with `transformers`.\n",
        "\n",
        "**Auth (required for gated model):** Add `HF_TOKEN=your_token` to your `.env` (never commit it). Accept the [Llama 3.2 license](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) on the Hub first; the notebook calls `login(token=HF_TOKEN)` before loading the model."
      ],
      "id": "5c7b603b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    login()\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "if not torch.cuda.is_available():\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "TOPICS = [\"RAG\", \"prompts\", \"evaluation\"]\n",
        "\n",
        "\n",
        "def llm_generate(user_prompt: str, max_new_tokens: int = 400) -> str:\n",
        "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.6, pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)[len(text):].strip()\n",
        "\n",
        "\n",
        "def generate_quiz(topics: list, questions_per_topic: int = 1) -> list:\n",
        "    prompt = f\"\"\"Generate a short quiz. Topics: {', '.join(topics)}. Create exactly {questions_per_topic} question(s) per topic. Output only a JSON array. Each item: {{\\\"topic\\\": \\\"<topic>\\\", \\\"question\\\": \\\"<question>\\\"}}\"\"\"\n",
        "    raw = llm_generate(prompt, max_new_tokens=500).strip()\n",
        "    if raw.startswith(\"```\"): raw = re.sub(r\"^```\\w*\\n?\", \"\", raw).rstrip(\"`\").strip()\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        m = re.search(r\"\\[[\\s\\S]*\\]\", raw)\n",
        "        return json.loads(m.group(0)) if m else [{\"topic\": t, \"question\": f\"Explain {t}.\"} for t in topics]\n",
        "\n",
        "\n",
        "def score_answers(questions_with_answers: list) -> list:\n",
        "    scored = []\n",
        "    for item in questions_with_answers:\n",
        "        prompt = f\"\"\"Score from 1 to 5. Reply with two lines: Score: <n> and Feedback: <sentence>. Question: {item['question']} Student's answer: {item['user_answer']}\"\"\"\n",
        "        text = llm_generate(prompt, max_new_tokens=120)\n",
        "        score, feedback = 3, text\n",
        "        for line in text.split(\"\\n\"):\n",
        "            if \"score:\" in line.lower() and re.search(r\"\\d+\", line): score = max(1, min(5, int(re.search(r\"\\d+\", line).group())))\n",
        "            elif \"feedback:\" in line.lower(): feedback = line.split(\":\", 1)[-1].strip() or text\n",
        "        scored.append({**item, \"score\": score, \"feedback\": feedback})\n",
        "    return scored\n",
        "\n",
        "\n",
        "def print_scores(scored: list):\n",
        "    for i, r in enumerate(scored, 1):\n",
        "        print(f\"Q{i} [{r['topic']}] Score: {r['score']}/5 - {r['feedback']}\")\n",
        "    print(f\"Total: {sum(r['score'] for r in scored)}/{len(scored) * 5}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a755a3f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "questions = generate_quiz(TOPICS, questions_per_topic=1)\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"Q{i} [{q['topic']}]: {q['question']}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "550cf3d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample_answers = [\"RAG retrieves docs and feeds them to the model to reduce hallucination.\", \"Use clear tool names and descriptions in the system prompt.\", \"Use a judge model to score answers.\"]\n",
        "answers = [{\"topic\": q[\"topic\"], \"question\": q[\"question\"], \"user_answer\": sample_answers[i] if i < len(sample_answers) else \"(no answer)\"} for i, q in enumerate(questions)]\n",
        "scored = score_answers(answers)\n",
        "print_scores(scored)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "22e59738"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}