{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Resume Generator using Multiple LLMs\n",
    "\n",
    "## Objective\n",
    "Generate synthetic resumes (Junior, Mid-Level, Senior) using:\n",
    "\n",
    "1. OpenRouter API (2 models)\n",
    "2. Hugging Face Transformers Pipeline (2 models)\n",
    "\n",
    "We compare:\n",
    "- JSON validity\n",
    "- Structure consistency\n",
    "- Experience differentiation\n",
    "- Diversity\n",
    "\n",
    "Author: [Your Name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de6669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m277 packages\u001b[0m \u001b[2min 9.53s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 3.27s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3.67s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv add accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n",
      "HuggingFace token found and looks good!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not openrouter_api_key:\n",
    "    print(\"No API key was found\")\n",
    "elif not openrouter_api_key.startswith(\"sk\"):\n",
    "    print(\"An API key was found, but it doesn't start with sk; please check you're using the right key\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n",
    "#get Hugging Face token\n",
    "if not hf_token:\n",
    "    print(\"No HuggingFace token found\")\n",
    "elif not hf_token.startswith(\"hf_\"):\n",
    "    print(\"HF token found but doesn't start with hf_; please check\")\n",
    "else:\n",
    "    print(\"HuggingFace token found and looks good!\")\n",
    "\n",
    "openrouter = OpenAI(base_url='https://openrouter.ai/api/v1', api_key=openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_SCHEMA = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"name\": \"string\",\n",
    "    \"experience_level\": \"Junior | Mid | Senior\",\n",
    "    \"education\": [\n",
    "      {\n",
    "        \"degree\": \"string\",\n",
    "        \"field\": \"string\",\n",
    "        \"institution\": \"string\",\n",
    "        \"year\": \"string\"\n",
    "      }\n",
    "    ],\n",
    "    \"skills\": [\"string\"],\n",
    "    \"work_experience\": [\n",
    "      {\n",
    "        \"job_title\": \"string\",\n",
    "        \"company\": \"string\",\n",
    "        \"years\": \"string\",\n",
    "        \"responsibilities\": [\"string\"],\n",
    "        \"achievements\": [\"string\"]\n",
    "      }\n",
    "    ],\n",
    "    \"certifications\": [\"string\"],\n",
    "    \"projects\": [\"string\"]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(experience_level, role):\n",
    "    return f\"\"\"\n",
    "You are a synthetic resume generator.\n",
    "\n",
    "Generate 1 fictional resume for a {experience_level} {role}.\n",
    "\n",
    "Requirements:\n",
    "- Ensure experience matches level.\n",
    "- Senior: 8+ years, leadership, measurable impact.\n",
    "- Mid: 3-7 years, strong contributions.\n",
    "- Junior: 0-2 years, internships/projects.\n",
    "- Make names diverse.\n",
    "- Do NOT use real people.\n",
    "- Return ONLY valid JSON (no markdown, no backticks, no explanation).\n",
    "- Follow this schema exactly:\n",
    "\n",
    "{RESUME_SCHEMA}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_resume_md(json_text, model_name):\n",
    "    \"\"\"Convert raw JSON resume text into nicely formatted markdown.\"\"\"\n",
    "    clean = json_text.strip()\n",
    "    if clean.startswith(\"```\"):\n",
    "        clean = clean.split(\"\\n\", 1)[-1]\n",
    "        clean = clean.rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "    try:\n",
    "        resumes = json.loads(clean)\n",
    "        if isinstance(resumes, dict):\n",
    "            resumes = [resumes]\n",
    "    except Exception as e:\n",
    "        return f\"### ‚ö†Ô∏è {model_name}\\n\\n**JSON parse error:** `{e}`\\n\\n```\\n{json_text[:600]}\\n```\"\n",
    "\n",
    "    md = f\"## ü§ñ {model_name}\\n\\n\"\n",
    "    for r in resumes:\n",
    "        md += f\"---\\n### üë§ {r.get('name', 'N/A')}\\n\"\n",
    "        md += f\"**Experience Level:** {r.get('experience_level', 'N/A')}\\n\\n\"\n",
    "\n",
    "        md += \"#### üéì Education\\n\"\n",
    "        for edu in r.get('education', []):\n",
    "            md += f\"- {edu.get('degree')} in {edu.get('field')} ‚Äî {edu.get('institution')} ({edu.get('year')})\\n\"\n",
    "\n",
    "        skills = r.get('skills', [])\n",
    "        if skills:\n",
    "            md += f\"\\n#### üõ†Ô∏è Skills\\n\"\n",
    "            md += \", \".join(f\"`{s}`\" for s in skills) + \"\\n\"\n",
    "\n",
    "        md += \"\\n#### üíº Work Experience\\n\"\n",
    "        for job in r.get('work_experience', []):\n",
    "            md += f\"**{job.get('job_title')}** @ {job.get('company')} _{job.get('years')}_\\n\"\n",
    "            for resp in job.get('responsibilities', []):\n",
    "                md += f\"  - {resp}\\n\"\n",
    "            for ach in job.get('achievements', []):\n",
    "                md += f\"  - ‚úÖ {ach}\\n\"\n",
    "            md += \"\\n\"\n",
    "\n",
    "        certs = r.get('certifications', [])\n",
    "        if certs:\n",
    "            md += \"#### üìú Certifications\\n\"\n",
    "            for c in certs:\n",
    "                md += f\"- {c}\\n\"\n",
    "\n",
    "        projects = r.get('projects', [])\n",
    "        if projects:\n",
    "            md += \"\\n#### üöÄ Projects\\n\"\n",
    "            for p in projects:\n",
    "                md += f\"- {p}\\n\"\n",
    "\n",
    "        md += \"\\n\"\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_openrouter(model, prompt, temperature):\n",
    "    \"\"\"Stream from OpenRouter, yielding incremental raw text.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You generate structured JSON resumes. Return ONLY raw JSON, no markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = openrouter.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    collected = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        collected += delta\n",
    "        yield collected\n",
    "\n",
    "\n",
    "def run_hf(model_name, prompt, temperature):\n",
    "    \"\"\"Run a HuggingFace model and return the generated text (no streaming).\"\"\"\n",
    "    generator = hf_pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        device_map=\"auto\",\n",
    "        token=hf_token\n",
    "    )\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=1200,\n",
    "        temperature=temperature,\n",
    "        do_sample=True\n",
    "    )\n",
    "    full = output[0][\"generated_text\"]\n",
    "    # Strip the prompt prefix that HF returns\n",
    "    return full[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_resumes(or_model_1, or_model_2, hf_model_1, hf_model_2,\n",
    "                     experience_level, role, temperature):\n",
    "    \"\"\"\n",
    "    Generator that yields (out1, out2, out3, out4) tuples so Gradio\n",
    "    updates all four panels incrementally as each model finishes.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(experience_level, role)\n",
    "    out1 = out2 = out3 = out4 = \"\"\n",
    "\n",
    "    # --- OpenRouter model 1 (streamed) ---\n",
    "    raw1 = \"\"\n",
    "    for raw1 in stream_openrouter(or_model_1, prompt, temperature):\n",
    "        out1 = f\"## ü§ñ {or_model_1}\\n\\n‚è≥ _Generating..._\\n\\n```json\\n{raw1}\\n```\"\n",
    "        yield out1, out2, out3, out4\n",
    "    out1 = format_resume_md(raw1, or_model_1)\n",
    "    yield out1, out2, out3, out4\n",
    "\n",
    "    # --- OpenRouter model 2 (streamed) ---\n",
    "    raw2 = \"\"\n",
    "    for raw2 in stream_openrouter(or_model_2, prompt, temperature):\n",
    "        out2 = f\"## ü§ñ {or_model_2}\\n\\n‚è≥ _Generating..._\\n\\n```json\\n{raw2}\\n```\"\n",
    "        yield out1, out2, out3, out4\n",
    "    out2 = format_resume_md(raw2, or_model_2)\n",
    "    yield out1, out2, out3, out4\n",
    "\n",
    "    # --- HuggingFace model 1 ---\n",
    "    out3 = f\"## ü§ó {hf_model_1}\\n\\n‚è≥ _Loading model ‚Äî this may take a moment..._\"\n",
    "    yield out1, out2, out3, out4\n",
    "    raw3 = run_hf(hf_model_1, prompt, temperature)\n",
    "    out3 = format_resume_md(raw3, hf_model_1)\n",
    "    yield out1, out2, out3, out4\n",
    "\n",
    "    # --- HuggingFace model 2 ---\n",
    "    out4 = f\"## ü§ó {hf_model_2}\\n\\n‚è≥ _Loading model ‚Äî this may take a moment..._\"\n",
    "    yield out1, out2, out3, out4\n",
    "    raw4 = run_hf(hf_model_2, prompt, temperature)\n",
    "    out4 = format_resume_md(raw4, hf_model_2)\n",
    "    yield out1, out2, out3, out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df24061fd81f47279d6cbaebcbcf8698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e01973df14403926f2dacabf0d27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5299d1fbd6547919f40f71b38975045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95392bd99418495b983296b6d53510c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e6f6acfa814d9799fd9249b8221635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd33894a9fe493fa96afb5d0c5acf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OPENROUTER_MODEL_CHOICES = [\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"x-ai/grok-4\",\n",
    "]\n",
    "\n",
    "HF_MODEL_CHOICES = [\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"google/gemma-7b-it\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Synthetic Resume Generator\") as demo:\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üìÑ Synthetic Resume Generator\n",
    "        Compare resume generation across **2 OpenRouter models** and **2 open-source HuggingFace models** side by side.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # ---- Controls ----\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            role_input = gr.Textbox(\n",
    "                label=\"üßë‚Äçüíº Role / Job Title\",\n",
    "                placeholder=\"e.g. Backend Engineer, Data Scientist, DevOps Engineer\",\n",
    "                value=\"Backend Engineer\"\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            experience_input = gr.Radio(\n",
    "                choices=[\"Junior\", \"Mid\", \"Senior\"],\n",
    "                value=\"Senior\",\n",
    "                label=\"üìä Experience Level\"\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            temperature_input = gr.Slider(\n",
    "                minimum=0.0, maximum=1.0, step=0.05, value=0.4,\n",
    "                label=\"üé® Variety / Creativity\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"#### üåê OpenRouter Models\")\n",
    "            or_model_1 = gr.Dropdown(\n",
    "                choices=OPENROUTER_MODEL_CHOICES,\n",
    "                value=OPENROUTER_MODEL_CHOICES[0],\n",
    "                label=\"OpenRouter Model 1\"\n",
    "            )\n",
    "            or_model_2 = gr.Dropdown(\n",
    "                choices=OPENROUTER_MODEL_CHOICES,\n",
    "                value=OPENROUTER_MODEL_CHOICES[1],\n",
    "                label=\"OpenRouter Model 2\"\n",
    "            )\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"#### ü§ó HuggingFace Models\")\n",
    "            hf_model_1 = gr.Dropdown(\n",
    "                choices=HF_MODEL_CHOICES,\n",
    "                value=HF_MODEL_CHOICES[0],\n",
    "                label=\"HuggingFace Model 1\"\n",
    "            )\n",
    "            hf_model_2 = gr.Dropdown(\n",
    "                choices=HF_MODEL_CHOICES,\n",
    "                value=HF_MODEL_CHOICES[1],\n",
    "                label=\"HuggingFace Model 2\"\n",
    "            )\n",
    "\n",
    "    generate_btn = gr.Button(\"üöÄ Generate Resumes\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "\n",
    "    # ---- Output panels ----\n",
    "    with gr.Row():\n",
    "        out1 = gr.Markdown(value=\"*Output will appear here once generated...*\")\n",
    "        out2 = gr.Markdown(value=\"*Output will appear here once generated...*\")\n",
    "\n",
    "    with gr.Row():\n",
    "        out3 = gr.Markdown(value=\"*Output will appear here once generated...*\")\n",
    "        out4 = gr.Markdown(value=\"*Output will appear here once generated...*\")\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=generate_resumes,\n",
    "        inputs=[or_model_1, or_model_2, hf_model_1, hf_model_2,\n",
    "                experience_input, role_input, temperature_input],\n",
    "        outputs=[out1, out2, out3, out4]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
