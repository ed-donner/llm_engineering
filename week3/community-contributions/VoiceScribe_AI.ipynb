{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üé§ VoiceScribe AI - Complete Implementation\n",
    "\n",
    "**Intelligent Audio Analysis Tool** - Transforms audio recordings into actionable insights\n",
    "\n",
    "This notebook contains the complete implementation that you can run step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Features:\n",
    "\n",
    "- üé§ Automatic Transcription with Whisper\n",
    "- üß† Smart Summarization with BART\n",
    "- üìå Key Points Extraction\n",
    "- üîç Searchable Transcript\n",
    "- üí¨ Chat with Audio (RAG Q&A)\n",
    "- üìÇ Gradio UI Interface\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ How to Use This Notebook\n",
    "\n",
    "Run each cell sequentially from top to bottom:\n",
    "1. Install dependencies (Step 1)\n",
    "2. Import libraries (Step 2)\n",
    "3. Define data structures (Step 3)\n",
    "4. Create the VoiceScribeAI class (Step 4)\n",
    "5. Build Gradio interface (Step 5)\n",
    "6. Launch the application (Step 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install all required packages. This may take 5-10 minutes.\n",
    "\n",
    "**What we're installing:**\n",
    "- `gradio` - Web UI framework\n",
    "- `torch` - Deep learning framework\n",
    "- `transformers` - AI models from Hugging Face\n",
    "- `langchain` - RAG framework for Q&A (with compatible versions)\n",
    "- `faiss-cpu` - Vector database for similarity search\n",
    "- `sentence-transformers` - Text embeddings\n",
    "- `librosa`, `soundfile` - Audio processing\n",
    "\n",
    "**Note:** If you see dependency conflicts, re-run this cell to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wg823aonyj",
   "metadata": {},
   "source": [
    "## Step 0: Fix Dependency Conflicts (Run This First If Needed)\n",
    "\n",
    "**Only run this cell if you're seeing dependency conflicts!** This will clean up conflicting packages.\n",
    "\n",
    "After running this, proceed to Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sepm9p1sgg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up conflicting langchain packages\n",
    "print(\"üßπ Cleaning up conflicting packages...\")\n",
    "\n",
    "!pip uninstall -y langchain-core langchain langchain-community langchain-huggingface langchain-text-splitters 2>/dev/null\n",
    "\n",
    "print(\"‚úÖ Cleanup complete! Now run Step 1 to install correct versions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q gradio torch transformers accelerate\n",
    "\n",
    "# Install audio processing libraries first\n",
    "!pip install -q librosa soundfile pydub\n",
    "\n",
    "# Install RAG and vector store with carefully managed versions\n",
    "# Install langchain-core first with the right version\n",
    "!pip install -q \"langchain-core>=0.3.72,<1.0.0\"\n",
    "\n",
    "# Install other langchain packages that depend on langchain-core < 1.0\n",
    "!pip install -q \"langchain>=0.3.0,<0.4.0\"\n",
    "!pip install -q \"langchain-community>=0.3.0,<0.4.0\"\n",
    "!pip install -q \"langchain-text-splitters>=0.3.0,<0.4.0\"\n",
    "\n",
    "# Install langchain-huggingface with compatible version (< 1.0.0)\n",
    "!pip install -q \"langchain-huggingface>=0.0.1,<1.0.0\"\n",
    "\n",
    "# Install FAISS and sentence transformers\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(\"‚úÖ Dependency conflicts resolved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_header",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for our application.\n",
    "\n",
    "**What each library does:**\n",
    "- `gradio` - Creates the web interface\n",
    "- `torch` - Handles GPU/CPU operations\n",
    "- `transformers` - Loads AI models (Whisper, BART, T5)\n",
    "- `langchain` - Builds the RAG Q&A system\n",
    "- `FAISS` - Stores and searches vector embeddings\n",
    "- `re` - Text pattern matching for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import re\n",
    "from typing import Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "import librosa  # For audio loading without ffmpeg\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress all deprecation and future warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*forced_decoder_ids.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# LangChain imports for RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline  # NEW: Both from langchain_huggingface\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üîç Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full_app",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Structure\n",
    "\n",
    "Create a data class to store transcription results and metadata.\n",
    "\n",
    "**TranscriptionResult stores:**\n",
    "- `text` - The full transcription\n",
    "- `summary` - Generated summary\n",
    "- `key_points` - List of extracted key points\n",
    "- `timestamps` - Timing information from Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_app",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    \"\"\"Store transcription results and metadata\"\"\"\n",
    "    text: str\n",
    "    summary: str = \"\"\n",
    "    key_points: List[str] = None\n",
    "    timestamps: dict = None\n",
    "\n",
    "print(\"‚úÖ Data structure defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technology",
   "metadata": {},
   "source": [
    "## Step 4: Create the VoiceScribe AI Class (Part 1 - Initialization)\n",
    "\n",
    "This is the main class that handles all AI functionality. We'll define it in parts:\n",
    "\n",
    "**Part 1 - `__init__` method:**\n",
    "- Detects GPU/CPU\n",
    "- Loads Whisper model for transcription\n",
    "- Loads BART model for summarization\n",
    "- Loads embeddings model for RAG\n",
    "- Loads FLAN-T5 model for Q&A\n",
    "\n",
    "**Note:** This will download ~7.5GB of models on first run and take 2-5 minutes to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lodpntpszo8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of VoiceScribeAI class - we'll build it piece by piece\n",
    "\n",
    "class VoiceScribeAI:\n",
    "    \"\"\"Main class for VoiceScribe AI functionality\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize models and components\"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.dtype = torch.float16 if self.device == 'cuda' else torch.float32\n",
    "\n",
    "        print(f\"üöÄ Initializing VoiceScribe AI on {self.device}...\")\n",
    "        print(\"‚è≥ This will take 2-5 minutes on first run...\\n\")\n",
    "\n",
    "        # Initialize transcription pipeline\n",
    "        print(\"üì• [1/4] Loading Whisper model...\")\n",
    "        self.transcription_pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=\"openai/whisper-medium.en\",\n",
    "            device=0 if self.device == 'cuda' else -1,  # 0 for GPU, -1 for CPU\n",
    "            return_timestamps=True\n",
    "        )\n",
    "        print(\"   ‚úÖ Whisper loaded!\\n\")\n",
    "\n",
    "        # Initialize summarization model\n",
    "        print(\"üì• [2/4] Loading BART summarization model...\")\n",
    "        self.summarization_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        self.summarization_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        if self.device == 'cuda':\n",
    "            self.summarization_model = self.summarization_model.to(self.device)\n",
    "        print(\"   ‚úÖ BART loaded!\\n\")\n",
    "\n",
    "        # Initialize embeddings for RAG\n",
    "        print(\"üì• [3/4] Loading embeddings model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        print(\"   ‚úÖ Embeddings loaded!\\n\")\n",
    "\n",
    "        # Initialize QA model\n",
    "        print(\"üì• [4/4] Loading Q&A model...\")\n",
    "        qa_model = \"google/flan-t5-base\"\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=qa_model,\n",
    "            device=0 if self.device == 'cuda' else -1,\n",
    "            max_length=512\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.qa_pipeline)\n",
    "        print(\"   ‚úÖ Q&A model loaded!\\n\")\n",
    "\n",
    "        # Storage for current session\n",
    "        self.current_transcription: Optional[TranscriptionResult] = None\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        self.qa_chain: Optional[RetrievalQA] = None\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚úÖ All models loaded successfully!\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "print(\"‚úÖ VoiceScribeAI class initialization defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bvz33b0bf4v",
   "metadata": {},
   "source": [
    "## Step 5: Add Transcription Method to VoiceScribeAI Class\n",
    "\n",
    "Now we'll add the main transcription method that:\n",
    "1. Takes an audio file as input\n",
    "2. Transcribes it using Whisper\n",
    "3. Generates a summary\n",
    "4. Extracts key points\n",
    "5. Sets up RAG for Q&A\n",
    "6. Returns all results\n",
    "\n",
    "**Note:** We're extending the class defined above, so make sure you ran the previous cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r072uy87kyj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transcription method to the class\n",
    "def transcribe_audio_method(self, audio_file_path: str, progress=gr.Progress()) -> Tuple[str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Transcribe audio file and return results\n",
    "    Returns: (transcription, summary, key_points, status_message)\n",
    "    \"\"\"\n",
    "    if not audio_file_path:\n",
    "        return \"\", \"\", \"\", \"‚ùå Please upload an audio file first.\"\n",
    "\n",
    "    try:\n",
    "        progress(0, desc=\"Starting transcription...\")\n",
    "\n",
    "        # Load audio using librosa (no ffmpeg needed!)\n",
    "        progress(0.1, desc=\"Loading audio file...\")\n",
    "        audio_array, sampling_rate = librosa.load(audio_file_path, sr=16000)\n",
    "        \n",
    "        # Transcribe audio\n",
    "        progress(0.2, desc=\"Transcribing audio (this may take a few minutes)...\")\n",
    "        result = self.transcription_pipe({\n",
    "            \"array\": audio_array,\n",
    "            \"sampling_rate\": sampling_rate\n",
    "        })\n",
    "        transcription = result[\"text\"]\n",
    "        timestamps = result.get(\"chunks\", [])\n",
    "\n",
    "        # Store transcription\n",
    "        self.current_transcription = TranscriptionResult(\n",
    "            text=transcription,\n",
    "            timestamps=timestamps\n",
    "        )\n",
    "\n",
    "        progress(0.5, desc=\"Generating summary...\")\n",
    "        summary = self.generate_summary(transcription)\n",
    "        self.current_transcription.summary = summary\n",
    "\n",
    "        progress(0.8, desc=\"Extracting key points...\")\n",
    "        key_points = self.extract_key_points(transcription)\n",
    "        self.current_transcription.key_points = key_points\n",
    "        key_points_text = \"\\n\".join([f\"‚Ä¢ {point}\" for point in key_points])\n",
    "\n",
    "        # Set up RAG for Q&A\n",
    "        progress(0.9, desc=\"Setting up Q&A system...\")\n",
    "        self.setup_rag(transcription)\n",
    "\n",
    "        progress(1.0, desc=\"Complete!\")\n",
    "\n",
    "        status = f\"‚úÖ Transcription complete! ({len(transcription.split())} words)\"\n",
    "        return transcription, summary, key_points_text, status\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error during transcription: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return \"\", \"\", \"\", error_msg\n",
    "\n",
    "# Attach method to class\n",
    "VoiceScribeAI.transcribe_audio = transcribe_audio_method\n",
    "print(\"‚úÖ Transcription method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuxkc1y6x2",
   "metadata": {},
   "source": [
    "## Step 6: Add Remaining Core Methods\n",
    "\n",
    "Now we'll add all the remaining methods to the VoiceScribeAI class:\n",
    "\n",
    "**Methods being added:**\n",
    "- `generate_summary()` - Uses BART to create summaries\n",
    "- `extract_key_points()` - Extracts main ideas\n",
    "- `setup_rag()` - Sets up vector database for Q&A\n",
    "- `answer_question()` - Answers questions using RAG\n",
    "- `search_transcript()` - Searches for keywords\n",
    "- `_split_text()` - Helper method for text chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5u3tpd62r9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all remaining methods to VoiceScribeAI class\n",
    "\n",
    "def generate_summary_method(self, text: str, max_length: int = 200) -> str:\n",
    "    \"\"\"Generate a concise summary of the text\"\"\"\n",
    "    try:\n",
    "        max_input_length = 1024\n",
    "        chunks = self._split_text(text, max_input_length)\n",
    "        summaries = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            inputs = self.summarization_tokenizer(\n",
    "                chunk, max_length=max_input_length, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            if self.device == 'cuda':\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            summary_ids = self.summarization_model.generate(\n",
    "                inputs[\"input_ids\"], max_length=max_length, min_length=50,\n",
    "                num_beams=4, length_penalty=2.0, early_stopping=True\n",
    "            )\n",
    "            summary = self.summarization_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "        \n",
    "        if len(summaries) > 1:\n",
    "            combined = \" \".join(summaries)\n",
    "            inputs = self.summarization_tokenizer(\n",
    "                combined, max_length=max_input_length, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            if self.device == 'cuda':\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            summary_ids = self.summarization_model.generate(\n",
    "                inputs[\"input_ids\"], max_length=max_length, min_length=50, num_beams=4\n",
    "            )\n",
    "            return self.summarization_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summaries[0]\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "def extract_key_points_method(self, text: str, num_points: int = 5) -> List[str]:\n",
    "    \"\"\"Extract key points from the text\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Extract {num_points} key points or main ideas from the following text.\n",
    "        List them as numbered points.\n",
    "\n",
    "        Text: {text[:2000]}\n",
    "\n",
    "        Key points:\"\"\"\n",
    "        \n",
    "        result = self.qa_pipeline(prompt, max_length=300, num_return_sequences=1)\n",
    "        points_text = result[0]['generated_text']\n",
    "        \n",
    "        points = []\n",
    "        lines = points_text.split('\\\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            line = re.sub(r'^\\\\d+[\\\\.\\\\)]\\\\s*', '', line)\n",
    "            if line and len(line) > 10:\n",
    "                points.append(line)\n",
    "        \n",
    "        if not points:\n",
    "            sentences = text.split('.')\n",
    "            points = [s.strip() + '.' for s in sentences[:num_points] if len(s.strip()) > 20]\n",
    "        \n",
    "        return points[:num_points]\n",
    "    except Exception as e:\n",
    "        sentences = text.split('.')\n",
    "        return [s.strip() + '.' for s in sentences[:num_points] if len(s.strip()) > 20]\n",
    "\n",
    "def setup_rag_method(self, text: str):\n",
    "    \"\"\"Set up RAG system for Q&A\"\"\"\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n",
    "        \n",
    "        qa_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        QA_PROMPT = PromptTemplate(template=qa_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm, chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up RAG: {e}\")\n",
    "        self.qa_chain = None\n",
    "\n",
    "def answer_question_method(self, question: str) -> str:\n",
    "    \"\"\"Answer questions about the transcription using RAG\"\"\"\n",
    "    if not self.current_transcription:\n",
    "        return \"‚ö†Ô∏è Please transcribe an audio file first before asking questions.\"\n",
    "    if not question or question.strip() == \"\":\n",
    "        return \"‚ö†Ô∏è Please enter a question.\"\n",
    "    if not self.qa_chain:\n",
    "        return \"‚ö†Ô∏è Q&A system not ready. Please try transcribing the audio again.\"\n",
    "    try:\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        return result.get('result', 'No answer found.')\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error answering question: {str(e)}\"\n",
    "\n",
    "def search_transcript_method(self, keyword: str) -> str:\n",
    "    \"\"\"Search for keyword in transcript\"\"\"\n",
    "    if not self.current_transcription:\n",
    "        return \"‚ö†Ô∏è Please transcribe an audio file first.\"\n",
    "    if not keyword or keyword.strip() == \"\":\n",
    "        return \"‚ö†Ô∏è Please enter a keyword to search.\"\n",
    "    try:\n",
    "        text = self.current_transcription.text\n",
    "        keyword = keyword.strip().lower()\n",
    "        sentences = text.split('.')\n",
    "        matches = []\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if keyword in sentence.lower():\n",
    "                context = sentence.strip()\n",
    "                matches.append(f\"[Match {len(matches) + 1}] ...{context}...\")\n",
    "        \n",
    "        if matches:\n",
    "            result = f\"Found {len(matches)} occurrence(s) of '{keyword}':\\\\n\\\\n\"\n",
    "            result += \"\\\\n\\\\n\".join(matches)\n",
    "            return result\n",
    "        else:\n",
    "            return f\"No occurrences of '{keyword}' found in the transcript.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error searching transcript: {str(e)}\"\n",
    "\n",
    "def split_text_method(self, text: str, max_length: int) -> List[str]:\n",
    "    \"\"\"Split text into chunks of max_length words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        current_length += 1\n",
    "        if current_length >= max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Attach all methods to class\n",
    "VoiceScribeAI.generate_summary = generate_summary_method\n",
    "VoiceScribeAI.extract_key_points = extract_key_points_method\n",
    "VoiceScribeAI.setup_rag = setup_rag_method\n",
    "VoiceScribeAI.answer_question = answer_question_method\n",
    "VoiceScribeAI.search_transcript = search_transcript_method\n",
    "VoiceScribeAI._split_text = split_text_method\n",
    "\n",
    "print(\"‚úÖ All methods added to VoiceScribeAI class!\")\n",
    "print(\"‚úÖ VoiceScribeAI class is now complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "od668ale2cr",
   "metadata": {},
   "source": [
    "## Step 7: Create Gradio Interface\n",
    "\n",
    "Build a beautiful web interface with multiple tabs:\n",
    "\n",
    "**Tabs included:**\n",
    "1. **Transcribe & Analyze** - Upload audio and get transcription, summary, and key points\n",
    "2. **Chat with Audio** - Ask questions about the transcription using RAG\n",
    "3. **Search Transcript** - Find keywords in the transcription\n",
    "4. **About** - Information about the application\n",
    "\n",
    "This function will create the complete UI and connect all the AI functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rwz9gy7zdx",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create and configure the Gradio interface\"\"\"\n",
    "    \n",
    "    # Initialize VoiceScribe AI (this loads all models)\n",
    "    vs_ai = VoiceScribeAI()\n",
    "    \n",
    "    # Custom CSS for styling\n",
    "    custom_css = \"\"\"\n",
    "    .gradio-container {\n",
    "        font-family: 'Arial', sans-serif;\n",
    "    }\n",
    "    .header {\n",
    "        text-align: center;\n",
    "        padding: 20px;\n",
    "        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);\n",
    "        color: white;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 20px;\n",
    "        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create interface\n",
    "    with gr.Blocks(css=custom_css, title=\"VoiceScribe AI\") as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üé§ VoiceScribe AI\n",
    "        ### Intelligent Audio Analysis Tool\n",
    "        Transform your audio recordings into actionable insights with AI-powered transcription, summarization, and Q&A.\n",
    "        \"\"\", elem_classes=\"header\")\n",
    "        \n",
    "        with gr.Tab(\"üìù Transcribe & Analyze\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Upload your audio file to get started\n",
    "            Supports MP3, WAV, M4A, and other common audio formats.\n",
    "            \"\"\")\n",
    "            \n",
    "            audio_input = gr.Audio(label=\"Upload Audio File\", type=\"filepath\")\n",
    "            transcribe_btn = gr.Button(\"üöÄ Transcribe & Analyze\", variant=\"primary\", size=\"lg\")\n",
    "            status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üìÑ Full Transcript\")\n",
    "                    transcript_output = gr.Textbox(\n",
    "                        label=\"Transcription\", lines=10,\n",
    "                        placeholder=\"Transcript will appear here...\", interactive=False\n",
    "                    )\n",
    "                \n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üìä Summary\")\n",
    "                    summary_output = gr.Textbox(\n",
    "                        label=\"Summary\", lines=5,\n",
    "                        placeholder=\"Summary will appear here...\", interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"### üéØ Key Points\")\n",
    "                    keypoints_output = gr.Textbox(\n",
    "                        label=\"Key Points\", lines=5,\n",
    "                        placeholder=\"Key points will appear here...\", interactive=False\n",
    "                    )\n",
    "        \n",
    "        # Combined Chat & Search Tab\n",
    "        with gr.Tab(\"üí¨ Chat & Search\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Interact with your transcription\n",
    "            Ask questions or search for specific keywords in your transcript.\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Left Column - Chat with Audio (Q&A)\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üí¨ Chat with Audio (Q&A)\")\n",
    "                    gr.Markdown(\"Ask natural language questions about your transcription.\")\n",
    "                    \n",
    "                    question_input = gr.Textbox(\n",
    "                        label=\"Ask a Question\",\n",
    "                        placeholder=\"e.g., What were the main topics discussed?\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                    ask_btn = gr.Button(\"ü§î Get Answer\", variant=\"primary\")\n",
    "                    answer_output = gr.Textbox(\n",
    "                        label=\"Answer\", lines=6,\n",
    "                        placeholder=\"Answer will appear here...\", interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"\"\"\n",
    "                    **Example Questions:**\n",
    "                    - What did they say about training data?\n",
    "                    - What are the action items mentioned?\n",
    "                    - What problems were discussed?\n",
    "                    \"\"\")\n",
    "                \n",
    "                # Right Column - Search Transcript\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üîç Search Transcript\")\n",
    "                    gr.Markdown(\"Find all occurrences of a keyword or phrase.\")\n",
    "                    \n",
    "                    search_input = gr.Textbox(\n",
    "                        label=\"Enter Keyword\",\n",
    "                        placeholder=\"e.g., deadline, budget, meeting\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                    search_btn = gr.Button(\"üîé Search\", variant=\"primary\")\n",
    "                    search_output = gr.Textbox(\n",
    "                        label=\"Search Results\", lines=6,\n",
    "                        placeholder=\"Search results will appear here...\", interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"\"\"\n",
    "                    **Search Tips:**\n",
    "                    - Use single keywords for best results\n",
    "                    - Try different variations (e.g., \"meet\", \"meeting\")\n",
    "                    - Search is case-insensitive\n",
    "                    \"\"\")\n",
    "        \n",
    "        with gr.Tab(\"‚ÑπÔ∏è About\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## About VoiceScribe AI\n",
    "            \n",
    "            VoiceScribe AI is an intelligent audio analysis tool that transforms your audio recordings into actionable insights.\n",
    "            \n",
    "            ### üîπ Key Features:\n",
    "            \n",
    "            - **üé§ Automatic Transcription**: Converts audio to text using OpenAI's Whisper model\n",
    "            - **üß† Smart Summarization**: Generates clear summaries using BART\n",
    "            - **üìå Key Points Extraction**: Lists the main topics and action items\n",
    "            - **üîç Searchable Transcript**: Quickly find parts of the recording by keyword\n",
    "            - **üí¨ Chat with Audio (Q&A Mode)**: Ask natural questions using RAG\n",
    "            \n",
    "            ### üõ†Ô∏è Technology Stack:\n",
    "            \n",
    "            - **Transcription**: OpenAI Whisper (medium.en)\n",
    "            - **Summarization**: Facebook BART-large-CNN\n",
    "            - **Q&A**: Google FLAN-T5 with LangChain RAG\n",
    "            - **Embeddings**: Sentence Transformers (all-MiniLM-L6-v2)\n",
    "            - **Vector Store**: FAISS\n",
    "            - **UI**: Gradio\n",
    "            \n",
    "            ### üìù Usage Tips:\n",
    "            \n",
    "            1. Upload your audio file in the \"Transcribe & Analyze\" tab\n",
    "            2. ‚è≥ **Wait for transcription to complete (may take a few minutes on CPU)**\n",
    "            3. View your transcript, summary, and key points\n",
    "            4. Go to \"Chat & Search\" tab to ask questions or search keywords\n",
    "            \n",
    "            ### üöÄ Powered by Open Source Models\n",
    "            \"\"\")\n",
    "        \n",
    "        # Connect functions to buttons\n",
    "        transcribe_btn.click(\n",
    "            fn=vs_ai.transcribe_audio,\n",
    "            inputs=[audio_input],\n",
    "            outputs=[transcript_output, summary_output, keypoints_output, status_output]\n",
    "        )\n",
    "        \n",
    "        ask_btn.click(\n",
    "            fn=vs_ai.answer_question,\n",
    "            inputs=[question_input],\n",
    "            outputs=[answer_output]\n",
    "        )\n",
    "        \n",
    "        search_btn.click(\n",
    "            fn=vs_ai.search_transcript,\n",
    "            inputs=[search_input],\n",
    "            outputs=[search_output]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "print(\"‚úÖ Gradio interface function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eivmz88eh",
   "metadata": {},
   "source": [
    "## Step 8: Launch the Application! üöÄ\n",
    "\n",
    "Now run this cell to launch VoiceScribe AI!\n",
    "\n",
    "**What will happen:**\n",
    "1. All AI models will be loaded (takes 2-5 minutes on first run)\n",
    "2. A web interface will open\n",
    "3. You'll get a local URL (e.g., http://127.0.0.1:7860)\n",
    "4. You'll also get a public URL for sharing (e.g., https://xxxxx.gradio.live)\n",
    "\n",
    "**After launching:**\n",
    "- Click on the local URL to open the interface\n",
    "- Upload an audio file in the \"Transcribe & Analyze\" tab\n",
    "- Wait for processing\n",
    "- Try asking questions and searching!\n",
    "\n",
    "**Note:** The cell will keep running until you stop it (use the stop button in Jupyter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2mq9omj9yq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch VoiceScribe AI!\n",
    "print(\"üöÄ Starting VoiceScribe AI...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create the interface (this will load all models)\n",
    "demo = create_gradio_interface()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ VoiceScribe AI is ready!\")\n",
    "print(\"üåê Launching interface...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Launch with public link\n",
    "demo.launch(\n",
    "    share=True,           # Create public shareable link\n",
    "    debug=True,           # Show debug info\n",
    "    show_error=True       # Show detailed errors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features_detail",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ How to Use the Application\n",
    "\n",
    "Once the application is running, follow these steps:\n",
    "\n",
    "### 1Ô∏è‚É£ Transcribe Audio\n",
    "\n",
    "1. Go to the **\"Transcribe & Analyze\"** tab\n",
    "2. Click to upload an audio file (or drag and drop)\n",
    "3. Click **\"üöÄ Transcribe & Analyze\"**\n",
    "4. Wait 2-5 minutes (depending on file length)\n",
    "5. View results: Transcript, Summary, and Key Points\n",
    "\n",
    "### 2Ô∏è‚É£ Ask Questions (Q&A)\n",
    "\n",
    "1. After transcribing, go to **\"Chat with Audio\"** tab\n",
    "2. Type your question (e.g., \"What were the main topics?\")\n",
    "3. Click **\"ü§î Get Answer\"**\n",
    "4. View AI-generated answer based on the transcript\n",
    "\n",
    "### 3Ô∏è‚É£ Search Transcript\n",
    "\n",
    "1. Go to **\"Search Transcript\"** tab\n",
    "2. Enter a keyword (e.g., \"deadline\")\n",
    "3. Click **\"üîé Search\"**\n",
    "4. View all occurrences with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Customization Options\n",
    "\n",
    "You can customize the application by modifying the code in the previous cells:\n",
    "\n",
    "### Change Whisper Model (Step 4):\n",
    "```python\n",
    "# In the __init__ method, replace:\n",
    "model=\"openai/whisper-medium.en\"\n",
    "\n",
    "# With one of these:\n",
    "model=\"openai/whisper-tiny.en\"    # Fastest (1GB)\n",
    "model=\"openai/whisper-small.en\"   # Balanced (2.5GB)  \n",
    "model=\"openai/whisper-large\"      # Best quality (10GB)\n",
    "```\n",
    "\n",
    "### Adjust Summary Length (Step 6):\n",
    "```python\n",
    "# In generate_summary_method, change:\n",
    "max_length=200  # Make this larger for longer summaries\n",
    "min_length=50   # Make this larger for minimum length\n",
    "```\n",
    "\n",
    "### Change Number of Key Points (Step 6):\n",
    "```python\n",
    "# In extract_key_points_method, change:\n",
    "num_points=5  # Change to 3, 7, 10, etc.\n",
    "```\n",
    "\n",
    "### Modify RAG Chunk Size (Step 6):\n",
    "```python\n",
    "# In setup_rag_method, change:\n",
    "chunk_size=500      # Larger = more context per chunk\n",
    "chunk_overlap=50    # Overlap between chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Use Cases\n",
    "\n",
    "VoiceScribe AI is perfect for:\n",
    "\n",
    "- **üìÖ Meeting Minutes**: Automatically transcribe and summarize team meetings\n",
    "- **üéôÔ∏è Podcast Analysis**: Extract key insights from podcast episodes\n",
    "- **üë• Interview Transcription**: Convert interviews to searchable text\n",
    "- **üìö Lecture Notes**: Transcribe educational content and generate study notes\n",
    "- **üî¨ Research Interviews**: Analyze qualitative research data\n",
    "- **‚úçÔ∏è Content Creation**: Extract quotes and key points for articles\n",
    "- **üìû Call Recordings**: Document important phone conversations\n",
    "- **üé¨ Video Content**: Transcribe YouTube videos or webinars\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è using Open Source AI Models by Rohit**\n",
    "\n",
    "**Happy Transcribing! üé§**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
