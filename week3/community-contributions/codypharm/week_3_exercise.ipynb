{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Link to Google Colab\n",
        "\n",
        "[Open in Google Colab](https://colab.research.google.com/drive/1JG6L7CsB081Zf9e6AQ5zim7F6vEJflbO?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** I was not allowed to access the Meta models despite having permission. Setting the token as part of the params made it work:\n",
        "\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA, token=hf_token)\n",
        "response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "```\n",
        ""
      ],
      "id": "8737a9f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate transformers==4.57.6"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e0157fa3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# imports\n",
        "\n",
        "import gradio as gr\n",
        "from IPython.display import  display, update_display\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b6500613"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Constants\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cb6cd3f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5c4206f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "system_prompt = \"\"\"You are a helpful assistant for pharmacists in a community pharmacy.\n",
        " You are able to review prescriptions and recommend to the pharmacist wether to dispence or not.\n",
        " You recommend to the pharmcist some safety measures like \"contact prescriber\" if there is an issue with the precription.\n",
        " Always consider drug -drug interaction and call it out.\n",
        "  \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d4434fc6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d9a21658"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "llm = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config, token=hf_token)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "aa7b131f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chat(message, history):\n",
        "  messages = [{\"role\": \"system\", \"content\":system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  outputs = llm.generate(inputs,max_new_tokens=300, streamer=streamer)\n",
        "  # Decode only the newly generated tokens and skip special tokens\n",
        "  response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "  return response"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "741ab92c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "39a46217"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}