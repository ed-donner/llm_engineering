{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_3lWj7s4gh8"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåÄ Q&A Generator\n",
        "\n",
        "This Colab project uses open-source **Hugging Face instruct model** to build a Q&A on LLM Engineering.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Context\n",
        "1. **Topics** a list of topics from Week 1-3 of LLM Engineering course by Ed Donner.  \n",
        "2. **Difficulty** three levels ranging from Beginner, Intermediate and Advanced.  \n",
        "3. **Interface** built with the beautiful Gradio UI.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Highlights\n",
        "- Generate helpful Q&A to assist learning with **Meta-Llama-3.1-8B-Instruct**.  \n",
        "- Supports **4-bit quantized loading** for efficiency, can be run on free T4 GPU via Google Colab.\n",
        "- Simple **Gradio user interface** with ability to download JSON and CSV files.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìò Notebook\n",
        "üëâ <a href=\"https://colab.research.google.com/drive/1A8mtfT_JyJQISWa96ZEduHsqNnpb5yGN?usp=share_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\" Open In Google Colab \"/></a>\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Tech Stack\n",
        "`Google Colab ¬∑ Hugging Face ¬∑ Transformers ¬∑ BitsAndBytes ¬∑ Pandas ¬∑ JSON ¬∑ Torch ¬∑ Gradio`\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Summary\n",
        "**Q&A Generator** shows how open-source LLM models can be used as a **powerful learning tool**, delivering **on-demand support** for students learning new topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW3p9OiJ320J"
      },
      "outputs": [],
      "source": [
        "# Built to run on Google Colab\n",
        "!pip install -q transformers accelerate bitsandbytes torch gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAp-9Frn4cr4"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY73WHUp4rEy"
      },
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "print(\"Successfully authenticated with Hugging Face\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78W25JdBSLCF"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01zPkguD5uDl"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# 4-bit quantization for efficiency on T4 GPU\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWfMXZX_SWMR"
      },
      "source": [
        "## Define learning curriculum topics and difficulty settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCe7fEZG_kZS"
      },
      "outputs": [],
      "source": [
        "# Topic definitions, revise and add as needed\n",
        "TOPICS = {\n",
        "    \"Week 1: LLM APIs & Prompting\": {\n",
        "        \"concepts\": [\n",
        "            \"Cursor IDE set up\",\n",
        "            \"Introduction to git and github\",\n",
        "            \"Different types of LLM: base, instruct, reasoning\",\n",
        "            \"OpenAI API usage and parameters\",\n",
        "            \"Ollama and OpenRouter usage\",\n",
        "            \"Prompt engineering techniques\",\n",
        "            \"System vs user messages\",\n",
        "            \"JSON mode and structured outputs\",\n",
        "            \"Token counting and API pricing\",\n",
        "            \"Encoding and decoding\",\n",
        "            \"Chat completions vs completions\",\n",
        "            \"Multi-shot prompting\",\n",
        "            \"Prompt caching\",\n",
        "        ]\n",
        "    },\n",
        "    \"Week 2: Function Calling & Agents\": {\n",
        "        \"concepts\": [\n",
        "            \"Function calling syntax and format\",\n",
        "            \"Tool definitions and schemas\",\n",
        "            \"Common use cases for Tools\",\n",
        "            \"Parallel function calling\",\n",
        "            \"Function calling best practices\",\n",
        "            \"Agent patterns and workflows\",\n",
        "            \"Structured outputs with Pydantic\",\n",
        "            \"Error handling in function calls\",\n",
        "            \"Use of Gradio UI\",\n",
        "        ]\n",
        "    },\n",
        "    \"Week 3: Transformers & Models\": {\n",
        "        \"concepts\": [\n",
        "            \"Difference between Hugging Face platform and libraries\",\n",
        "            \"Popular Hugging Face libraries\",\n",
        "            \"Tokenizers and tokenization strategies\",\n",
        "            \"Hugging Face pipelines\",\n",
        "            \"AutoModel and AutoTokenizer\",\n",
        "            \"Model quantization (4-bit, 8-bit)\",\n",
        "            \"Speech-to-text with Whisper\",\n",
        "            \"Local vs cloud model inference\",\n",
        "            \"Model architectures (encoder, decoder, encoder-decoder)\",\n",
        "            \"Introduction to Google Colab\",\n",
        "            \"Deep dive into LLM layers such as Llama 3.2\",\n",
        "            \"LLM temperature or do sample settings\",\n",
        "        ]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4bGXs_1fnGC"
      },
      "outputs": [],
      "source": [
        "# Difficulty definitions\n",
        "DIFFICULTY_LEVELS = {\n",
        "    \"Beginner\": \"Basic understanding of concepts and definitions\",\n",
        "    \"Intermediate\": \"Application of concepts with some technical depth\",\n",
        "    \"Advanced\": \"Edge cases, optimization, and deep technical understanding\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh8S2WDTScu7"
      },
      "source": [
        "## Main function to generate multiple-choice questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVI1PJilDeK1"
      },
      "outputs": [],
      "source": [
        "def generate_questions(topic, difficulty, num_questions):\n",
        "    \"\"\"\n",
        "    Generate Q&A questions using the LLM.\n",
        "\n",
        "    Args:\n",
        "        topic: Topic category to generate questions for based on the curriculum\n",
        "        difficulty: Difficulty level (Beginner/Intermediate/Advanced)\n",
        "        num_questions: Number of questions to generate\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing questions and answers\n",
        "    \"\"\"\n",
        "\n",
        "    # Get topic details\n",
        "    topic_info = TOPICS[topic]\n",
        "    concepts = \", \".join(topic_info[\"concepts\"])\n",
        "\n",
        "    # Build the system prompt\n",
        "    system_message = \"\"\"\n",
        "    You are an expert teacher creating high-quality multiple-choice questions for an LLM Engineering course.\n",
        "\n",
        "    Format each question EXACTLY as shown below:\n",
        "\n",
        "    QUESTION: [question text]\n",
        "    A) [option A]\n",
        "    B) [option B]\n",
        "    C) [option C]\n",
        "    D) [option D]\n",
        "    ANSWER: [correct letter]\n",
        "    EXPLANATION: [brief explanation]\n",
        "    ---\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the user prompt\n",
        "    user_prompt = f\"\"\"\n",
        "    Create {num_questions} multiple-choice questions about: {topic}\n",
        "\n",
        "    Difficulty Level: {difficulty}\n",
        "\n",
        "    Cover these concepts: {concepts}\n",
        "\n",
        "    Requirements:\n",
        "    - Questions should be practical and relevant to real LLM engineering\n",
        "    - All 4 options should be plausible\n",
        "    - Explanations should be clear and educational\n",
        "    - Vary the correct answer position\n",
        "\n",
        "    Generate {num_questions} questions now:\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare messages for LLM\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Tokenize using HF's apply_chat_template utility\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
        "\n",
        "    # Generate and set max tokens\n",
        "    print(f\"Generating {num_questions} questions...\")\n",
        "    max_tokens = min(2500, num_questions * 200)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the assistant's response\n",
        "    if \"assistant\" in response:\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # Debug: print what we got\n",
        "    print(\"Generated text preview:\")\n",
        "    print(response[:500] + \"...\" if len(response) > 500 else response)\n",
        "    print()\n",
        "\n",
        "    # Parse the questions\n",
        "    questions = parse_questions(response, topic, difficulty)\n",
        "\n",
        "    print(f\"Successfully generated {len(questions)} questions\")\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WazwKXn-LYDa"
      },
      "outputs": [],
      "source": [
        "def parse_questions(text, topic, difficulty):\n",
        "    \"\"\"\n",
        "    Parse the generated text into structured question objects.\n",
        "    More robust parsing that handles various formats.\n",
        "    \"\"\"\n",
        "    questions = []\n",
        "\n",
        "    # Split by \"QUESTION:\" to get individual question blocks\n",
        "    blocks = text.split(\"QUESTION:\")\n",
        "\n",
        "    for i, block in enumerate(blocks):\n",
        "        if not block.strip() or i == 0 and len(block) < 20:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Extract components\n",
        "            question_text = \"\"\n",
        "            options = {}\n",
        "            answer = \"\"\n",
        "            explanation = \"\"\n",
        "\n",
        "            lines = block.strip().split(\"\\n\")\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line or line == \"---\":\n",
        "                    continue\n",
        "\n",
        "                # Handle question text (first non-empty line before options)\n",
        "                if not question_text and not any(line.startswith(x) for x in [\"A)\", \"B)\", \"C)\", \"D)\", \"ANSWER:\", \"EXPLANATION:\", \"Answer:\", \"Explanation:\"]):\n",
        "                    question_text = line\n",
        "\n",
        "                # Handle options - be flexible with formatting\n",
        "                elif line.startswith(\"A)\") or line.startswith(\"A.\"):\n",
        "                    options[\"A\"] = line[2:].strip()\n",
        "                elif line.startswith(\"B)\") or line.startswith(\"B.\"):\n",
        "                    options[\"B\"] = line[2:].strip()\n",
        "                elif line.startswith(\"C)\") or line.startswith(\"C.\"):\n",
        "                    options[\"C\"] = line[2:].strip()\n",
        "                elif line.startswith(\"D)\") or line.startswith(\"D.\"):\n",
        "                    options[\"D\"] = line[2:].strip()\n",
        "\n",
        "                # Handle answer\n",
        "                elif line.upper().startswith(\"ANSWER:\"):\n",
        "                    answer = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "                # Handle explanation\n",
        "                elif line.upper().startswith(\"EXPLANATION:\"):\n",
        "                    explanation = line.split(\":\", 1)[1].strip()\n",
        "                elif explanation and len(explanation) < 200:\n",
        "                    # Continue multi-line explanation (up to reasonable length)\n",
        "                    explanation += \" \" + line\n",
        "\n",
        "            # Extract just the letter from answer\n",
        "            if answer:\n",
        "                answer_letter = \"\"\n",
        "                for char in answer.upper():\n",
        "                    if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                        answer_letter = char\n",
        "                        break\n",
        "                answer = answer_letter\n",
        "\n",
        "            # Only add if we have minimum required components\n",
        "            if question_text and len(options) >= 3 and answer:\n",
        "                # Fill missing option if needed\n",
        "                if len(options) == 3:\n",
        "                    for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "                        if letter not in options:\n",
        "                            options[letter] = \"Not applicable\"\n",
        "                            break\n",
        "\n",
        "                # Use placeholder explanation if none provided\n",
        "                if not explanation:\n",
        "                    explanation = f\"The correct answer is {answer}.\"\n",
        "\n",
        "                questions.append({\n",
        "                    \"id\": len(questions) + 1,\n",
        "                    \"topic\": topic,\n",
        "                    \"difficulty\": difficulty,\n",
        "                    \"question\": question_text,\n",
        "                    \"options\": options,\n",
        "                    \"correct_answer\": answer,\n",
        "                    \"explanation\": explanation.strip()\n",
        "                })\n",
        "                print(f\"Parsed question {len(questions)}\")\n",
        "            else:\n",
        "                print(f\"Skipped incomplete block: Q={bool(question_text)}, Opts={len(options)}, Ans={bool(answer)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing block {i+1}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TSRqJEZLoHN"
      },
      "outputs": [],
      "source": [
        "def format_questions_display(questions):\n",
        "    \"\"\"Format questions for display in Gradio.\"\"\"\n",
        "    if not questions:\n",
        "        return \"No questions generated.\"\n",
        "\n",
        "    output = f\"# Generated Questions\\n\\n\"\n",
        "    output += f\"**Total Questions:** {len(questions)}\\n\\n\"\n",
        "    output += \"---\\n\\n\"\n",
        "\n",
        "    for q in questions:\n",
        "        output += f\"## Question {q['id']}\\n\\n\"\n",
        "        output += f\"**Topic:** {q['topic']}  \\n\"\n",
        "        output += f\"**Difficulty:** {q['difficulty']}  \\n\\n\"\n",
        "        output += f\"**Q:** {q['question']}\\n\\n\"\n",
        "\n",
        "        for letter in ['A', 'B', 'C', 'D']:\n",
        "            # prefix currently NOT in use so answer is not obviously shown in UI\n",
        "            prefix = \"‚úÖ \" if letter == q['correct_answer'] else \"\"\n",
        "            output += f\"{letter}) {q['options'][letter]}\\n\\n\"\n",
        "\n",
        "        output += f\"**Answer:** {q['correct_answer']}\\n\\n\"\n",
        "        output += f\"**Explanation:** {q['explanation']}\\n\\n\"\n",
        "        output += \"---\\n\\n\"\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p96PtPVoOF4"
      },
      "outputs": [],
      "source": [
        "def export_to_json(questions):\n",
        "    \"\"\"Export questions to JSON file.\"\"\"\n",
        "    if not questions:\n",
        "        return None\n",
        "\n",
        "    filename = \"qa_dataset.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(questions, f, indent=2)\n",
        "\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrSwpHHCoQzy"
      },
      "outputs": [],
      "source": [
        "def export_to_csv(questions):\n",
        "    \"\"\"Export questions to CSV file.\"\"\"\n",
        "    if not questions:\n",
        "        return None\n",
        "\n",
        "    # Flatten the data for CSV\n",
        "    flattened = []\n",
        "    for q in questions:\n",
        "        flattened.append({\n",
        "            'id': q['id'],\n",
        "            'topic': q['topic'],\n",
        "            'difficulty': q['difficulty'],\n",
        "            'question': q['question'],\n",
        "            'option_A': q['options']['A'],\n",
        "            'option_B': q['options']['B'],\n",
        "            'option_C': q['options']['C'],\n",
        "            'option_D': q['options']['D'],\n",
        "            'correct_answer': q['correct_answer'],\n",
        "            'explanation': q['explanation']\n",
        "        })\n",
        "\n",
        "    filename = \"qa_dataset.csv\"\n",
        "    df = pd.DataFrame(flattened)\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg_knR0aSqAj"
      },
      "source": [
        "## Build Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p_qEJOlLtKq"
      },
      "outputs": [],
      "source": [
        "def gradio_generate(topic, difficulty, num_questions):\n",
        "    \"\"\"\n",
        "    Wrapper function for Gradio interface.\n",
        "    Generates questions and returns formatted output plus download files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate questions\n",
        "        questions = generate_questions(topic, difficulty, num_questions)\n",
        "\n",
        "        if not questions:\n",
        "            return \"Failed to generate questions. Please try again.\", None, None\n",
        "\n",
        "        # Format for display\n",
        "        display_text = format_questions_display(questions)\n",
        "\n",
        "        # Export files\n",
        "        json_file = export_to_json(questions)\n",
        "        csv_file = export_to_csv(questions)\n",
        "\n",
        "        return display_text, json_file, csv_file\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78uqJKndLx2j"
      },
      "outputs": [],
      "source": [
        "# Build the Gradio UI\n",
        "with gr.Blocks(title=\"Q&A Generator\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üìö Educational Q&A Dataset Generator\n",
        "    Generate high-quality multiple-choice questions for LLM Engineering topics\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Configuration\")\n",
        "\n",
        "            topic_dropdown = gr.Dropdown(\n",
        "                choices=list(TOPICS.keys()),\n",
        "                value=\"Week 1: LLM APIs & Prompting\",\n",
        "                label=\"Select Topic\",\n",
        "                info=\"Choose which week's content to generate questions for\"\n",
        "            )\n",
        "\n",
        "            difficulty_dropdown = gr.Dropdown(\n",
        "                choices=[\"Beginner\", \"Intermediate\", \"Advanced\"],\n",
        "                value=\"Intermediate\",\n",
        "                label=\"Difficulty Level\",\n",
        "                info=\"Select the difficulty of the questions\"\n",
        "            )\n",
        "\n",
        "            num_questions_slider = gr.Slider(\n",
        "                minimum=5,\n",
        "                maximum=20,\n",
        "                value=10,\n",
        "                step=5,\n",
        "                label=\"Number of Questions\",\n",
        "                info=\"How many questions to generate (5-20)\"\n",
        "            )\n",
        "\n",
        "            generate_btn = gr.Button(\"üöÄ Generate Questions\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            ### üì• Download Files\n",
        "            After generation, download your dataset in JSON or CSV format\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                json_download = gr.File(label=\"JSON File\", interactive=False)\n",
        "                csv_download = gr.File(label=\"CSV File\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üìù Generated Questions\")\n",
        "\n",
        "            output_display = gr.Markdown(\n",
        "                value=\"Click 'Generate Questions' to start...\",\n",
        "                label=\"Questions\"\n",
        "            )\n",
        "\n",
        "    # Connect the generate button\n",
        "    generate_btn.click(\n",
        "        fn=gradio_generate,\n",
        "        inputs=[topic_dropdown, difficulty_dropdown, num_questions_slider],\n",
        "        outputs=[output_display, json_download, csv_download]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### üí° Tips:\n",
        "    - Start with 5 questions to test the system\n",
        "    - Beginner questions cover definitions and basic concepts\n",
        "    - Intermediate questions test application and understanding\n",
        "    - Advanced questions explore edge cases and optimization\n",
        "    - Generation takes ~30-60 seconds depending on number of questions\n",
        "\n",
        "    ### üìä Output Formats:\n",
        "    - **JSON**: Structured data for programmatic use\n",
        "    - **CSV**: Easy to view in spreadsheets or import into other tools\n",
        "    \"\"\")\n",
        "\n",
        "print(\"‚úÖ Gradio interface configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhAkgy7ITkUg"
      },
      "outputs": [],
      "source": [
        "# Launch the Gradio app, create a public URL\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
