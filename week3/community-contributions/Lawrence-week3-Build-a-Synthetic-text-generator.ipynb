{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# A Synthetic Text Generator\n",
    "\n",
    "This section adds an interactive Gradio app that:\n",
    "\n",
    "- Uses Hugging Face `pipeline`\n",
    "- Allows model selection (2 open-source models)\n",
    "- Allows prompt type selection\n",
    "- Includes creativity control (temperature)\n",
    "- Runs with GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ03dQDl2h0D",
    "outputId": "9b4f585c-28ff-4ba1-9678-aa35aceb6fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Pip installs should come at the top line.\n",
    "# If your Kernel ever resets, you need to run this again.\n",
    "\n",
    "!pip install -q --upgrade datasets==3.6.0 transformers==4.57.6 gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bj_PlzfSd5d"
   },
   "outputs": [],
   "source": [
    "# Let's check the GPU - it should be a Tesla T4\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "  if gpu_info.find('Tesla T4') >= 0:\n",
    "    print(\"Success - Connected to a T4\")\n",
    "  else:\n",
    "    print(\"NOT CONNECTED TO A T4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTm7gpG7qhB7"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7GoH-tT6-xD"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "if hf_token and hf_token.startswith(\"hf_\"):\n",
    "  print(\"HF key looks good so far\")\n",
    "else:\n",
    "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a58dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS = {\n",
    "    \"GPT-2\": \"gpt2\",\n",
    "    \"GEMMA\": \"google/gemma-3-270m-it\"\n",
    "}\n",
    "\n",
    "PROMPT_TYPES = {\n",
    "    \"Movie Review\": \"Write a detailed movie review:\",\n",
    "    \"Blog Post\": \"Write a short blog post about productivity:\",\n",
    "    \"Product Review\": \"Write a product review for a new smartphone:\",\n",
    "    \"Customer Complaint\": \"Write a frustrated customer complaint email:\",\n",
    "    \"News Article\": \"Write a short news article about artificial intelligence:\"\n",
    "}\n",
    "\n",
    "def generate_text(model_choice, prompt_type, temperature):\n",
    "    model = MODELS[model_choice]\n",
    "    generator =  pipeline(\"text-generation\", model=model, device=\"cuda\")\n",
    "    prompt = PROMPT_TYPES[prompt_type]\n",
    "    output = generator(prompt, max_length=120, do_sample=True, temperature=temperature)\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\n",
    "        gr.Dropdown(list(MODELS.keys()), label=\"Choose Model\"),\n",
    "        gr.Dropdown(list(PROMPT_TYPES.keys()), label=\"Choose Prompt Type\"),\n",
    "        gr.Slider(0.3, 1.2, value=0.8, step=0.1, label=\"Creativity (Temperature)\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Generated Text:\"),\n",
    "    title=\"Synthetic Text Generator\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
