{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Week 3 Exercise - Synthetic Data Generator\n",
    "\n",
    "## Task\n",
    "Generate synthetic data for Pidgin and Yoruba languages from English input using HuggingFace transformers.\n",
    "\n",
    "### Requirements:\n",
    "- Use a variety of models and prompts for diverse outputs\n",
    "- Create a Gradio UI for your product\n",
    "- Generate translations in both Pidgin (Nigerian Pidgin) and Yoruba\n",
    "- Use HuggingFace transformers library (following week 3 day 3 & 4 patterns)\n",
    "\n",
    "### Approach:\n",
    "1. Multiple HuggingFace models (Llama, Phi, Qwen, Gemma)\n",
    "2. Different prompt strategies (direct translation, contextual, with examples)\n",
    "3. Batch generation capabilities\n",
    "4. Export generated data as CSV/JSON\n",
    "\n",
    "**Note:** This notebook is designed to run on Google Colab with a T4 GPU for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Google Colab)\n",
    "# !pip install -q --upgrade bitsandbytes accelerate transformers==4.57.6 gradio pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_hf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - Login to HuggingFace\n",
    "# Uncomment these lines when running on Colab\n",
    "\n",
    "# from google.colab import userdata\n",
    "# from huggingface_hub import login\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_selection",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We'll use multiple HuggingFace models to create diverse outputs:\n",
    "- **Llama 3.2**: Meta's efficient small model\n",
    "- **Phi-4**: Microsoft's compact but powerful model  \n",
    "- **Gemma**: Google's lightweight model\n",
    "- **Qwen**: Alibaba's multilingual model\n",
    "\n",
    "**Note:** For Llama models, you need to request access at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available models\n",
    "MODELS = {\n",
    "    \"Llama-3.2-1B\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"Phi-4-Mini\": \"microsoft/Phi-4-mini-instruct\",\n",
    "    \"Gemma-270M\": \"google/gemma-3-270m-it\",\n",
    "    \"Qwen-4B\": \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config for efficient memory usage (similar to week3/day4)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ") if device == \"cuda\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Translation Prompt Strategies\n",
    "\n",
    "We'll use different prompt strategies to create diverse outputs:\n",
    "1. **Direct Translation**: Simple, straightforward translation\n",
    "2. **Contextual Translation**: Consider context and tone\n",
    "3. **Few-Shot Translation**: Provide examples for better accuracy\n",
    "4. **Cultural Adaptation**: Adapt idioms and cultural references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates for different strategies\n",
    "\n",
    "DIRECT_PROMPT = {\n",
    "    \"pidgin\": \"\"\"Translate the following English text to Nigerian Pidgin English.\n",
    "Be accurate and natural. Only provide the translation, no explanations.\n",
    "\n",
    "English: {text}\n",
    "Nigerian Pidgin:\"\"\",\n",
    "    \n",
    "    \"yoruba\": \"\"\"Translate the following English text to Yoruba language.\n",
    "Be accurate and use proper Yoruba grammar. Only provide the translation, no explanations.\n",
    "\n",
    "English: {text}\n",
    "Yoruba:\"\"\"\n",
    "}\n",
    "\n",
    "CONTEXTUAL_PROMPT = {\n",
    "    \"pidgin\": \"\"\"You are an expert in Nigerian Pidgin English. Translate the following English text to Nigerian Pidgin,\n",
    "maintaining the tone, emotion, and cultural context. Make it sound natural and conversational.\n",
    "Provide only the translation.\n",
    "\n",
    "English: {text}\n",
    "Nigerian Pidgin:\"\"\",\n",
    "    \n",
    "    \"yoruba\": \"\"\"You are a Yoruba language expert. Translate the following English text to Yoruba,\n",
    "preserving the meaning, tone, and cultural nuances. Use appropriate Yoruba expressions.\n",
    "Provide only the translation.\n",
    "\n",
    "English: {text}\n",
    "Yoruba:\"\"\"\n",
    "}\n",
    "\n",
    "FEW_SHOT_PROMPT = {\n",
    "    \"pidgin\": \"\"\"Translate English to Nigerian Pidgin. Here are some examples:\n",
    "\n",
    "English: Good morning, how are you?\n",
    "Nigerian Pidgin: Mornin', how you dey?\n",
    "\n",
    "English: I am going to the market.\n",
    "Nigerian Pidgin: I dey go market.\n",
    "\n",
    "English: What are you doing?\n",
    "Nigerian Pidgin: Wetin you dey do?\n",
    "\n",
    "Now translate this (provide only the translation):\n",
    "English: {text}\n",
    "Nigerian Pidgin:\"\"\",\n",
    "    \n",
    "    \"yoruba\": \"\"\"Translate English to Yoruba. Here are some examples:\n",
    "\n",
    "English: Good morning\n",
    "Yoruba: E kaaro\n",
    "\n",
    "English: How are you?\n",
    "Yoruba: Bawo ni?\n",
    "\n",
    "English: Thank you\n",
    "Yoruba: E se\n",
    "\n",
    "Now translate this (provide only the translation):\n",
    "English: {text}\n",
    "Yoruba:\"\"\"\n",
    "}\n",
    "\n",
    "CULTURAL_PROMPT = {\n",
    "    \"pidgin\": \"\"\"Translate to Nigerian Pidgin English, adapting idioms and cultural references to Nigerian context.\n",
    "Make it sound like how a Nigerian would naturally speak in Pidgin.\n",
    "Provide only the translation.\n",
    "\n",
    "English: {text}\n",
    "Nigerian Pidgin:\"\"\",\n",
    "    \n",
    "    \"yoruba\": \"\"\"Translate to Yoruba, adapting cultural references and idioms to Yoruba tradition and culture.\n",
    "Use traditional Yoruba expressions where appropriate.\n",
    "Provide only the translation.\n",
    "\n",
    "English: {text}\n",
    "Yoruba:\"\"\"\n",
    "}\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Direct\": DIRECT_PROMPT,\n",
    "    \"Contextual\": CONTEXTUAL_PROMPT,\n",
    "    \"Few-Shot\": FEW_SHOT_PROMPT,\n",
    "    \"Cultural\": CULTURAL_PROMPT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e92304",
   "metadata": {},
   "source": [
    "## Model Loading and Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for loaded models to avoid reloading\n",
    "model_cache = {}\n",
    "tokenizer_cache = {}\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"Load model and tokenizer with caching\"\"\"\n",
    "    if model_name in model_cache:\n",
    "        return model_cache[model_name], tokenizer_cache[model_name]\n",
    "    \n",
    "    model_id = MODELS[model_name]\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with quantization if GPU available\n",
    "    if device == \"cuda\" and quant_config is not None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quant_config,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if device == \"cpu\":\n",
    "            model = model.to(device)\n",
    "    \n",
    "    model_cache[model_name] = model\n",
    "    tokenizer_cache[model_name] = tokenizer\n",
    "    \n",
    "    print(f\"{model_name} loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def clear_model_cache():\n",
    "    \"\"\"Clear model cache to free memory\"\"\"\n",
    "    global model_cache, tokenizer_cache\n",
    "    model_cache.clear()\n",
    "    tokenizer_cache.clear()\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Model cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "translate_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text: str, language: str, model_name: str, strategy: str = \"Contextual\") -> str:\n",
    "    \"\"\"Translate text using specified model and strategy\"\"\"\n",
    "    try:\n",
    "        # Get prompt template\n",
    "        lang_code = \"pidgin\" if \"Pidgin\" in language else \"yoruba\"\n",
    "        prompt_template = PROMPT_STRATEGIES[strategy][lang_code]\n",
    "        prompt = prompt_template.format(text=text)\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = load_model(model_name)\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the translation (remove the input prompt)\n",
    "        # This is a simple extraction - the response should contain the translation after the prompt\n",
    "        if lang_code == \"pidgin\":\n",
    "            if \"Nigerian Pidgin:\" in full_response:\n",
    "                translation = full_response.split(\"Nigerian Pidgin:\")[-1].strip()\n",
    "            else:\n",
    "                translation = full_response.split(prompt)[-1].strip() if prompt in full_response else full_response\n",
    "        else:\n",
    "            if \"Yoruba:\" in full_response:\n",
    "                translation = full_response.split(\"Yoruba:\")[-1].strip()\n",
    "            else:\n",
    "                translation = full_response.split(prompt)[-1].strip() if prompt in full_response else full_response\n",
    "        \n",
    "        # Clean up the translation\n",
    "        translation = translation.split('\\n')[0].strip()  # Take first line if multiple\n",
    "        \n",
    "        return translation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a887eb3",
   "metadata": {},
   "source": [
    "## Test Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple sentence\n",
    "test_text = \"Good morning! How are you doing today?\"\n",
    "\n",
    "print(\"Testing translation with Phi-4-Mini model...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pidgin_translation = translate_text(test_text, \"Nigerian Pidgin\", \"Phi-4-Mini\", \"Few-Shot\")\n",
    "print(f\"English: {test_text}\")\n",
    "print(f\"Pidgin: {pidgin_translation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "yoruba_translation = translate_text(test_text, \"Yoruba\", \"Phi-4-Mini\", \"Few-Shot\")\n",
    "print(f\"English: {test_text}\")\n",
    "print(f\"Yoruba: {yoruba_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fc55b",
   "metadata": {},
   "source": [
    "## Batch Generation for Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63230373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(english_texts: List[str], model_name: str = \"Phi-4-Mini\", \n",
    "                                strategy: str = \"Contextual\", progress_callback=None) -> pd.DataFrame:\n",
    "    \"\"\"Generate a complete synthetic dataset with multiple translations\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, text in enumerate(english_texts):\n",
    "        if progress_callback:\n",
    "            progress_callback(f\"Processing {i+1}/{len(english_texts)}: {text[:50]}...\")\n",
    "        else:\n",
    "            print(f\"Processing {i+1}/{len(english_texts)}: {text[:50]}...\")\n",
    "        \n",
    "        # Translate to Pidgin\n",
    "        pidgin = translate_text(text, \"Nigerian Pidgin\", model_name, strategy)\n",
    "        \n",
    "        # Translate to Yoruba\n",
    "        yoruba = translate_text(text, \"Yoruba\", model_name, strategy)\n",
    "        \n",
    "        data.append({\n",
    "            \"id\": i + 1,\n",
    "            \"english\": text,\n",
    "            \"pidgin\": pidgin,\n",
    "            \"yoruba\": yoruba,\n",
    "            \"model\": model_name,\n",
    "            \"strategy\": strategy,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English sentences for testing\n",
    "sample_sentences = [\n",
    "    \"Good morning! How are you doing today?\",\n",
    "    \"I am going to the market to buy some food.\",\n",
    "    \"Please, can you help me with this?\"\n",
    "]\n",
    "\n",
    "# Generate a test dataset (uncomment to run)\n",
    "# test_dataset = generate_synthetic_dataset(sample_sentences, \"Phi-4-Mini\", \"Few-Shot\")\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ed49d-7873-4a03-bdc8-bda1c23cb979",
   "metadata": {},
   "source": [
    "## Gradio UI for Synthetic Data Generator\n",
    "\n",
    "Create an interactive interface for:\n",
    "1. Single text translation\n",
    "2. Model and strategy selection\n",
    "3. Comparison across models\n",
    "4. Batch processing with export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d84b78-0a02-4a35-8f10-e3f7451d352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface functions\n",
    "\n",
    "def translate_single(english_text, target_language, model_choice, strategy):\n",
    "    \"\"\"Translate a single text\"\"\"\n",
    "    if not english_text.strip():\n",
    "        return \"Please enter some text to translate.\"\n",
    "    \n",
    "    result = translate_text(english_text, target_language, model_choice, strategy)\n",
    "    return result\n",
    "\n",
    "def compare_models(english_text, target_language, strategy):\n",
    "    \"\"\"Compare translations from different models\"\"\"\n",
    "    if not english_text.strip():\n",
    "        return \"Please enter some text to translate.\"\n",
    "    \n",
    "    output = f\"**English:** {english_text}\\n\\n\"\n",
    "    output += f\"**Target Language:** {target_language}\\n\"\n",
    "    output += f\"**Strategy:** {strategy}\\n\\n\"\n",
    "    output += \"---\\n\\n\"\n",
    "    \n",
    "    # Try different models (limit to 2-3 to avoid long wait times)\n",
    "    models_to_test = [\"Phi-4-Mini\", \"Gemma-270M\"]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        try:\n",
    "            translation = translate_text(english_text, target_language, model_name, strategy)\n",
    "            output += f\"**{model_name}:**\\n{translation}\\n\\n\"\n",
    "        except Exception as e:\n",
    "            output += f\"**{model_name}:** Error: {str(e)}\\n\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_translate(batch_text, model_choice, strategy, progress=gr.Progress()):\n",
    "    \"\"\"Translate multiple sentences (one per line)\"\"\"\n",
    "    if not batch_text.strip():\n",
    "        return None, \"Please enter text to translate (one sentence per line).\"\n",
    "    \n",
    "    sentences = [s.strip() for s in batch_text.split('\\n') if s.strip()]\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return None, \"No valid sentences found.\"\n",
    "    \n",
    "    # Progress callback\n",
    "    def progress_fn(msg):\n",
    "        progress(msg)\n",
    "    \n",
    "    df = generate_synthetic_dataset(sentences, model_choice, strategy, progress_fn)\n",
    "    \n",
    "    # Create summary\n",
    "    summary = f\"‚úÖ Generated {len(df)} translations\\n\\n\"\n",
    "    summary += f\"**Model:** {model_choice}\\n\"\n",
    "    summary += f\"**Strategy:** {strategy}\\n\\n\"\n",
    "    summary += \"You can download the full dataset as CSV below.\"\n",
    "    \n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e92304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio Interface\n",
    "\n",
    "with gr.Blocks(title=\"Synthetic Data Generator - Pidgin & Yoruba\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üåç Synthetic Data Generator for Nigerian Languages\n",
    "        \n",
    "        Generate synthetic translation data for **Nigerian Pidgin** and **Yoruba** from English text using HuggingFace transformers.\n",
    "        \n",
    "        ### Features:\n",
    "        - Multiple HuggingFace models (Llama, Phi, Gemma, Qwen)\n",
    "        - Different translation strategies (Direct, Contextual, Few-Shot, Cultural)\n",
    "        - Single or batch translation\n",
    "        - Export to CSV\n",
    "        \n",
    "        **Note:** First use of a model will take time to download and load. Subsequent uses will be faster.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Single Translation\n",
    "        with gr.Tab(\"üî§ Single Translation\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    single_input = gr.Textbox(\n",
    "                        label=\"English Text\",\n",
    "                        placeholder=\"Enter English text to translate...\",\n",
    "                        lines=5\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        single_lang = gr.Dropdown(\n",
    "                            choices=[\"Nigerian Pidgin\", \"Yoruba\"],\n",
    "                            label=\"Target Language\",\n",
    "                            value=\"Nigerian Pidgin\"\n",
    "                        )\n",
    "                        single_model = gr.Dropdown(\n",
    "                            choices=list(MODELS.keys()),\n",
    "                            label=\"Model\",\n",
    "                            value=\"Phi-4-Mini\"\n",
    "                        )\n",
    "                        single_strategy = gr.Dropdown(\n",
    "                            choices=[\"Direct\", \"Contextual\", \"Few-Shot\", \"Cultural\"],\n",
    "                            label=\"Strategy\",\n",
    "                            value=\"Few-Shot\"\n",
    "                        )\n",
    "                    translate_btn = gr.Button(\"Translate\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    single_output = gr.Textbox(\n",
    "                        label=\"Translation\",\n",
    "                        lines=5,\n",
    "                        interactive=False\n",
    "                    )\n",
    "            \n",
    "            translate_btn.click(\n",
    "                fn=translate_single,\n",
    "                inputs=[single_input, single_lang, single_model, single_strategy],\n",
    "                outputs=single_output\n",
    "            )\n",
    "            \n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"Good morning! How are you doing today?\", \"Nigerian Pidgin\", \"Phi-4-Mini\", \"Few-Shot\"],\n",
    "                    [\"I am going to the market to buy food.\", \"Yoruba\", \"Gemma-270M\", \"Contextual\"],\n",
    "                    [\"Please help me with this work.\", \"Nigerian Pidgin\", \"Phi-4-Mini\", \"Cultural\"],\n",
    "                ],\n",
    "                inputs=[single_input, single_lang, single_model, single_strategy],\n",
    "            )\n",
    "        \n",
    "        # Tab 2: Compare Models\n",
    "        with gr.Tab(\"üîÑ Compare Models\"):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                Compare translations from different HuggingFace models to see which produces better results.\n",
    "                \"\"\"\n",
    "            )\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    compare_input = gr.Textbox(\n",
    "                        label=\"English Text\",\n",
    "                        placeholder=\"Enter text to see translations from multiple models...\",\n",
    "                        lines=4\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        compare_lang = gr.Dropdown(\n",
    "                            choices=[\"Nigerian Pidgin\", \"Yoruba\"],\n",
    "                            label=\"Target Language\",\n",
    "                            value=\"Nigerian Pidgin\"\n",
    "                        )\n",
    "                        compare_strategy = gr.Dropdown(\n",
    "                            choices=[\"Direct\", \"Contextual\", \"Few-Shot\", \"Cultural\"],\n",
    "                            label=\"Strategy\",\n",
    "                            value=\"Few-Shot\"\n",
    "                        )\n",
    "                    compare_btn = gr.Button(\"Compare Models\", variant=\"primary\")\n",
    "            \n",
    "            compare_output = gr.Markdown(label=\"Comparison Results\")\n",
    "            \n",
    "            compare_btn.click(\n",
    "                fn=compare_models,\n",
    "                inputs=[compare_input, compare_lang, compare_strategy],\n",
    "                outputs=compare_output\n",
    "            )\n",
    "        \n",
    "        # Tab 3: Batch Generation\n",
    "        with gr.Tab(\"üìä Batch Generation\"):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ### Generate Synthetic Dataset\n",
    "                Enter multiple English sentences (one per line) to generate a complete dataset \n",
    "                with translations in both Pidgin and Yoruba.\n",
    "                \n",
    "                ‚ö†Ô∏è **Note:** Batch generation may take several minutes depending on the number of sentences.\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    batch_input = gr.Textbox(\n",
    "                        label=\"English Sentences (one per line)\",\n",
    "                        placeholder=\"Good morning!\\nHow are you?\\nI am fine.\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        batch_model = gr.Dropdown(\n",
    "                            choices=list(MODELS.keys()),\n",
    "                            label=\"Model\",\n",
    "                            value=\"Phi-4-Mini\"\n",
    "                        )\n",
    "                        batch_strategy = gr.Dropdown(\n",
    "                            choices=[\"Direct\", \"Contextual\", \"Few-Shot\", \"Cultural\"],\n",
    "                            label=\"Strategy\",\n",
    "                            value=\"Few-Shot\"\n",
    "                        )\n",
    "                    generate_btn = gr.Button(\"Generate Dataset\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    batch_summary = gr.Markdown(label=\"Summary\")\n",
    "            \n",
    "            batch_dataframe = gr.Dataframe(\n",
    "                label=\"Generated Dataset\",\n",
    "                wrap=True,\n",
    "                interactive=False\n",
    "            )\n",
    "            \n",
    "            # Store the dataframe state\n",
    "            df_state = gr.State()\n",
    "            \n",
    "            generate_btn.click(\n",
    "                fn=batch_translate,\n",
    "                inputs=[batch_input, batch_model, batch_strategy],\n",
    "                outputs=[df_state, batch_summary]\n",
    "            ).then(\n",
    "                fn=lambda x: x,\n",
    "                inputs=[df_state],\n",
    "                outputs=[batch_dataframe]\n",
    "            )\n",
    "            \n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"Good morning!\\nHow are you?\\nI am fine, thank you.\"],\n",
    "                    [\"The market is open.\\nI need to buy rice.\\nHow much does it cost?\"],\n",
    "                ],\n",
    "                inputs=[batch_input],\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ---\n",
    "        ### About Translation Strategies:\n",
    "        \n",
    "        - **Direct**: Simple, straightforward translation\n",
    "        - **Contextual**: Considers tone, emotion, and cultural context\n",
    "        - **Few-Shot**: Uses examples to improve translation accuracy\n",
    "        - **Cultural**: Adapts idioms and cultural references\n",
    "        \n",
    "        ### Supported Models:\n",
    "        \n",
    "        - **Llama-3.2-1B**: Meta's efficient small model (requires HuggingFace access approval)\n",
    "        - **Phi-4-Mini**: Microsoft's compact but powerful model\n",
    "        - **Gemma-270M**: Google's lightweight model\n",
    "        - **Qwen-4B**: Alibaba's multilingual model\n",
    "        \n",
    "        ### Tips:\n",
    "        - First model load will download weights (~1-4GB per model)\n",
    "        - Use GPU (Colab T4) for faster generation\n",
    "        - Few-Shot strategy typically gives best results\n",
    "        - Start with small batches to test before scaling up\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
