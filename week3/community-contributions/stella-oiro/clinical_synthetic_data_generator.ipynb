{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clinical Synthetic Data Generator\n",
        "### Week 3 Exercise — Stella Oiro (Andela AI Engineering Bootcamp)\n",
        "\n",
        "Generate realistic synthetic clinical datasets for research, education, and testing — no real patient data required.\n",
        "\n",
        "**Dataset types you can generate:**\n",
        "-  Patient Demographics & Diagnoses\n",
        "-  Prescription Records\n",
        "-  Clinical Trial Participants\n",
        "-  Adverse Drug Event (ADE) Reports\n",
        "-  Laboratory Results\n",
        "\n",
        "**Models supported:**\n",
        "-  HuggingFace: `meta-llama/Meta-Llama-3.1-8B-Instruct` (4-bit, T4 GPU)\n",
        "-  OpenAI: `gpt-4.1-mini` (via API key)\n",
        "\n",
        "> **Note:** This notebook is optimised for Google Colab with a T4 GPU.\n",
        "> For the HuggingFace model you need a `HF_TOKEN` saved in Colab Secrets.\n",
        "> For GPT you need an `OPENAI_API_KEY` saved in Colab Secrets."
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a1b2c3d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (Colab)\n",
        "%pip install -q transformers accelerate bitsandbytes torch gradio openai python-dotenv"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b2c3d4e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU availability\n",
        "import subprocess\n",
        "try:\n",
        "    gpu_info = subprocess.check_output([\"nvidia-smi\"], text=True)\n",
        "except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "    gpu_info = \"NVIDIA-SMI has failed\"\n",
        "if 'failed' in gpu_info.lower():\n",
        "    print('  No GPU detected — HuggingFace model will be slow. GPT mode recommended.')\n",
        "else:\n",
        "    print(gpu_info)\n",
        "    if 'T4' in gpu_info:\n",
        "        print(' Connected to T4 GPU — ready for HuggingFace model.')\n",
        "    else:\n",
        "        print(' GPU detected.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c3d4e5f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "# ── API keys ─────────────────────────────────────────────────────────────────\n",
        "# In Colab: use Secrets (key icon in left sidebar)\n",
        "# Locally:  use a .env file\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    HF_TOKEN       = userdata.get('HF_TOKEN')\n",
        "    IN_COLAB = True\n",
        "    print('Running in Google Colab')\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv(override=True)\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "    HF_TOKEN       = os.getenv('HF_TOKEN')\n",
        "    IN_COLAB = False\n",
        "    print('Running locally')\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    print(f'OpenAI key found: {OPENAI_API_KEY[:8]}...')\n",
        "else:\n",
        "    print('  No OpenAI key — GPT mode disabled.')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    print('HuggingFace token found.')\n",
        "else:\n",
        "    print('  No HF token — HuggingFace model disabled.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d4e5f6a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── HuggingFace login ─────────────────────────────────────────────────────────\n",
        "from huggingface_hub import login\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN, add_to_git_credential=True)\n",
        "    print('Logged in to HuggingFace.')\n",
        "else:\n",
        "    print('Skipping HuggingFace login (no token).')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e5f6a7b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Load HuggingFace model ────────────────────────────────────────────────────\n",
        "# Only loads if HF_TOKEN is present and GPU is available.\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "HF_MODEL_NAME = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "hf_model     = None\n",
        "hf_tokenizer = None\n",
        "\n",
        "if HF_TOKEN and torch.cuda.is_available():\n",
        "    print(f'Loading {HF_MODEL_NAME} with 4-bit quantisation...')\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    )\n",
        "    hf_tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
        "    hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "        HF_MODEL_NAME,\n",
        "        device_map='auto',\n",
        "        quantization_config=quant_config\n",
        "    )\n",
        "    print(' HuggingFace model ready.')\n",
        "else:\n",
        "    print('  HuggingFace model not loaded (no GPU or no HF token).')\n",
        "    print('    GPT mode will be used instead.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f6a7b8c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── OpenAI client ─────────────────────────────────────────────────────────────\n",
        "GPT_MODEL = 'gpt-4.1-mini'\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
        "print(f'GPT client ready: {bool(openai_client)}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a7b8c9d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Clinical dataset schemas ──────────────────────────────────────────────────\n",
        "# Each schema tells the LLM exactly what fields to generate.\n",
        "\n",
        "DATASET_SCHEMAS = {\n",
        "    \"Patient Demographics & Diagnoses\": {\n",
        "        \"description\": \"Synthetic patient records with demographics and primary diagnoses.\",\n",
        "        \"fields\": [\n",
        "            \"patient_id (e.g. PT-001)\",\n",
        "            \"age (18-90)\",\n",
        "            \"sex (Male/Female/Non-binary)\",\n",
        "            \"ethnicity\",\n",
        "            \"primary_diagnosis (ICD-10 name and code)\",\n",
        "            \"comorbidities (list of 1-3)\",\n",
        "            \"bmi\",\n",
        "            \"smoking_status (Never/Ex/Current)\",\n",
        "            \"date_of_admission (YYYY-MM-DD, 2022-2024)\"\n",
        "        ],\n",
        "        \"example\": {\n",
        "            \"patient_id\": \"PT-001\",\n",
        "            \"age\": 54,\n",
        "            \"sex\": \"Female\",\n",
        "            \"ethnicity\": \"Black British\",\n",
        "            \"primary_diagnosis\": \"Type 2 Diabetes Mellitus (E11)\",\n",
        "            \"comorbidities\": [\"Hypertension\", \"Obesity\"],\n",
        "            \"bmi\": 31.4,\n",
        "            \"smoking_status\": \"Ex\",\n",
        "            \"date_of_admission\": \"2023-07-14\"\n",
        "        }\n",
        "    },\n",
        "    \"Prescription Records\": {\n",
        "        \"description\": \"Synthetic prescriptions including drug, dose, and prescriber details.\",\n",
        "        \"fields\": [\n",
        "            \"prescription_id (e.g. RX-001)\",\n",
        "            \"patient_id\",\n",
        "            \"drug_name (generic)\",\n",
        "            \"drug_class\",\n",
        "            \"dose_mg\",\n",
        "            \"frequency (e.g. once daily, BD, TDS)\",\n",
        "            \"route (oral/IV/inhaled/topical)\",\n",
        "            \"indication\",\n",
        "            \"prescriber_role (GP/Consultant/Registrar/NP)\",\n",
        "            \"date_prescribed (YYYY-MM-DD)\",\n",
        "            \"duration_days\"\n",
        "        ],\n",
        "        \"example\": {\n",
        "            \"prescription_id\": \"RX-001\",\n",
        "            \"patient_id\": \"PT-001\",\n",
        "            \"drug_name\": \"metformin\",\n",
        "            \"drug_class\": \"Biguanide\",\n",
        "            \"dose_mg\": 500,\n",
        "            \"frequency\": \"twice daily\",\n",
        "            \"route\": \"oral\",\n",
        "            \"indication\": \"Type 2 Diabetes Mellitus\",\n",
        "            \"prescriber_role\": \"GP\",\n",
        "            \"date_prescribed\": \"2023-07-15\",\n",
        "            \"duration_days\": 90\n",
        "        }\n",
        "    },\n",
        "    \"Clinical Trial Participants\": {\n",
        "        \"description\": \"Synthetic clinical trial enrolment records with outcomes.\",\n",
        "        \"fields\": [\n",
        "            \"participant_id (e.g. CTP-001)\",\n",
        "            \"trial_id (e.g. TRIAL-2023-HF01)\",\n",
        "            \"trial_phase (I/II/III/IV)\",\n",
        "            \"intervention_arm (Drug/Placebo/Active comparator)\",\n",
        "            \"drug_name\",\n",
        "            \"dose_mg\",\n",
        "            \"age\",\n",
        "            \"sex\",\n",
        "            \"primary_endpoint_met (Yes/No)\",\n",
        "            \"serious_adverse_event (Yes/No)\",\n",
        "            \"withdrawal_reason (Completed/AE/Lost to follow-up/Withdrawn consent)\",\n",
        "            \"follow_up_weeks\"\n",
        "        ],\n",
        "        \"example\": {\n",
        "            \"participant_id\": \"CTP-001\",\n",
        "            \"trial_id\": \"TRIAL-2023-HF01\",\n",
        "            \"trial_phase\": \"III\",\n",
        "            \"intervention_arm\": \"Drug\",\n",
        "            \"drug_name\": \"empagliflozin\",\n",
        "            \"dose_mg\": 10,\n",
        "            \"age\": 67,\n",
        "            \"sex\": \"Male\",\n",
        "            \"primary_endpoint_met\": \"Yes\",\n",
        "            \"serious_adverse_event\": \"No\",\n",
        "            \"withdrawal_reason\": \"Completed\",\n",
        "            \"follow_up_weeks\": 52\n",
        "        }\n",
        "    },\n",
        "    \"Adverse Drug Event Reports\": {\n",
        "        \"description\": \"Synthetic pharmacovigilance reports modelled on WHO/CIOMS format.\",\n",
        "        \"fields\": [\n",
        "            \"report_id (e.g. ADE-001)\",\n",
        "            \"suspect_drug\",\n",
        "            \"indication\",\n",
        "            \"adverse_event (MedDRA preferred term)\",\n",
        "            \"severity (Mild/Moderate/Severe/Life-threatening)\",\n",
        "            \"seriousness (Serious/Non-serious)\",\n",
        "            \"causality (Certain/Probable/Possible/Unlikely)\",\n",
        "            \"patient_age\",\n",
        "            \"patient_sex\",\n",
        "            \"time_to_onset_days\",\n",
        "            \"outcome (Recovered/Recovering/Not recovered/Fatal/Unknown)\",\n",
        "            \"reporter_type (Physician/Pharmacist/Patient/Nurse)\"\n",
        "        ],\n",
        "        \"example\": {\n",
        "            \"report_id\": \"ADE-001\",\n",
        "            \"suspect_drug\": \"warfarin\",\n",
        "            \"indication\": \"Atrial fibrillation\",\n",
        "            \"adverse_event\": \"Gastrointestinal haemorrhage\",\n",
        "            \"severity\": \"Severe\",\n",
        "            \"seriousness\": \"Serious\",\n",
        "            \"causality\": \"Probable\",\n",
        "            \"patient_age\": 74,\n",
        "            \"patient_sex\": \"Female\",\n",
        "            \"time_to_onset_days\": 12,\n",
        "            \"outcome\": \"Recovered\",\n",
        "            \"reporter_type\": \"Physician\"\n",
        "        }\n",
        "    },\n",
        "    \"Laboratory Results\": {\n",
        "        \"description\": \"Synthetic lab panels including haematology and metabolic results.\",\n",
        "        \"fields\": [\n",
        "            \"lab_id (e.g. LAB-001)\",\n",
        "            \"patient_id\",\n",
        "            \"collection_date (YYYY-MM-DD)\",\n",
        "            \"haemoglobin_g_dL (normal 12-17)\",\n",
        "            \"wbc_10e9_L (normal 4-11)\",\n",
        "            \"platelets_10e9_L (normal 150-400)\",\n",
        "            \"sodium_mmol_L (normal 135-145)\",\n",
        "            \"potassium_mmol_L (normal 3.5-5)\",\n",
        "            \"creatinine_umol_L (normal 60-110)\",\n",
        "            \"egfr_mL_min_1.73m2\",\n",
        "            \"hba1c_mmol_mol\",\n",
        "            \"flag (Normal/Abnormal/Critical)\"\n",
        "        ],\n",
        "        \"example\": {\n",
        "            \"lab_id\": \"LAB-001\",\n",
        "            \"patient_id\": \"PT-001\",\n",
        "            \"collection_date\": \"2023-07-15\",\n",
        "            \"haemoglobin_g_dL\": 13.2,\n",
        "            \"wbc_10e9_L\": 6.8,\n",
        "            \"platelets_10e9_L\": 234,\n",
        "            \"sodium_mmol_L\": 138,\n",
        "            \"potassium_mmol_L\": 4.1,\n",
        "            \"creatinine_umol_L\": 89,\n",
        "            \"egfr_mL_min_1.73m2\": 72,\n",
        "            \"hba1c_mmol_mol\": 58,\n",
        "            \"flag\": \"Abnormal\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f'Schemas loaded for {len(DATASET_SCHEMAS)} dataset types.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b8c9d0e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Prompt builder ────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_prompt(dataset_type: str, num_records: int) -> tuple[str, str]:\n",
        "    \"\"\"Build system + user prompt for the chosen dataset type.\"\"\"\n",
        "    schema = DATASET_SCHEMAS[dataset_type]\n",
        "    fields_str = '\\n'.join(f'  - {f}' for f in schema['fields'])\n",
        "    example_str = json.dumps(schema['example'], indent=2)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a clinical data engineer generating realistic but entirely synthetic medical datasets.\\n\"\n",
        "        \"Your output MUST be valid JSON only — a JSON array of objects, no markdown, no commentary.\\n\"\n",
        "        \"Use realistic clinical values, proper medical terminology, and plausible variation between records.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"Generate exactly {num_records} synthetic {dataset_type} records.\\n\\n\"\n",
        "        f\"Description: {schema['description']}\\n\\n\"\n",
        "        f\"Required fields:\\n{fields_str}\\n\\n\"\n",
        "        f\"Example of ONE record (follow this exact structure):\\n{example_str}\\n\\n\"\n",
        "        f\"Return ONLY a JSON array of {num_records} objects. No extra text.\"\n",
        "    )\n",
        "\n",
        "    return system_prompt, user_prompt\n",
        "\n",
        "print('Prompt builder ready.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c9d0e1f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Generation functions ──────────────────────────────────────────────────────\n",
        "\n",
        "def generate_with_gpt(dataset_type: str, num_records: int) -> list:\n",
        "    \"\"\"Generate synthetic data using GPT-4.1-mini.\"\"\"\n",
        "    if not openai_client:\n",
        "        raise RuntimeError('OpenAI client not configured. Add OPENAI_API_KEY.')\n",
        "    system_prompt, user_prompt = build_prompt(dataset_type, num_records)\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            {'role': 'system',  'content': system_prompt},\n",
        "            {'role': 'user',    'content': user_prompt}\n",
        "        ],\n",
        "        response_format={'type': 'json_object'}\n",
        "    )\n",
        "    raw = response.choices[0].message.content\n",
        "    parsed = json.loads(raw)\n",
        "    # GPT json_object wraps arrays — unwrap if needed\n",
        "    if isinstance(parsed, dict):\n",
        "        for v in parsed.values():\n",
        "            if isinstance(v, list):\n",
        "                return v\n",
        "    return parsed\n",
        "\n",
        "\n",
        "def generate_with_hf(dataset_type: str, num_records: int) -> list:\n",
        "    \"\"\"Generate synthetic data using local HuggingFace model.\"\"\"\n",
        "    if hf_model is None or hf_tokenizer is None:\n",
        "        raise RuntimeError('HuggingFace model not loaded.')\n",
        "    system_prompt, user_prompt = build_prompt(dataset_type, num_records)\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_prompt},\n",
        "        {'role': 'user',   'content': user_prompt}\n",
        "    ]\n",
        "    input_ids = hf_tokenizer.apply_chat_template(\n",
        "        messages, return_tensors='pt', add_generation_prompt=True\n",
        "    ).to(hf_model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = hf_model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=min(3000, num_records * 250),\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=hf_tokenizer.eos_token_id\n",
        "        )\n",
        "    text = hf_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract JSON array from output\n",
        "    start = text.find('[')\n",
        "    end   = text.rfind(']') + 1\n",
        "    if start == -1 or end == 0:\n",
        "        raise ValueError('No JSON array found in model output.')\n",
        "    return json.loads(text[start:end])\n",
        "\n",
        "\n",
        "def generate_records(dataset_type: str, num_records: int, model_choice: str) -> list:\n",
        "    \"\"\"Route to the correct generation function.\"\"\"\n",
        "    if model_choice.startswith('GPT'):\n",
        "        return generate_with_gpt(dataset_type, num_records)\n",
        "    else:\n",
        "        return generate_with_hf(dataset_type, num_records)\n",
        "\n",
        "print('Generation functions ready.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d0e1f2a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Export helpers ────────────────────────────────────────────────────────────\n",
        "\n",
        "def records_to_csv(records: list, dataset_type: str) -> str:\n",
        "    \"\"\"Save records to CSV and return the file path.\"\"\"\n",
        "    filename = f\"{dataset_type.lower().replace(' ', '_').replace('&', 'and')}.csv\"\n",
        "    pd.DataFrame(records).to_csv(filename, index=False)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def records_to_json(records: list, dataset_type: str) -> str:\n",
        "    \"\"\"Save records to JSON and return the file path.\"\"\"\n",
        "    filename = f\"{dataset_type.lower().replace(' ', '_').replace('&', 'and')}.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(records, f, indent=2)\n",
        "    return filename\n",
        "\n",
        "\n",
        "print('Export helpers ready.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e1f2a3b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Gradio UI ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "MODEL_CHOICES = []\n",
        "if openai_client:\n",
        "    MODEL_CHOICES.append(f'GPT ({GPT_MODEL})')\n",
        "if hf_model:\n",
        "    MODEL_CHOICES.append('HuggingFace (Llama-3.1-8B)')\n",
        "if not MODEL_CHOICES:\n",
        "    MODEL_CHOICES = ['GPT (gpt-4.1-mini) — needs API key']\n",
        "\n",
        "\n",
        "def gradio_generate(dataset_type, num_records, model_choice):\n",
        "    \"\"\"Called by Gradio when the user clicks Generate.\"\"\"\n",
        "    try:\n",
        "        records = generate_records(dataset_type, int(num_records), model_choice)\n",
        "        df = pd.DataFrame(records)\n",
        "        csv_path  = records_to_csv(records,  dataset_type)\n",
        "        json_path = records_to_json(records, dataset_type)\n",
        "        status = f' Generated {len(records)} {dataset_type} records using {model_choice}.'\n",
        "        return status, df, csv_path, json_path\n",
        "    except Exception as e:\n",
        "        return f' Error: {e}', None, None, None\n",
        "\n",
        "\n",
        "with gr.Blocks(title='Clinical Synthetic Data Generator', theme=gr.themes.Soft()) as ui:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    #  Clinical Synthetic Data Generator\n",
        "    Generate realistic synthetic clinical datasets for research, education, and testing.\n",
        "    *All data is entirely synthetic — no real patient information is used or produced.*\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            dataset_dd = gr.Dropdown(\n",
        "                choices=list(DATASET_SCHEMAS.keys()),\n",
        "                value=list(DATASET_SCHEMAS.keys())[0],\n",
        "                label='Dataset Type'\n",
        "            )\n",
        "            num_slider = gr.Slider(\n",
        "                minimum=5, maximum=20, value=10, step=5,\n",
        "                label='Number of Records'\n",
        "            )\n",
        "            model_dd = gr.Dropdown(\n",
        "                choices=MODEL_CHOICES,\n",
        "                value=MODEL_CHOICES[0],\n",
        "                label='Model'\n",
        "            )\n",
        "            generate_btn = gr.Button('Generate Dataset', variant='primary')\n",
        "\n",
        "            gr.Markdown('### Download')\n",
        "            csv_file  = gr.File(label='CSV')\n",
        "            json_file = gr.File(label='JSON')\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            status_box = gr.Textbox(label='Status', interactive=False)\n",
        "            data_table = gr.Dataframe(label='Preview', wrap=True)\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            ['Patient Demographics & Diagnoses',  10, MODEL_CHOICES[0]],\n",
        "            ['Adverse Drug Event Reports',        10, MODEL_CHOICES[0]],\n",
        "            ['Laboratory Results',                 5, MODEL_CHOICES[0]],\n",
        "        ],\n",
        "        inputs=[dataset_dd, num_slider, model_dd]\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=gradio_generate,\n",
        "        inputs=[dataset_dd, num_slider, model_dd],\n",
        "        outputs=[status_box, data_table, csv_file, json_file]\n",
        "    )\n",
        "\n",
        "print('UI built.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f2a3b4c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ui.launch(share=True, debug=True)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a3b4c5d6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}