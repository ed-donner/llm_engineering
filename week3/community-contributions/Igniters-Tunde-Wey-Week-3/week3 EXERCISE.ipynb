{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24a20e6d",
      "metadata": {},
      "source": [
        "# Week 3: AI Tutor with Synthetic Data Generator\n",
        "\n",
        "Uses Hugging Face Hub (pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig) to generate synthetic teaching scenarios. The tutor weaves these into responses via the system prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d37a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes gradio python-dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc86576",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Environment: Colab Secrets vs local .env\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "    load_dotenv(override=True)\n",
        "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    from huggingface_hub import login\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Hugging Face login OK\")\n",
        "else:\n",
        "    print(\"Set HF_TOKEN in Colab Secrets or .env to use the synthetic generator.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01765830",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic generator: small model + 4-bit quant for Colab/local\n",
        "SYNTHETIC_GEN_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "MAX_SYNTHETIC_TOKENS = 256\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "synthetic_tokenizer = AutoTokenizer.from_pretrained(SYNTHETIC_GEN_MODEL_ID)\n",
        "if synthetic_tokenizer.pad_token is None:\n",
        "    synthetic_tokenizer.pad_token = synthetic_tokenizer.eos_token\n",
        "\n",
        "synthetic_model = AutoModelForCausalLM.from_pretrained(\n",
        "    SYNTHETIC_GEN_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "synthetic_text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=synthetic_model,\n",
        "    tokenizer=synthetic_tokenizer,\n",
        "    max_new_tokens=MAX_SYNTHETIC_TOKENS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241ab9b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_examples(topic: str) -> str:\n",
        "    \"\"\"Generate 1â€“2 short teaching scenarios or code examples for the given topic.\"\"\"\n",
        "    scenario_prompt = (\n",
        "        f\"Generate two very short teaching scenarios or concrete code examples \"\n",
        "        f\"for explaining: {topic}. Each in 1-2 sentences. No preamble.\"\n",
        "    )\n",
        "    pipe_output = synthetic_text_pipeline(scenario_prompt, do_sample=True, temperature=0.7)\n",
        "    generated_text = pipe_output[0][\"generated_text\"] if pipe_output else \"\"\n",
        "    if not generated_text:\n",
        "        return \"\"\n",
        "    # Strip the prompt from the model output and cap length\n",
        "    return generated_text.replace(scenario_prompt, \"\").strip()[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e414b39c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat backends: OpenRouter (Colab or local) and Ollama (local only)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "    openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "    load_dotenv(override=True)\n",
        "    openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "\n",
        "openrouter_client = OpenAI(api_key=openrouter_api_key, base_url=OPENROUTER_BASE_URL) if openrouter_api_key else None\n",
        "ollama_client = OpenAI(api_key=\"ollama\", base_url=OLLAMA_BASE_URL)\n",
        "\n",
        "# (display_label, model_id, backend_name)\n",
        "AVAILABLE_MODELS = [\n",
        "    (\"GPT-4o-mini (OpenRouter)\", \"openai/gpt-4o-mini\", \"openrouter\"),\n",
        "]\n",
        "if not IN_COLAB:\n",
        "    AVAILABLE_MODELS.append((\"Llama 3.2 (Ollama)\", \"llama3.2\", \"ollama\"))\n",
        "\n",
        "TUTOR_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a professional AI coding tutor. Give clear, step-by-step explanations with code examples. Use the following synthetic teaching scenarios or examples to enrich your answer when relevant. Weave them into your explanation.\n",
        "\n",
        "Synthetic scenarios/examples to use when helpful:\n",
        "{synthetic}\n",
        "\n",
        "Keep a friendly, expert tone. Respond in markdown (e.g. code blocks).\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659d8601",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_client_for_backend(backend: str):\n",
        "    if backend == \"openrouter\":\n",
        "        return openrouter_client\n",
        "    return ollama_client\n",
        "\n",
        "\n",
        "def stream_reply(conversation_history, user_message: str, selected_model_label: str, inject_synthetic: bool):\n",
        "    \"\"\"Stream chat completion; yields content chunks. conversation_history is list of (user, assistant) tuples.\"\"\"\n",
        "    label_to_model_and_backend = {\n",
        "        label: (mid, backend) for label, mid, backend in AVAILABLE_MODELS\n",
        "    }\n",
        "    selected_model_id, backend_name = label_to_model_and_backend.get(\n",
        "        selected_model_label, AVAILABLE_MODELS[0][1:]\n",
        "    )\n",
        "    llm_client = _get_client_for_backend(backend_name)\n",
        "\n",
        "    if llm_client is None:\n",
        "        if backend_name == \"openrouter\":\n",
        "            error_msg = (\n",
        "                \"**OpenRouter** key not found. In Colab: open the **Secrets** panel (key icon in left sidebar). \"\n",
        "                \"Locally: add `OPENROUTER_API_KEY` to .env.\"\n",
        "            )\n",
        "        else:\n",
        "            error_msg = \"Ollama is for local runs only. In Colab use OpenRouter.\"\n",
        "        yield error_msg\n",
        "        return\n",
        "\n",
        "    synthetic_scenarios = generate_synthetic_examples(user_message[:200]) if inject_synthetic else \"(none)\"\n",
        "    system_message_content = TUTOR_SYSTEM_PROMPT_TEMPLATE.format(synthetic=synthetic_scenarios)\n",
        "    chat_messages = [{\"role\": \"system\", \"content\": system_message_content}]\n",
        "    for prev_user, prev_assistant in conversation_history:\n",
        "        chat_messages.append({\"role\": \"user\", \"content\": prev_user})\n",
        "        chat_messages.append({\"role\": \"assistant\", \"content\": prev_assistant or \"\"})\n",
        "    chat_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    response_stream = llm_client.chat.completions.create(\n",
        "        model=selected_model_id, messages=chat_messages, stream=True\n",
        "    )\n",
        "    for stream_chunk in response_stream:\n",
        "        content_chunk = stream_chunk.choices[0].delta.content or \"\"\n",
        "        if content_chunk:\n",
        "            yield content_chunk\n",
        "\n",
        "\n",
        "def chat(user_input: str, conversation_history, selected_model_label: str, inject_synthetic: bool):\n",
        "    \"\"\"Gradio chat fn: streams reply by yielding accumulated content.\"\"\"\n",
        "    if not user_input or not user_input.strip():\n",
        "        return\n",
        "    accumulated_response = \"\"\n",
        "    for content_chunk in stream_reply(\n",
        "        conversation_history, user_input, selected_model_label, inject_synthetic\n",
        "    ):\n",
        "        accumulated_response += content_chunk\n",
        "        yield accumulated_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca221b96",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_selector = gr.Dropdown(\n",
        "    choices=[label for label, _, _ in AVAILABLE_MODELS],\n",
        "    value=AVAILABLE_MODELS[0][0],\n",
        "    label=\"Model\",\n",
        ")\n",
        "inject_synthetic_checkbox = gr.Checkbox(value=True, label=\"Inject synthetic teaching scenarios\")\n",
        "\n",
        "tutor_demo = gr.ChatInterface(\n",
        "    chat,\n",
        "    additional_inputs=[model_selector, inject_synthetic_checkbox],\n",
        "    title=\"Technical Q&A Tutor + Synthetic Data\",\n",
        "    description=\"Ask a coding question. Toggle to inject HuggingFace-generated scenarios into the answer.\",\n",
        ")\n",
        "tutor_demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
