{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41693bba",
      "metadata": {},
      "source": [
        "# Week 3 Exercise — Synthetic Buggy Code Factory\n",
        "\n",
        "**Building on the Bug Review Panel (Week 1) and the Bug Exterminator (Week 2)**, this exercise closes the loop by generating the very data those tools consume: realistic buggy Python code samples, produced entirely through LLM prompting.\n",
        "\n",
        "**Pipeline:** Generate buggy code (Week 3) → Analyze it with personas (Week 1) → Interactively debug it (Week 2)\n",
        "\n",
        "### What This Does\n",
        "\n",
        "- Uses the **HuggingFace Inference API** to call multiple models (Qwen Coder, Llama, Mistral) for diverse output\n",
        "- **Two-phase generation:** first expand a pool of algorithm descriptions, then infest each with realistic bugs\n",
        "- Covers **easy / medium / hard** difficulty levels and multiple Python error types (SyntaxError, NameError, IndentationError, TypeError, IndexError, LogicError)\n",
        "- Outputs structured JSON datasets ready for downstream training or evaluation\n",
        "- Full **Gradio UI** for configuring models, bug types, sample counts, previewing results, and downloading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2dea6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you get \"No module named pip\", run: python -m ensurepip --upgrade\n",
        "%pip install -q huggingface_hub gradio python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d693afc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tempfile\n",
        "\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc96cb65",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise EnvironmentError(\"HF_TOKEN not found in .env — add your HuggingFace token there.\")\n",
        "\n",
        "def get_client(model: str) -> InferenceClient:\n",
        "    return InferenceClient(model=model, token=hf_token, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef39481",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS = {\n",
        "    \"Qwen Coder 2.5 7B\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    \"Llama 3.1 8B\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"Mistral 7B v0.3\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "}\n",
        "\n",
        "LEVELS = [\"easy\", \"medium\", \"hard\"]\n",
        "\n",
        "BUG_TYPES = [\n",
        "    \"IndentationError\",\n",
        "    \"SyntaxError\",\n",
        "    \"TypeError\",\n",
        "    \"NameError\",\n",
        "    \"IndexError\",\n",
        "    \"LogicError\",\n",
        "]\n",
        "\n",
        "SEED_DESCRIPTIONS = {\n",
        "    \"easy\": [\n",
        "        \"counts the number of vowels in a string\",\n",
        "        \"counts the number of words in a string\",\n",
        "        \"counts the number of characters in a string\",\n",
        "        \"counts the number of sentences in a string\",\n",
        "        \"counts the number of paragraphs in a string\",\n",
        "        \"finds the maximum value in an array of numbers\",\n",
        "        \"finds the minimum value in an array\",\n",
        "        \"reverses a string\",\n",
        "        \"checks if a string is a palindrome\",\n",
        "        \"calculates the sum of all elements in a list\",\n",
        "    ],\n",
        "    \"medium\": [\n",
        "        \"sorts an array of numbers using bubble sort algorithm\",\n",
        "        \"sorts an array using merge sort algorithm\",\n",
        "        \"sorts an array using quick sort algorithm\",\n",
        "        \"searches for a target value in an array using linear search algorithm\",\n",
        "        \"searches for a target value in a sorted array using binary search algorithm\",\n",
        "        \"searches for a target value in an array using interpolation search algorithm\",\n",
        "        \"implements a stack data structure with push, pop and peek\",\n",
        "        \"implements a queue data structure with enqueue and dequeue\",\n",
        "        \"calculates the nth Fibonacci number recursively\",\n",
        "        \"removes duplicates from a sorted linked list\",\n",
        "    ],\n",
        "    \"hard\": [\n",
        "        \"calculates the factorial of a number\",\n",
        "        \"checks if a number is prime\",\n",
        "        \"generates all possible permutations of a string\",\n",
        "        \"finds the longest common subsequence of two strings\",\n",
        "        \"solves the traveling salesman problem using a genetic algorithm\",\n",
        "        \"implements Dijkstra's shortest path algorithm\",\n",
        "        \"solves the knapsack problem using dynamic programming\",\n",
        "        \"builds a trie data structure for prefix searching\",\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceed5bd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "DESCRIPTION_PROMPT = \"\"\"\\\n",
        "You are a Python programming curriculum designer.\n",
        "\n",
        "Generate exactly {count} unique, short descriptions of Python functions at the **{level}** difficulty level.\n",
        "Each description must start with a lowercase verb and be one sentence (e.g., \"sorts a list using insertion sort\").\n",
        "\n",
        "Difficulty guidelines:\n",
        "- easy: simple loops, string operations, basic math, list traversals\n",
        "- medium: classic sorting/searching algorithms, recursion, basic data structures\n",
        "- hard: dynamic programming, graph algorithms, complex recursion, advanced data structures\n",
        "\n",
        "Here are some existing descriptions to avoid duplicating:\n",
        "{existing}\n",
        "\n",
        "Respond with ONLY a JSON array of strings — no markdown fences, no explanation.\n",
        "Example: [\"reverses a linked list\", \"finds the mode of a list of numbers\"]\n",
        "\"\"\"\n",
        "\n",
        "BUGGY_CODE_PROMPT = \"\"\"\\\n",
        "You are a Python instructor creating buggy code samples for students to debug.\n",
        "\n",
        "Write a single Python function that {description}.\n",
        "Then inject exactly {num_bugs} bug(s) into it — choose from these error types: {bug_types}.\n",
        "\n",
        "Rules:\n",
        "- The function should be 5-25 lines and look like a real programmer's mistake.\n",
        "- Use realistic variable names; occasional typos in names count as NameError.\n",
        "- Do NOT add any comments explaining or hinting at the bugs.\n",
        "- Missing colons, wrong indentation, off-by-one errors, misspelled keywords — all fair game.\n",
        "- The code must be broken enough to fail but not so garbled it's unreadable.\n",
        "\n",
        "Respond with ONLY a valid JSON object — no markdown fences, no extra text:\n",
        "{{\n",
        "  \"level\": \"{level}\",\n",
        "  \"description\": \"{description}\",\n",
        "  \"buggy_code\": \"<the buggy code as a single string with \\\\n for newlines>\",\n",
        "  \"bug_types\": [\"<actual error types you injected>\"],\n",
        "  \"num_bugs\": {num_bugs}\n",
        "}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c579d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_llm_json(text: str):\n",
        "    \"\"\"Extract and parse JSON from LLM output, handling markdown fences and stray text.\"\"\"\n",
        "    text = text.strip()\n",
        "    fence = re.search(r\"```(?:json)?\\s*\\n?([\\s\\S]*?)```\", text)\n",
        "    if fence:\n",
        "        text = fence.group(1).strip()\n",
        "    for start_char, end_char in [(\"{\", \"}\"), (\"[\", \"]\")]:\n",
        "        first = text.find(start_char)\n",
        "        last = text.rfind(end_char)\n",
        "        if first != -1 and last != -1 and last > first:\n",
        "            candidate = text[first : last + 1]\n",
        "            try:\n",
        "                return json.loads(candidate)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return json.loads(text)\n",
        "\n",
        "\n",
        "_verbose = False\n",
        "_debug_log: list[str] = []\n",
        "\n",
        "\n",
        "def _log(msg: str):\n",
        "    if _verbose:\n",
        "        _debug_log.append(msg)\n",
        "\n",
        "\n",
        "def call_llm(client: InferenceClient, prompt: str, max_tokens: int = 1024) -> str:\n",
        "    \"\"\"Single LLM call with retry — backs off longer on timeouts (cold-start).\"\"\"\n",
        "    _log(f\"── PROMPT ({len(prompt)} chars) ──\\n{prompt[:300]}{'...' if len(prompt) > 300 else ''}\\n\")\n",
        "    for attempt in range(4):\n",
        "        try:\n",
        "            response = client.chat_completion(\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.8,\n",
        "            )\n",
        "            result = response.choices[0].message.content\n",
        "            _log(f\"── RESPONSE ({len(result)} chars) ──\\n{result[:400]}{'...' if len(result) > 400 else ''}\\n\")\n",
        "            return result\n",
        "        except Exception as exc:\n",
        "            is_timeout = \"timeout\" in str(exc).lower() or \"timed out\" in str(exc).lower()\n",
        "            if attempt == 3:\n",
        "                raise\n",
        "            wait = (10 if is_timeout else 2) * (2 ** attempt)\n",
        "            _log(f\"  RETRY {attempt+1} — {str(exc)[:120]}... waiting {wait}s\")\n",
        "            time.sleep(wait)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def generate_descriptions(client: InferenceClient, level: str, count: int, existing: list[str]) -> list[str]:\n",
        "    \"\"\"Use the LLM to generate new algorithm descriptions for a given level.\"\"\"\n",
        "    prompt = DESCRIPTION_PROMPT.format(\n",
        "        count=count,\n",
        "        level=level,\n",
        "        existing=json.dumps(existing[:10]),\n",
        "    )\n",
        "    raw = call_llm(client, prompt)\n",
        "    try:\n",
        "        descriptions = parse_llm_json(raw)\n",
        "        if isinstance(descriptions, list):\n",
        "            result = [d for d in descriptions if isinstance(d, str) and d not in existing]\n",
        "            _log(f\"  PARSED {len(result)} new descriptions\")\n",
        "            return result\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        _log(f\"  PARSE FAILED (descriptions): {e}\")\n",
        "    return []\n",
        "\n",
        "\n",
        "def generate_buggy_sample(\n",
        "    client: InferenceClient,\n",
        "    model_id: str,\n",
        "    level: str,\n",
        "    description: str,\n",
        "    bug_types: list[str],\n",
        "    num_bugs: int = 2,\n",
        ") -> dict | None:\n",
        "    \"\"\"Generate a single buggy code sample for the given description.\"\"\"\n",
        "    chosen_bugs = random.sample(bug_types, min(num_bugs, len(bug_types)))\n",
        "    _log(f\"  BUGS CHOSEN: {chosen_bugs}\")\n",
        "    prompt = BUGGY_CODE_PROMPT.format(\n",
        "        description=description,\n",
        "        num_bugs=num_bugs,\n",
        "        bug_types=\", \".join(chosen_bugs),\n",
        "        level=level,\n",
        "    )\n",
        "    raw = call_llm(client, prompt)\n",
        "    try:\n",
        "        sample = parse_llm_json(raw)\n",
        "        if isinstance(sample, dict) and \"buggy_code\" in sample:\n",
        "            sample[\"model\"] = model_id\n",
        "            sample.setdefault(\"level\", level)\n",
        "            sample.setdefault(\"description\", description)\n",
        "            _log(f\"  PARSED OK — {sample.get('num_bugs', '?')} bug(s), {len(sample.get('buggy_code', ''))} chars\")\n",
        "            return sample\n",
        "        _log(f\"  PARSE WARNING — JSON valid but missing 'buggy_code' key\")\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        _log(f\"  PARSE FAILED (sample): {e}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8affda",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(\n",
        "    model_id: str,\n",
        "    bug_types: list[str],\n",
        "    counts: dict[str, int],\n",
        "    expand_descriptions: bool,\n",
        "    progress_cb=None,\n",
        ") -> tuple[list[dict], str]:\n",
        "    \"\"\"\n",
        "    Orchestrate the full generation pipeline.\n",
        "    Returns (samples_list, status_log).\n",
        "    progress_cb(fraction, description) is called to update a progress bar.\n",
        "    \"\"\"\n",
        "    client = get_client(model_id)\n",
        "    samples = []\n",
        "    log_lines = []\n",
        "    total = sum(counts.values())\n",
        "    done = 0\n",
        "\n",
        "    def report(msg):\n",
        "        log_lines.append(msg)\n",
        "        frac = done / max(total, 1)\n",
        "        if progress_cb:\n",
        "            progress_cb(frac, msg)\n",
        "\n",
        "    for level in LEVELS:\n",
        "        target = counts.get(level, 0)\n",
        "        if target == 0:\n",
        "            continue\n",
        "\n",
        "        pool = list(SEED_DESCRIPTIONS.get(level, []))\n",
        "\n",
        "        if expand_descriptions and target > len(pool):\n",
        "            needed = target - len(pool)\n",
        "            report(f\"Expanding {level} descriptions (+{needed})...\")\n",
        "            extras = generate_descriptions(client, level, needed, pool)\n",
        "            pool.extend(extras)\n",
        "            report(f\"  Got {len(extras)} new descriptions\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        random.shuffle(pool)\n",
        "        descs_to_use = pool[:target]\n",
        "\n",
        "        if len(descs_to_use) < target:\n",
        "            descs_to_use = descs_to_use * ((target // len(descs_to_use)) + 1)\n",
        "            descs_to_use = descs_to_use[:target]\n",
        "\n",
        "        for i, desc in enumerate(descs_to_use):\n",
        "            report(f\"[{level}] {i+1}/{target}: {desc[:60]}...\")\n",
        "            num_bugs = random.randint(1, 3)\n",
        "            sample = generate_buggy_sample(client, model_id, level, desc, bug_types, num_bugs)\n",
        "            if sample:\n",
        "                samples.append(sample)\n",
        "                report(f\"  OK — {sample.get('num_bugs', '?')} bug(s)\")\n",
        "            else:\n",
        "                report(f\"  SKIP — failed to parse LLM output\")\n",
        "            done += 1\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    stats = {}\n",
        "    for s in samples:\n",
        "        lvl = s.get(\"level\", \"?\")\n",
        "        stats[lvl] = stats.get(lvl, 0) + 1\n",
        "    summary = f\"Done: {len(samples)}/{total} samples — \" + \", \".join(f\"{k}: {v}\" for k, v in stats.items())\n",
        "    log_lines.append(summary)\n",
        "\n",
        "    return samples, \"\\n\".join(log_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee19976b",
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_samples: list[dict] = []\n",
        "\n",
        "DETAIL_PLACEHOLDER = \"*Click a row in the table above to inspect the full sample.*\"\n",
        "\n",
        "\n",
        "def format_sample_markdown(sample: dict) -> str:\n",
        "    level = sample.get(\"level\", \"unknown\")\n",
        "    desc = sample.get(\"description\", \"\")\n",
        "    code = sample.get(\"buggy_code\", \"\")\n",
        "    bugs = sample.get(\"bug_types\", [])\n",
        "    if isinstance(bugs, list):\n",
        "        bugs = \", \".join(bugs)\n",
        "    num = sample.get(\"num_bugs\", \"?\")\n",
        "    model = sample.get(\"model\", \"\")\n",
        "\n",
        "    return (\n",
        "        f\"### {level.upper()} — {desc}\\n\\n\"\n",
        "        f\"**Bug types:** {bugs} &nbsp;|&nbsp; **Count:** {num} &nbsp;|&nbsp; **Model:** `{model}`\\n\\n\"\n",
        "        f\"```python\\n{code}\\n```\"\n",
        "    )\n",
        "\n",
        "\n",
        "def on_generate(model_name, bug_types, easy_n, med_n, hard_n, expand, verbose, progress=gr.Progress()):\n",
        "    global generated_samples, _verbose, _debug_log\n",
        "    _verbose = verbose\n",
        "    _debug_log = []\n",
        "\n",
        "    if not bug_types:\n",
        "        gr.Warning(\"Select at least one bug type.\")\n",
        "        return gr.update(), None, gr.update(interactive=True), DETAIL_PLACEHOLDER, \"\"\n",
        "\n",
        "    model_id = MODELS[model_name]\n",
        "    counts = {\"easy\": int(easy_n), \"medium\": int(med_n), \"hard\": int(hard_n)}\n",
        "    _log(f\"CONFIG — model={model_id}, counts={counts}, expand={expand}, bugs={bug_types}\")\n",
        "\n",
        "    def progress_cb(frac, msg):\n",
        "        progress(frac, desc=msg)\n",
        "\n",
        "    samples, _ = generate_dataset(model_id, bug_types, counts, expand, progress_cb)\n",
        "    generated_samples = samples\n",
        "\n",
        "    _log(f\"\\nFINISHED — {len(samples)} samples generated\")\n",
        "    log_text = \"\\n\".join(_debug_log) if _verbose else \"\"\n",
        "\n",
        "    if not samples:\n",
        "        return gr.update(value=None), None, gr.update(interactive=True), DETAIL_PLACEHOLDER, log_text\n",
        "\n",
        "    rows = []\n",
        "    for s in samples:\n",
        "        code_preview = s.get(\"buggy_code\", \"\")[:80].replace(\"\\n\", \"↵\") + \"...\"\n",
        "        bug_types_str = \", \".join(s.get(\"bug_types\", [])) if isinstance(s.get(\"bug_types\"), list) else str(s.get(\"bug_types\", \"\"))\n",
        "        rows.append([\n",
        "            s.get(\"level\", \"\"),\n",
        "            s.get(\"description\", \"\")[:60],\n",
        "            code_preview,\n",
        "            bug_types_str,\n",
        "            str(s.get(\"num_bugs\", \"?\")),\n",
        "            s.get(\"model\", \"\").split(\"/\")[-1],\n",
        "        ])\n",
        "\n",
        "    tmp = tempfile.NamedTemporaryFile(\n",
        "        mode=\"w\", suffix=\".json\", delete=False, prefix=\"buggy_dataset_\"\n",
        "    )\n",
        "    json.dump(samples, tmp, indent=2)\n",
        "    tmp.close()\n",
        "\n",
        "    first_detail = format_sample_markdown(samples[0])\n",
        "\n",
        "    return (\n",
        "        gr.update(value=rows),\n",
        "        tmp.name,\n",
        "        gr.update(interactive=True),\n",
        "        first_detail,\n",
        "        log_text,\n",
        "    )\n",
        "\n",
        "\n",
        "_selected_idx = -1\n",
        "\n",
        "VALIDATE_PROMPT = \"\"\"\\\n",
        "Analyze the following Python code. Identify ALL errors — syntax errors, \\\n",
        "logic errors, naming issues, indentation problems, or anything else wrong.\n",
        "\n",
        "For each error found, state:\n",
        "1. The line or area where it occurs\n",
        "2. What type of error it is\n",
        "3. A brief explanation\n",
        "\n",
        "Then give a corrected version of the full code.\n",
        "\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "VALIDATE_PLACEHOLDER = \"*Select a row and click **Validate** to independently check the code for errors.*\"\n",
        "\n",
        "\n",
        "def on_row_select(evt: gr.SelectData):\n",
        "    global _selected_idx\n",
        "    idx = evt.index[0] if isinstance(evt.index, (list, tuple)) else evt.index\n",
        "    _selected_idx = idx\n",
        "    if 0 <= idx < len(generated_samples):\n",
        "        return format_sample_markdown(generated_samples[idx]), VALIDATE_PLACEHOLDER\n",
        "    return DETAIL_PLACEHOLDER, VALIDATE_PLACEHOLDER\n",
        "\n",
        "\n",
        "def on_validate(model_name):\n",
        "    if _selected_idx < 0 or _selected_idx >= len(generated_samples):\n",
        "        gr.Warning(\"Select a row first.\")\n",
        "        return VALIDATE_PLACEHOLDER\n",
        "    sample = generated_samples[_selected_idx]\n",
        "    code = sample.get(\"buggy_code\", \"\")\n",
        "    if not code.strip():\n",
        "        return \"No code to validate.\"\n",
        "\n",
        "    model_id = MODELS[model_name]\n",
        "    client = get_client(model_id)\n",
        "    prompt = VALIDATE_PROMPT.format(code=code)\n",
        "    try:\n",
        "        result = call_llm(client, prompt, max_tokens=1500)\n",
        "        return f\"### Independent Validation\\n\\n{result}\"\n",
        "    except Exception as e:\n",
        "        return f\"**Validation failed:** {e}\"\n",
        "\n",
        "\n",
        "def on_download_jsonl():\n",
        "    global generated_samples\n",
        "    if not generated_samples:\n",
        "        gr.Warning(\"Generate a dataset first.\")\n",
        "        return None\n",
        "    tmp = tempfile.NamedTemporaryFile(\n",
        "        mode=\"w\", suffix=\".jsonl\", delete=False, prefix=\"buggy_dataset_\"\n",
        "    )\n",
        "    for s in generated_samples:\n",
        "        tmp.write(json.dumps(s) + \"\\n\")\n",
        "    tmp.close()\n",
        "    return tmp.name\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Buggy Code Factory\") as app:\n",
        "    gr.Markdown(\n",
        "        \"## Synthetic Buggy Code Factory\\n\"\n",
        "        \"Configure the generation parameters below, pick a model, select bug types, \"\n",
        "        \"set how many samples per difficulty level, and hit **Generate**.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            model_dd = gr.Dropdown(\n",
        "                choices=list(MODELS.keys()),\n",
        "                value=list(MODELS.keys())[0],\n",
        "                label=\"Model\",\n",
        "            )\n",
        "            bug_cb = gr.CheckboxGroup(\n",
        "                choices=BUG_TYPES,\n",
        "                value=BUG_TYPES[:3],\n",
        "                label=\"Bug Types to Inject\",\n",
        "            )\n",
        "            expand_ck = gr.Checkbox(\n",
        "                value=True,\n",
        "                label=\"Expand description pool via LLM\",\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            easy_sl = gr.Slider(0, 20, value=3, step=1, label=\"Easy samples\")\n",
        "            med_sl = gr.Slider(0, 20, value=3, step=1, label=\"Medium samples\")\n",
        "            hard_sl = gr.Slider(0, 20, value=2, step=1, label=\"Hard samples\")\n",
        "            verbose_ck = gr.Checkbox(value=False, label=\"Show debug logs (prompts & responses)\")\n",
        "\n",
        "    gen_btn = gr.Button(\"Generate Dataset\", variant=\"primary\")\n",
        "\n",
        "    preview_df = gr.DataFrame(\n",
        "        label=\"Click a row to inspect\",\n",
        "        headers=[\"Level\", \"Description\", \"Code Preview\", \"Bug Types\", \"Bugs\", \"Model\"],\n",
        "        interactive=False,\n",
        "    )\n",
        "    detail_md = gr.Markdown(DETAIL_PLACEHOLDER)\n",
        "    validate_btn = gr.Button(\"Validate Selected Code\", variant=\"secondary\")\n",
        "    validate_md = gr.Markdown(VALIDATE_PLACEHOLDER)\n",
        "\n",
        "    with gr.Accordion(\"Debug Logs\", open=False, visible=True):\n",
        "        debug_box = gr.Textbox(lines=12, max_lines=12, interactive=False, show_label=False, placeholder=\"Enable the debug toggle and generate to see logs here.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        json_file = gr.File(label=\"Download JSON\", interactive=False)\n",
        "        jsonl_btn = gr.Button(\"Export as JSONL\")\n",
        "        jsonl_file = gr.File(label=\"Download JSONL\", interactive=False)\n",
        "\n",
        "    gen_btn.click(\n",
        "        fn=lambda: gr.update(interactive=False),\n",
        "        outputs=[gen_btn],\n",
        "    ).then(\n",
        "        fn=on_generate,\n",
        "        inputs=[model_dd, bug_cb, easy_sl, med_sl, hard_sl, expand_ck, verbose_ck],\n",
        "        outputs=[preview_df, json_file, gen_btn, detail_md, debug_box],\n",
        "    )\n",
        "\n",
        "    preview_df.select(fn=on_row_select, outputs=[detail_md, validate_md])\n",
        "    validate_btn.click(fn=on_validate, inputs=[model_dd], outputs=[validate_md])\n",
        "    jsonl_btn.click(fn=on_download_jsonl, outputs=[jsonl_file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2efe9ab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "app.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
