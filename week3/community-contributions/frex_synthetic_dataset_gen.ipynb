{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import csv\n",
        "import io\n",
        "import os\n",
        "from huggingface_hub import InferenceClient, login\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "cG4xfuhy8GuO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Setup\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(HF_TOKEN, add_to_git_credential=True)\n",
        "\n",
        "# HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "DEFAULT_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "AVAILABLE_MODELS = [\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "FXaOep0c8NJS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_messages(description: str, num_rows: int, schema: str):\n",
        "    schema_hint = f\"\\nUse this schema/columns: {schema}\" if schema.strip() else \"\"\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a synthetic dataset generator. \"\n",
        "                \"When asked, return ONLY a valid JSON array of objects ‚Äî no markdown, no explanation, no code fences. \"\n",
        "                \"Each object must have identical keys. Make data realistic, diverse, and non-repetitive.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Generate exactly {num_rows} rows of synthetic data.\\n\"\n",
        "                f\"Dataset description: {description}{schema_hint}\\n\\n\"\n",
        "                \"Return ONLY the raw JSON array.\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "aaD1PYOl8Zit"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uY2-Tswt7WGE"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(description, num_rows, schema, model_name, output_format, progress=gr.Progress()):\n",
        "    if not HF_TOKEN:\n",
        "        return None, None, \"‚ùå HF_TOKEN secret is not configured on this Space.\"\n",
        "\n",
        "    if not description.strip():\n",
        "        return None, None, \"‚ùå Please describe your dataset.\"\n",
        "\n",
        "    num_rows = max(1, min(int(num_rows), 200))\n",
        "    client = InferenceClient(model=model_name, token=HF_TOKEN)\n",
        "\n",
        "    all_rows = []\n",
        "    batch_size = min(20, num_rows)\n",
        "    batches = (num_rows + batch_size - 1) // batch_size\n",
        "\n",
        "    for _ in progress.tqdm(range(batches), desc=\"Generating batches\"):\n",
        "        rows_needed = min(batch_size, num_rows - len(all_rows))\n",
        "        messages = build_messages(description, rows_needed, schema)\n",
        "        try:\n",
        "            response = client.chat_completion(\n",
        "                messages=messages,\n",
        "                max_tokens=2048,\n",
        "                temperature=0.8,\n",
        "            )\n",
        "            text = response.choices[0].message.content.strip()\n",
        "            # Strip markdown code fences if model adds them\n",
        "            if text.startswith(\"```\"):\n",
        "                text = text.split(\"```\")[1]\n",
        "                if text.startswith(\"json\"):\n",
        "                    text = text[4:]\n",
        "            start = text.find(\"[\")\n",
        "            end = text.rfind(\"]\") + 1\n",
        "            if start == -1 or end == 0:\n",
        "                return None, None, f\"‚ùå Model didn't return valid JSON. Raw response:\\n{text[:500]}\"\n",
        "            batch_rows = json.loads(text[start:end])\n",
        "            all_rows.extend(batch_rows)\n",
        "        except json.JSONDecodeError as e:\n",
        "            return None, None, f\"‚ùå JSON parse error: {e}\\n\\nRaw output:\\n{text[:500]}\"\n",
        "        except Exception as e:\n",
        "            return None, None, f\"‚ùå Generation error: {str(e)}\"\n",
        "\n",
        "    if not all_rows:\n",
        "        return None, None, \"‚ùå No data was generated.\"\n",
        "\n",
        "    if output_format == \"JSON\":\n",
        "        output_str = json.dumps(all_rows, indent=2, ensure_ascii=False)\n",
        "        filename = \"frexrator_dataset.json\"\n",
        "    else:\n",
        "        keys = list(all_rows[0].keys())\n",
        "        buf = io.StringIO()\n",
        "        writer = csv.DictWriter(buf, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        for row in all_rows:\n",
        "            flat = {k: json.dumps(v) if isinstance(v, (dict, list)) else v for k, v in row.items()}\n",
        "            writer.writerow(flat)\n",
        "        output_str = buf.getvalue()\n",
        "        filename = \"frexrator_dataset.csv\"\n",
        "\n",
        "    tmp_path = f\"/tmp/{filename}\"\n",
        "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(output_str)\n",
        "\n",
        "    preview = output_str[:3000] + (\"\\n...(truncated)\" if len(output_str) > 3000 else \"\")\n",
        "    return tmp_path, preview, f\"‚úÖ Generated {len(all_rows)} rows successfully!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "with gr.Blocks(\n",
        "    title=\"üß™ Frexrator ‚Äì Synthetic Dataset Generator\",\n",
        "    theme=gr.themes.Soft(primary_hue=\"violet\"),\n",
        "    css=\".output-preview { font-family: monospace; font-size: 12px; }\"\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üß™ Frexrator\n",
        "    ### Synthetic Dataset Generator powered by Open-Source LLMs\n",
        "    Describe the dataset you want in plain English and get structured synthetic data instantly.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            description = gr.Textbox(\n",
        "                label=\"üìù Dataset Description\",\n",
        "                placeholder=\"e.g. 'A customer support ticket dataset with fields for ticket ID, customer name, issue category, priority level, and resolution status. Include a mix of resolved and open tickets across different product categories.'\",\n",
        "                lines=4,\n",
        "            )\n",
        "            schema = gr.Textbox(\n",
        "                label=\"üìã Column Schema (optional)\",\n",
        "                placeholder=\"e.g. ticket_id, customer_name, issue, priority (low/medium/high), status, created_at\",\n",
        "                lines=2,\n",
        "            )\n",
        "            with gr.Row():\n",
        "                num_rows = gr.Slider(1, 200, value=20, step=1, label=\"üî¢ Number of Rows\")\n",
        "                output_format = gr.Radio([\"JSON\", \"CSV\"], value=\"JSON\", label=\"üìÑ Output Format\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            model_name = gr.Dropdown(\n",
        "                AVAILABLE_MODELS,\n",
        "                value=DEFAULT_MODEL,\n",
        "                label=\"ü§ñ LLM Model\",\n",
        "            )\n",
        "            gr.Markdown(\"\"\"\n",
        "            **Tips:**\n",
        "            - Be specific about data types and value ranges\n",
        "            - Mention relationships between fields\n",
        "            - Specify any domain (medical, finance, e-commerce...)\n",
        "            - Add constraints like date ranges or enums\n",
        "            \"\"\")\n",
        "\n",
        "    generate_btn = gr.Button(\"‚ö° Generate Dataset\", variant=\"primary\", size=\"lg\")\n",
        "    status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        download_file = gr.File(label=\"‚¨áÔ∏è Download Dataset\")\n",
        "        preview = gr.Textbox(\n",
        "            label=\"üëÅÔ∏è Preview\",\n",
        "            lines=20,\n",
        "            interactive=False,\n",
        "            elem_classes=[\"output-preview\"]\n",
        "        )\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"E-commerce product reviews with product_id, product_name, category, rating (1-5), review_text, helpful_votes, verified_purchase, date\", 25, \"product_id, product_name, category, rating, review_text, helpful_votes, verified_purchase, date\"],\n",
        "            [\"Medical patient records with anonymized patient_id, age, gender, diagnosis (ICD codes), medications, blood_pressure, bmi, and admission_date\", 15, \"\"],\n",
        "            [\"Job postings dataset with company, job_title, location, salary_range, required_skills, experience_years, remote_option, posted_date\", 20, \"\"],\n",
        "            [\"Financial transactions with transaction_id, timestamp, merchant, amount, currency, category, is_fraud flag\", 30, \"\"],\n",
        "        ],\n",
        "        inputs=[description, num_rows, schema],\n",
        "        label=\"Example Prompts\"\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=generate_dataset,\n",
        "        inputs=[description, num_rows, schema, model_name, output_format],\n",
        "        outputs=[download_file, preview, status_box],\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgV8nLVg8pRR",
        "outputId": "a56c4f92-8bd2-44b7-de7e-c0d75e175ee8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-712/3567219499.py:2: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-712/3567219499.py:2: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ilFenG1t8sqK",
        "outputId": "a856ab0b-6b71-40b3-d82c-3458cb1ac480"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dcfde182e31c681b50.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcfde182e31c681b50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}