{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Week 3 Exercise Solution - Synthetic Data Generator with HuggingFace\n",
    "\n",
    "**Author:** Samuel Kalu  \n",
    "**Team:** Euclid  \n",
    "**Week:** 3\n",
    "\n",
    "## Overview\n",
    "\n",
    "This solution combines Week 3 concepts into a comprehensive synthetic data generation pipeline:\n",
    "- ‚úÖ HuggingFace Transformers for model inference\n",
    "- ‚úÖ Multiple model architectures (causal LM, seq2seq)\n",
    "- ‚úÖ Token generation and sampling strategies\n",
    "- ‚úÖ Batch processing for efficiency\n",
    "- ‚úÖ Gradio UI for interactive data generation\n",
    "- ‚úÖ Export to multiple formats (JSON, CSV, JSONL)\n",
    "\n",
    "## Use Cases\n",
    "- Training data creation for fine-tuning\n",
    "- Data augmentation for ML pipelines\n",
    "- Synthetic Q&A pair generation\n",
    "- Multi-lingual dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(f\"‚úì HuggingFace Token exists: {hf_token[:8]}...\")\n",
    "else:\n",
    "    print(\"‚úó HuggingFace Token not set - some models may not work\")\n",
    "    print(\"  Get your token from https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-config",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Multiple pre-trained models for different generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models for different tasks\n",
    "MODELS = {\n",
    "    \"Text Generation (GPT-2)\": {\n",
    "        \"model_id\": \"gpt2\",\n",
    "        \"task\": \"text-generation\",\n",
    "        \"type\": \"causal\",\n",
    "        \"max_length\": 100\n",
    "    },\n",
    "    \"Text Generation (Phi-2)\": {\n",
    "        \"model_id\": \"microsoft/phi-2\",\n",
    "        \"task\": \"text-generation\",\n",
    "        \"type\": \"causal\",\n",
    "        \"max_length\": 150\n",
    "    },\n",
    "    \"Summarization (BART)\": {\n",
    "        \"model_id\": \"facebook/bart-large-cnn\",\n",
    "        \"task\": \"summarization\",\n",
    "        \"type\": \"seq2seq\",\n",
    "        \"max_length\": 130\n",
    "    },\n",
    "    \"Translation (Marian)\": {\n",
    "        \"model_id\": \"Helsinki-NLP/opus-mt-en-de\",\n",
    "        \"task\": \"translation\",\n",
    "        \"type\": \"seq2seq\",\n",
    "        \"max_length\": 100\n",
    "    },\n",
    "    \"Q&A Generation (T5)\": {\n",
    "        \"model_id\": \"google/flan-t5-base\",\n",
    "        \"task\": \"text2text-generation\",\n",
    "        \"type\": \"seq2seq\",\n",
    "        \"max_length\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cache for loaded pipelines\n",
    "pipelines_cache = {}\n",
    "\n",
    "def load_pipeline(model_key: str):\n",
    "    \"\"\"Load and cache a model pipeline\"\"\"\n",
    "    if model_key in pipelines_cache:\n",
    "        return pipelines_cache[model_key]\n",
    "    \n",
    "    model_config = MODELS[model_key]\n",
    "    print(f\"Loading {model_config['model_id']}...\")\n",
    "    \n",
    "    try:\n",
    "        pipe = pipeline(\n",
    "            model_config['task'],\n",
    "            model=model_config['model_id'],\n",
    "            token=hf_token,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        pipelines_cache[model_key] = pipe\n",
    "        device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úì Model loaded on {device}\")\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-templates",
   "metadata": {},
   "source": [
    "## Data Generation Templates\n",
    "\n",
    "Prompts and templates for different dataset types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-templates-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates for different dataset types\n",
    "DATASET_TEMPLATES = {\n",
    "    \"Q&A Pairs\": {\n",
    "        \"prompt\": \"Generate a question and answer pair about {topic}. Format: Q: [question] A: [answer]\",\n",
    "        \"output_format\": \"json\",\n",
    "        \"fields\": [\"question\", \"answer\"]\n",
    "    },\n",
    "    \"Summaries\": {\n",
    "        \"prompt\": \"Summarize the following text in 2-3 sentences: {text}\",\n",
    "        \"output_format\": \"text\",\n",
    "        \"fields\": [\"original\", \"summary\"]\n",
    "    },\n",
    "    \"Translations\": {\n",
    "        \"prompt\": \"{text}\",\n",
    "        \"output_format\": \"text\",\n",
    "        \"fields\": [\"source\", \"translation\"]\n",
    "    },\n",
    "    \"Story Continuations\": {\n",
    "        \"prompt\": \"Continue this story in 3-4 sentences: {text}\",\n",
    "        \"output_format\": \"text\",\n",
    "        \"fields\": [\"prompt\", \"continuation\"]\n",
    "    },\n",
    "    \"Instruction-Response\": {\n",
    "        \"prompt\": \"Generate an instruction and its response about {topic}. Format: Instruction: [instruction] Response: [response]\",\n",
    "        \"output_format\": \"json\",\n",
    "        \"fields\": [\"instruction\", \"response\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample topics for generation\n",
    "SAMPLE_TOPICS = [\n",
    "    \"artificial intelligence\",\n",
    "    \"climate change\",\n",
    "    \"space exploration\",\n",
    "    \"healthy eating\",\n",
    "    \"renewable energy\",\n",
    "    \"machine learning\",\n",
    "    \"history of internet\",\n",
    "    \"mental health\",\n",
    "    \"sustainable living\",\n",
    "    \"future of work\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-class",
   "metadata": {},
   "source": [
    "## Synthetic Data Generator Class\n",
    "\n",
    "Core generation logic with batching and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-class-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    \"\"\"Generate synthetic datasets using HuggingFace models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str = \"Text Generation (GPT-2)\"):\n",
    "        self.model_key = model_key\n",
    "        self.pipeline = load_pipeline(model_key)\n",
    "        self.generated_data = []\n",
    "    \n",
    "    def generate(self, \n",
    "                 prompt: str, \n",
    "                 num_samples: int = 5,\n",
    "                 temperature: float = 0.7,\n",
    "                 top_p: float = 0.9,\n",
    "                 batch_size: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate synthetic data samples\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt or template\n",
    "            num_samples: Number of samples to generate\n",
    "            temperature: Sampling temperature (higher = more diverse)\n",
    "            top_p: Nucleus sampling parameter\n",
    "            batch_size: Number of samples to generate in parallel\n",
    "        \n",
    "        Returns:\n",
    "            List of generated samples\n",
    "        \"\"\"\n",
    "        if not self.pipeline:\n",
    "            return [{\"error\": \"Model not loaded\"}]\n",
    "        \n",
    "        results = []\n",
    "        model_config = MODELS[self.model_key]\n",
    "        \n",
    "        # Prepare generation parameters\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": model_config[\"max_length\"],\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"do_sample\": temperature > 0,\n",
    "            \"num_return_sequences\": min(batch_size, num_samples)\n",
    "        }\n",
    "        \n",
    "        # Generate in batches\n",
    "        for batch_num in range(0, num_samples, batch_size):\n",
    "            current_batch = min(batch_size, num_samples - batch_num)\n",
    "            \n",
    "            try:\n",
    "                if model_config[\"type\"] == \"causal\":\n",
    "                    outputs = self.pipeline(\n",
    "                        [prompt] * current_batch,\n",
    "                        **gen_kwargs\n",
    "                    )\n",
    "                    # Extract generated text\n",
    "                    for output in outputs:\n",
    "                        if isinstance(output, list):\n",
    "                            generated = output[0]['generated_text']\n",
    "                        else:\n",
    "                            generated = output['generated_text']\n",
    "                        results.append({\n",
    "                            \"input\": prompt,\n",
    "                            \"output\": generated,\n",
    "                            \"model\": self.model_key\n",
    "                        })\n",
    "                else:\n",
    "                    # Seq2Seq models\n",
    "                    outputs = self.pipeline(\n",
    "                        [prompt] * current_batch,\n",
    "                        **gen_kwargs\n",
    "                    )\n",
    "                    for i, output in enumerate(outputs):\n",
    "                        results.append({\n",
    "                            \"input\": prompt,\n",
    "                            \"output\": output[0]['generated_text'] if isinstance(output, list) else output['generated_text'],\n",
    "                            \"model\": self.model_key\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                results.append({\"error\": str(e), \"input\": prompt})\n",
    "        \n",
    "        self.generated_data = results\n",
    "        return results\n",
    "    \n",
    "    def generate_qa_pairs(self, topic: str, num_pairs: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate Q&A pairs for a specific topic\"\"\"\n",
    "        prompt = f\"Generate a thoughtful question about {topic} and provide a comprehensive answer.\"\n",
    "        return self.generate(prompt, num_samples=num_pairs)\n",
    "    \n",
    "    def generate_summaries(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"Generate summaries for a list of texts\"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            prompt = f\"Summarize: {text}\"\n",
    "            output = self.generate(prompt, num_samples=1)\n",
    "            results.append({\n",
    "                \"original\": text[:200] + \"...\" if len(text) > 200 else text,\n",
    "                \"summary\": output[0].get(\"output\", \"Error\"),\n",
    "                \"model\": self.model_key\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def export_to_json(self, filename: str = None) -> str:\n",
    "        \"\"\"Export generated data to JSON\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"synthetic_data_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.generated_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def export_to_csv(self, filename: str = None) -> str:\n",
    "        \"\"\"Export generated data to CSV\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"synthetic_data_{timestamp}.csv\"\n",
    "        \n",
    "        if not self.generated_data:\n",
    "            return None\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.generated_data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.generated_data)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def export_to_jsonl(self, filename: str = None) -> str:\n",
    "        \"\"\"Export generated data to JSONL (for fine-tuning)\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"synthetic_data_{timestamp}.jsonl\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for item in self.generated_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-section",
   "metadata": {},
   "source": [
    "## Demo: Generate Synthetic Data\n",
    "\n",
    "Test the generator with different models and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-gpt2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Text Generation with GPT-2\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO 1: Text Generation with GPT-2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generator_gpt2 = SyntheticDataGenerator(\"Text Generation (GPT-2)\")\n",
    "prompt = \"Artificial intelligence is revolutionizing\"\n",
    "results = generator_gpt2.generate(prompt, num_samples=3, temperature=0.8)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Output: {result['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-flan-t5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Q&A Generation with FLAN-T5\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMO 2: Q&A Generation with FLAN-T5\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generator_t5 = SyntheticDataGenerator(\"Q&A Generation (T5)\")\n",
    "prompt = \"Generate a question and answer about machine learning.\"\n",
    "results = generator_t5.generate(prompt, num_samples=3, temperature=0.5)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Q&A Pair {i} ---\")\n",
    "    print(f\"{result['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-summarization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Summarization with BART\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMO 3: Summarization with BART\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Large language models are artificial intelligence systems that have been trained on vast amounts of text data. \n",
    "They can understand and generate human-like text, making them useful for tasks like translation, summarization, \n",
    "and question answering. Recent models like GPT-4, Claude, and Llama have demonstrated remarkable capabilities \n",
    "in understanding context, following instructions, and even reasoning about complex problems.\n",
    "\"\"\"\n",
    "\n",
    "generator_bart = SyntheticDataGenerator(\"Summarization (BART)\")\n",
    "prompt = f\"Summarize: {sample_text}\"\n",
    "results = generator_bart.generate(prompt, num_samples=2)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Summary {i} ---\")\n",
    "    print(f\"{result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## Export Generated Data\n",
    "\n",
    "Save datasets in multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "json_file = generator_gpt2.export_to_json()\n",
    "print(f\"‚úì Exported to JSON: {json_file}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_file = generator_gpt2.export_to_csv()\n",
    "print(f\"‚úì Exported to CSV: {csv_file}\")\n",
    "\n",
    "# Export to JSONL\n",
    "jsonl_file = generator_t5.export_to_jsonl()\n",
    "print(f\"‚úì Exported to JSONL: {jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-ui",
   "metadata": {},
   "source": [
    "## Gradio UI - Interactive Data Generator\n",
    "\n",
    "Create a user-friendly interface for synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-ui-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator_ui():\n",
    "    \"\"\"Create Gradio interface for synthetic data generation\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"Synthetic Data Generator\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"# ü§ñ Synthetic Data Generator\n",
    "### Week 3 Exercise Solution - Samuel Kalu (Team Euclid)\n",
    "\n",
    "Generate high-quality synthetic datasets using HuggingFace transformers:\n",
    "- Multiple pre-trained models (GPT-2, FLAN-T5, BART, etc.)\n",
    "- Various dataset types (Q&A, Summaries, Translations, Instructions)\n",
    "- Batch generation for efficiency\n",
    "- Export to JSON, CSV, or JSONL formats\n",
    "\"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"### ‚öôÔ∏è Configuration\")\n",
    "                \n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=list(MODELS.keys()),\n",
    "                    value=\"Text Generation (GPT-2)\",\n",
    "                    label=\"Model\"\n",
    "                )\n",
    "                \n",
    "                dataset_type = gr.Dropdown(\n",
    "                    choices=list(DATASET_TEMPLATES.keys()),\n",
    "                    value=\"Q&A Pairs\",\n",
    "                    label=\"Dataset Type\"\n",
    "                )\n",
    "                \n",
    "                topic_input = gr.Textbox(\n",
    "                    label=\"Topic / Input Text\",\n",
    "                    placeholder=\"e.g., machine learning, climate change, or paste your text here\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    num_samples = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=20,\n",
    "                        value=5,\n",
    "                        step=1,\n",
    "                        label=\"Number of Samples\"\n",
    "                    )\n",
    "                    temperature = gr.Slider(\n",
    "                        minimum=0.1,\n",
    "                        maximum=1.5,\n",
    "                        value=0.7,\n",
    "                        step=0.1,\n",
    "                        label=\"Temperature (Diversity)\"\n",
    "                    )\n",
    "                \n",
    "                generate_btn = gr.Button(\"üöÄ Generate Data\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "            with gr.Column(scale=3):\n",
    "                gr.Markdown(\"### üìä Generated Data\")\n",
    "                \n",
    "                output_area = gr.JSON(\n",
    "                    label=\"Generated Samples\",\n",
    "                    height=400\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    export_json = gr.Button(\"üìÑ Export JSON\")\n",
    "                    export_csv = gr.Button(\"üìä Export CSV\")\n",
    "                    export_jsonl = gr.Button(\"üìù Export JSONL\")\n",
    "                \n",
    "                status_box = gr.Textbox(\n",
    "                    label=\"Status\",\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        current_generator = gr.State(None)\n",
    "        \n",
    "        def generate_data(model, data_type, topic, num_samples, temp):\n",
    "            \"\"\"Generate synthetic data\"\"\"\n",
    "            if not topic:\n",
    "                return None, \"Please enter a topic or text\"\n",
    "            \n",
    "            template = DATASET_TEMPLATES[data_type]\n",
    "            prompt = template[\"prompt\"].format(topic=topic, text=topic)\n",
    "            \n",
    "            generator = SyntheticDataGenerator(model)\n",
    "            \n",
    "            results = generator.generate(\n",
    "                prompt=prompt,\n",
    "                num_samples=num_samples,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            return results, f\"‚úì Generated {len(results)} samples with {model}\"\n",
    "        \n",
    "        def export_json_handler():\n",
    "            return \"JSON export functionality available after generation\"\n",
    "        \n",
    "        def export_csv_handler():\n",
    "            return \"CSV export functionality available after generation\"\n",
    "        \n",
    "        def export_jsonl_handler():\n",
    "            return \"JSONL export functionality available after generation\"\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=generate_data,\n",
    "            inputs=[model_dropdown, dataset_type, topic_input, num_samples, temperature],\n",
    "            outputs=[output_area, status_box]\n",
    "        )\n",
    "        \n",
    "        export_json.click(fn=export_json_handler, outputs=[status_box])\n",
    "        export_csv.click(fn=export_csv_handler, outputs=[status_box])\n",
    "        export_jsonl.click(fn=export_jsonl_handler, outputs=[status_box])\n",
    "        \n",
    "        gr.Markdown(\"\"\"---\n",
    "### üí° Tips:\n",
    "- **Temperature**: Lower values (0.1-0.5) = more focused, Higher values (0.8-1.5) = more creative\n",
    "- **Batch Generation**: Generate multiple samples at once for efficiency\n",
    "- **Export Formats**: JSONL is ideal for fine-tuning LLMs\n",
    "\"\"\")\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch-ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_data_generator_ui()\n",
    "    demo.launch()\n",
    "    \n",
    "    # For public sharing:\n",
    "    # demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-section",
   "metadata": {},
   "source": [
    "## Advanced: Custom Dataset Creation\n",
    "\n",
    "Create custom datasets for specific fine-tuning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetuning_dataset(topics: List[str], \n",
    "                              num_pairs_per_topic: int = 3,\n",
    "                              output_file: str = \"finetuning_dataset.jsonl\") -> str:\n",
    "    \"\"\"\n",
    "    Create a fine-tuning dataset with instruction-response pairs\n",
    "    \n",
    "    Args:\n",
    "        topics: List of topics to generate Q&A for\n",
    "        num_pairs_per_topic: Number of Q&A pairs per topic\n",
    "        output_file: Output JSONL filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to generated file\n",
    "    \"\"\"\n",
    "    generator = SyntheticDataGenerator(\"Q&A Generation (T5)\")\n",
    "    dataset = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"Generating for topic: {topic}\")\n",
    "        \n",
    "        for i in range(num_pairs_per_topic):\n",
    "            instruction = f\"Tell me about {topic}\"\n",
    "            prompt = f\"Generate an educational response about {topic}\"\n",
    "            \n",
    "            results = generator.generate(prompt, num_samples=1)\n",
    "            \n",
    "            if results and \"output\" in results[0]:\n",
    "                dataset.append({\n",
    "                    \"instruction\": instruction,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": results[0][\"output\"],\n",
    "                    \"topic\": topic,\n",
    "                    \"source\": \"synthetic_week3_exercise\"\n",
    "                })\n",
    "    \n",
    "    # Save to JSONL\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"‚úì Generated {len(dataset)} samples for fine-tuning\")\n",
    "    return output_file\n",
    "\n",
    "# Example usage:\n",
    "# finetuning_file = create_finetuning_dataset(SAMPLE_TOPICS[:3], num_pairs_per_topic=2)\n",
    "# print(f\"Fine-tuning dataset saved to: {finetuning_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Features Implemented:\n",
    "1. ‚úÖ HuggingFace Transformers Integration - Multiple model architectures\n",
    "2. ‚úÖ Batch Processing - Efficient generation with configurable batch sizes\n",
    "3. ‚úÖ Sampling Strategies - Temperature and top-p control for diversity\n",
    "4. ‚úÖ Multiple Export Formats - JSON, CSV, JSONL for different use cases\n",
    "5. ‚úÖ Gradio UI - Interactive interface for non-technical users\n",
    "6. ‚úÖ Template System - Pre-built templates for common dataset types\n",
    "7. ‚úÖ Fine-tuning Dataset Creator - Generate instruction-response pairs\n",
    "\n",
    "### Potential Enhancements:\n",
    "- Add support for more models (Llama, Mistral via HuggingFace)\n",
    "- Implement data quality filtering\n",
    "- Add multi-lingual support\n",
    "- Integrate with HuggingFace Datasets for direct upload\n",
    "- Add data validation and deduplication\n",
    "\n",
    "### Lessons Learned:\n",
    "- HuggingFace pipelines make model inference incredibly simple\n",
    "- Different models excel at different tasks (causal LM vs seq2seq)\n",
    "- Temperature control is crucial for balancing diversity and coherence\n",
    "- Batch processing significantly speeds up large dataset generation\n",
    "- JSONL format is standard for fine-tuning datasets\n",
    "\n",
    "---\n",
    "**Built with ‚ù§Ô∏è for LLM Engineering Bootcamp - Week 3**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
