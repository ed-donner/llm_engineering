{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5b46928",
      "metadata": {},
      "source": [
        "# Week 3 Exercise — Synthetic Data Generator (Emmanuel)\n",
        "\n",
        "Chat UI to generate **synthetic data of any type** using an LLM. Choose a model from the dropdown and describe what data you need (e.g. tabular customer records, JSON configs, CSV, sample emails, survey responses). The assistant is system-prompted to produce valid, consistent synthetic data in the format you request.\n",
        "\n",
        "**Requirements:** For OpenAI models set `OPENAI_API_KEY` in your environment or a `.env` file. For open-source models (Ollama), run Ollama locally (`ollama serve`, then `ollama pull <model>`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc306d22",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai_client = OpenAI() if openai_api_key else None\n",
        "\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
        "\n",
        "# Dropdown: (display label, model id for API). OpenAI first, then Ollama.\n",
        "MODEL_CHOICES = [\n",
        "    (\"OpenAI — GPT-4.1 Mini\", \"gpt-4.1-mini\"),\n",
        "    (\"OpenAI — GPT-4o\", \"gpt-4o\"),\n",
        "    (\"OpenAI — GPT-4 Turbo\", \"gpt-4-turbo\"),\n",
        "    (\"Ollama — Llama 3.2\", \"llama3.2\"),\n",
        "    (\"Ollama — Llama 3.1\", \"llama3.1\"),\n",
        "    (\"Ollama — Mistral\", \"mistral\"),\n",
        "    (\"Ollama — Qwen 2.5 7B\", \"qwen2.5:7b\"),\n",
        "    (\"Ollama — Phi-3\", \"phi3\"),\n",
        "]\n",
        "DEFAULT_MODEL_ID = \"gpt-4.1-mini\"\n",
        "OLLAMA_MODELS = {\"llama3.2\", \"llama3.1\", \"mistral\", \"qwen2.5:7b\", \"phi3\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6c5adc",
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an expert synthetic data generator. Your role is to produce realistic, consistent synthetic data of any type the user requests.\n",
        "\n",
        "You must:\n",
        "- Generate data that matches the user's description (schema, format, volume, domain).\n",
        "- Output in the exact format requested: CSV, JSON, JSONL, YAML, markdown tables, or plain text.\n",
        "- Ensure internal consistency (e.g. dates, IDs, referential integrity where relevant).\n",
        "- Avoid real personal data; use clearly fake names, emails, and identifiers.\n",
        "- If the user specifies a number of rows or examples, produce at least that many unless they ask for a small sample.\n",
        "\n",
        "When the user's request is ambiguous, ask one short clarifying question (e.g. number of rows, format, or fields) then generate. Otherwise generate directly.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a088851e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_messages(message, history):\n",
        "    sys = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    history_msgs = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "    return sys + history_msgs + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "\n",
        "def chat(message, history, model):\n",
        "    use_ollama = model in OLLAMA_MODELS\n",
        "    client = ollama_client if use_ollama else openai_client\n",
        "    if not client:\n",
        "        if use_ollama:\n",
        "            yield \"Ollama is not available. Run `ollama serve` and `ollama pull <model>` (e.g. ollama pull llama3.2).\"\n",
        "        else:\n",
        "            yield \"Set OPENAI_API_KEY in your environment or .env to use OpenAI models.\"\n",
        "        return\n",
        "    messages = build_messages(message, history)\n",
        "    try:\n",
        "        stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
        "    except Exception as e:\n",
        "        yield f\"Error calling model: {e}\"\n",
        "        return\n",
        "    accumulated = \"\"\n",
        "    for chunk in stream:\n",
        "        if chunk.choices and chunk.choices[0].delta.content:\n",
        "            accumulated += chunk.choices[0].delta.content\n",
        "            yield accumulated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496bc906",
      "metadata": {},
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Synthetic Data Generator\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"### Synthetic Data Generator — pick a model, then chat below.\")\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=MODEL_CHOICES,\n",
        "            value=DEFAULT_MODEL_ID,\n",
        "            label=\"Model\",\n",
        "            info=\"OpenAI models need OPENAI_API_KEY; Ollama models need local Ollama running.\",\n",
        "            allow_custom_value=False,\n",
        "        )\n",
        "    gr.ChatInterface(\n",
        "        fn=chat,\n",
        "        type=\"messages\",\n",
        "        additional_inputs=[model_dropdown],\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24977b4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-engineering (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
