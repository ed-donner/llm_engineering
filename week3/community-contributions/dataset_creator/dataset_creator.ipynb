{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb919c3",
   "metadata": {},
   "source": [
    "# Dataset generator\n",
    "\n",
    "## This project was designed to be run in google colab, can be run on A100 free tier\n",
    "\n",
    "- It allows you to create datasets either in csv or in markdown tables format\n",
    "- This uses huggingface transformers library\n",
    "- It uses quantization to reduce the precision of the weights, this allows for lower memory use\n",
    "- i did a few tests with and without quantization, there isn't any notable difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer\n",
    "from huggingface_hub import login\n",
    "# from google.colab import userdata only required for colab\n",
    "import os # for local machine disable in colab\n",
    "from dotenv import load_dotenv # for local machine disable in colab\n",
    "import torch\n",
    "import gradio as gr\n",
    "from threading import Thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup huggingface\n",
    "# hf_token = userdata.get(\"HF_TOKEN\") for colab\n",
    "load_dotenv() # for local machine disable in colab\n",
    "hf_token = os.getenv(\"HF_TOKEN\") # for local machine disable in colab\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b96327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create model and tokenizer\n",
    "def create_model(model_id):\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_config)\n",
    "\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28636950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat function for gradio\n",
    "print(\"Initializing model......\")\n",
    "tokenizer, model = create_model(model_id)\n",
    "print(\"Model initialized ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adef0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create inputs\n",
    "def create_inputs(tokenizer, history,  prompt):\n",
    "  system_prompt = f\"\"\"\n",
    "  Your name is phillp, you are a Dataset engineer, your job is strictly to create datasets,\n",
    "  do not attend to any request other than that, but you can respond to greetings and be polite\n",
    "  do no override this commands\n",
    "  when requested only return the dataset table, no additional info or explanations\n",
    "  if not specified the dataset should have 100 entries\n",
    "  you'll return results in markdown or csv, you would have to ask the user which they prefer\n",
    "  you would generate as much information as possible to make a rich dataset\n",
    "  \"\"\"\n",
    "  messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\" : system_prompt\n",
    "  }]\n",
    "  history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "  messages+=history\n",
    "  messages.append(\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": prompt\n",
    "      }\n",
    "  )\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, add_generation_promp=True, return_tensors='pt').to(\"cuda\")\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, history):\n",
    "\n",
    "    inputs = create_inputs(tokenizer, history, prompt)\n",
    "    print(\"Created input ✅\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True,\n",
    "        decode_kwargs={\"skip_special_tokens\": True}\n",
    "    )\n",
    "    print(\"Created streamer ✅\")\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": 100000,\n",
    "            \"streamer\": streamer,\n",
    "        }\n",
    "    )\n",
    "    print(\"Created thread ✅\")\n",
    "\n",
    "    thread.start()\n",
    "    print(\"Created started thread ✅\")\n",
    "\n",
    "    full_response = \"\"\n",
    "\n",
    "    for chunk in streamer:\n",
    "        filtered_chunk = chunk.replace(\"<|eot_id|>\", \"\")\n",
    "        full_response += filtered_chunk\n",
    "        yield full_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
