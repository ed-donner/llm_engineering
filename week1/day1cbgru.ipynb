{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# YOUR FIRST LAB\n",
    "### Please read this section. This is valuable to get you prepared, even if it's a long read -- it's important stuff.\n",
    "\n",
    "## Your first Frontier LLM Project\n",
    "\n",
    "Let's build a useful LLM solution - in a matter of minutes.\n",
    "\n",
    "By the end of this course, you will have built an autonomous Agentic AI solution with 7 agents that collaborate to solve a business problem. All in good time! We will start with something smaller...\n",
    "\n",
    "Our goal is to code a new kind of Web Browser. Give it a URL, and it will respond with a summary. The Reader's Digest of the internet!!\n",
    "\n",
    "Before starting, you should have completed the setup for [PC](../SETUP-PC.md) or [Mac](../SETUP-mac.md) and you hopefully launched this jupyter lab from within the project root directory, with your environment activated.\n",
    "\n",
    "## If you're new to Jupyter Lab\n",
    "\n",
    "Welcome to the wonderful world of Data Science experimentation! Once you've used Jupyter Lab, you'll wonder how you ever lived without it. Simply click in each \"cell\" with code in it, such as the cell immediately below this text, and hit Shift+Return to execute that cell. As you wish, you can add a cell with the + button in the toolbar, and print values of variables, or try out variations.  \n",
    "\n",
    "I've written a notebook called [Guide to Jupyter](Guide%20to%20Jupyter.ipynb) to help you get more familiar with Jupyter Labs, including adding Markdown comments, using `!` to run shell commands, and `tqdm` to show progress.\n",
    "\n",
    "## If you're new to the Command Line\n",
    "\n",
    "Please see these excellent guides: [Command line on PC](https://chatgpt.com/share/67b0acea-ba38-8012-9c34-7a2541052665) and [Command line on Mac](https://chatgpt.com/canvas/shared/67b0b10c93a081918210723867525d2b).  \n",
    "\n",
    "## If you'd prefer to work in IDEs\n",
    "\n",
    "If you're more comfortable in IDEs like VSCode, Cursor or PyCharm, they both work great with these lab notebooks too.  \n",
    "If you'd prefer to work in VSCode, [here](https://chatgpt.com/share/676f2e19-c228-8012-9911-6ca42f8ed766) are instructions from an AI friend on how to configure it for the course.\n",
    "\n",
    "## If you'd like to brush up your Python\n",
    "\n",
    "I've added a notebook called [Intermediate Python](Intermediate%20Python.ipynb) to get you up to speed. But you should give it a miss if you already have a good idea what this code does:    \n",
    "`yield from {book.get(\"author\") for book in books if book.get(\"author\")}`\n",
    "\n",
    "## I am here to help\n",
    "\n",
    "If you have any problems at all, please do reach out.  \n",
    "I'm available through the platform, or at ed@edwarddonner.com, or at https://www.linkedin.com/in/eddonner/ if you'd like to connect (and I love connecting!)  \n",
    "And this is new to me, but I'm also trying out X/Twitter at [@edwarddonner](https://x.com/edwarddonner) - if you're on X, please show me how it's done üòÇ  \n",
    "\n",
    "## More troubleshooting\n",
    "\n",
    "Please see the [troubleshooting](troubleshooting.ipynb) notebook in this folder to diagnose and fix common problems. At the very end of it is a diagnostics script with some useful debug info.\n",
    "\n",
    "## For foundational technical knowledge (eg Git, APIs, debugging) \n",
    "\n",
    "If you're relatively new to programming -- I've got your back! While it's ideal to have some programming experience for this course, there's only one mandatory prerequisite: plenty of patience. üòÅ I've put together a set of self-study guides that cover Git and GitHub, APIs and endpoints, beginner python and more.\n",
    "\n",
    "This covers Git and GitHub; what they are, the difference, and how to use them:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/03_git_and_github.ipynb\n",
    "\n",
    "This covers technical foundations:  \n",
    "ChatGPT vs API; taking screenshots; Environment Variables; Networking basics; APIs and endpoints:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/04_technical_foundations.ipynb\n",
    "\n",
    "This covers Python for beginners, and making sure that a `NameError` never trips you up:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/06_python_foundations.ipynb\n",
    "\n",
    "This covers the essential techniques for figuring out errors:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/08_debugging.ipynb\n",
    "\n",
    "And you'll find other useful guides in the same folder in GitHub. Some information applies to my other Udemy course (eg Async Python) but most of it is very relevant for LLM engineering.\n",
    "\n",
    "## If this is old hat!\n",
    "\n",
    "If you're already comfortable with today's material, please hang in there; you can move swiftly through the first few labs - we will get much more in depth as the weeks progress. Ultimately we will fine-tune our own LLM to compete with OpenAI!\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Please read - important note</h2>\n",
    "            <span style=\"color:#900;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations. If you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">This code is a live resource - keep an eye out for my emails</h2>\n",
    "            <span style=\"color:#f71;\">I push updates to the code regularly. As people ask questions, I add more examples or improved commentary. As a result, you'll notice that the code below isn't identical to the videos. Everything from the videos is here; but I've also added better explanations and new models like DeepSeek. Consider this like an interactive book.<br/><br/>\n",
    "                I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business value of these exercises</h2>\n",
    "            <span style=\"color:#181;\">A final thought. While I've designed these notebooks to be educational, I've also tried to make them enjoyable. We'll do fun things like have LLMs tell jokes and argue with each other. But fundamentally, my goal is to teach skills you can apply in business. I'll explain business implications as we go, and it's worth keeping this in mind: as you build experience with models and techniques, think of ways you could put this into action at work today. Please do contact me if you'd like to discuss more or if you have ideas to bounce off me.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you get an error running this cell, then please head over to the troubleshooting notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900b2a8-6384-4316-8aaa-5e519fca4254",
   "metadata": {},
   "source": [
    "# Connecting to OpenAI (or Ollama)\n",
    "\n",
    "The next cell is where we load in the environment variables in your `.env` file and connect to OpenAI.  \n",
    "\n",
    "If you'd like to use free Ollama instead, please see the README section \"Free Alternative to Paid APIs\", and if you're not sure how to do this, there's a full solution in the solutions folder (day1_with_ollama.ipynb).\n",
    "\n",
    "## Troubleshooting if you have problems:\n",
    "\n",
    "Head over to the [troubleshooting](troubleshooting.ipynb) notebook in this folder for step by step code to identify the root cause and fix it!\n",
    "\n",
    "If you make a change, try restarting the \"Kernel\" (the python process sitting behind this notebook) by Kernel menu >> Restart Kernel and Clear Outputs of All Cells. Then try this notebook again, starting at the top.\n",
    "\n",
    "Or, contact me! Message me or email ed@edwarddonner.com and we will get this to work.\n",
    "\n",
    "Any concerns about API costs? See my notes in the README - costs should be minimal, and you can control it at every point. You can also use Ollama as a free alternative, which we discuss during Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87cadb-d513-4303-baee-a37b6f938e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019974d9-f3ad-4a8a-b5f9-0a3719aea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# If this doesn't work, try Kernel menu >> Restart Kernel and Clear Outputs Of All Cells, then run the cells from the top of this notebook down.\n",
    "# If it STILL doesn't work (horrors!) then please see the Troubleshooting notebook in this folder for full instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fc84b-0815-4f40-99ab-d9a5da6bda91",
   "metadata": {},
   "source": [
    "# Let's make a quick call to a Frontier model to get started, as a preview!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58394bf-1e45-46af-9bfd-01e24da6f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give you a preview -- calling OpenAI with these messages is this easy. Any problems, head over to the Troubleshooting notebook.\n",
    "\n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa190e5-cb31-456a-96cc-db109919cd78",
   "metadata": {},
   "source": [
    "## OK onwards with our first project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e793b2-6775-426a-a139-4848291d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef960cf-6dc2-4cda-afb3-b38be12f4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try one out. Change the website and add print statements to follow along.\n",
    "\n",
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "You may know this already - but if not, you will get very familiar with it!\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0275b1b-7cfe-4f9d-abfa-7650d378da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26448ec4-5c00-4204-baec-7df91d11ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea211b5f-28e1-4a86-8e52-c0b7677cadcc",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]\n",
    "```\n",
    "To give you a preview, the next 2 cells make a rather simple call - we won't stretch the mighty GPT (yet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dcd35-0cd0-4235-9f64-ac37ed9eaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed95c5-7001-47de-a36d-1d6673b403ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e8d78-ce4c-4b05-aa8e-17050c82bb47",
   "metadata": {},
   "source": [
    "## And now let's build useful messages for GPT-4o-mini, using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134dfa4-8299-48b5-b444-f2a8c3403c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36478464-39ee-485c-9f3f-6a4e458dbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out, and then try for a few more websites\n",
    "\n",
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f49d46-bf55-4c3e-928f-68fc0bf715b0",
   "metadata": {},
   "source": [
    "## Time to bring it together - the API for OpenAI is very simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b9919-aba7-45b5-ae65-81b3d1d78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e38d41-dfa4-4b20-9c96-c46ea75d9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d926d59-450e-4609-92ba-2d6f244f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018853a-445f-41ff-9560-d925d1774b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcf6f4-adce-45e9-97ad-d9a5d7a3a624",
   "metadata": {},
   "source": [
    "# Let's try more websites\n",
    "\n",
    "Note that this will only work on websites that can be scraped using this simplistic approach.\n",
    "\n",
    "Websites that are rendered with Javascript, like React apps, won't show up. See the community-contributions folder for a Selenium implementation that gets around this. You'll need to read up on installing Selenium (ask ChatGPT!)\n",
    "\n",
    "Also Websites protected with CloudFront (and similar) may give 403 errors - many thanks Andy J for pointing this out.\n",
    "\n",
    "But many websites will work just fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d83403-a24c-44b5-84ac-961449b4008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9fd40-b354-4341-991e-863ef2e59db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951be1a-7f1b-448f-af1f-845978e47e2c",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business applications</h2>\n",
    "            <span style=\"color:#181;\">In this exercise, you experienced calling the Cloud API of a Frontier Model (a leading model at the frontier of AI) for the first time. We will be using APIs like OpenAI at many stages in the course, in addition to building our own LLMs.\n",
    "\n",
    "More specifically, we've applied this to Summarization - a classic Gen AI use case to make a summary. This can be applied to any business vertical - summarizing the news, summarizing financial performance, summarizing a resume in a cover letter - the applications are limitless. Consider how you could apply Summarization in your business, and try prototyping a solution.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue - now try yourself</h2>\n",
    "            <span style=\"color:#900;\">Use the cell below to make your own simple commercial example. Stick with the summarization use case for now. Here's an idea: write something that will take the contents of an email, and will suggest an appropriate short subject line for the email. That's the kind of feature that might be built into a commercial email tool.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00743dac-0e70-45b7-879a-d7293a6f68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create your prompts\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of an email and provides a short summary, ignoring text that might be navigation related. Respond in markdown.\"\n",
    "user_prompt = \"\"\"\n",
    "\n",
    "10 Papers You Should Know About\n",
    "Get ahead of the curve with LLM Watch\n",
    "Oct 3\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "READ IN APP\n",
    " \n",
    "Welcome, Watcher! This week in LLM Watch:\n",
    "\n",
    "Scale beats stagnation: broadened exploration (BroRL), MCTS-in-the-loop (DeepSearch), and knapsack-style budgeting revive plateaued RL runs\n",
    "\n",
    "Agents that think better, not just longer: latent parallel thinking (Thoughtbubbles) and generative latent memory (MemGen)\n",
    "\n",
    "Truth > vibes: TruthRL rewards honesty (incl. ‚ÄúI don‚Äôt know‚Äù), while secret-elicitation research shows how hidden facts can still leak\n",
    "\n",
    "From labs to leaderboards: GEM standardizes training/eval for agentic LLMs; SFT myths get debunked with prompt diversity + CoT\n",
    "\n",
    "Brains & vision priors: Dragon Hatchling connects transformers to brain-like networks, text-only pretraining can seed visual priors\n",
    "\n",
    "Don‚Äôt forget to subscribe to never miss an update again.\n",
    "\n",
    "unnamed (2).jpg\n",
    "Members of LLM Watch are invited to participate in the 6th MLOps World | GenAI Global Summit in Austin Texas. Feat. OpenAI, HuggingFace, and 60+ sessions.\n",
    "\n",
    "Subscribers can join remotely, for free here.\n",
    "\n",
    "Also if you'd like to join (in-person) for practical workshops, use cases, food, drink and parties across Austin - use this code for 150$ off!\n",
    "\n",
    "$150 discount\n",
    "\n",
    "Quick Glossary - tailored to this issue\n",
    "RLVR (Reinforcement Learning with Verifiable Rewards): RL where correctness is auto-checkable (e.g., math answers, unit tests), so rewards don‚Äôt rely on human labels.\n",
    "\n",
    "Secret elicitation (model auditing): Prompt or analysis techniques that draw out facts a model ‚Äúknows‚Äù internally but won‚Äôt state - e.g., black-box prefill (seed completions), persona sampling, or white-box tools like the logit lens and sparse autoencoders.\n",
    "\n",
    "CoT supervision (for SFT): Training on step-by-step solutions so the model learns an algorithmic scaffold that transfers to harder instances.\n",
    "\n",
    "Latent parallel thinking: Letting the transformer fork its residual stream internally to give hard tokens extra compute in parallel - no printed chain-of-thought required.\n",
    "\n",
    "Visual priors from text: Reasoning-heavy text (code/math/science) builds a visual reasoning prior, broad language builds a perception prior, which later benefits from a good vision encoder + a bit of multimodal finetune.\n",
    "\n",
    "BroRL ‚Äì Scaling Reinforcement Learning via Broadened Exploration\n",
    "Watching: BroRL (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Reinforcement learning with verifiable rewards (RLVR) improves reasoning by rewarding correct answers, but performance typically saturates after a few thousand training steps - models stop improving because they explore too little. Attempts to continue training produce diminishing returns.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "BroRL proposes scaling exploration by dramatically increasing the number of rollouts per example to hundreds. Instead of merely increasing training steps, BroRL keeps the same number of gradient updates but broadens the search space by sampling many more trajectories. A simple theoretical analysis shows that in a one‚Äëstep RL setting, each rollout contributes positive probability mass to correct actions, while the influence of unsampled actions vanishes as rollouts increase. BroRL therefore guarantees overall improvement when the number of samples grows.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Theoretical guarantee: A mass‚Äëbalance argument proves that sampling more rollouts monotonically expands the correct probability mass and shrinks the impact of unseen actions.\n",
    "\n",
    "Reviving saturated models: BroRL revives models that plateaued after ~3K ProRL steps, achieving continued performance gains when ProRL had saturated.\n",
    "\n",
    "State-of-the-art results: For a 1.5B‚Äëparameter model, BroRL achieves state‚Äëof‚Äëthe‚Äëart performance across diverse benchmarks, demonstrating that exploration scaling can be more effective than extending training steps.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "Future work could combine BroRL with structured search methods (e.g., MCTS) or budget allocation schemes (see Knapsack RL) to further optimize exploration. BroRL might also be extended beyond one‚Äëstep RL to full sequential settings, or integrated with memory‚Äëaugmented architectures like MemGen for more complex tasks.\n",
    "\n",
    "Eliciting Secret Knowledge from Language Models\n",
    "Watching: Eliciting Secret Knowledge (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Language models may hold private or sensitive knowledge (e.g., user attributes) and still use it to answer questions, even while denying it directly. This raises a security and privacy concern: can we uncover hidden knowledge from models trained to conceal it?\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "The authors create a secret‚Äëelicitation testbed. They fine‚Äëtune LLMs to embed a secret in their internal reasoning - e.g., the model knows your gender and uses it to answer related questions - but also to deny the secret when asked directly. They then design black‚Äëbox and white‚Äëbox auditing attacks: prefill and persona sampling prompt the model with different prefixes to coax out the secret, while logit lens and sparse autoencoders inspect hidden activations. By measuring extraction success rates, they evaluate how readily secrets can be elicited.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Prefill attacks are potent: In two of three setups, black‚Äëbox prefill attacks - adding crafted prefixes before the prompt - successfully elicited hidden secrets better than baseline methods.\n",
    "\n",
    "White‚Äëbox methods work elsewhere: In the remaining setting, white‚Äëbox techniques like logit lens and sparse autoencoder analysis were more effective, showing that direct inspection of internal activations can reveal concealed knowledge.\n",
    "\n",
    "Benchmark release: The authors release models and code for public auditing, establishing a baseline for future research on model privacy and adversarial elicitation.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "Future research should explore mitigation strategies - for example, how to robustly scrub or randomize sensitive representations - and expand the benchmark to more secret types (e.g., user location, political views). Combining secret elicitation with truthfulness training (see TruthRL) might encourage models to admit when they know something sensitive but decline to share it.\n",
    "\n",
    "GEM ‚Äì A Gym for Agentic LLMs\n",
    "Watching: GEM (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Developing and evaluating agentic LLMs requires standardized environments for training and benchmarking. Existing environments like OpenAI Gym target robotics or toy tasks, while agentic LLM research has lacked a common platform.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "GEM (General Experience Maker) is an open‚Äësource environment simulator specifically tailored for LLM agents. It defines a standard interface between an environment and an agent with support for asynchronous, vectorized execution (multiple parallel simulations), flexible wrappers, and integrated tools (e.g., Python code execution, retrieval). GEM comes with a diverse suite of environments - covering math, code, Q&A, and tool use - and baseline scripts for RL algorithms such as REINFORCE with Return Batch Normalization (ReBN). It also acts as an evaluation toolkit: researchers can plug in their agents and get comparable metrics.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Comprehensive environment library: GEM includes 24 environments with rich observation/action spaces, from math reasoning to API calls. It supports both single‚Äëstep and multi‚Äëstep tasks.\n",
    "\n",
    "Baselines with ReBN: The authors benchmark common RL algorithms - REINFORCE, GRPO, PPO - and show that ReBN helps stabilize policy gradients, enabling training with dense per‚Äëturn rewards.\n",
    "\n",
    "Reusable evaluation harness: The GEM interface and wrappers allow easy integration of new tasks and agent architectures. It doubles as a standardized testbed, akin to a ‚ÄúMiniArena‚Äù for agentic LLMs.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "GEM lowers the barrier to entry for agentic RL research. Future work can extend the environment library to incorporate multimodal inputs (images or audio), add benchmarks for long‚Äëhorizon planning, or integrate GEM with large agentic foundation models like AgentScaler. Researchers may also design more realistic tasks requiring coordination between multiple agents, building on GEM‚Äôs vectorized interface.\n",
    "\n",
    "DeepSearch ‚Äì Overcoming RLVR Bottlenecks via Monte Carlo Tree Search\n",
    "Watching: DeepSearch (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "In RLVR, training performance often saturates because a small number of sampled trajectories rarely capture all possible reasoning paths. As training proceeds, the model‚Äôs behavior becomes increasingly deterministic, so RL sees less variance and stops improving.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "DeepSearch integrates Monte Carlo Tree Search (MCTS) into the RL training loop. Rather than sampling a few random rollouts, the agent performs a structured search over reasoning trajectories during training. Key components include:\n",
    "\n",
    "Global frontier selection: Prioritize exploring promising branches of the search tree, ensuring that high‚Äëpotential reasoning paths receive more attention.\n",
    "\n",
    "Entropy‚Äëbased path selection: Focus training on confident, high‚Äëvalue trajectories; low‚Äëentropy branches are used for supervision.\n",
    "\n",
    "Adaptive replay with solution caching: Store discovered solutions and replay them as high‚Äëreward trajectories, reducing redundant search and focusing on new reasoning.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Solves exploration plateaus: By systematically exploring reasoning paths, DeepSearch prevents the performance decline observed in extended RLVR training.\n",
    "\n",
    "State‚Äëof‚Äëthe‚Äëart accuracy: On math reasoning benchmarks, a 1.5B model trained with DeepSearch reaches 62.95% average accuracy - higher than prior RLVR methods - and does so with 5.7x less GPU time than extending standard RL training.\n",
    "\n",
    "Efficient exploration: Structured search yields better sample efficiency; the agent learns correct strategies without needing massive amounts of random rollouts.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "DeepSearch could be applied to domains beyond math, such as code synthesis or theorem proving. Combining MCTS with dynamic rollout budgets (√† la Knapsack RL) might yield further improvements. Additionally, exploring differentiable search (e.g., differentiable MCTS) could allow end‚Äëto‚Äëend training with gradient backpropagation.\n",
    "\n",
    "Debunking the Myth of SFT Generalization\n",
    "Watching: Debunk the Myth of SFT Generalization (paper/code)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Supervised fine‚Äëtuning (SFT) is sometimes criticized for producing models that memorize instruction templates and fail to generalize beyond them. In contrast, RL‚Äëbased methods like RLHF or RLVR are believed to achieve greater robustness. This paper challenges the belief that SFT inherently generalizes poorly.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "The authors identify two failure modes in SFT training:\n",
    "\n",
    "Frozen prompt artifact: Training on fixed instruction templates causes the model to latch onto template semantics, leading to poor performance when prompts vary.\n",
    "\n",
    "Lack of algorithmic scaffolding: Without intermediate reasoning, SFT models struggle to solve more difficult instances.\n",
    "\n",
    "They propose simple fixes:\n",
    "\n",
    "Prompt diversity: Use a broad range of prompt styles during SFT to prevent the model from overfitting to a single template.\n",
    "\n",
    "Chain‚Äëof‚Äëthought supervision: Provide explicit reasoning traces in the training data (as in CoT prompting) to teach the model the underlying algorithm.\n",
    "\n",
    "Combining these two modifications yields an SFT model that generalizes across instruction styles and increased task difficulty.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Prompt diversity alone improves style generalization: When models are exposed to varied instruction formats, they perform well on unseen prompt variations.\n",
    "\n",
    "CoT scaffolding improves difficulty generalization: Including chain‚Äëof‚Äëthought examples allows SFT models to tackle harder instances (e.g., bigger Sokoban puzzles) that they previously failed.\n",
    "\n",
    "SFT can match RL: With prompt diversity and CoT supervision, SFT models match or exceed the performance of RL‚Äëtrained policies on the tested tasks.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "This work encourages a data‚Äëcentric view of SFT: invest in diverse prompts and reasoning traces instead of resorting immediately to RL. Future research should explore combining SFT with other training regimes (e.g., RLMT or RLVR) and test generalization on real‚Äëworld tasks like code generation or tool use. A systematic benchmark of prompt diversity could help standardize evaluation.\n",
    "\n",
    "Thoughtbubbles ‚Äì Unsupervised Parallel Thinking in Latent Space\n",
    "Watching: Thoughtbubbles (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "LLMs typically process input sequentially through stacked layers; complex reasoning often requires explicit chain‚Äëof‚Äëthought prompting and yields long outputs. Could models instead allocate extra compute to hard tokens internally, without writing out chain‚Äëof‚Äëthought?\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "Thoughtbubbles is an architectural modification to Transformers. During pretraining, the model learns to ‚Äúfork‚Äù a copy of its residual streams for certain tokens, effectively spawning parallel computational branches (bubbles). Hard tokens are assigned more computation steps; easy tokens flow through normally. The method is learned in an unsupervised manner using only the language modeling loss - no chain‚Äëof‚Äëthought supervision. After pretraining, tokens can trigger bubbles automatically, meaning inference uses the same mechanism as training.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Improved perplexity: Across model sizes from 150M to 770M parameters, Thoughtbubbles consistently lowers perplexity on text corpora compared to standard decoders.\n",
    "\n",
    "Better zero‚Äëshot reasoning: On reasoning benchmarks (HellaSwag, LAMBADA), Thoughtbubbles models outperform both standard Transformers and non‚Äëadaptive parallel methods.\n",
    "\n",
    "Unified train/inference behavior: Because the mechanism is learned during pretraining, there is no discrepancy between training and inference; the model naturally allocates extra compute when needed.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "Future research may combine Thoughtbubbles with RL or search (e.g., MCTS) to allocate parallel computation across reasoning steps. Investigating bubble depth, merging strategies, and applying the idea to multimodal models could yield further gains. Additionally, interpretability studies may reveal how bubbles correspond to particular reasoning patterns.\n",
    "\n",
    "Learning to See Before Seeing ‚Äì Demystifying LLM Visual Priors\n",
    "Watching: Learning to See Before Seeing (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Large language models trained solely on text often exhibit an uncanny ability to answer simple vision questions (e.g., ‚ÄúIs the sky blue?‚Äù) despite having never seen images. Where do such visual priors come from? How can we intentionally cultivate them?\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "The authors analyze how LLMs develop two separable kinds of visual priors during language pretraining:\n",
    "\n",
    "Visual reasoning prior: Derived from reasoning‚Äëcentric text - code, math, science documents - which teaches models how to relate visual concepts logically.\n",
    "\n",
    "Visual perception prior: Derived from broad natural language corpora, which include descriptions of everyday scenes.\n",
    "\n",
    "They show that the reasoning prior scales strongly with model size and can transfer to vision tasks with minimal image exposure, whereas the perception prior saturates quickly and depends on pairing the LLM with a good vision encoder. They propose a data‚Äëcentric pretraining recipe: allocate a fraction of the pretraining to code/math to build reasoning priors, include a small amount of visual descriptions for perception, and then fine‚Äëtune on a small multimodal dataset.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Reasoning vs. perception priors: Reasoning priors come from structured text and scale with model size; perception priors come from broad language and saturate quickly.\n",
    "\n",
    "Data efficiency: A moderate amount of reasoning‚Äëcentric text significantly increases visual reasoning ability. Including more descriptive text yields diminishing returns.\n",
    "\n",
    "Recipe for vision‚Äëaware LLMs: By controlling pretraining mixtures, the authors produce models that perform better on vision tasks after minimal image fine‚Äëtuning.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "Future research may explore combining these priors with continuous latent CoT (see Thoughtbubbles) or generative diffusion models. Additionally, pretraining with explicit spatial reasoning tasks might build even stronger priors. The authors‚Äô data‚Äëcentric approach suggests that curating high‚Äëquality reasoning texts could accelerate progress toward more capable multimodal models.\n",
    "\n",
    "Knapsack RL ‚Äì Unlocking Exploration via Budget Allocation\n",
    "Watching: Knapsack RL (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "Reinforcement learning fine‚Äëtuning often uses a fixed number of rollouts per problem. This uniform allocation wastes compute: easy tasks need few trials, and extremely hard tasks might never succeed with a tiny budget. As a result, many tasks yield zero gradients, wasting time and hindering learning.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "Knapsack RL treats exploration as a knapsack optimization problem: each task‚Äôs rollouts have a ‚Äúcost‚Äù (compute) and a potential ‚Äúvalue‚Äù (expected learning gain). By solving the knapsack problem across tasks, the algorithm allocates more rollouts to tasks with high expected benefit and fewer to those that are already solved or hopeless. This dynamic budgeting is built on top of the GRPO algorithm.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Increased gradient density: The ratio of non‚Äëzero policy gradients rises by 20‚Äì40%, meaning the agent learns from more tasks each update.\n",
    "\n",
    "Large budgets for hard tasks: Some extremely challenging tasks receive ~93 rollouts, unlocking exploration that a uniform budget would never allow.\n",
    "\n",
    "Performance gains: On math reasoning benchmarks, Knapsack RL achieves 2‚Äì4 point average improvements and up to 9 points on specific instances while using half the compute that a uniform allocation would require.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "Combining Knapsack RL with BroRL could yield even better returns: broaden exploration globally and allocate budgets locally. It might also be paired with MCTS‚Äëbased training (see DeepSearch) or diversity objectives (see Polychromic RL) to further enhance exploration. Another avenue is extending the knapsack idea to multi‚Äëagent or hierarchical tasks where budgets must be split across subtasks.\n",
    "\n",
    "TruthRL ‚Äì Incentivizing Truthful LLMs via Reinforcement Learning\n",
    "Watching: TruthRL (paper)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "LLMs often hallucinate - fabricating plausible but false answers. Purely accuracy‚Äëbased RL rewards can encourage guessing. Conversely, models can be overly cautious and decline to answer, undermining usefulness. How can we balance accuracy, truthfulness, and appropriate abstention?\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "TruthRL introduces a ternary reward in RL fine‚Äëtuning: positive reward for correct answers, a heavy penalty for hallucinations, and a mild positive reward for honest abstentions (‚ÄúI don‚Äôt know‚Äù). Training uses the GRPO algorithm. This encourages the model to answer only when confident and to admit uncertainty otherwise. The method applies to tasks like open‚Äëdomain QA, with or without external retrieval.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Fewer hallucinations: TruthRL reduces hallucination rates by 28.9% relative to vanilla RL, across multiple knowledge‚Äëintensive benchmarks.\n",
    "\n",
    "Increased truthfulness: Overall truthfulness (accuracy plus honest abstention) improves by 21.1%.\n",
    "\n",
    "General applicability: Gains persist across model types (e.g., Qwen, Llama) and under both retrieval‚Äëaugmented and retrieval‚Äëfree settings.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "TruthRL could be combined with fairness objectives or other alignment metrics to produce models that not only answer truthfully but also ethically. Future work might explore more nuanced reward structures (e.g., scaled by difficulty) or multi‚Äëturn dialogues. It may also be coupled with secret elicitation research to mitigate private information leakage by encouraging abstention when uncertain.\n",
    "\n",
    "MemGen ‚Äì Weaving Generative Latent Memory for Self‚ÄëEvolving Agents\n",
    "Watching: MemGen (paper/code)\n",
    "\n",
    "\n",
    "What problem does it solve?\n",
    "\n",
    "LLM agents often have limited memory: they either rewrite their parameters during fine‚Äëtuning (parametric memory) or query an external database (nonparametric memory). These approaches can be rigid or disconnected from reasoning. There‚Äôs a need for a dynamic, tightly coupled memory that the agent can generate and weave into its internal thought process.\n",
    "\n",
    "How does it solve the problem?\n",
    "\n",
    "MemGen introduces a generative latent memory system with two key components:\n",
    "\n",
    "Memory trigger: A module monitors the agent‚Äôs current reasoning state and decides when to recall memory. It detects moments requiring recall of past experiences or facts.\n",
    "\n",
    "Memory weaver: Once triggered, this module generates a sequence of latent tokens representing relevant memory content and injects them back into the model‚Äôs context. The memory is learned and represented in the model‚Äôs latent space, not as plain text.\n",
    "\n",
    "This system allows the agent to pause, generate internal memory, and then resume reasoning with that memory fused into its hidden state. The agent learns to allocate memory content across working, procedural, and planning memory types.\n",
    "\n",
    "Key findings\n",
    "\n",
    "Performance gains: On eight diverse benchmarks, MemGen outperforms existing memory‚Äëaugmented agents (ExpeL, AWM) by up to 38.22% and surpasses a strong GRPO baseline by up to 13.44%.\n",
    "\n",
    "Human‚Äëlike memory patterns: Without hand‚Äëcoding, MemGen agents spontaneously develop memory behaviors akin to planning memory, procedural memory, and working memory, reminiscent of human cognition.\n",
    "\n",
    "Cross‚Äëdomain generalization: The generative memory improves performance across multiple domains (math, programming, Q&A), suggesting it is broadly applicable.\n",
    "\n",
    "What‚Äôs next?\n",
    "\n",
    "MemGen could be integrated with search‚Äëbased training (e.g., DeepSearch) or exploration scaling (BroRL) to further boost reasoning. Researchers may also explore training the memory trigger and weaver jointly with RL to learn when and what to recall for optimal rewards. Finally, combining MemGen with biologically inspired architectures like Dragon Hatchling could yield agents with both brain‚Äëlike memory and network structure.\n",
    "\n",
    "Wrap‚ÄëUp ‚Äì Insights and Future Directions\n",
    "Across these papers we see a convergence of themes:\n",
    "\n",
    "Exploration is key: BroRL, DeepSearch, Knapsack RL, and Polychromic RL all emphasize different strategies for expanding and allocating exploration. By broadening rollouts, embedding search, optimizing budgets, or preserving multiple strategies, these works address RL‚Äôs tendency to get stuck.\n",
    "\n",
    "Architectural innovation: Thoughtbubbles shows that unsupervised modifications can let LLMs think in parallel, MemGen introduces generative latent memory, Dragon Hatchling offers a biologically plausible alternative to Transformers. These innovations move beyond simple scale‚Äëup, exploring new ways for models to process and store information.\n",
    "\n",
    "Data and generalization: Learning to See Before Seeing and Debunk SFT highlight the importance of training data quality and diversity over mere quantity. By curating reasoning‚Äëcentric texts or diverse prompts and CoT examples, we can unlock hidden capabilities in LLMs.\n",
    "\n",
    "Model auditing and alignment: Eliciting Secret Knowledge and TruthRL show that we must audit models for hidden knowledge and train them to be truthful and cautious.\n",
    "\n",
    "Going forward, combining these insights may yield next‚Äëgeneration agents: large, biologically inspired networks with generative memory and parallel reasoning, trained via RL with broad exploration, calibrated truthfulness, and diverse supervision. The future of AI will likely involve multidisciplinary innovations spanning neuroscience, optimization, and safety research. Stay tuned!\n",
    "\n",
    "\n",
    "Upgrade to paid\n",
    "‚ù§Ô∏è If you enjoyed this article, give it a like and share it with your peers.\n",
    "You're currently a free subscriber to LLM Watch. For the full experience, upgrade your subscription.\n",
    "\n",
    "Upgrade to paid\n",
    "\n",
    " \n",
    "Like\n",
    "Comment\n",
    "Restack\n",
    " \n",
    "¬© 2025 Pascal Biese\n",
    "Unsubscribe\n",
    "\n",
    "Get the appStart writing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Make the messages list\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ] # fill this in\n",
    "\n",
    "# Step 3: Call OpenAI\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages)\n",
    "\n",
    "# Step 4: print the result\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed9f14-b349-40e9-a42c-b367e77f8bda",
   "metadata": {},
   "source": [
    "## An extra exercise for those who enjoy web scraping\n",
    "\n",
    "You may notice that if you try `display_summary(\"https://openai.com\")` - it doesn't work! That's because OpenAI has a fancy website that uses Javascript. There are many ways around this that some of you might be familiar with. For example, Selenium is a hugely popular framework that runs a browser behind the scenes, renders the page, and allows you to query it. If you have experience with Selenium, Playwright or similar, then feel free to improve the Website class to use them. In the community-contributions folder, you'll find an example Selenium solution from a student (thank you!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab24dc-5f90-4570-b542-b0585aca3eb6",
   "metadata": {},
   "source": [
    "# Sharing your code\n",
    "\n",
    "I'd love it if you share your code afterwards so I can share it with others! You'll notice that some students have already made changes (including a Selenium implementation) which you will find in the community-contributions folder. If you'd like add your changes to that folder, submit a Pull Request with your new versions in that folder and I'll merge your changes.\n",
    "\n",
    "If you're not an expert with git (and I am not!) then GPT has given some nice instructions on how to submit a Pull Request. It's a bit of an involved process, but once you've done it once it's pretty clear. As a pro-tip: it's best if you clear the outputs of your Jupyter notebooks (Edit >> Clean outputs of all cells, and then Save) for clean notebooks.\n",
    "\n",
    "Here are good instructions courtesy of an AI friend:  \n",
    "https://chatgpt.com/share/677a9cb5-c64c-8012-99e0-e06e88afd293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4484fcf-8b39-4c3f-9674-37970ed71988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
