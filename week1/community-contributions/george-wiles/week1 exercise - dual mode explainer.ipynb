{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a83e63a9465ad10",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,\n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889141761f266c91",
   "metadata": {},
   "source": [
    "### Dual-Mode Explainer\n",
    "This notebook runs dual explainer runs two llm models side by side (html) and vertically (markdown) in response to technical/coding questions.\n",
    "* Allows simple variable toggles for controlling model configuration dynamically (eg temperature)\n",
    "* runs dual models side by side for comparison with some markdown to html trickery\n",
    "* provides two question template, coding and general technical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1699ee624f73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os          # OS utilities, environment variables, file paths\n",
    "import threading   # Run parallel threads for streaming (dual mode explainer)\n",
    "\n",
    "from IPython.display import HTML, Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "# dual model constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'gemma3:4b'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3781fa123ff74c4",
   "metadata": {},
   "source": [
    "### Configuration for models\n",
    "I want a set of default conifiguration that I can override at runtime for side by side comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2beed3c181ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # OpenAI\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    OPENAI_MODEL = MODEL_GPT\n",
    "    OPENAI_TEMPERATURE = 0.7\n",
    "\n",
    "    # Ollama\n",
    "    OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "    OLLAMA_MODEL = MODEL_LLAMA\n",
    "    OLLAMA_TEMPERATURE = 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc4a7c2f1999d9",
   "metadata": {},
   "source": [
    "### Shared Utilities for streaming and creating clients.\n",
    "LLM's produce markdown, which can be streamed, but in a linear format. However this presents a problem when trying to display LLM question/answers side by side, as markdown down not really have a concept of layout as per HTML/css.  So I have a few simple functions that allow LLM results to be chunked in parallel and written to an HTML table, with all the markdown goodness removed.\n",
    "\n",
    "The alternative would be to install a markdown -> html converter, or create a function to do that, but that seems a world of hurt.. htmls tags are open/close in nature and markdown is far simpler, which would required matching a markdown tag for open, and maybe a newline for close tag, but then you would hit #, ##, <h1></h1>, greedy regex, and now instead one problem you have many problems... I am sure an AI agent could eventually solve the algorithm without any unit tests... but nup... move on.. for this don't need formatting goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93876cdae7a3eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client(api_key=None, base_url=None, model=None, temperature=None):\n",
    "    \"\"\"\n",
    "    Create a client+model+params bundle as a dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"client\": OpenAI(api_key=api_key, base_url=base_url),\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "def get_stream(client_bundle, question: str):\n",
    "    return client_bundle[\"client\"].chat.completions.create(\n",
    "        model=client_bundle[\"model\"],\n",
    "        temperature=client_bundle[\"temperature\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a technical explainer.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "\n",
    "def stream_answer_into_table(client_bundle, question: str, column_id: str, display_handle):\n",
    "    stream = get_stream(client_bundle, question)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        html = markdown_to_html(response)\n",
    "        js = f\"\"\"\n",
    "        <script>\n",
    "        document.getElementById(\"{column_id}\").innerHTML = `{html}`;\n",
    "        </script>\n",
    "        \"\"\"\n",
    "        update_display(HTML(js), display_id=display_handle.display_id)\n",
    "\n",
    "    return response\n",
    "\n",
    "def stream_answer_markdown(client_bundle, question: str) -> str:\n",
    "    stream = stream = get_stream(client_bundle, question)\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def markdown_to_html(md: str) -> str:\n",
    "    html = md\n",
    "\n",
    "    # HEADINGS\n",
    "    html = re.sub(r'^#{3}\\s*(.*)', r'<h3>\\1</h3>', html, flags=re.MULTILINE)\n",
    "    html = re.sub(r'^#{2}\\s*(.*)', r'<h2>\\1</h2>', html, flags=re.MULTILINE)\n",
    "    html = re.sub(r'^#{1}\\s*(.*)', r'<h1>\\1</h1>', html, flags=re.MULTILINE)\n",
    "\n",
    "    # HORIZONTAL RULE (---)\n",
    "    html = re.sub(r'^\\s*---+\\s*$', r'<hr>', html, flags=re.MULTILINE)\n",
    "\n",
    "    # BOLD\n",
    "    html = re.sub(r'\\*\\*(.*?)\\*\\*', r'<b>\\1</b>', html)\n",
    "\n",
    "    # ITALIC\n",
    "    html = re.sub(r'\\*(.*?)\\*', r'<i>\\1</i>', html)\n",
    "\n",
    "    # INLINE CODE\n",
    "    html = re.sub(r'`(.*?)`', r'<code>\\1</code>', html)\n",
    "\n",
    "    # NEWLINES → <br>\n",
    "    html = html.replace('\\n', '<br>')\n",
    "\n",
    "    return html\n",
    "\n",
    "def stream_parallel_table(client_a, client_b, question):\n",
    "    # Display initial empty table\n",
    "    table_html = f\"\"\"\n",
    "    <table style=\"width:100%; border-collapse:collapse;\">\n",
    "        <tr>\n",
    "            <th>{client_a['model']}</th>\n",
    "            <th>{client_b['model']}</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td id=\"colA\" style=\"white-space:pre-wrap;\"></td>\n",
    "            <td id=\"colB\" style=\"white-space:pre-wrap;\"></td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    handle = display(HTML(table_html), display_id=True)\n",
    "\n",
    "    # Launch threads to stream each model into its column\n",
    "    thread_a = threading.Thread(target=stream_answer_into_table, args=(client_a, question, \"colA\", handle))\n",
    "    thread_b = threading.Thread(target=stream_answer_into_table, args=(client_b, question, \"colB\", handle))\n",
    "\n",
    "    thread_a.start()\n",
    "    thread_b.start()\n",
    "\n",
    "    thread_a.join()\n",
    "    thread_b.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905252cea97b4473",
   "metadata": {},
   "source": [
    "### Allow dynamic override of config for comparisons\n",
    "I would like to override specific properties like temperature dynamically, see what the results are etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac8620be3d039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_openai_client(config: Config, *, model=None, temperature=None):\n",
    "    return create_client(\n",
    "        api_key=config.OPENAI_API_KEY,\n",
    "        base_url=None,\n",
    "        model=model or config.OPENAI_MODEL,\n",
    "        temperature=temperature or config.OPENAI_TEMPERATURE\n",
    "    )\n",
    "\n",
    "def build_ollama_client(config: Config, *, model=None, temperature=None):\n",
    "    return create_client(\n",
    "        api_key=\"ollama\",\n",
    "        base_url=config.OLLAMA_BASE_URL,\n",
    "        model=model or config.OLLAMA_MODEL,\n",
    "        temperature=temperature or config.OLLAMA_TEMPERATURE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ac0de9f892f89",
   "metadata": {},
   "source": [
    "### A Coding specific question template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9f6b4853723c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"\"\"\n",
    "# Please explain what this code does and why:\n",
    "#     stream = client_bundle[\"client\"].chat.completions.create(\n",
    "#         model=client_bundle[\"model\"],\n",
    "#         temperature=client_bundle[\"temperature\"],\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a technical explainer.\"},\n",
    "#             {\"role\": \"user\", \"content\": question}\n",
    "#         ],\n",
    "#         stream=True\n",
    "# # \"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please modify the following code and rather than strip out markdown symbols, wrap symbol in open and close html equvelanets:\n",
    "    def strip_markdown(md: str) -> str:\n",
    "        symbols = [\"**\", \"*\", \"`\", \"###\", \"##\", \"#\"]\n",
    "        result = md\n",
    "        for sym in symbols:\n",
    "            result = result.replace(sym, \"\")\n",
    "        return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e25ee5c211357",
   "metadata": {},
   "source": [
    "### An Architectural and Concepts question template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908737d66dfe88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please explain what this concept is and why is it important:\n",
    "\"Setting a MODEL temperature for a specific coding question, within a coding / deterministic context, what effect does temperature variation have?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12679209a4316f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "overrideTemperature =  0.9 # no creativity\n",
    "openai_client = build_openai_client(config, temperature=overrideTemperature)\n",
    "ollama_client = build_ollama_client(config, temperature=overrideTemperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34534e46957b2211",
   "metadata": {},
   "source": [
    "### Run Ollama and OpenAI clients in parallel\n",
    "Compare results of different models side by side (dual mode explainer)\n",
    "Issues:\n",
    "* Displayed as html, therefor current implementation loses formatting (heading, bullet points) and leads to a lesser semantic understanding\n",
    "* Could retain or convert markdown -> html by introducing new libraries such as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7589b8e9aeb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display side by side in table format\n",
    "table_html = f\"\"\"\n",
    "<style>\n",
    "    h1, h2, h3, h4, h5, h6 {{\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 0;\n",
    "    }}\n",
    "\n",
    "    p {{\n",
    "        margin: 0;\n",
    "    }}\n",
    "\n",
    "    ul, ol {{\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 0;\n",
    "        padding-left: 20px;\n",
    "    }}\n",
    "</style>\n",
    "\n",
    "<table style=\"width:100%; border-collapse:collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:left; vertical-align:top;\"><h1>{openai_client[\"model\"]} - Temp: {openai_client[\"temperature\"]}</h1></th>\n",
    "    <th style=\"text-align:left; vertical-align:top;\"><h1>{ollama_client[\"model\"]} - Temp: {ollama_client[\"temperature\"]}</h1></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td id=\"colA\" style=\"white-space:pre-wrap; text-align:left; vertical-align:top;\"></td>\n",
    "    <td id=\"colB\" style=\"white-space:pre-wrap; text-align:left; vertical-align:top;\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "handle = display(HTML(table_html), display_id=True)\n",
    "\n",
    "stream_parallel_table(openai_client, ollama_client, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859054e42418ad35",
   "metadata": {},
   "source": [
    "### Run sequentially with streams as pure markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266522134db4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"# Model: {openai_client['model']} — Temp: {openai_client['temperature']}\"))\n",
    "stream_answer_markdown(openai_client, question)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "display(Markdown(f\"# Model: {ollama_client['model']} — Temp: {ollama_client['temperature']}\"))\n",
    "stream_answer_markdown(ollama_client, question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
