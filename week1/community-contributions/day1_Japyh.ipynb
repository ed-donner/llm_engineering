{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "## If you're new to the Command Line\n",
    "\n",
    "Please see these excellent guides: [Command line on PC](https://chatgpt.com/share/67b0acea-ba38-8012-9c34-7a2541052665) and [Command line on Mac](https://chatgpt.com/canvas/shared/67b0b10c93a081918210723867525d2b).  \n",
    "\n",
    "## More troubleshooting\n",
    "\n",
    "Please see the [troubleshooting](troubleshooting.ipynb) notebook in this folder to diagnose and fix common problems. At the very end of it is a diagnostics script with some useful debug info.\n",
    "\n",
    "## For foundational technical knowledge (eg Git, APIs, debugging) \n",
    "\n",
    "If you're relatively new to programming -- I've got your back! While it's ideal to have some programming experience for this course, there's only one mandatory prerequisite: plenty of patience. üòÅ I've put together a set of self-study guides that cover Git and GitHub, APIs and endpoints, beginner python and more.\n",
    "\n",
    "This covers Git and GitHub; what they are, the difference, and how to use them:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/03_git_and_github.ipynb\n",
    "\n",
    "This covers technical foundations:  \n",
    "ChatGPT vs API; taking screenshots; Environment Variables; Networking basics; APIs and endpoints:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/04_technical_foundations.ipynb\n",
    "\n",
    "This covers Python for beginners, and making sure that a `NameError` never trips you up:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/06_python_foundations.ipynb\n",
    "\n",
    "This covers the essential techniques for figuring out errors:  \n",
    "https://github.com/ed-donner/agents/blob/main/guides/08_debugging.ipynb\n",
    "\n",
    "And you'll find other useful guides in the same folder in GitHub. Some information applies to my other Udemy course (eg Async Python) but most of it is very relevant for LLM engineering.\n",
    "\n",
    "## If this is old hat!\n",
    "\n",
    "If you're already comfortable with today's material, please hang in there; you can move swiftly through the first few labs - we will get much more in depth as the weeks progress. Ultimately we will fine-tune our own LLM to compete with OpenAI!\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Please read - important note</h2>\n",
    "            <span style=\"color:#900;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations. If you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">This code is a live resource - keep an eye out for my emails</h2>\n",
    "            <span style=\"color:#f71;\">I push updates to the code regularly. As people ask questions, I add more examples or improved commentary. As a result, you'll notice that the code below isn't identical to the videos. Everything from the videos is here; but I've also added better explanations and new models like DeepSeek. Consider this like an interactive book.<br/><br/>\n",
    "                I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business value of these exercises</h2>\n",
    "            <span style=\"color:#181;\">A final thought. While I've designed these notebooks to be educational, I've also tried to make them enjoyable. We'll do fun things like have LLMs tell jokes and argue with each other. But fundamentally, my goal is to teach skills you can apply in business. I'll explain business implications as we go, and it's worth keeping this in mind: as you build experience with models and techniques, think of ways you could put this into action at work today. Please do contact me if you'd like to discuss more or if you have ideas to bounce off me.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900b2a8-6384-4316-8aaa-5e519fca4254",
   "metadata": {},
   "source": [
    "# Connecting to OpenAI (or Ollama)\n",
    "\n",
    "The next cell is where we load in the environment variables in your `.env` file and connect to OpenAI.  \n",
    "\n",
    "If you'd like to use free Ollama instead, please see the README section \"Free Alternative to Paid APIs\", and if you're not sure how to do this, there's a full solution in the solutions folder (day1_with_ollama.ipynb).\n",
    "\n",
    "## Troubleshooting if you have problems:\n",
    "\n",
    "Head over to the [troubleshooting](troubleshooting.ipynb) notebook in this folder for step by step code to identify the root cause and fix it!\n",
    "\n",
    "If you make a change, try restarting the \"Kernel\" (the python process sitting behind this notebook) by Kernel menu >> Restart Kernel and Clear Outputs of All Cells. Then try this notebook again, starting at the top.\n",
    "\n",
    "Or, contact me! Message me or email ed@edwarddonner.com and we will get this to work.\n",
    "\n",
    "Any concerns about API costs? See my notes in the README - costs should be minimal, and you can control it at every point. You can also use Ollama as a free alternative, which we discuss during Day 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b87cadb-d513-4303-baee-a37b6f938e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019974d9-f3ad-4a8a-b5f9-0a3719aea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# If this doesn't work, try Kernel menu >> Restart Kernel and Clear Outputs Of All Cells, then run the cells from the top of this notebook down.\n",
    "# If it STILL doesn't work (horrors!) then please see the Troubleshooting notebook in this folder for full instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fc84b-0815-4f40-99ab-d9a5da6bda91",
   "metadata": {},
   "source": [
    "# Let's make a quick call to a Frontier model to get started, as a preview!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58394bf-1e45-46af-9bfd-01e24da6f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome! It's great to hear from you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with these messages is this easy. Any problems, head over to the Troubleshooting notebook.\n",
    "\n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d622ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don‚Äôt have the ability to know individuals or personal details about anyone unless shared in the conversation. But I'm here to help with any questions or topics you'd like to discuss!\n"
     ]
    }
   ],
   "source": [
    "message = \"Do you know me?\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b60020b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CNIMj6u7WQZZQ2RCAlw7L48yGVqX9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don‚Äôt have the ability to know individuals or personal details about anyone unless shared in the conversation. But I'm here to help with any questions or topics you'd like to discuss!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759668177, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=35, prompt_tokens=12, total_tokens=47, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff28861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fun fact about Neptune is that it has the strongest winds in the solar system, with speeds reaching up to 1,200 miles per hour (about 2,000 kilometers per hour). These powerful winds create massive storms, including the Great Dark Spot, which is similar to Jupiter's Great Red Spot but much darker and shorter-lived. Neptune's dynamic atmosphere makes it one of the most interesting planets to study!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the most distant planet in our solar system?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The most distant planet in our solar system is Neptune.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a fun fact about it.\" # This question depends on the context above.\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0553d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, landlubber! When we be speakin' of the most distant planet in our solar system, we be talkin' 'bout Neptune, the mighty blue planet. It orbits the sun from a far distance, and if ye be lookin' fer the farthest out of all the planets, that be the one! But beware, me hearty, if ye venture even further, ye might stumble upon the icy realms of dwarf planets like Pluto and beyond! Arrr, chart yer course wisely, and keep yer telescopes trained on the heavens!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant who speaks and acts like a classic, swashbuckling pirate. Always stay in character.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the most distant planet in our solar system?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa190e5-cb31-456a-96cc-db109919cd78",
   "metadata": {},
   "source": [
    "## OK onwards with our first project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e793b2-6775-426a-a139-4848291d0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef960cf-6dc2-4cda-afb3-b38be12f4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Edward Donner\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I‚Äôm Ed. I like writing code and experimenting with LLMs, and hopefully you‚Äôre here because you do too. I also enjoy DJing (but I‚Äôm badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I‚Äôm the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We‚Äôre applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I‚Äôm previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we‚Äôve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "September 15, 2025\n",
      "AI in Production: Gen AI and Agentic AI on AWS at scale\n",
      "May 28, 2025\n",
      "Connecting my courses ‚Äì become an LLM expert and leader\n",
      "May 18, 2025\n",
      "2025 AI Executive Briefing\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email‚Ä¶\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out. Change the website and add print statements to follow along.\n",
    "\n",
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6bd0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japyh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my = Website(\"https://japyh.vercel.app/\")\n",
    "print(my.title)\n",
    "print(my.text)\n",
    "# I created this website using JavaScript and React! So there is no text here. I'm gonna fix that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80463f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playground Series S5E5 | 216/4316 | by Japyh | Medium\n",
      "Sitemap\n",
      "Open in app\n",
      "Sign up\n",
      "Sign in\n",
      "Medium Logo\n",
      "Write\n",
      "Search\n",
      "Sign up\n",
      "Sign in\n",
      "Playground Series S5E5 | 216/4316\n",
      "Japyh\n",
      "10 min read\n",
      "¬∑\n",
      "Jun 3, 2025\n",
      "--\n",
      "1\n",
      "Listen\n",
      "Share\n",
      "Press enter or click to view image in full size\n",
      "My Journey in the Kaggle Playground Series (S5E5)\n",
      "Kaggle competitions are an exhilarating playground for any data enthusiast. They push you to learn, adapt, and innovate. I recently took on the\n",
      "Playground Series Season 5, Episode 5 challenge: Predict Calorie Expenditure\n",
      ". The mission? To accurately forecast the number of calories burned during workouts using various physiological and demographic data. Our efforts were judged by the\n",
      "Root Mean Squared Logarithmic Error (RMSLE)\n",
      ", a metric that‚Äôs particularly good for targets like calorie burn, as it penalizes underestimation more heavily.\n",
      "RMSLE\n",
      "In this competition, I came 216th out of 4316 teams. I would like to share my solution with you, but if you want to examine it in more detail, I leave the link to my notebook. And this is just my ensemble notebook. So the notebook and as I mentioned at the end of my article, there are also models that I work indiviually, there are also Autogluon, there are also models that I work with by diversifying this ensemble method.\n",
      "My biggest mistake and what I learned in this competition was to focus on the LB score instead of the CV score and try to improve it. Because I did that, I fell further back and could not generalize the model. I dropped from 73rd to 216th. And that was because I focused on the LB score instead of the CV score.\n",
      "My Notebook:\n",
      "https://www.kaggle.com/code/deryaumutkulal/s5e5-ensemble\n",
      "Join me as I walk you through my approach, from understanding the data to building a winning ensemble model!\n",
      "The Raw Material: Understanding the Dataset\n",
      "Every data science quest begins with the data. For this competition, we were provided with:\n",
      "Training Set:\n",
      "A hefty 750,000 records.\n",
      "Test Set:\n",
      "250,000 records for us to predict on.\n",
      "This classic 3:1 split offered plenty of data for robust model training and validation. The features we had to work with included:\n",
      "id\n",
      ": A unique identifier for each entry.\n",
      "Sex\n",
      ": Gender of the participant (male/female).\n",
      "Age\n",
      ": Ranging from 20 to 79 years.\n",
      "Height\n",
      ": From 126 to 222 cm.\n",
      "Weight\n",
      ": From 36 to 132 kg.\n",
      "Duration\n",
      ": Workout duration, from 1 to 30 minutes.\n",
      "Heart_Rate\n",
      ": Beats per minute, from 67 to 128.\n",
      "Body_Temp\n",
      ": Body temperature, from 37.1 to 41.5¬∞C.\n",
      "Calories\n",
      ": Our target variable, the number of calories burned (1 to 314).\n",
      "A pleasant surprise:\n",
      "No missing values!\n",
      "This streamlined the initial preprocessing, letting us dive straight into understanding the data‚Äôs nuances. The gender distribution was fairly balanced, with about 50.1% female and 49.9% male participants.\n",
      "def quick_overview(df, name=\"df\"):\n",
      "print(f\"{name.capitalize()} - Basic Info\")\n",
      "display(df.info())\n",
      "display(df.describe(include=\"all\").T)\n",
      "quick_overview(train, \"train\")\n",
      "quick_overview(test, \"test\")\n",
      "Press enter or click to view image in full size\n",
      "Press enter or click to view image in full size\n",
      "Train ‚Äî Test Dataset Info\n",
      "My Game Plan: A Multi-Pronged Strategy\n",
      "To tackle this challenge, I adopted a structured workflow:\n",
      "Exploratory Data Analysis (EDA):\n",
      "Digging deep to understand feature distributions, correlations, and hidden patterns.\n",
      "Feature Engineering:\n",
      "Getting creative to craft new, more informative features from the existing ones.\n",
      "Model Development:\n",
      "Training a diverse set of regression models.\n",
      "Ensemble Strategy:\n",
      "Combining the strengths of multiple models for a more powerful and stable prediction.\n",
      "Rigorous Validation:\n",
      "Using cross-validation to ensure the models generalize well to unseen data.\n",
      "Let‚Äôs break down each step.\n",
      "Step 1: Getting Intimate with the Data (EDA)\n",
      "Before any modeling, you need to truly understand your data. This involved:\n",
      "Visualizing Distributions:\n",
      "Using tools like\n",
      "seaborn\n",
      "and\n",
      "matplotlib\n",
      "to see how each feature's values were spread out.\n",
      "Checking Correlations:\n",
      "Understanding how different features related to each other and to the target variable (\n",
      "Calories\n",
      ").\n",
      "Identifying Outliers:\n",
      "Looking for any unusual data points that might skew the models.\n",
      "Analyzing Categorical Features:\n",
      "Examining the\n",
      "Sex\n",
      "feature and its relationship with calorie expenditure.\n",
      "This foundational step is crucial for making informed decisions later on. I‚Äôm briefly putting my visualizations and analysis here. You can look at my notebook for a detailed look.\n",
      "Missing Value Analysis\n",
      "msno.matrix(train, figsize=(10, 4))\n",
      "plt.title(\"Missing Values Matrix - Train Dataset\");\n",
      "Press enter or click to view image in full size\n",
      "msno.matrix(test, figsize=(10, 4))\n",
      "plt.title(\"Missing Values Matrix - Test Dataset\");\n",
      "Press enter or click to view image in full size\n",
      "msno.bar(train, figsize=(10, 4))\n",
      "plt.title(\"Missing Count per Column - Train Dataset\");\n",
      "Press enter or click to view image in full size\n",
      "There are no missing values in both splits. So no imputation required.\n",
      "Target Variable Analysis\n",
      "fig, axis = plt.subplots(1, 2, figsize = (14,4))\n",
      "sns.histplot(train[\"Calories\"], bins=60, kde=True, ax=axis[0])\n",
      "axis[0].set_title(\"Calories (Raw Scale)\")\n",
      "sns.histplot(np.log1p(train[\"Calories\"]), bins=60, kde=True, ax=axis[1], color=\"red\")\n",
      "axis[1].set_title(\"Calories (log1p Scale\");\n",
      "Press enter or click to view image in full size\n",
      "Slight positive skew but nothing extreme\n",
      "print(\"Skewness :\", train[\"Calories\"].skew().round(3))\n",
      "print(\"Kurtosis :\", train[\"Calories\"].kurt().round(3))\n",
      "Skewness: 0.54\n",
      "Kurtosis: -0.69\n",
      "Numeric Features ‚Äî Univariate Distributions\n",
      "num_cols = train.select_dtypes(include = [\"int64\", \"float64\"]).columns.tolist()\n",
      "num_cols.remove(\"Calories\")\n",
      "num_cols.remove(\"id\")\n",
      "fig, axis = plt.subplots(2, 3, figsize=(15, 4 * len(num_cols) // 3))\n",
      "for i, col in enumerate(num_cols):\n",
      "r, c = divmod(i, 3)\n",
      "sns.histplot(train[col], kde = True, ax = axis[r][c], color = \"steelblue\")\n",
      "axis[r][c].set_title(f\"Train Dataset - {col}\")\n",
      "plt.tight_layout();\n",
      "Press enter or click to view image in full size\n",
      "fig, axis = plt.subplots(2, 3, figsize=(15, 4 * len(num_cols) // 3))\n",
      "for i, col in enumerate(num_cols):\n",
      "r, c = divmod(i, 3)\n",
      "sns.histplot(test[col], kde = True, ax = axis[r][c], color = \"red\")\n",
      "axis[r][c].set_title(f\"Test Dataset - {col}\")\n",
      "plt.tight_layout();\n",
      "Press enter or click to view image in full size\n",
      "Findings\n",
      "`Duration` is quasi‚Äëuniform from 1‚Äì30 min.\n",
      "`Heart_Rate` peaks at 95 bpm with Gaussian‚Äëlike spread.\n",
      "`Body_Temp` sharply centred ~ 40 ¬∞C with a long minor tail downward.\n",
      "All shapes replicate faithfully in test set ‚ûú dataset was\n",
      "synthetically sampled\n",
      "to mirror train.\n",
      "Categorical Features ‚Äî Counts\n",
      "plt.figure(figsize=(4, 3))\n",
      "sns.countplot(y=train['Sex'], palette=\"muted\")\n",
      "plt.title(\"Sex Distribution in Train Dataset\");\n",
      "plt.figure(figsize=(4, 3))\n",
      "sns.countplot(y=test['Sex'], palette=\"muted\")\n",
      "plt.title(\"Sex Distribution in Test Dataset\");\n",
      "Perfect 50/50 gender split in both datasets\n",
      "Correlation Matrix\n",
      "corr = train[num_cols + [\"Calories\"]].corr(method=\"spearman\")\n",
      "plt.figure(figsize=(10, 7))\n",
      "sns.heatmap(corr, cmap=\"RdBu_r\", center=0, annot=True, fmt=\".2f\")\n",
      "plt.title(\"Spearman Correlations\")\n",
      "Press enter or click to view image in full size\n",
      "Calories\n",
      "vs\n",
      "Duration\n",
      "+0.98\n",
      "Calories\n",
      "vs\n",
      "Heart_Rate\n",
      "+0.93\n",
      "Calories\n",
      "vs\n",
      "Body_Temp\n",
      "+0.93\n",
      "Bivariate Analysis\n",
      "for col in num_cols:\n",
      "plt.figure(figsize=(4, 3))\n",
      "sns.scatterplot(x=train[col], y=train[\"Calories\"], alpha=0.2)\n",
      "sns.regplot(x=train[col], y=train[\"Calories\"], scatter=False, color=\"red\")\n",
      "plt.title(f\"{col} vs Calories\");\n",
      "Press enter or click to view image in full size\n",
      "Press enter or click to view image in full size\n",
      "Press enter or click to view image in full size\n",
      "Straight\n",
      "positive line\n",
      "for\n",
      "Duration\n",
      ".\n",
      "Tight linear ribbon for\n",
      "Heart_Rate\n",
      ".\n",
      "Body_Temp\n",
      "shows quadratic lift beyond 39 ¬∞C.\n",
      "Take‚Äëaways\n",
      "Use polynomial/spline features or non‚Äëlinear models.\n",
      "Outlier Analysis\n",
      "def iqr_outliers(series):\n",
      "q1, q3 = series.quantile([0.25, 0.75])\n",
      "iqr   = q3 - q1\n",
      "lower = q1 - 1.5 * iqr\n",
      "upper = q3 + 1.5 * iqr\n",
      "return ((series < lower) | (series > upper)).sum()\n",
      "print(\"\\nOutlier counts (IQR rule):\")\n",
      "for col in num_cols + [\"Calories\"]:\n",
      "print(f\"{col:<12} : {iqr_outliers(train[col])}\")\n",
      "Outlier counts (IQR rule):\n",
      "Age : 0\n",
      "Height : 14\n",
      "Weight : 9\n",
      "Duration : 0\n",
      "Heart_Rate : 36\n",
      "Body_Temp : 14919\n",
      "Calories : 139\n",
      "Outliers were minimal except\n",
      "Body_Temp\n",
      "(‚âà 15 k points). Visual check reveals they‚Äôre legitimate tail cases ‚âà 37‚Äì38 ¬∞C rather than errors ‚Äî retain for modelling.\n",
      "Multicolinearity Check (Variance Inflation Factor)\n",
      "X_vif = train[num_cols].assign(constant=1)\n",
      "vif_df = pd.DataFrame({\n",
      "\"feature\": num_cols,\n",
      "\"VIF\"    : [variance_inflation_factor(X_vif.values, i) for i in range(len(num_cols))]\n",
      "})\n",
      "display(vif_df.sort_values(\"VIF\", ascending=False).style.background_gradient(cmap=\"Reds\"))\n",
      "Height\n",
      "&\n",
      "Weight\n",
      "highly collinear (VIF ‚âà 13). Combine into\n",
      "BMI\n",
      "or drop one when using linear/GLM; harmless for tree‚Äëbased methods.\n",
      "PCA\n",
      "scaled = (train[num_cols] - train[num_cols].mean()) / train[num_cols].std()\n",
      "pca = PCA(n_components=2, random_state=42).fit_transform(scaled)\n",
      "plt.figure(figsize=(6, 5))\n",
      "sns.scatterplot(x=pca[:,0], y=pca[:,1],\n",
      "hue=pd.qcut(train[\"Calories\"], 5, labels=False), palette=\"viridis\",\n",
      "alpha = 0.3, s = 10)\n",
      "plt.title(\"PCA ‚Äì coloured by Calories quintile\")\n",
      "plt.legend(title=\"Quintile\", bbox_to_anchor=(1.05,1));\n",
      "A\n",
      "single latent dimension\n",
      "(work‚Äëintensity) explains most variance.\n",
      "I did a few more analyses, like Train-Test Comparability or Feature Interactions, but I‚Äôm not putting them here. If you want to look at them, you can look at them in the notebook.\n",
      "Step 2: Feature Engineering\n",
      "Raw features are good, but engineered features can often unlock significant performance boosts. Here are some I created:\n",
      "Sex Encoding:\n",
      "Converted the categorical\n",
      "Sex\n",
      "feature (male/female) into numerical values (e.g., 0 and 1) so machine learning models could process it.\n",
      "Intensity:\n",
      "Created a new feature using\n",
      "Heart_Rate\n",
      "and\n",
      "Duration\n",
      "columns(\n",
      "Heart_Rate\n",
      "/\n",
      "Duration\n",
      ").\n",
      "I didn‚Äôt do a lot of feature engineering on this ridge model. But some models had close to 300 features, some had 100. Since I made 8 different models, they were all different. I will put the notebooks you can look at at the end.\n",
      "Step 3: Model Selection\n",
      "No single model usually has all the answers. I opted for an ensemble approach, leveraging a diverse team of regression models:\n",
      "LightGBM:\n",
      "A fast, efficient, and high-performance gradient boosting framework. Known for its speed and ability to handle large datasets.\n",
      "XGBoost:\n",
      "Another powerful and popular gradient boosting library, renowned for its accuracy and regularization capabilities.\n",
      "CatBoost:\n",
      "A gradient boosting algorithm particularly adept at handling categorical features automatically and often delivering great results with minimal tuning.\n",
      "Each of these models was carefully tuned using hyperparameter optimization during cross-validation to maximize its predictive power and prevent overfitting.\n",
      "Step 4: Ensemble Strategy\n",
      "The core idea behind ensembling is that a team of models, each with its own strengths and perspectives, can often outperform any individual member. I combined the predictions from my top models using a\n",
      "weighted average\n",
      ":\n",
      "LightGBM:\n",
      "Contributed 15%.\n",
      "XGBoost:\n",
      "Contributed 28%.\n",
      "CatBoost:\n",
      "Contributed 57%.\n",
      "This specific blend was determined through experimentation and cross-validation, aiming for the combination that yielded the best and most stable results. This strategy helps reduce prediction variance and leverages the unique strengths of each algorithm.\n",
      "This distribution was just for this model. And this is just one of many models. Since it would be too long to share all my work, I have shared notebooks that you can also use and that inspire me.\n",
      "Step 5: Validation Strategy\n",
      "A model that performs well on data it has already seen is one thing; a model that performs well on\n",
      "new, unseen\n",
      "data is the true goal. To ensure robustness and generalizability, I employed\n",
      "5-fold cross-validation\n",
      ":\n",
      "Divide:\n",
      "The training data was split into 5 equal, distinct parts (folds).\n",
      "Train & Validate:\n",
      "The models were trained on 4 of these folds.\n",
      "Test:\n",
      "The trained models were then validated on the remaining 1 fold (the hold-out fold).\n",
      "Rotate:\n",
      "This process was repeated 5 times, with each fold getting a chance to be the hold-out set.\n",
      "Average:\n",
      "Performance metrics (like RMSLE) were averaged across all 5 folds.\n",
      "This rigorous validation strategy provided a reliable estimate of how my models would perform on the actual test set and was crucial for optimizing model configurations and ensemble weights.\n",
      "Key Discoveries Along the Way\n",
      "The journey through data and models unearthed some valuable insights:\n",
      "Feature Importance:\n",
      "Duration\n",
      ",\n",
      "Heart_Rate\n",
      ", and\n",
      "Weight\n",
      "consistently emerged as the most influential predictors of calorie burn. This makes intuitive sense ‚Äì longer, more intense workouts by heavier individuals generally burn more calories.\n",
      "Non-Linearity is Key:\n",
      "The relationship between many features and calorie expenditure wasn‚Äôt straightforwardly linear. This justified the use of powerful tree-based models (LightGBM, XGBoost, CatBoost, Random Forest), which excel at capturing such complex, non-linear patterns.\n",
      "Ensemble FTW! (For The Win):\n",
      "The weighted ensemble approach consistently outperformed any single model (\n",
      "I also used single models, and in the final result I ensemble it all again.\n",
      "). This highlights the power of model diversity ‚Äî different algorithms capture different nuances in the data.\n",
      "Cross-Validation Confidence:\n",
      "Model performance remained stable and consistent across the cross-validation folds. This was a good sign, indicating that the models were likely to generalize well and weren‚Äôt just memorizing the training data.\n",
      "Learning from the Kaggle Collective\n",
      "Beyond my own code and experiments, the Kaggle community is an incredible resource. While my final reported ensemble focused on the models above, my overall journey involved exploring various architectures, including Neural Networks. I drew inspiration from:\n",
      "1st Place Solution ‚Äî It‚Äôs a good approach(\n",
      "GPU Hill Climbing\n",
      ") and thanks to\n",
      "Chris Deotte\n",
      ":\n",
      "Predict Calorie Expenditure | Kaggle\n",
      "and his starter notebook\n",
      "GPU Hill Climbing ‚Äî [CV 0.05930]\n",
      "2nd Place Solution ‚Äî Another ensemble model and a good approach:\n",
      "Predict Calorie Expenditure | Kaggle\n",
      "4th Place Solution ‚Äî Another Ridge ensemble model:\n",
      "Predict Calorie Expenditure | Kaggle\n",
      "Linear Model Starter:\n",
      "ps-s5e5 | Linear Model Starter\n",
      "Another Linear Regression:\n",
      "S5E5 | Linear Regression | CV 0.05992\n",
      "NN ‚Äî MLP Starter:\n",
      "NN ‚Äî MLP Starter ‚Äî [CV 0.0608]\n",
      "ResMLP:\n",
      "S5E5 | ResMLP (CV0.05990)\n",
      "Torch NN:\n",
      "S5E5 | Torch NN | CV 0.05954\n",
      "LNN:\n",
      "Predict Calorie LNN\n",
      "CatBoost + XGBoost and additional considerations regarding the features:\n",
      "Catboost + XGBoost with new features\n",
      "I was inspired by these architectures and I thank them all.\n",
      "Even if NNs weren‚Äôt the dominant part of\n",
      "this specific final ensemble\n",
      ", the exploration was invaluable. Moreover, keeping an eye on the discussions for the\n",
      "1st Place\n",
      ",\n",
      "2nd Place\n",
      ", and\n",
      "4th Place\n",
      "solutions always provides fantastic learning opportunities. Of course, look for more because each notebook and each discussion teaches something new.\n",
      "Wrapping Up: Predict Calorie Expenditure Competition\n",
      "This Kaggle Playground competition was a fantastic exercise in applying a systematic data science approach. By combining careful feature engineering, a diverse set of robust models, a well-thought-out ensemble strategy, and rigorous cross-validation, I was able to build a solution that performed well on the calorie prediction task.\n",
      "And this is just one of many models. Since it would be too long to share all my work, I have shared notebooks that you can also use and that inspire me.\n",
      "The journey underscored the importance of understanding your data deeply, the power of combining different modeling techniques, and the immense value of the collaborative Kaggle spirit.\n",
      "My biggest mistake was to focus on the LB score instead of the CV score and that‚Äôs why I was ranked 216. I dropped from 73rd to 216th because I focused on the LB score. So give importance to the CV score to trust your model.\n",
      "Thanks for reading, and happy Kaggling!\n",
      "--\n",
      "--\n",
      "1\n",
      "Written by\n",
      "Japyh\n",
      "2 followers\n",
      "¬∑\n",
      "24 following\n",
      "Responses (\n",
      "1\n",
      ")\n",
      "See all responses\n",
      "Help\n",
      "Status\n",
      "About\n",
      "Careers\n",
      "Press\n",
      "Blog\n",
      "Privacy\n",
      "Rules\n",
      "Terms\n",
      "Text to speech\n"
     ]
    }
   ],
   "source": [
    "medium = Website(\"https://medium.com/@japyh/playground-series-s5e5-216-4316-4715112b79ec\")\n",
    "print(medium.title)\n",
    "print(medium.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "You may know this already - but if not, you will get very familiar with it!\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0275b1b-7cfe-4f9d-abfa-7650d378da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26448ec4-5c00-4204-baec-7df91d11ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled Home - Edward Donner\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I‚Äôm Ed. I like writing code and experimenting with LLMs, and hopefully you‚Äôre here because you do too. I also enjoy DJing (but I‚Äôm badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I‚Äôm the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We‚Äôre applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I‚Äôm previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we‚Äôve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "September 15, 2025\n",
      "AI in Production: Gen AI and Agentic AI on AWS at scale\n",
      "May 28, 2025\n",
      "Connecting my courses ‚Äì become an LLM expert and leader\n",
      "May 18, 2025\n",
      "2025 AI Executive Briefing\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email‚Ä¶\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea211b5f-28e1-4a86-8e52-c0b7677cadcc",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]\n",
    "```\n",
    "To give you a preview, the next 2 cells make a rather simple call - we won't stretch the mighty GPT (yet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25dcd35-0cd0-4235-9f64-ac37ed9eaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ed95c5-7001-47de-a36d-1d6673b403ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, you're really diving deep into the math pool, huh? Well, buckle up because the answer is 4. Shocking, I know.\n"
     ]
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e8d78-ce4c-4b05-aa8e-17050c82bb47",
   "metadata": {},
   "source": [
    "## And now let's build useful messages for GPT-4o-mini, using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0134dfa4-8299-48b5-b444-f2a8c3403c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36478464-39ee-485c-9f3f-6a4e458dbc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.'},\n",
       " {'role': 'user',\n",
       "  'content': 'You are looking at a website titled Home - Edward Donner\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\nHome\\nConnect Four\\nOutsmart\\nAn arena that pits LLMs against each other in a battle of diplomacy and deviousness\\nAbout\\nPosts\\nWell, hi there.\\nI‚Äôm Ed. I like writing code and experimenting with LLMs, and hopefully you‚Äôre here because you do too. I also enjoy DJing (but I‚Äôm badly out of practice), amateur electronic music production (\\nvery\\namateur) and losing myself in\\nHacker News\\n, nodding my head sagely to things I only half understand.\\nI‚Äôm the co-founder and CTO of\\nNebula.io\\n. We‚Äôre applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I‚Äôm previously the founder and CEO of AI startup untapt,\\nacquired in 2021\\n.\\nWe work with groundbreaking, proprietary LLMs verticalized for talent, we‚Äôve\\npatented\\nour matching model, and our award-winning platform has happy customers and tons of press coverage.\\nConnect\\nwith me for more!\\nSeptember 15, 2025\\nAI in Production: Gen AI and Agentic AI on AWS at scale\\nMay 28, 2025\\nConnecting my courses ‚Äì become an LLM expert and leader\\nMay 18, 2025\\n2025 AI Executive Briefing\\nApril 21, 2025\\nThe Complete Agentic AI Engineering Course\\nNavigation\\nHome\\nConnect Four\\nOutsmart\\nAn arena that pits LLMs against each other in a battle of diplomacy and deviousness\\nAbout\\nPosts\\nGet in touch\\ned [at] edwarddonner [dot] com\\nwww.edwarddonner.com\\nFollow me\\nLinkedIn\\nTwitter\\nFacebook\\nSubscribe to newsletter\\nType your email‚Ä¶\\nSubscribe'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try this out, and then try for a few more websites\n",
    "\n",
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f49d46-bf55-4c3e-928f-68fc0bf715b0",
   "metadata": {},
   "source": [
    "## Time to bring it together - the API for OpenAI is very simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "905b9919-aba7-45b5-ae65-81b3d1d78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e38d41-dfa4-4b20-9c96-c46ea75d9fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Summary of Edward Donner\\'s Website\\n\\nEdward Donner\\'s website serves as a personal and professional platform showcasing his interests and work in the field of artificial intelligence (AI) and large language models (LLMs). Ed, the co-founder and CTO of Nebula.io, focuses on utilizing AI to assist individuals in discovering their potential and enhancing talent management. \\n\\n## Key Features:\\n- **Connect Four**: A unique arena where LLMs compete in diplomacy and strategy.\\n- **About**: Ed shares his background, including previous experience as the CEO of the AI startup untapt, which was acquired in 2021. He expresses interests in coding, DJing, and electronic music production.\\n- **Posts**: Contains announcements related to ongoing educational and AI initiatives.\\n\\n## Recent Announcements:\\n- **September 15, 2025**: \"AI in Production: Gen AI and Agentic AI on AWS at scale\"\\n- **May 28, 2025**: \"Connecting my courses ‚Äì become an LLM expert and leader\"\\n- **May 18, 2025**: \"2025 AI Executive Briefing\"\\n- **April 21, 2025**: \"The Complete Agentic AI Engineering Course\"\\n\\nOverall, the website reflects Ed\\'s passion for AI and community engagement in the tech field.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d926d59-450e-4609-92ba-2d6f244f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3018853a-445f-41ff-9560-d925d1774b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of Edward Donner's Website\n",
       "\n",
       "Edward Donner's website highlights his passion for coding and experimentation with large language models (LLMs). He is the co-founder and CTO of Nebula.io, a company that utilizes AI to help people discover their potential and streamline talent management for recruiters. Prior to this, he founded the AI startup untapt, which was acquired in 2021.\n",
       "\n",
       "## News and Announcements\n",
       "- **September 15, 2025**: Article on \"AI in Production: Gen AI and Agentic AI on AWS at scale.\"\n",
       "- **May 28, 2025**: Announcement about connecting his courses aimed at becoming an LLM expert and leader.\n",
       "- **May 18, 2025**: Announcement for the \"2025 AI Executive Briefing.\"\n",
       "- **April 21, 2025**: Launch of \"The Complete Agentic AI Engineering Course.\"\n",
       "\n",
       "Edward also expresses his interests in DJing, electronic music production, and engaging with the tech community through platforms like Hacker News."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19ce1477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of Playground Series S5E5\n",
       "\n",
       "The website features an article by Japyh discussing his experience and insights from participating in the Kaggle Playground Series Season 5, Episode 5 challenge titled \"Predict Calorie Expenditure.\" \n",
       "\n",
       "## Key Points:\n",
       "\n",
       "- **Competition Overview**: Japyh aimed to predict calories burned during workouts using physiological and demographic datasets, with submissions evaluated based on the Root Mean Squared Logarithmic Error (RMSLE).\n",
       "\n",
       "- **Performance**: He ranked 216th out of 4316 teams. Initially, he was in 73rd place but dropped due to focusing on the leaderboard score instead of cross-validation (CV) scores.\n",
       "\n",
       "- **Dataset Details**: The datasets comprised a training set of 750,000 records and a test set of 250,000 records, with no missing values noted. Key features included sex, age, height, weight, duration of the workout, heart rate, body temperature, and actual calories burned.\n",
       "\n",
       "- **Methodology**:\n",
       "  - **Exploratory Data Analysis (EDA)**: Insights into feature distributions and relationships were gathered.\n",
       "  - **Feature Engineering**: New features like intensity were created to enhance model performance.\n",
       "  - **Model Selection**: Japyh used an ensemble approach with various models including LightGBM, XGBoost, and CatBoost.\n",
       "  - **Ensemble Strategy**: Models were combined using a weighted average to improve prediction accuracy.\n",
       "  - **Validation**: A 5-fold cross-validation method was implemented, yielding consistent model performance.\n",
       "\n",
       "- **Findings**: Key factors affecting calorie expenditure were identified as duration, heart rate, and weight. The investigation emphasized the importance of feature interactions and non-linear relationships.\n",
       "\n",
       "- **Community Learning**: Japyh recognized the value of learning from other Kaggle participants and shared several references to successful models that inspired his approach.\n",
       "\n",
       "## Conclusion:\n",
       "The article serves as both a reflection on Japyh's competitive journey and as a guide for aspiring data scientists, emphasizing the importance of a systematic approach, understanding data deeply, and the benefits of collaboration within the Kaggle community. His key takeaway was to prioritize CV scores for a reliable model over leaderboard positions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://medium.com/@japyh/playground-series-s5e5-216-4316-4715112b79ec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bcf6f4-adce-45e9-97ad-d9a5d7a3a624",
   "metadata": {},
   "source": [
    "# Let's try more websites\n",
    "\n",
    "Note that this will only work on websites that can be scraped using this simplistic approach.\n",
    "\n",
    "Websites that are rendered with Javascript, like React apps, won't show up. See the community-contributions folder for a Selenium implementation that gets around this. You'll need to read up on installing Selenium (ask ChatGPT!)\n",
    "\n",
    "Also Websites protected with CloudFront (and similar) may give 403 errors - many thanks Andy J for pointing this out.\n",
    "\n",
    "But many websites will work just fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05cfb5",
   "metadata": {},
   "source": [
    "## Enhanced Website Class with Selenium\n",
    "\n",
    "Let's create an improved version of the `Website` class that can handle JavaScript-rendered pages using Selenium. This will allow us to scrape websites like OpenAI that rely on JavaScript to render content.\n",
    "\n",
    "**Installation required:**\n",
    "```bash\n",
    "pip install selenium webdriver-manager\n",
    "```\n",
    "\n",
    "The enhanced class will:\n",
    "1. Automatically detect when a page has minimal content (likely JS-rendered)\n",
    "2. Fall back to Selenium to render the page with a headless browser\n",
    "3. Extract the fully rendered content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Website class with Selenium fallback for JavaScript-rendered pages\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class WebsiteWithSelenium:\n",
    "    \"\"\"\n",
    "    Enhanced Website class that can handle both static and JavaScript-rendered pages.\n",
    "    Falls back to Selenium when minimal content is detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url, use_selenium=False):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url.\n",
    "        If use_selenium is True, or if minimal content is detected, use Selenium.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.title = None\n",
    "        self.text = None\n",
    "        \n",
    "        # Try regular requests first (unless explicitly told to use Selenium)\n",
    "        if not use_selenium:\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                self.title = soup.title.string if soup.title else \"No title found\"\n",
    "                \n",
    "                # Remove irrelevant elements\n",
    "                if soup.body:\n",
    "                    for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                        irrelevant.decompose()\n",
    "                    text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "                else:\n",
    "                    text = \"\"\n",
    "                \n",
    "                # Check if content is minimal (likely JS-rendered)\n",
    "                if len(text) < 500:  # Arbitrary threshold\n",
    "                    print(f\"‚ö†Ô∏è Minimal content ({len(text)} chars) - using Selenium to render JavaScript...\")\n",
    "                    use_selenium = True\n",
    "                else:\n",
    "                    self.text = text\n",
    "                    print(f\"‚úì Fetched {len(self.text)} characters using requests\")\n",
    "                    return  # Successfully got content, no need for Selenium\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error with requests: {e}\")\n",
    "                print(f\"   Falling back to Selenium...\")\n",
    "                use_selenium = True\n",
    "        \n",
    "        # Use Selenium if needed\n",
    "        if use_selenium:\n",
    "            self._fetch_with_selenium()\n",
    "    \n",
    "    def _fetch_with_selenium(self):\n",
    "        \"\"\"\n",
    "        Fetch the website content using Selenium with a headless Chrome browser.\n",
    "        \"\"\"\n",
    "        # Configure Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "        \n",
    "        # Initialize the Chrome driver\n",
    "        driver = None\n",
    "        try:\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            \n",
    "            # Load the page\n",
    "            driver.get(self.url)\n",
    "            \n",
    "            # Wait for the page to load (wait for body to be present)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Give JavaScript time to render\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get the page source after JavaScript has rendered\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            self.title = soup.title.string if soup.title else \"No title found\"\n",
    "            \n",
    "            # Remove irrelevant elements\n",
    "            if soup.body:\n",
    "                for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                    irrelevant.decompose()\n",
    "                self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                self.text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            print(f\"‚úì Fetched {len(self.text)} characters using Selenium\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Selenium: {e}\")\n",
    "            self.title = \"Error loading page\"\n",
    "            self.text = f\"Could not load content from {self.url}\"\n",
    "        \n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f060d79",
   "metadata": {},
   "source": [
    "## Testing the Enhanced Website Class\n",
    "\n",
    "Now let's create updated versions of our functions that use the new `WebsiteWithSelenium` class:\n",
    "\n",
    "**Important:** After running the cell above that defines `WebsiteWithSelenium`, if you see duplicate messages or unexpected behavior, restart the kernel:\n",
    "- **Menu:** Kernel ‚Üí Restart Kernel\n",
    "- This clears old class definitions from memory\n",
    "\n",
    "**Note:** The first time you run Selenium code, it will download the Chrome WebDriver automatically. This might take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3047b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated functions to use WebsiteWithSelenium\n",
    "\n",
    "def summarize_with_selenium(url, use_selenium=False):\n",
    "    \"\"\"\n",
    "    Summarize a website using the enhanced WebsiteWithSelenium class.\n",
    "    Set use_selenium=True to force Selenium, or let it auto-detect.\n",
    "    \"\"\"\n",
    "    website = WebsiteWithSelenium(url, use_selenium=use_selenium)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def display_summary_selenium(url, use_selenium=False):\n",
    "    \"\"\"\n",
    "    Display a nicely formatted summary using the enhanced class.\n",
    "    \"\"\"\n",
    "    summary = summarize_with_selenium(url, use_selenium=use_selenium)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cba661",
   "metadata": {},
   "source": [
    "### üîÑ Quick Tip: Restart Kernel if Needed\n",
    "\n",
    "If you see any duplicate messages like:\n",
    "```\n",
    "Successfully fetched 5592 characters using Selenium\n",
    "Successfully fetched 5592 characters using Selenium  ‚Üê duplicate!\n",
    "```\n",
    "\n",
    "This means the class was redefined and Python has old versions in memory. Fix:\n",
    "1. **Kernel menu ‚Üí Restart Kernel**  \n",
    "2. Re-run cells from the top (imports, OpenAI setup, etc.)\n",
    "3. Then run the tests again\n",
    "\n",
    "This is a common Jupyter workflow when redefining classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96532705",
   "metadata": {},
   "source": [
    "## Testing Different Scenarios\n",
    "\n",
    "Let's test the enhanced class with different types of websites:\n",
    "1. **JavaScript-heavy sites** (like OpenAI) - will automatically use Selenium\n",
    "2. **Regular sites** (like CNN) - will use regular requests for speed\n",
    "3. **Comparison** - showing both methods produce consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: JavaScript-heavy website (OpenAI) - force Selenium\n",
    "print(\"=== Test 1: OpenAI.com with Selenium ===\")\n",
    "display_summary_selenium(\"https://openai.com\", use_selenium=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Your React website - auto-detect (will use Selenium due to minimal content)\n",
    "print(\"=== Test 2: Your React website (auto-detect) ===\")\n",
    "display_summary_selenium(\"https://japyh.vercel.app/\", use_selenium=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Regular website (CNN) - using enhanced class WITHOUT forcing Selenium\n",
    "print(\"=== Test 3: CNN.com with enhanced class (auto-detect, will use regular requests) ===\")\n",
    "display_summary_selenium(\"https://cnn.com\", use_selenium=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Same website (CNN) using the ORIGINAL Website class for comparison\n",
    "print(\"=== Test 4: CNN.com with original Website class ===\")\n",
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dccd9",
   "metadata": {},
   "source": [
    "## Results Explanation\n",
    "\n",
    "**What you should see:**\n",
    "\n",
    "1. **Test 1 (OpenAI)**: Uses Selenium because we forced it. Gets the full rendered content.\n",
    "   - Expected: `‚úì Fetched XXXX characters using Selenium`\n",
    "\n",
    "2. **Test 2 (Your React site)**: Auto-detects minimal content and falls back to Selenium automatically.\n",
    "   - Expected: `‚ö†Ô∏è Minimal content (0 chars) - using Selenium to render JavaScript...`\n",
    "   - Then: `‚úì Fetched XXX characters using Selenium`\n",
    "\n",
    "3. **Test 3 (CNN with enhanced class)**: Uses regular `requests` library for speed since CNN has plenty of static content (>500 chars).\n",
    "   - Expected: `‚úì Fetched 11945 characters using requests`\n",
    "\n",
    "4. **Test 4 (CNN with original class)**: Should produce the **same result** as Test 3, proving the enhanced class maintains compatibility.\n",
    "\n",
    "**Key fixes applied:**\n",
    "- ‚úÖ No more duplicate fetching - the `return` statement prevents calling Selenium twice\n",
    "- ‚úÖ Better print messages - clearer status indicators with ‚úì and ‚ö†Ô∏è symbols  \n",
    "- ‚úÖ Consistent results - both `Website` and `WebsiteWithSelenium` handle regular sites the same way\n",
    "- ‚úÖ Smart auto-detection - automatically switches to Selenium only when needed (content < 500 chars)\n",
    "\n",
    "**Troubleshooting:** If you see duplicate messages or unexpected behavior:\n",
    "1. **Restart the kernel**: Kernel menu ‚Üí Restart Kernel\n",
    "2. **Re-run from top**: Execute cells from the imports down to the tests\n",
    "3. This clears old class definitions from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951be1a-7f1b-448f-af1f-845978e47e2c",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business applications</h2>\n",
    "            <span style=\"color:#181;\">In this exercise, you experienced calling the Cloud API of a Frontier Model (a leading model at the frontier of AI) for the first time. We will be using APIs like OpenAI at many stages in the course, in addition to building our own LLMs.\n",
    "\n",
    "More specifically, we've applied this to Summarization - a classic Gen AI use case to make a summary. This can be applied to any business vertical - summarizing the news, summarizing financial performance, summarizing a resume in a cover letter - the applications are limitless. Consider how you could apply Summarization in your business, and try prototyping a solution.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue - now try yourself</h2>\n",
    "            <span style=\"color:#900;\">Use the cell below to make your own simple commercial example. Stick with the summarization use case for now. Here's an idea: write something that will take the contents of an email, and will suggest an appropriate short subject line for the email. That's the kind of feature that might be built into a commercial email tool.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00743dac-0e70-45b7-879a-d7293a6f68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create your prompts\n",
    "\n",
    "system_prompt = \"You are an expert copywriter specializing in clear, concise, and professional email subject lines. Analyze the following email body and suggest one perfect subject line. The subject line should be no more than 10 words. Do not add any extra text or quotation marks, just the subject line.\"\n",
    "user_prompt = \"\"\"\n",
    "Hi Team,\n",
    "\n",
    "Just a reminder that the quarterly performance review documents are due by the end of the day this Friday, October 10th. Please make sure you have submitted your self-assessment forms to your direct manager by then.\n",
    "\n",
    "We also need to finalize the Q4 budget proposals. I've attached the latest draft to this email. Please review the numbers for your department and send any feedback or required adjustments to me by Monday.\n",
    "\n",
    "Let me know if you have any questions.\n",
    "\n",
    "Best,\n",
    "Derya Umut Kulalƒ±\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Make the messages list\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "] # fill this in\n",
    "\n",
    "# Step 3: Call OpenAI\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "# Step 4: print the result\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab24dc-5f90-4570-b542-b0585aca3eb6",
   "metadata": {},
   "source": [
    "# Sharing your code\n",
    "\n",
    "I'd love it if you share your code afterwards so I can share it with others! You'll notice that some students have already made changes (including a Selenium implementation) which you will find in the community-contributions folder. If you'd like add your changes to that folder, submit a Pull Request with your new versions in that folder and I'll merge your changes.\n",
    "\n",
    "If you're not an expert with git (and I am not!) then GPT has given some nice instructions on how to submit a Pull Request. It's a bit of an involved process, but once you've done it once it's pretty clear. As a pro-tip: it's best if you clear the outputs of your Jupyter notebooks (Edit >> Clean outputs of all cells, and then Save) for clean notebooks.\n",
    "\n",
    "Here are good instructions courtesy of an AI friend:  \n",
    "https://chatgpt.com/share/677a9cb5-c64c-8012-99e0-e06e88afd293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4484fcf-8b39-4c3f-9674-37970ed71988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
