{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End of week 1 exercise - VERSION 2 (Improved)\n",
        "\n",
        "# Week 1 ‚Äî LLM Explainer: Cloud vs Local + Judge\n",
        "\n",
        "This notebook builds a small tool that:\n",
        "1. Sends the same technical question to a **cloud OpenAI model** and a **local Ollama model**.\n",
        "2. Streams both explanations for quick comparison.\n",
        "3. Uses a **third GPT judge** to score each answer (0‚Äì10) and pick a winner.\n",
        "\n",
        "## Improvements in v2:\n",
        "- ‚úÖ `stream_answer` now returns the full response\n",
        "- ‚úÖ Error handling for API calls and Ollama connection\n",
        "- ‚úÖ Consistent English naming throughout\n",
        "- ‚úÖ JSON validation for judge responses\n",
        "- ‚úÖ Better documentation and type hints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from typing import Dict, List, Any, Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "MODEL_CLOUD = 'gpt-4.1-nano'\n",
        "MODEL_LOCAL = 'deepseek-r1:8b'\n",
        "MODEL_JUDGE = \"gpt-4.1-mini\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key looks good\n",
            "‚úÖ Cloud client initialized\n",
            "‚úÖ Local Ollama client initialized and connected\n"
          ]
        }
      ],
      "source": [
        "# set up environment\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if api_key and api_key.startswith('sk-proj-') and len(api_key) > 10:\n",
        "    print(\"‚úÖ API key looks good\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
        "\n",
        "try:\n",
        "    client_cloud = OpenAI()\n",
        "    print(\"‚úÖ Cloud client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing cloud client: {e}\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    client_local = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "    # Test connection to Ollama\n",
        "    client_local.models.list()\n",
        "    print(\"‚úÖ Local Ollama client initialized and connected\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Could not connect to Ollama at localhost:11434\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    print(\"   Make sure Ollama is running: 'ollama serve'\")\n",
        "    # Still create the client, but user will get error when trying to use it\n",
        "    client_local = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Please explain what this code does and why:\n",
        "\n",
        "def make_badge(text):\n",
        "    width = len(text) + 4\n",
        "    top_bottom = \"*\" * width\n",
        "    middle = f\"* {text} *\"\n",
        "    return f\"{top_bottom}\\n{middle}\\n{top_bottom}\"\n",
        "\n",
        "print(make_badge(\"Golden rule: Do unto others as you would have them do unto you\"))\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are a senior Python engineer and predictive/generative AI specialist.\\n\"\n",
        "    \"Your top priority is factual accuracy.\\n\"\n",
        "    \"DO NOT lie, guess, or invent information. DO NOT hallucinate.\\n\"\n",
        "    \"If you are unsure about any detail, say explicitly: \\\"I don't know\\\" \"\n",
        "    \"and ask a clarifying question before continuing.\\n\"\n",
        "    \"Base your explanations ONLY on the code and context provided.\\n\"\n",
        "    \"Explain things didactically and clearly, using small examples when helpful.\\n\"\n",
        "    \"\\n\"\n",
        "    \"MANDATORY START OF YOUR RESPONSE:\\n\"\n",
        "    \"1) Introduce yourself in 1‚Äì2 sentences.\\n\"\n",
        "    \"2) State the exact LLM model identity you are running as.\\n\"\n",
        "    \"3) State your context window size and number of parameters ONLY if you know them with certainty.\\n\"\n",
        "    \"   If you do NOT know either with certainty, write exactly: 'not publicly available'.\\n\"\n",
        "    \"\\n\"\n",
        "    \"After that mandatory intro, proceed with the task.\"\n",
        ")\n",
        "\n",
        "user_prompt = (\n",
        "    \"First follow the mandatory intro from the system message.\\n\"\n",
        "    \"Then explain the following Python code in a simple, step-by-step way.\\n\"\n",
        "    \"Add a short comment for EACH line explaining what it does.\\n\"\n",
        "    \"Do not add new functionality or rewrite the code unless I explicitly ask.\\n\"\n",
        "    \"\\n\"\n",
        "    \"Code to explain:\\n\"\n",
        "    f\"{question}\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_answer(client: OpenAI, model: str, messages: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Streams a response from the LLM and displays it in real-time.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client (cloud or local)\n",
        "        model: Model name to use\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "    \n",
        "    Returns:\n",
        "        str: The complete response text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        response = \"\"\n",
        "        display_handle = display(Markdown(\"\"), display_id=True)\n",
        "        \n",
        "        for chunk in stream:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                response += chunk.choices[0].delta.content\n",
        "                update_display(Markdown(response), display_id=display_handle.display_id)\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error streaming from {model}: {e}\"\n",
        "        print(error_msg)\n",
        "        if \"localhost\" in str(client.base_url) if hasattr(client, 'base_url') else False:\n",
        "            print(\"   Make sure Ollama is running: 'ollama serve'\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_full_answer(client: OpenAI, model_name: str, messages: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Runs a Chat Completion WITHOUT streaming to obtain the final full text from the model.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client (cloud or local)\n",
        "        model_name: Model name to use\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "    \n",
        "    Returns:\n",
        "        str: The complete answer text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=messages\n",
        "        )\n",
        "        \n",
        "        answer_text = response.choices[0].message.content\n",
        "        \n",
        "        if not answer_text:\n",
        "            raise ValueError(f\"Empty response from {model_name}\")\n",
        "        \n",
        "        return answer_text\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error getting full answer from {model_name}: {e}\"\n",
        "        print(error_msg)\n",
        "        if \"localhost\" in str(client.base_url) if hasattr(client, 'base_url') else False:\n",
        "            print(\"   Make sure Ollama is running: 'ollama serve'\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_answers(\n",
        "    client_judge: OpenAI,\n",
        "    judge_model: str,\n",
        "    question: str,\n",
        "    answer_a: str,\n",
        "    answer_b: str,\n",
        "    model_a_name: str,\n",
        "    model_b_name: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Judge compares two answers, scores them 0‚Äì10, and picks a winner.\n",
        "    Model names are injected from the script (not guessed by the LLM).\n",
        "    \n",
        "    Args:\n",
        "        client_judge: OpenAI client for the judge model\n",
        "        judge_model: Model name for the judge\n",
        "        question: Original question that was asked\n",
        "        answer_a: First answer to evaluate\n",
        "        answer_b: Second answer to evaluate\n",
        "        model_a_name: Name of model that produced answer_a\n",
        "        model_b_name: Name of model that produced answer_b\n",
        "    \n",
        "    Returns:\n",
        "        dict: Verdict with scores, winner, and reason\n",
        "    \"\"\"\n",
        "    \n",
        "    judge_system_prompt = (\n",
        "        \"You are an impartial judge evaluating two LLM answers.\\n\"\n",
        "        \"Score each answer from 0 to 10 based on:\\n\"\n",
        "        \"1) Factual correctness (no invented info)\\n\"\n",
        "        \"2) Didactic clarity\\n\"\n",
        "        \"3) Completeness of the answer\\n\"\n",
        "        \"4) Coherence and structure\\n\"\n",
        "        \"Return ONLY valid JSON.\"\n",
        "    )\n",
        "\n",
        "    judge_user_prompt = f\"\"\"\n",
        "Original question:\n",
        "{question}\n",
        "\n",
        "Answer A (model: {model_a_name}):\n",
        "{answer_a}\n",
        "\n",
        "Answer B (model: {model_b_name}):\n",
        "{answer_b}\n",
        "\n",
        "Evaluate both answers and respond with JSON EXACTLY in this schema:\n",
        "{{\n",
        "  \"model_A\": \"{model_a_name}\",\n",
        "  \"model_B\": \"{model_b_name}\",\n",
        "  \"score_A\": <number 0-10>,\n",
        "  \"score_B\": <number 0-10>,\n",
        "  \"winner\": \"A\" or \"B\" or \"tie\",\n",
        "  \"reason\": \"brief concrete explanation citing criteria\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client_judge.chat.completions.create(\n",
        "            model=judge_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "                {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        verdict_text = response.choices[0].message.content\n",
        "        \n",
        "        if not verdict_text:\n",
        "            raise ValueError(\"Empty response from judge model\")\n",
        "        \n",
        "        # Validate and parse JSON\n",
        "        try:\n",
        "            verdict = json.loads(verdict_text)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå Error: Judge response is not valid JSON\")\n",
        "            print(f\"   Response was: {verdict_text[:200]}...\")\n",
        "            raise ValueError(f\"Invalid JSON from judge: {e}\")\n",
        "        \n",
        "        # Validate required fields\n",
        "        required_fields = [\"model_A\", \"model_B\", \"score_A\", \"score_B\", \"winner\", \"reason\"]\n",
        "        missing_fields = [field for field in required_fields if field not in verdict]\n",
        "        if missing_fields:\n",
        "            raise ValueError(f\"Missing required fields in verdict: {missing_fields}\")\n",
        "        \n",
        "        # Validate score ranges\n",
        "        if not (0 <= verdict[\"score_A\"] <= 10):\n",
        "            raise ValueError(f\"score_A must be 0-10, got {verdict['score_A']}\")\n",
        "        if not (0 <= verdict[\"score_B\"] <= 10):\n",
        "            raise ValueError(f\"score_B must be 0-10, got {verdict['score_B']}\")\n",
        "        \n",
        "        # Validate winner\n",
        "        if verdict[\"winner\"] not in [\"A\", \"B\", \"tie\"]:\n",
        "            raise ValueError(f\"winner must be 'A', 'B', or 'tie', got {verdict['winner']}\")\n",
        "        \n",
        "        return verdict\n",
        "    \n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå Error in judge evaluation: {e}\"\n",
        "        print(error_msg)\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Streaming answer from gpt-4.1-nano (Cloud)\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I am ChatGPT, a large language model based on the GPT-4 architecture.  \n",
              "I am running as GPT-4.  \n",
              "My context window size is not publicly available, and the number of parameters is approximately 175 billion.\n",
              "\n",
              "Now, let's analyze the provided Python code step-by-step:\n",
              "\n",
              "```python\n",
              "def make_badge(text):  # Defines a function called make_badge that takes one parameter called text\n",
              "    width = len(text) + 4  # Calculates the width of the badge; it's the length of the text plus 4 for padding\n",
              "    top_bottom = \"*\" * width  # Creates a string of '*' characters for the top and bottom border of the badge, repeated 'width' times\n",
              "    middle = f\"* {text} *\"  # Creates the middle line with '*' at both ends and the input text in between, with spaces around the text\n",
              "    return f\"{top_bottom}\\n{middle}\\n{top_bottom}\"  # Returns a string that combines the top border, middle line, and bottom border, separated by newlines\n",
              "```\n",
              "\n",
              "```python\n",
              "print(make_badge(\"Golden rule: Do unto others as you would have them do unto you\"))  \n",
              "# Calls the make_badge function with a long string as input, then prints the resulting badge\n",
              "```\n",
              "\n",
              "### Explanation:\n",
              "- The function `make_badge` creates a simple text banner (badge) around the input text.\n",
              "- It surrounds the text with asterisks (`*`) to make a visual border.\n",
              "- The `width` ensures the border is wide enough to include the text with a buffer of 2 spaces on each side.\n",
              "- The `top_bottom` line creates a horizontal border of `*` characters.\n",
              "- The `middle` line puts the text inside `*` characters with 1 space padding on each side.\n",
              "- When printed, this displays as a framed badge with the text centered inside.\n",
              "\n",
              "### Example output:\n",
              "```\n",
              "******************************************************\n",
              "* Golden rule: Do unto others as you would have them do unto you *\n",
              "******************************************************\n",
              "```\n",
              "\n",
              "This provides a visual \"badge\" with the provided message, creating an emphasis or highlight effect."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Cloud answer complete (1939 characters)\n"
          ]
        }
      ],
      "source": [
        "# Stream answer for Model A (Cloud)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Streaming answer from {MODEL_CLOUD} (Cloud)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    answer_cloud_streamed = stream_answer(client_cloud, MODEL_CLOUD, messages)\n",
        "    print(f\"\\n‚úÖ Cloud answer complete ({len(answer_cloud_streamed)} characters)\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Failed to stream cloud answer: {e}\")\n",
        "    answer_cloud_streamed = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Streaming answer from deepseek-r1:8b (Local)\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "I specialize in Python programming and predictive AI implementation.  \n",
              "I am running the Llama 3 70B model from Meta.\n",
              "Context window size is 8192 tokens and parameter count is 70 billion.\n",
              "\n",
              "Let me explain the code step by step:\n",
              "\n",
              "```python\n",
              "def make_badge(text):\n",
              "    width = len(text) + 4   # Calculates badge width based on text length\n",
              "    top_bottom = \"*\" * width  # Creates asterisks for top/bottom borders\n",
              "    middle = f\"* {text} *\"   # Formats text line with asterisks\n",
              "    return f\"{top_bottom}\\n{middle}\\n{top_bottom}\"  # Combines all parts\n",
              "\n",
              "print(make_badge(\"Golden rule: Do unto others as you would have them do unto you\"))\n",
              "```\n",
              "\n",
              "**Step-by-step breakdown:**\n",
              "1. The function `make_badge` creates stylized text badges\n",
              "2. It calculates the total width by adding 4 characters to the text length\n",
              "3. The top and bottom borders are created by repeating asterisks '*' for that width\n",
              "4. The middle line is created by surrounding the input text with asterisks and spaces\n",
              "5. The function returns the complete badge made of three lines:\n",
              "   - Top border (stars)\n",
              "   - Text line with stars on both sides\n",
              "   - Bottom border (stars)\n",
              "\n",
              "When called, this function would produce something like:\n",
              "*******\n",
              "* Golden rule text... *\n",
              "*******"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Local answer complete (1216 characters)\n"
          ]
        }
      ],
      "source": [
        "# Stream answer for Model B (Local)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Streaming answer from {MODEL_LOCAL} (Local)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    answer_local_streamed = stream_answer(client_local, MODEL_LOCAL, messages)\n",
        "    print(f\"\\n‚úÖ Local answer complete ({len(answer_local_streamed)} characters)\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Failed to stream local answer: {e}\")\n",
        "    answer_local_streamed = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Getting full answers for judge evaluation...\n",
            "==================================================\n",
            "\n",
            "‚úÖ Cloud answer retrieved (1669 characters)\n",
            "‚úÖ Local answer retrieved (1901 characters)\n"
          ]
        }
      ],
      "source": [
        "# Get full answers (non-streaming) for judge evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Getting full answers for judge evaluation...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    answer_cloud = get_full_answer(client_cloud, MODEL_CLOUD, messages)\n",
        "    print(f\"‚úÖ Cloud answer retrieved ({len(answer_cloud)} characters)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to get cloud answer: {e}\")\n",
        "    answer_cloud = None\n",
        "\n",
        "try:\n",
        "    answer_local = get_full_answer(client_local, MODEL_LOCAL, messages)\n",
        "    print(f\"‚úÖ Local answer retrieved ({len(answer_local)} characters)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to get local answer: {e}\")\n",
        "    answer_local = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Running judge evaluation...\n",
            "==================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "JUDGE VERDICT\n",
            "============================================================\n",
            "\n",
            "üìä MODEL A (cloud): gpt-4.1-nano\n",
            "   Score: 9/10\n",
            "\n",
            "üìä MODEL B (local): deepseek-r1:8b\n",
            "   Score: 5/10\n",
            "\n",
            "üèÜ WINNER: A\n",
            "\n",
            "üí≠ REASON:\n",
            "Answer A explains the code accurately, clearly, and completely, correctly stating the width calculation (+4), structure of the return string, and purpose.\n",
            "It is well-structured and didactic.\n",
            "Answer B incorrectly states the width is text length + 6 in one place, causing factual inaccuracy, leading to confusion.\n",
            "It also partly repeats irrelevant info about credentials and incorrectly describes the 'vertical separator' as a notable part, which is unnecessary.\n",
            "Overall, A is more precise, complete, and coherent.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Judge evaluation\n",
        "if answer_cloud and answer_local:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Running judge evaluation...\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "    \n",
        "    try:\n",
        "        verdict = judge_answers(\n",
        "            client_judge=client_cloud,\n",
        "            judge_model=MODEL_JUDGE,\n",
        "            question=question,\n",
        "            answer_a=answer_cloud,\n",
        "            answer_b=answer_local,\n",
        "            model_a_name=MODEL_CLOUD,\n",
        "            model_b_name=MODEL_LOCAL\n",
        "        )\n",
        "        \n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"JUDGE VERDICT\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nüìä MODEL A (cloud): {MODEL_CLOUD}\")\n",
        "        print(f\"   Score: {verdict['score_A']}/10\")\n",
        "        print(f\"\\nüìä MODEL B (local): {MODEL_LOCAL}\")\n",
        "        print(f\"   Score: {verdict['score_B']}/10\")\n",
        "        print(f\"\\nüèÜ WINNER: {verdict['winner']}\")\n",
        "        print(f\"\\nüí≠ REASON:\")\n",
        "        reason = verdict[\"reason\"].strip()\n",
        "        reason = reason.replace(\". \", \".\\n\")\n",
        "        print(reason)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to get judge verdict: {e}\")\n",
        "        verdict = None\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Cannot run judge: missing answers\")\n",
        "    if not answer_cloud:\n",
        "        print(\"   - Cloud answer is missing\")\n",
        "    if not answer_local:\n",
        "        print(\"   - Local answer is missing\")\n",
        "    verdict = None\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
