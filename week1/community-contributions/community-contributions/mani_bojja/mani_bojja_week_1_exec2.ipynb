{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell:\n",
    "# 1) Ensures the Ollama model `llama3.2:1b` is available locally\n",
    "# 2) Scrapes the contents of a target webpage\n",
    "# 3) Sends the contents to a locally hosted LLM via the OpenAI-compatible API\n",
    "# 4) Prints a Markdown-formatted summary of the page\n",
    "\n",
    "# Make sure Ollama is installed, running, and listening on localhost:11434\n",
    "# This pulls the `llama3.2:1b` model if it's not already present.\n",
    "\n",
    "!ollama pull llama3.2:1b\n",
    "from openai import OpenAI\n",
    "from scraper import fetch_website_contents\n",
    "\n",
    "# Configuration for the local Ollama server and target webpage\n",
    "ollama_url = \"http://localhost:11434/v1/\"\n",
    "webpage = \"https://edwarddonner.com/about-me-and-about-nebula/\"\n",
    "\n",
    "# Fetch the raw contents of the target webpage as a string\n",
    "webpage_contents = fetch_website_contents(webpage)\n",
    "\n",
    "# Construct chat-style messages following the OpenAI API format:\n",
    "# - system: instructions for how the model should behave\n",
    "# - user: the actual data we want summarized (the webpage contents)\n",
    "prompts = [\n",
    "    {\"role\":\"system\",\"content\":\"You provide and excellent summary of the webpage contents in a Markdown format\"},\n",
    "    {\"role\":\"user\",\"content\":f\"Here are the webpage contents: {webpage_contents}\"}\n",
    "]\n",
    "\n",
    "# Create an OpenAI client that talks to the local Ollama server instead of the real OpenAI API.\n",
    "# The api_key value is ignored by Ollama but must be non-empty to satisfy the client.\n",
    "ollama = OpenAI(base_url = ollama_url, api_key = \"ollama\")\n",
    "\n",
    "# Call the chat completion endpoint on the local model\n",
    "response = ollama.chat.completions.create(messages=prompts, model=\"llama3.2:1b\")\n",
    "\n",
    "# The model's summary is in the first choice's message content.\n",
    "# This should be a Markdown-formatted summary of the original webpage.\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
