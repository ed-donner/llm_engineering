{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efeac1ed",
   "metadata": {},
   "source": [
    "# giving gpt different personas and hav a three way conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f54ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87356cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30292f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411473ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0d19f",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    \"Alex: Hi Blake and Charlie\",\n",
    "    \"Blake: Hi Alex and Charlie\",\n",
    "    \"Charlie: Hi Blake and Alex\",\n",
    "]\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "\n",
    "alex_system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "short response\n",
    "\"\"\"\n",
    "\n",
    "alex_user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "\n",
    "def alex(conversation):\n",
    "    alex_user_prompt = f\"\"\"\n",
    "    Conversation so far:\n",
    "    {chr(10).join(conversation)}\n",
    "\n",
    "    Respond as Alex.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": alex_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": alex_user_prompt})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return \"\\nAlex:\" + response.choices[0].message.content + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "blake_system_prompt = \"\"\"\n",
    "You are Blake, a chatbot who is very nice; you agree with anything in the conversation and you dont challenge everything, and respond in a nice way.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "short response\n",
    "\"\"\"\n",
    "\n",
    "blake_user_prompt = f\"\"\"\n",
    "You are Blake, in conversation with Alex and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Blake.\n",
    "\"\"\"\n",
    "\n",
    "def blake(conversation):\n",
    "    blake_user_prompt = f\"\"\"\n",
    "    Conversation so far:\n",
    "    {chr(10).join(conversation)}\n",
    "\n",
    "    Respond as Blake.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": blake_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": blake_user_prompt})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return \"\\nBlake:\" + response.choices[0].message.content + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "charlie_system_prompt = \"\"\"\n",
    "You are Charlie, a chatbot who is cool; you niether agree or disagree with anything in the conversation and you dont have any opinion, and respond in a cool way.\n",
    "You are in a conversation with Alex and Blake.\n",
    "short response\n",
    "\"\"\"\n",
    "\n",
    "charlie_user_prompt = f\"\"\"\n",
    "You are Charlie, in conversation with Alex and Blake.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Charlie.\n",
    "\"\"\"\n",
    "\n",
    "def charlie(conversation):\n",
    "    charlie_user_prompt = f\"\"\"\n",
    "    Conversation so far:\n",
    "    {chr(10).join(conversation)}\n",
    "\n",
    "    Respond as Charlie.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": charlie_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": charlie_user_prompt})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return \"\\nCharlie:\" + response.choices[0].message.content + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b70946",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_conv = f\"\\n{conversation[0]}\\n\\n{conversation[1]}\\n\\n{conversation[2]}\\n\"\n",
    "\n",
    "display(Markdown(initial_conv))\n",
    "\n",
    "for _ in range(5):\n",
    "    alexRes = alex(conversation)\n",
    "    conversation.append(alexRes)\n",
    "    display(Markdown(alexRes))\n",
    "\n",
    "    blakeRes = blake(conversation)\n",
    "    conversation.append(blakeRes)\n",
    "    display(Markdown(blakeRes))\n",
    "\n",
    "    charlieRes = charlie(conversation)\n",
    "    conversation.append(charlieRes)\n",
    "    display(Markdown(charlieRes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332b919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
