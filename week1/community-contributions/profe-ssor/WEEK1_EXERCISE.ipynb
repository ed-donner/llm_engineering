{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ae0ba09",
      "metadata": {},
      "source": [
        "# Week 1 Exercise — Technical Q&A + Comparison Tool\n",
        "\n",
        "## What I built\n",
        "\n",
        "This notebook is a **technical question-and-answer tool** that uses two different models to explain things, then compares their answers.\n",
        "\n",
        "1. **You ask one technical question** (e.g. “Explain this code” or “What is X?”). You type it in the `question` variable below.\n",
        "\n",
        "2. **Two models answer the same question:**\n",
        "   - **GPT** (via OpenAI or OpenRouter) — answers with **streaming**, so the text appears as it’s generated.\n",
        "   - **Llama 3.2** (via Ollama, running locally) — returns the full answer in one go.\n",
        "\n",
        "3. **The tool then compares the two answers:** it sends both explanations back to the model and asks: *“Which explanation is clearer, and why?”* You get a short comparison so you can see which one helped more.\n",
        "\n",
        "## What it does (when you run it)\n",
        "\n",
        "- **Setup:** Loads your API key from `.env` and connects to OpenAI or OpenRouter (if your key starts with `sk-or-v1-`).\n",
        "- **Your question:** Edit the `question` cell and re-run from there to ask something new.\n",
        "- **GPT answer:** Streams the explanation and saves it as `response`.\n",
        "- **Llama answer:** Gets the explanation from Ollama and saves it as `reply`.\n",
        "- **Compare:** Calls the API again with both answers and displays which is clearer and why.\n",
        "\n",
        "## In one sentence\n",
        "\n",
        "**One technical question → two explanations (GPT + Llama) → one short comparison of which explanation is clearer.** You can reuse this during the course whenever you want two perspectives on a technical topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0ce1bf24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b69097ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "# Use llama3.2:1b if you get \"model requires more system memory\" (needs ~1.2 GiB). Use llama3.2 if you have 2.5+ GiB free.\n",
        "# MODEL_LLAMA = 'llama3.2:1b'\n",
        "MODEL_LLAMA = 'llama3.2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4665e163",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key and api_key.startswith(\"sk-or-v1-\"):\n",
        "    openai = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
        "    model = \"openai/gpt-4o-mini\"\n",
        "else:\n",
        "    openai = OpenAI()\n",
        "    model = MODEL_GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d20a3d94",
      "metadata": {},
      "outputs": [],
      "source": [
        "# here is the question; type over this to ask something new\n",
        "\n",
        "question = \"\"\"\n",
        "Please explain what this code does and when you would use it:\n",
        "@functools.lru_cache(maxsize=128)\n",
        "def fib(n):\n",
        "    if n < 2:\n",
        "        return n\n",
        "    return fib(n - 1) + fib(n - 2)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "cdc16239",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs.\"\n",
        "user_prompt = \"Please give a detailed explanation to the following question: \" + question\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "fe7ad0b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The code you've provided defines a function called `fib` that computes Fibonacci numbers using recursion, while also utilizing a technique called \"memoization\" through the `lru_cache` decorator from the `functools` module. Let's break this down step by step.\n",
              "\n",
              "### Explanation of the Code\n",
              "\n",
              "1. **Decorators in Python**: \n",
              "   - The `@functools.lru_cache(maxsize=128)` line is a decorator that applies the Least Recently Used (LRU) cache to the `fib` function. This cache helps store the results of the function calls to avoid redundant computations for the same input.\n",
              "   - The `maxsize` parameter defines how many results should be cached. In this case, it can cache up to 128 recent calls.\n",
              "\n",
              "2. **Fibonacci Function**:\n",
              "   - The function `fib(n)` computes the n-th Fibonacci number. The Fibonacci sequence is defined as:\n",
              "     - Fib(0) = 0\n",
              "     - Fib(1) = 1\n",
              "     - Fib(n) = Fib(n - 1) + Fib(n - 2) for n ≥ 2\n",
              "   - The first two numbers of the Fibonacci series are 0 and 1, and every subsequent number is the sum of the two preceding numbers.\n",
              "\n",
              "3. **Base Cases**:\n",
              "   - The line `if n < 2: return n` handles the base cases of the Fibonacci sequence. If `n` is either 0 or 1, the function will return `n` itself.\n",
              "\n",
              "4. **Recursive Calls**:\n",
              "   - If `n` is greater than or equal to 2, the function calls itself recursively to calculate the Fibonacci numbers for `n - 1` and `n - 2`, then adds them together.\n",
              "\n",
              "### How `lru_cache` Works\n",
              "\n",
              "- **Efficiency**: Without memoization, calculating `fib(n)` results in an exponential time complexity, specifically O(2^n), due to the repeated calculations of the same Fibonacci values for different calls. Adding the `lru_cache` decorator changes the time complexity to O(n) for this recursive implementation because it stores already computed Fibonacci values. Consequently, when the function is called with the same argument again, it retrieves the value from the cache, rather than calculating it from scratch.\n",
              "  \n",
              "- **Storage**: The cache is stored in a fixed-size list, and when it exceeds the specified `maxsize`, the least recently used entries will be discarded.\n",
              "\n",
              "### When You Would Use This Code\n",
              "\n",
              "1. **Calculating Fibonacci Numbers**: This code is particularly useful when you need to compute Fibonacci numbers, especially for larger values of `n`, where the recursive approach would normally be inefficient.\n",
              "\n",
              "2. **Dynamic Programming**: `lru_cache` effectively transforms this recursive approach into a dynamic programming technique by storing intermediate results, thus optimizing the function.\n",
              "\n",
              "3. **General Memoization Needs**: While this is specifically implemented for Fibonacci numbers, the memoization strategy can be applied to other recursive functions where there are overlapping subproblems.\n",
              "\n",
              "4. **Performance Considerations**: If you're working in an environment where performance is critical, such as algorithms that are part of larger data processing, machine learning applications, or real-time systems, using `lru_cache` can significantly speed up recursive computations.\n",
              "\n",
              "### Practical Use Case Example\n",
              "You might use this function in scenarios where you are creating applications that analyze numerical sequences, such as in financial models that rely on Fibonacci for technical analysis, or in generating sequences for algorithmic solutions where Fibonacci logic is pivotal.\n",
              "\n",
              "### Final Note\n",
              "It's worth noting that Python has other ways of calculating Fibonacci numbers in a more efficient manner than recursion (e.g., iterative methods or matrix exponentiation), but using `lru_cache` provides a clear understanding of how memoization works and its benefits in recursive functions."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Get GPT answer (streaming)\n",
        "\n",
        "stream = openai.chat.completions.create(model=model, messages=messages, stream=True)\n",
        "\n",
        "response = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "for chunk in stream:\n",
        "    response += chunk.choices[0].delta.content or \"\"\n",
        "    update_display(Markdown(response), display_id=display_handle.display_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a597e551",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Memoized Fibonacci Function**\n",
              "\n",
              "The given code implements a memoized version of the Fibonacci function, which is a classic example of dynamic programming. Here's a breakdown of what it does and when to use it:\n",
              "\n",
              "### What is Memoization?\n",
              "\n",
              "Memoization is an optimization technique that stores the results of expensive function calls so that they can be reused instead of recalculated. This approach avoids redundant calculations and reduces computation time.\n",
              "\n",
              "### The Code\n",
              "\n",
              "```python\n",
              "@functools.lru_cache(maxsize=128)\n",
              "def fib(n):\n",
              "    if n < 2:\n",
              "        return n\n",
              "    return fib(n - 1) + fib(n - 2)\n",
              "```\n",
              "\n",
              "This code defines a function `fib` that calculates the nth Fibonacci number. The `@functools.lru_cache` decorator is used to implement memoization.\n",
              "\n",
              "Here's how it works:\n",
              "\n",
              "1. The `lru_cache` decorator caches the results of function calls so they can be reused.\n",
              "2. When the function `fib` is called with a value of `n`, Python checks if the result for that `n` is already cached.\n",
              "3. If it is, the cached result is returned immediately.\n",
              "4. If not, the function calls itself recursively to calculate the result for `n-1` and `n-2`.\n",
              "5. The results are then combined using addition (`fib(n - 1) + fib(n - 2)`).\n",
              "6. Once both recursive calls have completed, the final result is returned.\n",
              "\n",
              "### Why Use Memoization?\n",
              "\n",
              "Memoization can significantly speed up performance when dealing with:\n",
              "\n",
              "*   **Computational expensive functions**: When the function has a high number of redundant calculations, memoization can reduce computation time.\n",
              "*   **Recursive functions**: Memoization is particularly useful for recursive functions that have overlapping subproblems (like the Fibonacci sequence).\n",
              "*   **Highly dynamic data**: In situations where data is being generated or updated frequently, memoization ensures that you're using up-to-date values.\n",
              "\n",
              "### Example Use Case\n",
              "\n",
              "Let's say you need to calculate the 10th Fibonacci number:\n",
              "```python\n",
              "fib_n = fib(10)\n",
              "print(fib_n)  # Output: 55\n",
              "```\n",
              "\n",
              "Without memoization, calculating the 10th Fibonacci number could involve redundant calculations (e.g., `fib(8)` is calculated multiple times). With memoization, the result for `n=10` will be cached immediately, avoiding these redundant calculations.\n",
              "\n",
              "### Potential Drawbacks\n",
              "\n",
              "While memoization can be a powerful optimization technique, it's essential to consider the following:\n",
              "\n",
              "*   **Cache size**: The `maxsize` parameter in the `lru_cache` decorator controls how many results are stored. If this number is too small, you might end up with a significant cache size, which could lead to memory issues.\n",
              "*   **Cache invalidation**: When data changes (e.g., when input values change), memoized results may become outdated.\n",
              "\n",
              "To mitigate these concerns, consider using techniques like:\n",
              "\n",
              "*   **Dynamic caching**: Update the cache only when necessary (i.e., when input values change).\n",
              "*   **Lazy evaluation**: Only calculate results on demand to avoid initial high memory usage."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get Ollama answer\n",
        "ollama_response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
        "reply = ollama_response[\"message\"][\"content\"]\n",
        "display(Markdown(reply))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97061fe",
      "metadata": {},
      "source": [
        "Compare the two answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "30800a9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the two answers\n",
        "compare_system = \"You are a clear, fair reviewer. Compare two explanations and say which is clearer and why, in a short paragraph.\"\n",
        "compare_user = \"Question: \" + question + \"\\n\\nAnswer A (GPT):\\n\" + response + \"\\n\\nAnswer B (Llama):\\n\" + reply\n",
        "compare_messages = [{\"role\": \"system\", \"content\": compare_system}, {\"role\": \"user\", \"content\": compare_user}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "fdb93628",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Answer A provides a more comprehensive and clearer explanation of the code than Answer B. It breaks down each component step-by-step, detailing not just the functionality of the `fib` function itself but also how the `lru_cache` decorator works and enhances performance through memoization. Additionally, Answer A elaborates on the potential applications of the code, discussing its relevance in dynamic programming and performance-critical situations, which gives context to its use. On the other hand, Answer B, while still informative, is less detailed and doesn’t explore the implications and broader contexts as thoroughly, making it less effective for someone looking for a full understanding of the code’s purpose and applications."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "comparison_response = openai.chat.completions.create(model=model, messages=compare_messages)\n",
        "comparison_text = comparison_response.choices[0].message.content\n",
        "display(Markdown(comparison_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40408b2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4c0d9d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
