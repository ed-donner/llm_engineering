{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "source": [
    "# imports\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "source": [
    "# set up environment\n",
    "load_dotenv()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt = \"You are a senior AI engineer and can answer questions about python code, software engineering, data science, machine learning, large language models. You also strive to give clear and concise answers to help unblock others. Respond in Markdown.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper function for streaming in Markdown\n",
    "\n",
    "def stream_in_md(any_stream):\n",
    "    md = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in any_stream:\n",
    "        if 'message' in dir(chunk): # from ollama\n",
    "            md += chunk.message.content or ''\n",
    "        elif 'choices' in dir(chunk): # from openAI\n",
    "            md += chunk.choices[0].delta.content or ''\n",
    "        md = md.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(md), display_id=display_handle.display_id)"
   ],
   "id": "729deae371d1757",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "openai = OpenAI()\n",
    "\n",
    "openAI_stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)\n",
    "stream_in_md(openAI_stream)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "llama_stream = ollama.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "stream_in_md(llama_stream)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
