{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter API key loaded.\n",
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# --- OpenRouter setup ---\n",
    "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openrouter_base = os.getenv(\"OPENROUTER_BASE_URL\") or \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "if not openrouter_key:\n",
    "    print(\"OpenRouter API key not found. Check your .env file.\")\n",
    "else:\n",
    "    print(\"OpenRouter API key loaded.\")\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    api_key=openrouter_key,\n",
    "    base_url=openrouter_base\n",
    ")\n",
    "\n",
    "# --- Ollama setup ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "system_role = \"\"\"\n",
    "You are a clear and precise technical assistant.\n",
    "Explain code behavior accurately, including why it is written that way.\n",
    "Use structured formatting where helpful.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini response:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the code snippet you've provided:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "### Code Explanation\n",
       "\n",
       "1. **Set Comprehension**:\n",
       "   ```python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   ```\n",
       "   - This part of the code is a *set comprehension* that iterates over a collection called `books`.\n",
       "   - For each `book` in `books`, it attempts to get the value associated with the `\"author\"` key using `book.get(\"author\")`.\n",
       "   - The `if book.get(\"author\")` condition filters out any books where the `\"author\"` key either doesn't exist or has a value that evaluates as `False` (e.g., `None`, an empty string, etc.).\n",
       "   - The result of this comprehension is a set containing the authors of the books that do have valid author entries.\n",
       "\n",
       "2. **Yielding Values**:\n",
       "   ```python\n",
       "   yield from ...\n",
       "   ```\n",
       "   - The `yield from` statement is used in a generator function to yield all values from an iterable. In this case, the iterable is the set created by the comprehension.\n",
       "   - This means that the generator will produce each author found in the set one at a time, allowing the calling context to iterate over these authors as they are generated.\n",
       "\n",
       "### Why It Is Written This Way\n",
       "\n",
       "- **Efficiency**: \n",
       "  - Using a set comprehension automatically removes any duplicate authors since sets do not allow duplicate entries. Therefore, this code will yield a unique list of authors.\n",
       "\n",
       "- **Conditional Filtering**:\n",
       "  - The `if book.get(\"author\")` ensures that only books with a valid author are included, which is critical for preventing `None` or empty entries in the output.\n",
       "\n",
       "- **Generator Functionality**:\n",
       "  - By using `yield from`, you make the generator function capable of yielding each author without needing to individually loop through the set and yield each author manually. This makes the code more concise and maintains clear logic flow.\n",
       "\n",
       "### Summary\n",
       "\n",
       "In summary, this code defines a generator that yields unique authors from a list of `books`, filtering out any entries that do not contain valid author information. The set comprehension ensures uniqueness, while `yield from` facilitates efficient iteration over the resulting set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming \n",
    "\n",
    "print(\"GPT-4o-mini response:\\n\")\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "stream = openai_client.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        response += content\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Llama response:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "This is a Python code snippet that uses a combination of generators, dictionaries, and list comprehension. Here's what it does:\n",
       "\n",
       "*   It extracts only the `author` field values from a list of books.\n",
       "*   The extracted values are collected into a set (`{...}`).\n",
       "*   If there are multiple authors for a book with no author (or an empty string), their contribution will be ignored.\n",
       "\n",
       "**Breaking Down the Code**\n",
       "\n",
       "1.  **Dictionary comprehension**: `{book.get(\"author\") for book in books if book.get(\"author\")}`:\n",
       "    *   This creates a set containing `author` field values extracted from each book.\n",
       "    *   Only books with authors are included (i.e., `if book.get(\"author\")` filter) to avoid unnecessary processing.\n",
       "\n",
       "2.  **Generator expression**: `yield from {...}`\n",
       "    *   This takes the set created by the dictionary comprehension and yields its elements one by one.\n",
       "\n",
       "3.  **Set used for deduplication and filtering**:\n",
       "    *   The resulting generator's contribution is an empty iterable if there are duplicate `author` field values, which means we drop these duplicates.\n",
       "    *   If a book has no `author`, it will not be included in the processed set.\n",
       "\n",
       "**Example Usage**\n",
       "\n",
       "Suppose you have a list of books where each book can be represented by a dictionary. Here's how you might use this code to iterate through authors:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book A\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book C\", \"author\": \"Author D\"},\n",
       "    {\"title\": \"Book E\", \"\",},  # No author provided\n",
       "]\n",
       "\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "In this example, the code will output `Author B` and `Author D`, but it won't include `Book E's` non-existent author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "print(\"\\n\\nLlama response:\\n\")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "stream = ollama_client.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        response += content\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
