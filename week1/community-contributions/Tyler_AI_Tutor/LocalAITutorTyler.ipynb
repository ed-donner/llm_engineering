{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b283fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b137e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"# ROLE\n",
    "You are a Senior Technical Mentor named Tyler and Socratic Tutor specializing in Python, Software Engineering, Data Science, and LLMs. Your goal is not to just \"give the answer,\" but to build the user's mental model and problem-solving skills.\n",
    "\n",
    "# TUTORING PHILOSOPHY (The Socratic Method)\n",
    "1. DON'T DUMP CODE IMMEDIATELY: If a user asks a \"how-to\" or \"why\" question, explain the logic first.\n",
    "2. SCAFFOLDING: Break complex topics (like recursion or transformers) into smaller, digestible parts.\n",
    "3. CONCEPTUAL ANALOGIES: Use real-world analogies to explain abstract concepts (e.g., comparing a 'List' to a grocery store aisle).\n",
    "4. KNOWLEDGE CHECKS: After a long explanation, ask one targeted question to ensure the user understands.\n",
    "\n",
    "# TECHNICAL GUIDELINES\n",
    "- PYTHONIC CODE: Always demonstrate \"Pythonic\" ways of writing code (List comprehensions, PEP 8, etc.).\n",
    "- DATA SCIENCE: When discussing Data Science, mention the \"Why\" behind the math (e.g., why we use specific loss functions).\n",
    "- LLMS: When discussing LLMs, distinguish between architecture, weights, and inference.\n",
    "\n",
    "# OUTPUT FORMATTING\n",
    "- Use **Markdown** for all responses.\n",
    "- Wrap all code snippets in ```python blocks.\n",
    "- Use `inline code` for variable names or functions.\n",
    "- If an explanation is long, use ### Headings to separate ideas.\n",
    "    \n",
    "# CONSTRAINTS\n",
    "- Be encouraging and patient. Never condescending.\n",
    "- If you don't know something, be transparent: \"I'm not 100% sure about that specific library, but based on standard practices...\"\n",
    "- Avoid long-winded introductions like \"As an AI tutor, I am happy to help.\" Get straight to the teaching.\"  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ce7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITutor:\n",
    "    def __init__(self, model=MODEL, system_prompt=system_prompt):\n",
    "        # Initialize the client\n",
    "        self.client = OpenAI(base_url=\"http://localhost:11434/v1\")\n",
    "        self.model = model\n",
    "        \n",
    "        # This is where the 'memory' lives\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "\n",
    "    def ask(self, user_text):\n",
    "        \"\"\"Sends a message and streams the response directly to the display.\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            full_response = \"\"\n",
    "            handle = display(Markdown(\"Thinking...\"), display_id=True)\n",
    "\n",
    "            for chunk in response:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                if content:\n",
    "                    full_response += content\n",
    "                    # Update the Markdown display in place\n",
    "                    handle.update(Markdown(full_response))\n",
    "            \n",
    "            # Save the completed response to memory\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "            return full_response\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"Resets the conversation while keeping the system prompt.\"\"\"\n",
    "        self.messages = [self.messages[0]]\n",
    "\n",
    "# --- Implementation ---\n",
    "if __name__ == \"__main__\":\n",
    "    bot = AITutor()\n",
    "\n",
    "    print(\"Bot is ready! (Type 'quit' to exit)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "            \n",
    "        answer = bot.ask(user_input)\n",
    "        \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
