{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# WEB Page Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96001ff-f6ad-4c74-a918-f4d308ff94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe189a-f175-47ee-8b28-7f29ae43aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c630e-265d-4ee0-9e57-2cce4783a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url, timeout=10, delay=1):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \n",
    "        Args:\n",
    "            url (str): The website URL to scrape\n",
    "            timeout (int): Request timeout in seconds\n",
    "            delay (int): Delay between requests to be polite\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.timeout = timeout\n",
    "        \n",
    "        # Initialize all attributes with default values\n",
    "        self.title = \"No title found\"\n",
    "        self.text = \"No content available\"\n",
    "        self.domain = None\n",
    "        self.soup = None\n",
    "        self.response = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # Be polite - add delay between requests\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        self._fetch_url()\n",
    "        if self.response and self.response.status_code == 200:\n",
    "            self._parse_content()\n",
    "    \n",
    "    def _fetch_url(self):\n",
    "        \"\"\"Fetch the URL with error handling\"\"\"\n",
    "        try:\n",
    "            self.response = requests.get(\n",
    "                self.url, \n",
    "                headers=headers, \n",
    "                timeout=self.timeout,\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            self.response.raise_for_status()\n",
    "            \n",
    "            # Extract domain for reference\n",
    "            parsed_url = urlparse(self.url)\n",
    "            self.domain = parsed_url.netloc\n",
    "            \n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching {self.url}: {e}\")\n",
    "            self.response = None\n",
    "    \n",
    "    def _parse_content(self):\n",
    "        \"\"\"Parse the HTML content\"\"\"\n",
    "        try:\n",
    "            self.soup = BeautifulSoup(self.response.content, 'html.parser')\n",
    "            self._extract_title()\n",
    "            self._clean_content()\n",
    "            self._extract_text()\n",
    "            self._extract_metadata()\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content from {self.url}: {e}\")\n",
    "    \n",
    "    def _extract_title(self):\n",
    "        \"\"\"Extract and clean the page title\"\"\"\n",
    "        try:\n",
    "            if self.soup.title and self.soup.title.string:\n",
    "                self.title = self.soup.title.string.strip()\n",
    "            else:\n",
    "                # Try to find title in h1 if no title tag\n",
    "                h1 = self.soup.find('h1')\n",
    "                self.title = h1.get_text().strip() if h1 else \"No title found\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting title: {e}\")\n",
    "            self.title = \"Error extracting title\"\n",
    "    \n",
    "    def _clean_content(self):\n",
    "        \"\"\"Remove irrelevant elements from the page\"\"\"\n",
    "        if not self.soup or not self.soup.body:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Elements to remove\n",
    "            elements_to_remove = [\n",
    "                \"script\", \"style\", \"img\", \"input\", \"button\", \n",
    "                \"nav\", \"footer\", \"header\", \"aside\", \"form\"\n",
    "            ]\n",
    "            \n",
    "            for element in elements_to_remove:\n",
    "                for tag in self.soup.body.find_all(element):\n",
    "                    tag.decompose()\n",
    "            \n",
    "            # Remove comments\n",
    "            for comment in self.soup.body.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "                comment.extract()\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning content: {e}\")\n",
    "    \n",
    "    def _extract_text(self):\n",
    "        \"\"\"Extract and clean text content\"\"\"\n",
    "        if not self.soup or not self.soup.body:\n",
    "            self.text = \"No content found\"\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get text with better formatting\n",
    "            text = self.soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Clean up excessive whitespace\n",
    "            lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "            self.text = '\\n'.join(lines) if lines else \"No text content found\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text: {e}\")\n",
    "            self.text = \"Error extracting text content\"\n",
    "    \n",
    "    def _extract_metadata(self):\n",
    "        \"\"\"Extract additional metadata\"\"\"\n",
    "        self.metadata = {}\n",
    "        \n",
    "        try:\n",
    "            # Meta description\n",
    "            meta_desc = self.soup.find('meta', attrs={'name': 'description'})\n",
    "            self.metadata['description'] = meta_desc.get('content') if meta_desc else None\n",
    "            \n",
    "            # Keywords\n",
    "            meta_keywords = self.soup.find('meta', attrs={'name': 'keywords'})\n",
    "            self.metadata['keywords'] = meta_keywords.get('content') if meta_keywords else None\n",
    "            \n",
    "            # Language\n",
    "            html_tag = self.soup.find('html')\n",
    "            self.metadata['language'] = html_tag.get('lang') if html_tag else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting metadata: {e}\")\n",
    "    \n",
    "    def get_links(self):\n",
    "        \"\"\"Extract all links from the page\"\"\"\n",
    "        if not self.soup:\n",
    "            return []\n",
    "        \n",
    "        links = []\n",
    "        try:\n",
    "            for link in self.soup.find_all('a', href=True):\n",
    "                links.append({\n",
    "                    'text': link.get_text(strip=True),\n",
    "                    'url': link['href'],\n",
    "                    'title': link.get('title', '')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting links: {e}\")\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def get_status_info(self):\n",
    "        \"\"\"Get HTTP status information\"\"\"\n",
    "        if not self.response:\n",
    "            return {\"error\": \"No response available\"}\n",
    "        \n",
    "        return {\n",
    "            \"status_code\": self.response.status_code,\n",
    "            \"content_type\": self.response.headers.get('content-type', ''),\n",
    "            \"content_length\": len(self.response.content),\n",
    "            \"encoding\": self.response.encoding\n",
    "        }\n",
    "    \n",
    "    def is_successful(self):\n",
    "        \"\"\"Check if the website was successfully fetched and parsed\"\"\"\n",
    "        return self.response is not None and self.response.status_code == 200\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the Website object\"\"\"\n",
    "        return f\"Website(url='{self.url}', title='{self.title}')\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Usage example\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     try:\n",
    "#         site = Website(\"https://example.com\")\n",
    "        \n",
    "#         print(f\"Title: {site.title}\")\n",
    "#         print(f\"Domain: {site.domain}\")\n",
    "#         print(f\"Status: {site.get_status_info()}\")\n",
    "#         print(f\"Text length: {len(site.text)} characters\")\n",
    "#         print(f\"Successful: {site.is_successful()}\")\n",
    "        \n",
    "#         # Get first few links\n",
    "#         links = site.get_links()[:5]\n",
    "#         print(\"Sample links:\")\n",
    "#         for link in links:\n",
    "#             print(f\"  - {link['text']}: {link['url']}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to process website: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de1032-dc98-43b1-896d-5b3a57a63a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    try:\n",
    "        site = Website(\"https://www.geeksforgeeks.org/blogs/interview-preparation/\")\n",
    "        \n",
    "        print(f\"Title: {site.title}\")\n",
    "        print(f\"Domain: {site.domain}\")\n",
    "        print(f\"Status: {site.get_status_info()}\")\n",
    "        print(f\"Text length: {len(site.text)} characters\")\n",
    "        \n",
    "        # Get first few links\n",
    "        links = site.get_links()[:5]\n",
    "        print(\"Sample links:\")\n",
    "        for link in links:\n",
    "            print(f\"  - {link['text']}: {link['url']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process website: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eca754-c3e6-4d7e-9ef4-dd3037a811a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265c1f9-a1ff-4106-b6d9-4b425d0fe941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    user_prompt += website.domain\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b7260-1a66-4dad-a124-541e0ec78b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(user_prompt_for(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c17375-dfa6-402b-8a28-f54a119b5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91293d10-6291-43e7-9b2c-1603d9e76f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try this out, and then try for a few more websites\n",
    "\n",
    "messages_for(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c9915-9ce9-4e2d-90a9-ca4f38dcc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080d4fb-b8c7-4450-8e51-cc2bb89bffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7158d5-6244-4e07-b321-105fd1eb4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75458c9f-2d3a-49c4-9b6d-569b5e9958d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://www.geeksforgeeks.org/blogs/interview-preparation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202e7e0-baac-4024-8837-01189c2ca8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
