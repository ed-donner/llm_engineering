{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b1314",
   "metadata": {},
   "source": [
    "### Setup API key and Initiate Ollama and GPT client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key=os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('OPENAI_API_KEY not set. Populate .env or set the environment variable.')\n",
    "open_gpt_ai=OpenAI(api_key=api_key,base_url='https://api.openai.com/v1')\n",
    "ollama_ai=OpenAI(api_key=os.getenv('LLAMA_API_KEY'), base_url='http://localhost:11434/v1')\n",
    "if not open_gpt_ai or not ollama_ai:\n",
    "    raise ValueError('Something wrong with GPT AI or Ollama AI')\n",
    "\n",
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt=\"\"\"\n",
    "You are a technical expert assistant that can answer any question about the coding questions, architecture and design. \n",
    "You can teach a complex question easily with help of many examples or analogies.\n",
    "You can also use humor and wittiness to make the explanation more engaging and interesting.\n",
    "Do not use any markdown formatting or longer explanation unless necessary. \n",
    "Always use  code samples to showcase various functions and use cases\n",
    "You can use below example as references:\n",
    "\n",
    "Question: What is \"for book in books\" mean in python?\n",
    "Answer: It is a for loop which iterates over the list. Example ['book1_name', 'book2_name'] . The loop will iterate from left to right.  \n",
    "\n",
    "\n",
    "Question: What is \"books.sort(reverse=True)\" mean in python?\n",
    "Answer: It is a python function which can reverse the list , You can give custom sort keys as needed. Example: books.sort(key=lambda b: b[:1]) picks the sort key as first letter and sort it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b9be6",
   "metadata": {},
   "source": [
    "### Creates helper function for user prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "def get_user_prompt(question):\n",
    "    return f\"\"\"\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "def messages_for_personal_tutor(question):\n",
    "    return [{\n",
    "        \"role\":\"system\", \"content\":system_prompt\n",
    "    },{\n",
    "        \"role\":\"user\",\"content\": get_user_prompt(question)\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569f942",
   "metadata": {},
   "source": [
    "### Creates GPT helper method for answering questions. We have used stream with help of Stream=True\n",
    "\n",
    "- We also have used Markdown and display_handle methods of IPython.display module.\n",
    "- Error handling incase of network error or something wrong happens with try /catch block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def get_apt_answer(question):       \n",
    "    stream_chunks=open_gpt_ai.chat.completions.create(model=MODEL_GPT, messages=messages_for_personal_tutor(question), stream=True)\n",
    "    result=\"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream_chunks:\n",
    "        try:\n",
    "            content = chunk.choices[0].delta.content or ''\n",
    "            if content:\n",
    "                result += content\n",
    "                update_display(Markdown(result), display_id=display_handle.display_id)\n",
    "        except Exception as e:\n",
    "            print(\"Streaming error:\", e)\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc9df7",
   "metadata": {},
   "source": [
    "### Ollama use case\n",
    "- Show cases usage of ollama, Idea is to make our system prompt good enough to have same/similar answer from GPT\n",
    "- Doesnt have try/catch since ollama runs on local\n",
    "- Uses markdown and stream \n",
    "- Great for speed and cost saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def get_llama_answer(question):\n",
    "    # print(ollama_ai.chat.completions.create(model=MODEL_LLAMA, messages=[{\"role\":\"user\", \"content\":f\"Hello {question}\"}]).choices[0].message.content)       \n",
    "    stream_chunks=ollama_ai.chat.completions.create(model=MODEL_LLAMA, messages=messages_for_personal_tutor(question), stream=True)\n",
    "    result=\"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream_chunks:\n",
    "        if chunk:\n",
    "            result+= chunk.choices[0].delta.content or ''\n",
    "            update_display( Markdown(result),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4312949",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_llama_answer(\"What is deque?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_apt_answer(\"What is deque?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
