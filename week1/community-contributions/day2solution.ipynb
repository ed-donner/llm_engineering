{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49cfd69",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from scraper import fetch_website_contents # importing the fucntion that was used in day2 \n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the prompt \n",
    "\n",
    "u_prompt = \"\"\"\n",
    "You are a helpful assistant that analyzes the contents of a website,\n",
    "and provides a short summary, ignoring text that might be navigation related.\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\"\n",
    "s_prompt = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e8c23",
   "metadata": {},
   "source": [
    "### we can run ollama in two ways \n",
    "- by creating a ollama method , which will run ollama locally (ollama.chat)\n",
    "- by creating an openai client object that talks to ollamas server(chat.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now doing the same task we did in day1 , summarazing a website using a llm but this time we are\n",
    "# doing it with a open sourse llm llama3.2\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": s_prompt},\n",
    "        {\"role\": \"user\", \"content\": u_prompt + website}\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "def summarize(url):\n",
    "    website = fetch_website_contents(url)\n",
    "    response = ollama.chat.completions.create(\n",
    "        model = \"llama3.2\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "summarize(\"https://edwarddonner.com\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cda033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display this nicely using markdown\n",
    "\n",
    "def display_summ(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summ(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a summary from the python org page\n",
    "\n",
    "display_summ(\"https://docs.python.org/3/tutorial/introduction.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ec357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
