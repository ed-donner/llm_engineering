{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End of week 1 exercise\n",
        "\n",
        "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,\n",
        "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See [README.md](README.md) in this folder for full exercise details and requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_LLAMA = 'llama3.2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment\n",
        "load_dotenv(\".env\", override=True)\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY is missing. Add it to your environment or .env file.\")\n",
        "\n",
        "# OpenAI client\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Ollama OpenAI-compatible client (requires `ollama serve` running locally)\n",
        "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# here is the question; type over this to ask something new\n",
        "\n",
        "question = \"\"\"\n",
        "What is Retrieval-Augmented Generation (RAG), and when should I choose it instead of fine-tuning an LLM?\n",
        "Please explain in a beginner-friendly way and include:\n",
        "- A simple definition of RAG\n",
        "- How RAG works step-by-step (embedding, retrieval, context injection, generation)\n",
        "- RAG vs fine-tuning: cost, speed, maintenance, and data freshness\n",
        "- One customer-support chatbot example (knowledge base + typical user question)\n",
        "- One common mistake and how to avoid it\n",
        "- A short checklist for deciding between RAG and fine-tuning\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get gpt-4o-mini to answer, with streaming\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an AI tools mentor helping new developers. Use clear, beginner-friendly language and structure the answer with short headings. Cover: (1) simple definition, (2) step-by-step workflow, (3) trade-offs, (4) practical example, (5) common mistake + fix, and (6) quick decision checklist. Keep the answer practical and concise.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": question},\n",
        "]\n",
        "\n",
        "stream = openai_client.chat.completions.create(\n",
        "    model=MODEL_GPT,\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "gpt_chunks = []\n",
        "for chunk in stream:\n",
        "    delta = chunk.choices[0].delta.content or \"\"\n",
        "    if delta:\n",
        "        gpt_chunks.append(delta)\n",
        "        print(delta, end=\"\")\n",
        "\n",
        "gpt_answer = \"\".join(gpt_chunks)\n",
        "\n",
        "display(Markdown(\"## GPT-4o-mini Answer\\n\\n\" + gpt_answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Llama 3.2 to answer\n",
        "llama_response = ollama_client.chat.completions.create(\n",
        "    model=MODEL_LLAMA,\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "llama_answer = llama_response.choices[0].message.content\n",
        "display(Markdown(\"## Llama 3.2 Answer\\n\\n\" + llama_answer))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
