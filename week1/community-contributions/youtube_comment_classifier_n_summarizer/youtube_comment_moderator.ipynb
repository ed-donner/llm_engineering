{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e50e9ac",
   "metadata": {},
   "source": [
    "# ðŸŽ¬ YouTube Comment Moderator AI\n",
    "\n",
    "## Tone Classification & Summary Workflow\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš¦ Moderation Flow\n",
    "\n",
    "1ï¸âƒ£ **User enters YouTube link**\n",
    "\n",
    "2ï¸âƒ£ **System fetches video comments**\n",
    "\n",
    "3ï¸âƒ£ **AI classifies comments**\n",
    "   - Toxicity\n",
    "   - Spam\n",
    "   - Hate\n",
    "   - Positive\n",
    "   - Neutral\n",
    "\n",
    "4ï¸âƒ£ **AI summarizes overall sentiment & issues**\n",
    "   - Highlights dominant tones\n",
    "   - Flags problematic trends\n",
    "   - Provides actionable insights\n",
    "\n",
    "5ï¸âƒ£ **Show moderation report**\n",
    "   - Visual summary of tone distribution\n",
    "   - Key findings & recommendations\n",
    "\n",
    "---\n",
    "\n",
    "> **Example Output:**\n",
    ">\n",
    "> - **Tone Distribution:** ![Pie Chart Icon](https://img.icons8.com/color/48/000000/pie-chart.png)\n",
    "> - **Summary:** \"Most comments are positive, but 12% show signs of toxicity. Spam detected in 8%.\"\n",
    "> - **Recommendations:** \"Consider moderating toxic and spam comments. Overall sentiment is positive.\"\n",
    "\n",
    "---\n",
    "\n",
    "âœ¨ Use this workflow to generate other cells for each step, making your notebook interactive and visually appealing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59400dec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 1: Import Required Libraries\n",
    "\n",
    "Below are the libraries needed for the YouTube Comment Moderator AI, as listed in `requirements.txt`:\n",
    "\n",
    "- **google-api-python-client**: For accessing YouTube API and fetching comments\n",
    "- **pandas**: For data manipulation and analysis\n",
    "- **nltk**: For natural language processing and tone classification\n",
    "- **scikit-learn**: For machine learning models and sentiment analysis\n",
    "- **ollama**: For advanced AI and LLM integration\n",
    "\n",
    "> **Next:** Add a code cell to import these libraries and check their installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058cc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import required libraries for YouTube Comment Moderator AI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ollama  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd73d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=3lS6ITUXQAI\"  # Example video URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013484e8",
   "metadata": {},
   "source": [
    "##  STEP 2: Extract Video ID from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6ee8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3lS6ITUXQAI'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def get_video_id(video_url):\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
    "    match = re.search(pattern, video_url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "video_id = get_video_id(video_url)\n",
    "video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba5f42",
   "metadata": {},
   "source": [
    "##  STEP 2: Fetch Comments from YouTube with your youtube API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92732e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "youtube_api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "if not youtube_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not youtube_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=youtube_api_key)\n",
    "\n",
    "def get_comments(video_id, max_comments=100):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    while request and len(comments) < max_comments:\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                maxResults=100,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "comments = get_comments(video_id, 500)\n",
    "comments[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08d57f",
   "metadata": {},
   "source": [
    "##  STEP 3: Classify Comments using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90499b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classify_comment(comment):\n",
    "    user_prompt = f\"\"\"\n",
    "    Classify this YouTube comment into one category:\n",
    "\n",
    "    Categories:\n",
    "    - Positive\n",
    "    - Neutral\n",
    "    - Toxic\n",
    "    - Spam\n",
    "    - Hate Speech\n",
    "\n",
    "    Comment: {comment}\n",
    "\n",
    "    Return only the category name.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='gemma3:1b',\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "\n",
    "    return response['message']['content'].strip()\n",
    "\n",
    "# test\n",
    "classify_comment(\"This video is amazing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae75fd0",
   "metadata": {},
   "source": [
    "## Step 4: Classify All Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for comment in comments:\n",
    "    label = classify_comment(comment)\n",
    "    results.append((comment, label))\n",
    "\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8d54b",
   "metadata": {},
   "source": [
    "## Step 5: Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Comment\", \"Category\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23856d34",
   "metadata": {},
   "source": [
    "## Step 6: Moderation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_counts = df[\"Category\"].value_counts()\n",
    "summary_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7fc0a",
   "metadata": {},
   "source": [
    "## Step 7: AI Summary of Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(summary):\n",
    "    system_prompt = f\"\"\"\n",
    "    Summarize the moderation results:\n",
    "\n",
    "    {summary.to_dict()}\n",
    "\n",
    "    Explain the overall sentiment and moderation concerns.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='gemma3:1b',\n",
    "        messages=[{\"role\": \"user\", \"content\": system_prompt}]\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "summarize_results(summary_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
