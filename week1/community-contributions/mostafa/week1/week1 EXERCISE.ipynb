{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "from scraper import fetch_website_contents, fetch_website_links\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT_4O_MINI = 'gpt-4o-mini'\n",
    "MODEL_GPT_5_NANO = 'gpt-5-nano'\n",
    "CLIENT_OLLAMA = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key and api_key.startswith('sk-'):\n",
    "    print('OPENAI_API_KEY is set')\n",
    "    client = OpenAI(api_key=api_key)\n",
    "else:\n",
    "    raise ValueError('OPENAI_API_KEY is not set')\n",
    "\n",
    "system_prompt = ''' \n",
    "You are a helpful assistant that can answer questions about the code.\n",
    "By default, make sure to explain the answer in a way that is easy to understand.\n",
    "If the user tells you that they are an advanced user or a professional, \n",
    "make sure to explain the answer in a way that is technical and detailed.\n",
    "If the user tells you they are coming from a different programming language, or mentioned another programming language,\n",
    "make sure to explain the answer in a way that is relevant to the mentioned language.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_models():\n",
    "    return [model['model'] for model in ollama.list()['models']]\n",
    "\n",
    "get_local_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df278e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_list():\n",
    "    remote_models = [{'name': MODEL_GPT_4O_MINI, 'type': 'remote'}, {'name': MODEL_GPT_5_NANO, 'type': 'remote'}]\n",
    "    local_models = [{'name': model, 'type': 'local'} for model in get_local_models()]\n",
    "    models_dict = {f'{i+1}': model for i, model in enumerate(remote_models + local_models)}\n",
    "    return models_dict\n",
    "\n",
    "def print_models_list():\n",
    "    models_dict = get_models_list()\n",
    "    return '\\n'.join([f'{i}. {v.get('name')}' for i, v in models_dict.items()])\n",
    "\n",
    "print_models_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a sample question. will be used if the user provides an empty question.\n",
    "\n",
    "sample_question = ''' \n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a remote model to answer, with streaming. By default, use gpt-4o-mini.\n",
    "def ask_gpt(question, model=MODEL_GPT_4O_MINI):\n",
    "    stream = client.chat.completions.create( \n",
    "        model=model, \n",
    "        messages=[ \n",
    "            {'role': 'system', 'content': system_prompt}, \n",
    "            {'role': 'user', 'content': question}, \n",
    "            ], \n",
    "            stream=True,\n",
    "    ) \n",
    "    response = '' \n",
    "    display_handle = display(Markdown(''), display_id=True) \n",
    "    for chunk in stream: \n",
    "        response += chunk.choices[0].delta.content or '' \n",
    "        display_handle.update(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a local model to answer. By default, use the first local model.\n",
    "def ask_ollama(question, model=get_local_models()[0]): \n",
    "    user_prompt = question\n",
    "    stream = CLIENT_OLLAMA.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    response = ''\n",
    "    display_handle = display(Markdown(''), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        display_handle.update(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = get_models_list()\n",
    "print(f'Enter the number of the model you want to use:\\n{print_models_list()}')\n",
    "\n",
    "choice = input('Enter the number of the model you want to use: ')\n",
    "model = models_list.get(choice)\n",
    "\n",
    "if not model:\n",
    "    print('Invalid choice. Please try again.')\n",
    "    raise ValueError('Invalid choice. Please try again.')\n",
    "\n",
    "question = input('Enter the question you want to ask: ')\n",
    "\n",
    "if not question:\n",
    "    question = sample_question\n",
    "\n",
    "if model['type'] == 'remote':\n",
    "    print(f'\\n{20 * '='}\\ngenerating response using {model['name']}')\n",
    "    ask_gpt(question)\n",
    "else:\n",
    "    print(f'\\n{20 * '='}\\ngenerating response using {model['name']}')\n",
    "    ask_ollama(question, model=model['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd10cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
