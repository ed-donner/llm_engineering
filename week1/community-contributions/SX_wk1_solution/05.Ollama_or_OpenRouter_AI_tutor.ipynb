{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise OpenAI client using Ollama\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='hohoho')\n",
    "OL_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "# Initialise OpenAI client using OpenRouter\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openrouter = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=openrouter_api_key)\n",
    "OR_MODEL = 'xiaomi/mimo-v2-flash:free'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which way to access LLM\n",
    "LLM_access = \"ollama\"\n",
    "\n",
    "if LLM_access == \"ollama\":\n",
    "    myAI = ollama\n",
    "    MODEL = OL_MODEL\n",
    "    print(\"We will use OLLAMA.\")\n",
    "elif LLM_access == \"openrouter\":\n",
    "    myAI = openrouter\n",
    "    MODEL = OR_MODEL\n",
    "    print(\"We will use OPENROUTER\")\n",
    "else:\n",
    "    print(\"Please set to ollama or openrouter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443eda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define call_LLM function\n",
    "def call_LLM(sys_prompt, user_prompt):\n",
    "    stream = myAI.chat.completions.create(\n",
    "        model= MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430972cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set system_prompt\n",
    "system_prompt = \"You are an expert, patient, and encouraging AI tutor. \\\n",
    "    Your goal is to help me master AI engineering through a single answer to each question. \\\n",
    "    Here are two main principles you would follow when answering questions:\\n \\\n",
    "    1) Use Analogies: \\\n",
    "    Use analogies, real-world examples, and, if appropriate, simple visuals to explain abstract concepts.\\n \\\n",
    "    2) Maintain Engagement: \\\n",
    "    Use a friendly, encouraging tone. Try and give me a related, interesting fact each time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user for input\n",
    "user_prompt = input(\"What would you like me to explain today?\")\n",
    "user_prompt += \"\\nPlease explain this to me in a single answer.\"\n",
    "\n",
    "# Exercise question\n",
    "# Please explain what this code does and why: yield from {book.get(\"author\") for book in books if book.get(\"author\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the AI tutor chat going\n",
    "check = \"y\"\n",
    "\n",
    "while check == \"y\":\n",
    "    # Run this if user_prompt is a string\n",
    "    if isinstance(user_prompt, str):\n",
    "        call_LLM(\n",
    "            sys_prompt= system_prompt,\n",
    "            user_prompt= user_prompt\n",
    "        )\n",
    " \n",
    "    # Check if the user wants to keep chatting\n",
    "    check = input(\"Shall we keep going? Enter 'y' or 'n'.\")\n",
    "    if check == \"y\":\n",
    "        # Add previous chat output to user_prompt to create illusion of memory\n",
    "        user_prompt = input(\"What would you like me to explain today?\")\n",
    "        user_prompt += \"\\nPlease explain this to me in a single answer.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
