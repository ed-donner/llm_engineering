{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MS9vPBDDkd6"
      },
      "source": [
        "# Welcome to your first use of an LLM!\n",
        "\n",
        "If this is your first use of Google Colab, welcome to that, too! In later weeks we will explore this in more detail. For now, just follow the instructions and let me know any questions.\n",
        "\n",
        "First, press the `Connect` or `Reconnect` button near the top right of this window to start a new CPU box running in the Cloud.\n",
        "\n",
        "Next click in the \"cell\" immediately below this one, and press Shift+Enter to execute the cell. Ollama should be downloaded and installed. The exclamation mark at the start of the line indicates that this is a shell command, not python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZcqFHYODOBO",
        "outputId": "a4907366-a988-405b-9553-55fc5ac8092e"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3-RFec0EMwI"
      },
      "source": [
        "## When this completes..\n",
        "\n",
        "The cell above should now say: `Install complete. Run \"ollama\" from the command line.`\n",
        "\n",
        "Now click in the cell below and press Shift+Enter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j3vwuDHDwxI",
        "outputId": "c42bed0f-8d62-40ce-d632-33a2b8b02e12"
      },
      "outputs": [],
      "source": [
        "# Start Ollama server\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Start the command in a background process\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# The kernel can continue execution while the process runs in the background\n",
        "print(\"The 'ollama serve' process is running in the background.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx6JAHmKEkOs"
      },
      "source": [
        "# Great!\n",
        "\n",
        "It should now say above: `The 'ollama serve' process is running in the background`.\n",
        "\n",
        "Now press Shift+Enter in each of the cells below in turn to try out Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3VSOQYiGzco",
        "outputId": "69d0ee27-1e18-4ddf-9482-e0ccaf1c1aa3"
      },
      "outputs": [],
      "source": [
        "# Download the model that we want to use - this might take a couple of mins\n",
        "# The exclamation mark at the start of the line indicates that this is a shell command, not python\n",
        "\n",
        "!ollama pull llama3.2:1b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr4iQ6alDqdf",
        "outputId": "c2ea1c2c-1748-4236-e0ee-5e4f9af018fc"
      },
      "outputs": [],
      "source": [
        "# install the Ollama python API package\n",
        "\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7EoToiDqaq"
      },
      "outputs": [],
      "source": [
        "# import the package\n",
        "\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA2T87uWCMKl"
      },
      "outputs": [],
      "source": [
        "# Your first message to your very own LLM running \"locally\" (well, on your cloud box)\n",
        "\n",
        "message = \"Hi there! This is my first message to an LLM!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIRqzV2jDLLJ"
      },
      "outputs": [],
      "source": [
        "# Put this message into the format expected by Ollama\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": message}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qehl86LOAuzQ",
        "outputId": "2ec3de2b-842b-4650-f03f-a35aad11fcf3"
      },
      "outputs": [],
      "source": [
        "# Let's do this!!\n",
        "\n",
        "response = ollama.chat(model=\"llama3.2:1b\", messages=messages)\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9GZvqvzFTmU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
