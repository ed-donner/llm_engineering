{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585f935",
   "metadata": {},
   "source": [
    "*Kindly note that this exercise use OpenRouter for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from scraper import fetch_website_links, fetch_website_contents\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f2c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n",
      "Base URL was found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "openrouter_base_url = os.getenv('OPENROUTER_BASE_URL')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not openrouter_api_key:\n",
    "    print(\"No API key was found\")\n",
    "elif not openrouter_api_key.startswith(\"sk\"):\n",
    "    print(\"An API key was found, but it doesn't start with sk; please check you're using the right key\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n",
    "# Check the base url\n",
    "\n",
    "if not openrouter_base_url:\n",
    "    print(\"No Base URL was found\")\n",
    "elif not openrouter_base_url.startswith(\"https://\"):\n",
    "    print(\"Base URL was found, but it doesn't start with https\")\n",
    "else:\n",
    "    print(\"Base URL was found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-oss-120b'\n",
    "MODEL_LLAMA = 'meta-llama/llama-3.2-3b-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b778b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "openAI = OpenAI(base_url=openrouter_base_url, api_key=openrouter_api_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user and system prompt\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Please explain why this code breaks and the solution code:\n",
    "def calculate_average(numbers)\n",
    "    total = 0\n",
    "    for i in range(0, len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    average = total / len(numbers\n",
    "    return average\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You will be provided with some code, and a question about the code.\n",
    "Your job is to explain the code in a way that is easy to understand and why it works.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1771757284-kSv5jXFchGHphiWIK1i7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Code Explanation**\\n\\nThis line of code uses a combination of generator expressions and the `yield from` syntax in Python.\\n\\n**Breakdown:**\\n\\n* `yield from {...}`: This is the `yield from` expression, which allows a generator to delegate iteration to another generator or iterable.\\n* `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression, which creates an iterable sequence of values. It consists of two parts:\\n\\t+ `book.get(\"author\")`: This is an expression that gets the value of the `\"author\"` key from a dictionary (`book`) and returns it. The `get()` method returns `None` if the key is not present in the dictionary.\\n\\t+ `for book in books`: This is a `for` loop that iterates over an iterable (`books`).\\n\\t+ `if book.get(\"author\")`: This is a conditional clause that filters the iteration based on the value returned by `book.get(\"author\")`. Only books with an `\"author\"` key will be processed.\\n\\n**How it works:**\\n\\n1. The `yield from` expression delegates the iteration to the generator expression inside it.\\n2. The generator expression iterates over the `books` iterable, filtering out books without an `\"author\"` key.\\n3. For each book with an `\"author\"` key, the expression `book.get(\"author\")` is evaluated, and its value is yielded by the generator expression.\\n4. The `yield from` expression collects these yielded values and makes them available to the outer generator, which can then iterate over them.\\n\\n**Example Use Case:**\\n\\nSuppose you have a list of books, where each book is a dictionary with an `\"author\"` key:\\n```python\\nbooks = [\\n    {\"title\": \"Book 1\", \"author\": \"Author 1\"},\\n    {\"title\": \"Book 2\", \"author\": \"Author 2\"},\\n    {\"title\": \"Book 3\", \"author\": None},  # No author\\n    {\"title\": \"Book 4\"}\\n]\\n```\\nThe code would yield a generator that produces the `\"author\"` values for books with an `\"author\"` key:\\n```python\\ngenerator = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\\nfor author in generator:\\n    print(author)  # Output: Author 1, Author 2\\n```\\nThis code is concise and efficient, as it avoids creating an intermediate list of authors and only yields the values as they are needed.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1771757284, model='meta-llama/llama-3.2-3b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=523, prompt_tokens=77, total_tokens=600, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=1.2e-05, is_byok=False, cost_details={'upstream_inference_cost': 1.2e-05, 'upstream_inference_prompt_cost': 1.54e-06, 'upstream_inference_completions_cost': 1.046e-05}), provider='DeepInfra')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "This line of code uses a combination of generator expressions and the `yield from` syntax in Python.\n",
       "\n",
       "**Breakdown:**\n",
       "\n",
       "* `yield from {...}`: This is the `yield from` expression, which allows a generator to delegate iteration to another generator or iterable.\n",
       "* `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression, which creates an iterable sequence of values. It consists of two parts:\n",
       "\t+ `book.get(\"author\")`: This is an expression that gets the value of the `\"author\"` key from a dictionary (`book`) and returns it. The `get()` method returns `None` if the key is not present in the dictionary.\n",
       "\t+ `for book in books`: This is a `for` loop that iterates over an iterable (`books`).\n",
       "\t+ `if book.get(\"author\")`: This is a conditional clause that filters the iteration based on the value returned by `book.get(\"author\")`. Only books with an `\"author\"` key will be processed.\n",
       "\n",
       "**How it works:**\n",
       "\n",
       "1. The `yield from` expression delegates the iteration to the generator expression inside it.\n",
       "2. The generator expression iterates over the `books` iterable, filtering out books without an `\"author\"` key.\n",
       "3. For each book with an `\"author\"` key, the expression `book.get(\"author\")` is evaluated, and its value is yielded by the generator expression.\n",
       "4. The `yield from` expression collects these yielded values and makes them available to the outer generator, which can then iterate over them.\n",
       "\n",
       "**Example Use Case:**\n",
       "\n",
       "Suppose you have a list of books, where each book is a dictionary with an `\"author\"` key:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author 2\"},\n",
       "    {\"title\": \"Book 3\", \"author\": None},  # No author\n",
       "    {\"title\": \"Book 4\"}\n",
       "]\n",
       "```\n",
       "The code would yield a generator that produces the `\"author\"` values for books with an `\"author\"` key:\n",
       "```python\n",
       "generator = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "for author in generator:\n",
       "    print(author)  # Output: Author 1, Author 2\n",
       "```\n",
       "This code is concise and efficient, as it avoids creating an intermediate list of authors and only yields the values as they are needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "response = openAI.chat.completions.create(model = MODEL_LLAMA, messages = [{'role':'system','content':f'{system_prompt}'}, {'role':'user','content':f'{user_prompt}'}])\n",
    "print(response)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4408749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a **self‑contained, optimized Python script** that\n",
       "\n",
       "* loads only the three required sections of a site – the home page, the “about” page, and **up to five** blog‑post pages,\n",
       "* extracts the page **title**, **main text content**, and **URL**,\n",
       "* returns a **JSON list** where each entry is an object with those three fields,\n",
       "* respects simple polite‑scraping practices (user‑agent header, short request delay, optional robots.txt check).\n",
       "\n",
       "You can run it as a stand‑alone script or import the `scrape_site()` function into another program.\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "scrape_site.py\n",
       "\n",
       "Scrape a website's home page, about page and up to 5 blog posts.\n",
       "Produces a JSON array – one object per page with:\n",
       "    {\"url\": \"...\", \"title\": \"...\", \"content\": \"...\"}\n",
       "\"\"\"\n",
       "\n",
       "import json\n",
       "import time\n",
       "import argparse\n",
       "from urllib.parse import urljoin, urlparse\n",
       "\n",
       "import requests\n",
       "from bs4 import BeautifulSoup\n",
       "from ratelimit import limits, sleep_and_retry\n",
       "\n",
       "# --------------------------------------------------------------\n",
       "# Configuration\n",
       "# --------------------------------------------------------------\n",
       "USER_AGENT = \"MyScraperBot/1.0 (+https://example.com/contact)\"\n",
       "REQUESTS_PER_MINUTE = 30          # polite rate‑limit\n",
       "REQUEST_TIMEOUT = 15              # seconds\n",
       "MAX_BLOG_POSTS = 5                 # per the requirement\n",
       "BLOG_LINK_SELECTOR = \"a[href*='blog'], a[href*='post']\"  # simple heuristic\n",
       "ABOUT_PAGE_PATHS = [\"/about\", \"/about-us\", \"/aboutme\"]   # common slugs\n",
       "# --------------------------------------------------------------\n",
       "\n",
       "@sleep_and_retry\n",
       "@limits(calls=REQUESTS_PER_MINUTE, period=60)\n",
       "def fetch(url: str) -> requests.Response:\n",
       "    \"\"\"GET a URL with a nice user‑agent and timeout.\"\"\"\n",
       "    headers = {\"User-Agent\": USER_AGENT}\n",
       "    return requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
       "\n",
       "\n",
       "def clean_text(soup: BeautifulSoup) -> str:\n",
       "    \"\"\"\n",
       "    Return the visible text of a page, collapsing whitespace.\n",
       "    Tries to focus on primary article content:\n",
       "        * <article> tag if present\n",
       "        * otherwise the biggest <div>/<section> by text length\n",
       "    \"\"\"\n",
       "    # 1️⃣ Prefer <article>\n",
       "    article = soup.find(\"article\")\n",
       "    if article:\n",
       "        txt = article.get_text(separator=\" \", strip=True)\n",
       "        if txt:\n",
       "            return \" \".join(txt.split())\n",
       "\n",
       "    # 2️⃣ Fallback – largest text block\n",
       "    candidates = soup.find_all([\"div\", \"section\"], recursive=True)\n",
       "    best = \"\"\n",
       "    for cand in candidates:\n",
       "        txt = cand.get_text(separator=\" \", strip=True)\n",
       "        if len(txt) > len(best):\n",
       "            best = txt\n",
       "    return \" \".join(best.split())\n",
       "\n",
       "\n",
       "def get_title(soup: BeautifulSoup) -> str:\n",
       "    \"\"\"Extract the <title> tag text, falling back to first h1.\"\"\"\n",
       "    if soup.title and soup.title.string:\n",
       "        return soup.title.string.strip()\n",
       "    h1 = soup.find(\"h1\")\n",
       "    return h1.get_text(strip=True) if h1 else \"\"\n",
       "\n",
       "\n",
       "def extract_page(url: str) -> dict:\n",
       "    \"\"\"Download a page and return a dict with url, title, content.\"\"\"\n",
       "    resp = fetch(url)\n",
       "    resp.raise_for_status()\n",
       "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
       "    return {\n",
       "        \"url\": url,\n",
       "        \"title\": get_title(soup),\n",
       "        \"content\": clean_text(soup)\n",
       "    }\n",
       "\n",
       "\n",
       "def is_blog_link(href: str, base_netloc: str) -> bool:\n",
       "    \"\"\"\n",
       "    Very lightweight filter:\n",
       "      * same domain,\n",
       "      * contains typical blog keywords,\n",
       "      * does not point to about/contact/etc.\n",
       "    \"\"\"\n",
       "    if not href:\n",
       "        return False\n",
       "    parsed = urlparse(href)\n",
       "    # ignore external links\n",
       "    if parsed.netloc and parsed.netloc != base_netloc:\n",
       "        return False\n",
       "    # normalize path\n",
       "    path = parsed.path.lower()\n",
       "    # exclude obvious non‑blog pages\n",
       "    exclude = [\"about\", \"contact\", \"login\", \"signup\", \"privacy\"]\n",
       "    if any(term in path for term in exclude):\n",
       "        return False\n",
       "    # include if it looks like a blog post\n",
       "    return any(term in path for term in [\"blog\", \"post\", \"article\", \"/202\", \"/20\"])\n",
       "    \n",
       "\n",
       "def discover_blog_links(home_soup: BeautifulSoup, base_url: str, limit: int = MAX_BLOG_POSTS) -> list:\n",
       "    \"\"\"\n",
       "    Scan the home page (or any start page) for blog‑post URLs.\n",
       "    Returns a list of absolute URLs, deduplicated, limited to *limit* entries.\n",
       "    \"\"\"\n",
       "    base_parsed = urlparse(base_url)\n",
       "    found = []\n",
       "    for a in home_soup.select(BLOG_LINK_SELECTOR):\n",
       "        href = a.get(\"href\")\n",
       "        if is_blog_link(href, base_parsed.netloc):\n",
       "            full = urljoin(base_url, href)\n",
       "            if full not in found:\n",
       "                found.append(full)\n",
       "        if len(found) >= limit:\n",
       "            break\n",
       "    return found[:limit]\n",
       "\n",
       "\n",
       "def find_about_page(base_url: str) -> str:\n",
       "    \"\"\"\n",
       "    Try a handful of common about‑page slugs.\n",
       "    Returns the first URL that returns a 200 status, otherwise falls back to base_url.\n",
       "    \"\"\"\n",
       "    for slug in ABOUT_PAGE_PATHS:\n",
       "        candidate = urljoin(base_url, slug)\n",
       "        try:\n",
       "            r = fetch(candidate)\n",
       "            if r.status_code == 200:\n",
       "                return candidate\n",
       "        except Exception:\n",
       "            continue\n",
       "    # fallback – maybe the site’s root is the “about” page\n",
       "    return base_url\n",
       "\n",
       "\n",
       "def scrape_site(start_url: str) -> list:\n",
       "    \"\"\"\n",
       "    Main entry point.\n",
       "    Returns a list of JSON‑serialisable dicts (one per scraped page).\n",
       "    \"\"\"\n",
       "    results = []\n",
       "\n",
       "    # ------------------------------------------------------------------\n",
       "    # 1️⃣ Home page\n",
       "    # ------------------------------------------------------------------\n",
       "    home = extract_page(start_url)\n",
       "    results.append(home)\n",
       "\n",
       "    # Parse its soup once – needed for blog discovery\n",
       "    home_soup = BeautifulSoup(requests.get(start_url,\n",
       "                                            headers={\"User-Agent\": USER_AGENT},\n",
       "                                            timeout=REQUEST_TIMEOUT).text,\n",
       "                             \"html.parser\")\n",
       "\n",
       "    # ------------------------------------------------------------------\n",
       "    # 2️⃣ About page\n",
       "    # ------------------------------------------------------------------\n",
       "    about_url = find_about_page(start_url)\n",
       "    if about_url != start_url:                      # avoid double‑scraping home\n",
       "        about = extract_page(about_url)\n",
       "        results.append(about)\n",
       "\n",
       "    # ------------------------------------------------------------------\n",
       "    # 3️⃣ Blog posts (max 5)\n",
       "    # ------------------------------------------------------------------\n",
       "    blog_urls = discover_blog_links(home_soup, start_url, limit=MAX_BLOG_POSTS)\n",
       "    for bu in blog_urls:\n",
       "        try:\n",
       "            results.append(extract_page(bu))\n",
       "        except Exception as e:\n",
       "            # gracefully skip broken links\n",
       "            print(f\"[WARN] Could not scrape {bu}: {e}\")\n",
       "\n",
       "    return results\n",
       "\n",
       "\n",
       "# ----------------------------------------------------------------------\n",
       "# CLI helper\n",
       "# ----------------------------------------------------------------------\n",
       "def main():\n",
       "    parser = argparse.ArgumentParser(\n",
       "        description=\"Scrape a site’s home, about and up to 5 blog posts.\"\n",
       "    )\n",
       "    parser.add_argument(\n",
       "        \"url\",\n",
       "        help=\"Root URL of the website (e.g. https://example.com/)\",\n",
       "    )\n",
       "    parser.add_argument(\n",
       "        \"-o\",\n",
       "        \"--output\",\n",
       "        default=\"scraped.json\",\n",
       "        help=\"Path for the JSON output file (default: scraped.json)\",\n",
       "    )\n",
       "    args = parser.parse_args()\n",
       "\n",
       "    # Normalise the URL (ensure trailing slash)\n",
       "    start_url = args.url.rstrip(\"/\") + \"/\"\n",
       "\n",
       "    data = scrape_site(start_url)\n",
       "\n",
       "    with open(args.output, \"w\", encoding=\"utf-8\") as fp:\n",
       "        json.dump(data, fp, ensure_ascii=False, indent=2)\n",
       "\n",
       "    print(f\"✅ Done – {len(data)} pages saved to {args.output}\")\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "### How it works\n",
       "| Step | What the script does | Why it matters |\n",
       "|------|----------------------|----------------|\n",
       "| **Fetch** (`fetch`) | Sends a GET request with a custom *User‑Agent* and respects a **30‑req/min** rate‑limit. | Prevents over‑loading the target server and looks legitimate to the site. |\n",
       "| **Parse** (`BeautifulSoup`) | Parses the HTML with the fast **html.parser** backend. | No external parsers needed; keeps the script lightweight. |\n",
       "| **Extract title** (`get_title`) | Uses the `<title>` tag, falling back to the first `<h1>`. | Guarantees a readable title even if the `<title>` tag is missing. |\n",
       "| **Extract content** (`clean_text`) | Prioritises `<article>` text; otherwise selects the largest `<div>`/`<section>` block and collapses whitespace. | Gives you the main body without navigation, footers, or scripts. |\n",
       "| **Discover blog URLs** (`discover_blog_links`) | Looks for `<a>` elements whose `href` contains common blog keywords while staying on‑domain, and stops after **5** unique links. | Meets the “only 5 blog posts” rule while still finding likely articles automatically. |\n",
       "| **Find About page** (`find_about_page`) | Tries a short list of typical about‑page slugs (`/about`, `/about-us`, …). | Works for many sites without hard‑coding a specific URL. |\n",
       "| **JSON output** | Returns a list of objects `{url, title, content}` and writes them to a file (`scraped.json` by default). | Exactly the format you asked for. |\n",
       "\n",
       "### Customising the script\n",
       "* **Different blog selectors** – adjust `BLOG_LINK_SELECTOR` if the site uses a unique class/id for post links.\n",
       "* **More polite crawling** – uncomment the (optional) `robots.txt` check with `urllib.robotparser` if you need stricter compliance.\n",
       "* **Deeper content extraction** – integrate readability libraries such as `readability-lxml` for even cleaner article bodies.\n",
       "\n",
       "### Running the script\n",
       "```bash\n",
       "# Save the script as scrape_site.py, make it executable, then:\n",
       "python scrape_site.py https://example.com/ -o example_data.json\n",
       "```\n",
       "\n",
       "The resulting `example_data.json` will look like:\n",
       "\n",
       "```json\n",
       "[\n",
       "  {\n",
       "    \"url\": \"https://example.com/\",\n",
       "    \"title\": \"Example Domain\",\n",
       "    \"content\": \"This domain is for use in illustrative examples ...\"\n",
       "  },\n",
       "  {\n",
       "    \"url\": \"https://example.com/about\",\n",
       "    \"title\": \"About Us\",\n",
       "    \"content\": \"We are a community of …\"\n",
       "  },\n",
       "  {\n",
       "    \"url\": \"https://example.com/blog/first-post\",\n",
       "    \"title\": \"First Blog Post\",\n",
       "    \"content\": \"Lorem ipsum dolor sit amet, consectetur …\"\n",
       "  },\n",
       "  ...\n",
       "]\n",
       "```\n",
       "\n",
       "Feel free to embed the `scrape_site` function in larger projects, add authentication headers, or extend the heuristics for finding blog pages—just keep the **home, about, and max‑5‑blog‑post** constraint in mind. Happy scraping!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get GPT OSS 120b to answer, with streaming\n",
    "stream = openAI.chat.completions.create(model = MODEL_GPT, messages = [{'role':'system','content':f'{system_prompt}'}, {'role':'user','content':f'{user_prompt}'}], stream=True)\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "response = \"\"\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
