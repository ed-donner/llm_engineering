{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise Solution comparing two LLM's with a third LLM\n",
    "\n",
    "Compare and contrast the difference response between two models when asked a technical question using a third LLM\n",
    "\n",
    "This notebook demonstrates how to compare the responses of two local Large Language Models (LLMs) using Deepseek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MODEL_Phi3 = 'phi3:latest'\n",
    "MODEL_LLAMA1b = \"llama3.2:1b\"\n",
    "MODEL_DEEPSEEK = \"deepseek-r1:1.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an assistant that takes a technical question and respond with an explanation.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "please explain the difference between quantization and tokenization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Answer the question in detail but keep it:\n",
    "\n",
    "- Brief\n",
    "- Bullet points\n",
    "- Concise\n",
    "- Maximum 300 words\n",
    "\n",
    "Question:\n",
    "\"\"\" + question\n",
    "\n",
    "def message():\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_phi3_response():\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_Phi3,\n",
    "        messages=message()\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "def get_llama_response():\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA1b,\n",
    "        messages=message()\n",
    "    )\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342a470c-9aab-4051-ad21-514dceec76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compare the two models\n",
    "\n",
    "def compare_models():\n",
    "    phi_response = get_phi3_response()\n",
    "    llama_response = get_llama_response()\n",
    "\n",
    "    # Display Model A Response\n",
    "    display(Markdown(\"## Response A — Phi3\\n\"))\n",
    "    display(Markdown(phi_response))\n",
    "\n",
    "    # Display Model B Response\n",
    "    display(Markdown(\"\\n---\\n\"))\n",
    "    display(Markdown(\"## Response B — Llama 3.2 1B\\n\"))\n",
    "    display(Markdown(llama_response))\n",
    "\n",
    "\n",
    "    comparison_prompt = f\"\"\"\n",
    "    You are evaluating two LLM responses to the same technical question.\n",
    "\n",
    "    Response A (Phi3):\n",
    "    {phi_response}\n",
    "\n",
    "    Response B (Llama 3.2 1B):\n",
    "    {llama_response}\n",
    "\n",
    "    Do the following:\n",
    "    1. List similarities\n",
    "    2. List differences\n",
    "    3. Compare technical depth\n",
    "    4. Compare clarity\n",
    "    5. Compare correctness\n",
    "    6. Recommend which model performed better and why\n",
    "\n",
    "    Be structured and objective.\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation = ollama.chat(\n",
    "        model=MODEL_DEEPSEEK,   # you can also use llama here\n",
    "        messages=[{\"role\": \"user\", \"content\": comparison_prompt}]\n",
    "    )\n",
    "\n",
    "    display(Markdown(\"\\n---\\n\"))\n",
    "    display(Markdown(\"## Model Evaluation\\n\"))\n",
    "    display(Markdown(evaluation['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9377b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5956a3",
   "metadata": {},
   "source": [
    "Execute function to compare the Phi3 and Ollama models using Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dad8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
