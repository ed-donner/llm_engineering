{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654d18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "ollama_client = OpenAI(api_key=api_key, base_url=\"http://localhost:11434/v1\")\n",
    "openai_client = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8606d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" \n",
    "You are a technical assistant and you can give answers and explanation to techinical questions such as code, algorithms, etc.\n",
    "You can also help with general questions.\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(user_prompt):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63582504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GPT with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92fed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is a critical process in natural language processing (NLP) and is especially important in the context of large language models (LLMs) like GPT-3 or BERT. It involves breaking down text into smaller units, called tokens, which can then be processed by these models. Here's a breakdown of what tokenization entails:\n",
      "\n",
      "### 1. **Purpose of Tokenization**\n",
      "   - **Text Preprocessing**: Tokenization simplifies the handling of text by converting it from a continuous string into discrete units.\n",
      "   - **Standardization**: It allows for consistent processing of input text, regardless of its original format.\n",
      "\n",
      "### 2. **Types of Tokenization**\n",
      "   - **Word Tokenization**: Splits text into individual words or terms. For example, \"Hello, world!\" becomes [\"Hello\", \"world\"].\n",
      "   - **Subword Tokenization**: Breaks down words into smaller components (subwords) that capture the morphological structure of languages. For example, \"unhappiness\" may be tokenized into [\"un\", \"happiness\"]. This approach helps handle rare words better.\n",
      "   - **Character Tokenization**: Treats every character as a token. For example, \"Hi!\" would be tokenized as [\"H\", \"i\", \"!\"].\n",
      "\n",
      "### 3. **Methods of Tokenization**\n",
      "   - **Byte Pair Encoding (BPE)**: A popular method for subword tokenization that keeps the most frequent pairs of characters or character sequences together.\n",
      "   - **WordPiece**: Similar to BPE, but often used in models like BERT. It constructs a vocabulary based on frequency and often leads to handling out-of-vocabulary words effectively.\n",
      "   - **SentencePiece**: Another subword tokenization approach that can generate tokens based on using the entire input corpus, allowing for more flexible and language-agnostic tokenization.\n",
      "\n",
      "### 4. **Tokenization in LLMs**\n",
      "   - **Vocabulary**: Tokenization relies on a fixed vocabulary, which is the set of tokens that the model can recognize. Each token corresponds to a unique integer ID.\n",
      "   - **Input Representation**: Once tokenized, each token is mapped to a vector representation (embedding), which is used as input for the model.\n",
      "   - **Handling Context**: Tokenization can affect how context is understood and managed in language models, as it influences how meaning is preserved across and within sentences.\n",
      "\n",
      "### 5. **Challenges with Tokenization**\n",
      "   - **Language Variability**: Different languages and writing systems may require different tokenization strategies.\n",
      "   - **Ambiguity**: Tokenization may face challenges with homographs, contractions, and idioms, which can complicate the meaning extraction process.\n",
      "   - **Out-of-Vocabulary (OOV) Issues**: Ensuring that the model can efficiently work with uncommon or newly coined words is a concern that subword tokenization helps mitigate.\n",
      "\n",
      "In summary, tokenization is a foundational step in preparing text data for large language models, allowing them to understand, generate, and manipulate human language effectively."
     ]
    }
   ],
   "source": [
    "# GPT with streaming\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=create_prompt(\"What is tokenization in large language models?\"),\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc71d5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5a2674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is a crucial preprocessing step for large language models (LLMs). In essence, it's the process of breaking down text into individual units, called tokens. These tokens can be words, subwords, characters, or even symbols.\n",
      "\n",
      "Here's why tokenization is important in LLMs:\n",
      "\n",
      "**Goals of Tokenization:**\n",
      "\n",
      "1.  **Input representation**: Tokenization allows the model to process input text as a sequence of tokens, which can be fed into the neural network.\n",
      "2.  **Output generation**: When generating output from a language model, tokens represent individual words or phrases.\n",
      "\n",
      "**Types of Tokens:**\n",
      "\n",
      "1.  **Word tokens**: Individual words (e.g., \"hello\").\n",
      "2.  **Subword tokens**: Smaller units within words that capture sub-linguistic aspects, such as prefixes and suffixes (e.g., \".in\" for \"into\", \"word\", etc.). Subwords help with out-of-vocabulary (OOV) word handling.\n",
      "3.  **Character-level tokenization**: Breaking down text into individual characters.\n",
      "\n",
      "**Tokenization Challenges:**\n",
      "\n",
      "1.  **Word boundaries**: Identifying where to split words, ensuring that subwords are not created unnecessarily.\n",
      "2.  **Punctuation and special characters**: Handling punctuation marks (e.g., periods) or special characters (e.g., !), which might be treated as separate tokens or ignored.\n",
      "\n",
      "**Common Tokenization Techniques:**\n",
      "\n",
      "1.  **WordPiece tokenization**: Used by BERT (Bidirectional Encoder Representations from Transformers). This method split words into subword units based on character likelihoods.\n",
      "2.  **WordPiece + BytePair encoding**: A variation of wordpiece tokenization, which uses both character and word frequencies for better coverage.\n",
      "\n",
      "**Tokenization in Practice:**\n",
      "\n",
      "1.  Text preprocessing scripts (e.g., using the NLTK library).\n",
      "2.  Pre-trained LLMs typically apply the optimal tokenization scheme during inference or training phases.\n",
      "\n",
      "By understanding how tokenization works, you can develop more efficient language models that excel at processing natural language inputs and generating meaningful outputs!\n"
     ]
    }
   ],
   "source": [
    "# `llama3.2`\n",
    "response = ollama_client.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=create_prompt(\"What is tokenization in large language models?\")\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8998087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
