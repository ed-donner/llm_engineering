{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Tutor de Espanhol para Brasileiros com Ollama\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Um tutor interativo de espanhol feito sob medida para falantes de portugues brasileiro, rodando 100% local via **Ollama**.\n",
    "\n",
    "O tutor:\n",
    "- Responde em PT-BR, incorporando exemplos em espanhol naturalmente\n",
    "- Foca nas diferencas e similaridades entre portugues e espanhol\n",
    "- Corrige erros com gentileza e da dicas praticas\n",
    "- Mantem o contexto da conversa (memoria entre celulas)\n",
    "\n",
    "## Pre-requisitos\n",
    "\n",
    "- Ollama instalado e rodando (`ollama serve`)\n",
    "- Modelo `gemma3:4b` baixado (ou sera baixado automaticamente)\n",
    "- Pacote Python: `openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "MODEL = \"gemma3:4b\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "print(f\"Cliente Ollama configurado com modelo: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "installed = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "if MODEL not in installed.stdout:\n",
    "    print(f\"Modelo {MODEL} nao encontrado. Baixando...\")\n",
    "    subprocess.run([\"ollama\", \"pull\", MODEL])\n",
    "    print(f\"Modelo {MODEL} instalado com sucesso!\")\n",
    "else:\n",
    "    print(f\"Modelo {MODEL} ja esta instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Voce e um tutor de espanhol paciente, amigavel e descontraido, especializado em ensinar espanhol para brasileiros.\n",
    "\n",
    "Regras:\n",
    "- Responda sempre em portugues brasileiro, usando linguagem informal e acessivel.\n",
    "- Incorpore palavras, frases e exemplos em espanhol naturalmente nas suas respostas.\n",
    "- Destaque as diferencas e similaridades entre portugues brasileiro e espanhol.\n",
    "- Quando o usuario cometer erros, corrija com gentileza e explique o porquê.\n",
    "- De dicas praticas e exemplos do dia a dia.\n",
    "- Use comparacoes com o portugues para facilitar o aprendizado.\n",
    "- Responda em markdown. Nao coloque o markdown dentro de um bloco de codigo - responda direto com o markdown.\n",
    "\"\"\"\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "print(\"System prompt e historico de conversa configurados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_message):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    response = ollama.chat.completions.create(model=MODEL, messages=conversation_history)\n",
    "    reply = response.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Oi! Quero aprender espanhol. Por onde comeco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Me ensine algumas frases basicas para usar no dia a dia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Quais sao os falsos cognatos mais comuns entre portugues e espanhol?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes-markdown",
   "metadata": {},
   "source": [
    "## Como funciona a memoria do tutor\n",
    "\n",
    "A cada chamada de `chat()`, a mensagem do usuario e a resposta do modelo sao adicionadas ao `conversation_history`. Na proxima chamada, **todo o historico** e enviado ao modelo, permitindo que ele mantenha o contexto e faca referencias a temas ja discutidos.\n",
    "\n",
    "Isso significa que a 2a e 3a interacao levam em conta o que foi dito antes — o tutor \"lembra\" da conversa.\n",
    "\n",
    "## Continue explorando!\n",
    "\n",
    "Use `chat()` nas celulas abaixo para continuar a conversa. Algumas ideias:\n",
    "\n",
    "- `chat(\"Como eu me apresento em espanhol?\")`\n",
    "- `chat(\"Me ajuda a pedir comida em um restaurante?\")`\n",
    "- `chat(\"Qual a diferenca entre 'ser' e 'estar' em espanhol?\")`\n",
    "- `chat(\"Me ensina os numeros de 1 a 20\")`\n",
    "- `chat(\"Como eu pergunto as horas em espanhol?\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-free",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Como eu me apresento em espanhol?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
