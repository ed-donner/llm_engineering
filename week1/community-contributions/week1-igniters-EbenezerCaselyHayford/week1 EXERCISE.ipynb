{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {},
      "source": [
        "## üìü CodeSmell: The Passive-Aggressive Senior Dev Tutor\n",
        "\n",
        "üìü CodeSmell: The Passive-Aggressive Senior Dev Tutor\n",
        "CodeSmell is a real-time Python tutor designed for developers who find standard linters too polite. Powered by OpenAI‚Äôs Chat Completions API, this tool acts as a jaded, over-caffeinated Senior Developer who has no patience for your un-Pythonic habits.\n",
        "\n",
        "Instead of dry error codes, CodeSmell delivers:\n",
        "The Roast: A biting, snarky critique of your code's anti-patterns.\n",
        "The Redemption: A streamed, optimized, and \"Pythonic\" rewrite of your logic.\n",
        "Instant Feedback: Utilizing One-Shot Prompting and Streaming API calls for near-zero latency code reviews.\n",
        "\n",
        "Stop writing code that makes your seniors cry. Submit your scripts, take the roast, and learn to write Python the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1070317-3ed9-4659-abe3-828943230e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_GEMMA = 'gemma3:270m'\n",
        "# Pull the model if not already pulled\n",
        "!ollama list | grep -q \"$MODEL_GEMMA\" && echo \"‚úÖ $MODEL_GEMMA already exists\" || (echo \"üöÄ Pulling $MODEL_GEMMA model...\" && echo \"‚è≥ This may take a few minutes depending on size...\" && ollama pull \"$MODEL_GEMMA\" && echo \"‚úÖ Download completed! $MODEL_GEMMA is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment\n",
        "load_dotenv(override=True)\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
      "metadata": {},
      "outputs": [],
      "source": [
        "# here is the question; type over this to ask something new\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a brilliant but deeply impatient Senior Python Developer who has been reviewing junior code for 20 years. \n",
        "You hate inefficient loops, global variables, and poor naming conventions.\n",
        "A junior is showing you their code. Roast the anti-pattern you see and provide a better way.\n",
        "Response in a markdown format.\n",
        "if the code is not python, respond politely to the user that you are only trained to answer Python related code questions.\n",
        "Format your response as follows:\n",
        "ROAST: [Your snarky comment]\n",
        "FIX: [The Pythonic code]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6352dec",
      "metadata": {},
      "outputs": [],
      "source": [
        "openai = OpenAI()\n",
        "\n",
        "def review_code(user_prompt,use_local_model=False):\n",
        "    print(f\"Using {MODEL_GPT if not use_local_model else MODEL_GEMMA} model\")\n",
        "    if use_local_model:\n",
        "        openai = OpenAI(base_url=OLLAMA_BASE_URL,api_key=OLLAMA_API_KEY)\n",
        "    else:\n",
        "        openai = OpenAI()\n",
        "    stream = openai.chat.completions.create(\n",
        "        model=MODEL_GPT if not use_local_model else MODEL_GEMMA,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "    print(\"--- LIVE CODE REVIEW ---\")\n",
        "    response = \"\"\n",
        "    display_handle = display(Markdown(\"\"), display_id=True)\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
        "    print(\"\\n-----------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62da021",
      "metadata": {},
      "outputs": [],
      "source": [
        "python_code_prompt = \"\"\"\n",
        "Analyze the following Python code, identify the issues with it and provide a refactored version of the code.\n",
        "def process_data(new_item, data_list=[]):\n",
        "    data_list.append(new_item)\n",
        "    f = open(\"log.txt\", \"a\")\n",
        "    report = \"\"\n",
        "    for i in range(len(data_list)):\n",
        "        report += \"Item \" + str(i) + \": \" + str(data_list[i]) + \"\\n\"\n",
        "        \n",
        "    f.write(report)\n",
        "    f.close() \n",
        "    # (Note: if f.write fails, f.close never runs!)\n",
        "    \n",
        "    return data_list\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff1ab41",
      "metadata": {},
      "outputs": [],
      "source": [
        "non_python_code_prompt = \"\"\"\n",
        "Analyze the following code, identify the issues with it and provide a refactored version of the code.\n",
        "\n",
        "using System;\n",
        "using System.Collections.Generic;\n",
        "using System.IO;\n",
        "using System.Linq;\n",
        "\n",
        "public class DataProcessor\n",
        "{\n",
        "    // Use 'null' as default for lists to avoid shared state issues\n",
        "    public List<string> ProcessData(string newItem, List<string>? dataList = null)\n",
        "    {\n",
        "        // 1. Null Coalescing: Ensure we have a valid list instance\n",
        "        dataList ??= new List<string>();\n",
        "        dataList.Add(newItem);\n",
        "\n",
        "        // 2. Resource Management: 'using' statement ensures the file is closed/disposed\n",
        "        // even if an exception occurs during writing.\n",
        "        using (StreamWriter sw = new StreamWriter(\"log.txt\", append: true))\n",
        "        {\n",
        "            // 3. LINQ & Interpolation: Replaces manual for-loops and '+' concatenation.\n",
        "            // Select maps the list to indexed strings; Join combines them efficiently.\n",
        "            var reportLines = dataList\n",
        "                .Select((item, index) => $\"Item {index}: {item}\");\n",
        "\n",
        "            string finalReport = string.Join(Environment.NewLine, reportLines);\n",
        "            \n",
        "            sw.WriteLine(finalReport);\n",
        "        }\n",
        "\n",
        "        return dataList;\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get gpt-4o-mini to answer, with streaming\n",
        "# you can try with the non_python_code_prompt or python_code_prompt to see how it works\n",
        "review_code(python_code_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get gemma3:270m to answer\n",
        "# you can try with the non_python_code_prompt or python_code_prompt to see how it works\n",
        "review_code(non_python_code_prompt, use_local_model=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
