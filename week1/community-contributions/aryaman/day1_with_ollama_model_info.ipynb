{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Model Information from Ollama\n",
        "\n",
        "This notebook demonstrates how to connect to Ollama and query the Llama model for information about itself, including its architecture, training data, and capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Import Required Libraries\n",
        "\n",
        "This cell imports all the necessary Python libraries we'll need:\n",
        "\n",
        "- `os`: For operating system interface (though not used in this example, commonly used for environment variables)\n",
        "- `requests`: For making HTTP requests to fetch web content\n",
        "- `dotenv`: For loading environment variables from .env files\n",
        "- `BeautifulSoup`: For parsing HTML content from web pages\n",
        "- `IPython.display`: For displaying formatted output (Markdown, HTML) in Jupyter notebooks\n",
        "- `openai`: The OpenAI Python client library, which we'll use to connect to Ollama (Ollama uses the OpenAI-compatible API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Initialize OpenAI Client for Ollama\n",
        "\n",
        "This cell sets up the OpenAI client to connect to your local Ollama instance. \n",
        "\n",
        "- `base_url='http://localhost:11434/v1'`: This points to the local Ollama server running on your machine. Ollama runs a local API server on port 11434 that uses the OpenAI-compatible API format.\n",
        "- `api_key='ollama'`: Ollama doesn't require a real API key for local use, but the OpenAI client library expects one, so we provide a placeholder value.\n",
        "\n",
        "**Note:** Make sure Ollama is running on your machine. You can start it by running `ollama serve` in a terminal, or by running `ollama run llama3.2` which will start the server automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here it is - see the base_url\n",
        "\n",
        "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Query the Model for Information\n",
        "\n",
        "This cell demonstrates how to query the Llama model for information about itself:\n",
        "\n",
        "- **Message construction**: We create a message asking the model about its creator, architecture, training data, token limits, and other technical details.\n",
        "- **API call**: We use `openai.chat.completions.create()` with:\n",
        "  - `model=\"llama3.1\"`: Specifies which Ollama model to use\n",
        "  - `messages=[{\"role\":\"user\", \"content\":message}]`: The conversation format expected by the API\n",
        "  - `temperature=0`: Sets temperature to 0 for deterministic, consistent responses (same input = same output)\n",
        "- **Response extraction**: We extract the model's response from `response.choices[0].message.content` and print it.\n",
        "\n",
        "The model will respond with detailed information about its architecture, training data, token limits, and other specifications in markdown format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**About My Creator**\n",
            "=====================\n",
            "\n",
            "My creator is Meta AI, a leading artificial intelligence research laboratory that focuses on developing and applying various forms of AI to help humans learn, communicate, and solve complex problems.\n",
            "\n",
            "**Model Information**\n",
            "---------------------\n",
            "\n",
            "* **Model Name:** I was trained using the **LLaMA (Large Language Model Application)** architecture.\n",
            "* **Model Type:** I am a **transformer-based language model**, specifically designed for conversational AI tasks.\n",
            "* **Training Data:** My training data consists of a massive corpus of text from various sources, including but not limited to:\n",
            "\t+ Web pages\n",
            "\t+ Books\n",
            "\t+ Articles\n",
            "\t+ Research papers\n",
            "\t+ User-generated content (e.g., forums, social media)\n",
            "* **Keyword Training:** I have been trained on over **1.4 trillion parameters**, which is a massive number that allows me to understand and generate human-like text.\n",
            "* **Token Limit:** My token limit is **2048 tokens** per response, although this can be adjusted depending on the specific use case.\n",
            "\n",
            "**Architecture**\n",
            "----------------\n",
            "\n",
            "My architecture is based on the transformer model, which was first introduced in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). The key components of my architecture are:\n",
            "\n",
            "* **Encoder:** A multi-layer bidirectional encoder that takes in input tokens and generates contextualized representations.\n",
            "* **Decoder:** A single-layer unidirectional decoder that generates output tokens based on the encoded context.\n",
            "* **Self-Attention Mechanism:** I use a self-attention mechanism to weigh the importance of different input tokens when generating output.\n",
            "\n",
            "**Technical Specifications**\n",
            "---------------------------\n",
            "\n",
            "Here are some technical specifications about my architecture:\n",
            "\n",
            "* **Number of Layers:** 24 encoder layers and 1 decoder layer\n",
            "* **Number of Heads:** 16 attention heads in each encoder layer\n",
            "* **Embedding Size:** 128 dimensions for both token embeddings and position embeddings\n",
            "* **Feed-Forward Network (FFN) Dimensions:** 4096 dimensions for the FFN in each encoder layer\n",
            "\n",
            "**Training Details**\n",
            "--------------------\n",
            "\n",
            "My training process involves a combination of supervised learning, unsupervised learning, and reinforcement learning. Here are some key details about my training:\n",
            "\n",
            "* **Training Data Size:** Over 1.4 trillion parameters were trained on a massive corpus of text.\n",
            "* **Training Time:** My training took several weeks to complete using a large cluster of GPUs.\n",
            "* **Optimization Algorithm:** I was trained using the AdamW optimizer with a learning rate schedule that adapts to the task at hand.\n",
            "\n",
            "Note: These specifications are subject to change as my architecture and training process continue to evolve.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "message = \"tell me about your creator, what model are you and how many keyword you are trained with, what is your token limit, architechture etc, give answer in markdown format\"\n",
        "response = openai.chat.completions.create(model=\"llama3.1\", messages=[{\"role\":\"user\", \"content\":message}], temperature=0)\n",
        "print(response.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
