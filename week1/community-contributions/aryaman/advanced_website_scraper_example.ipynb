{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Website Scraper - Usage Examples\n",
        "\n",
        "This notebook demonstrates how to use the `AdvancedWebsite` class for web scraping.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Handles JavaScript-rendered pages** (React, Vue, etc.) using Playwright\n",
        "- **Automatic fallback** to BeautifulSoup for static sites (faster)\n",
        "- **Better error handling** with retry logic\n",
        "- **Enhanced text extraction** with improved cleaning\n",
        "- **Metadata extraction** (description, keywords, Open Graph tags)\n",
        "- **Improved link extraction** with validation and normalization\n",
        "- **Built-in summarization** using Ollama via OpenAI-compatible API\n",
        "\n",
        "## Important Note\n",
        "\n",
        "If you update `advanced_website_scraper.py` and the changes don't appear, **re-run Cell 2** (the import cell) to reload the module. The import cell includes automatic reloading, but you may need to re-run it after making changes to the Python file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "If you want to use Playwright for JavaScript-rendered pages, you'll need to install it:\n",
        "\n",
        "```bash\n",
        "pip install playwright\n",
        "playwright install\n",
        "```\n",
        "\n",
        "Note: Playwright is optional. The scraper will fall back to requests + BeautifulSoup if Playwright is not available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AdvancedWebsite class imported successfully\n",
            "✓ Available methods: ['display_images', 'get_contents', 'get_fetch_method', 'get_images', 'get_links', 'get_metadata', 'summarize', 'summarize_with_ollama']\n"
          ]
        }
      ],
      "source": [
        "# Import the AdvancedWebsite class\n",
        "# Note: This assumes advanced_website_scraper.py is in the same directory\n",
        "# If running from a different location, you may need to adjust the import path\n",
        "import sys\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "# Add current directory to path if needed\n",
        "current_dir = os.path.dirname(os.path.abspath(''))\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "# Import the module\n",
        "import advanced_website_scraper\n",
        "\n",
        "# Reload the module to pick up any changes (useful during development)\n",
        "importlib.reload(advanced_website_scraper)\n",
        "\n",
        "# Import the class\n",
        "from advanced_website_scraper import AdvancedWebsite\n",
        "\n",
        "print(\"✓ AdvancedWebsite class imported successfully\")\n",
        "print(f\"✓ Available methods: {[m for m in dir(AdvancedWebsite) if not m.startswith('_')]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your website URL and model name here. Change these variables to test different websites and models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Website URL: https://streamlit.io\n",
            "  Model Name: llama3.1\n",
            "  Use JavaScript: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables - Change these to test different websites and models\n",
        "WEBSITE_URL = \"https://streamlit.io\"  # Change this to any website URL\n",
        "MODEL_NAME = \"llama3.1\"  # Change this to your preferred Ollama model (e.g., \"llama3.2\", \"llama3.1\", etc.)\n",
        "USE_JS = True  # Set to True to force JavaScript rendering (requires Playwright)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Website URL: {WEBSITE_URL}\")\n",
        "print(f\"  Model Name: {MODEL_NAME}\")\n",
        "print(f\"  Use JavaScript: {USE_JS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Scraping a Static Website (Fast)\n",
        "\n",
        "For static websites, the scraper will use requests + BeautifulSoup, which is faster and lighter.\n",
        "\n",
        "**Note:** This example uses the `WEBSITE_URL` variable defined in the configuration cell above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch method: playwright\n",
            "\n",
            "Title: Streamlit • A faster way to build and share data apps\n",
            "\n",
            "Content preview (first 500 chars):\n",
            "Turn your data scripts into shareable web apps in minutes.\n",
            "All in pure Python. No front‑end experience required.\n",
            "Get started\n",
            "Try the live playground!\n",
            "Learn more with the\n",
            "Streamlit crash course on YouTube\n",
            "Trusted by\n",
            "over 90% of Fortune 50\n",
            "companies\n",
            "Get started in under a minute\n",
            "Streamlit is an\n",
            "open-source\n",
            "app framework that is a breeze to get started with.\n",
            "Just install it like any other Python library:\n",
            "pip install streamlit\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "streamlit hello\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "A\n",
            "\n",
            "Number of links found: 87\n"
          ]
        }
      ],
      "source": [
        "# Scrape a static website using the configured URL\n",
        "website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "\n",
        "print(f\"Fetch method: {website.get_fetch_method()}\")\n",
        "print(f\"\\nTitle: {website.title}\")\n",
        "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
        "print(f\"\\nNumber of links found: {len(website.get_links())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Scraping a JavaScript-Rendered Website\n",
        "\n",
        "For websites that use JavaScript to render content (like React, Vue, etc.), use Playwright mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-31 (worker):\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/week1/community-contributions/aryaman/advanced_website_scraper.py\", line 185, in worker\n",
            "    success = self._fetch_with_playwright_sync()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/week1/community-contributions/aryaman/advanced_website_scraper.py\", line 202, in _fetch_with_playwright_sync\n",
            "    page.goto(self.url, wait_until=\"networkidle\", timeout=self.timeout * 1000)\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/sync_api/_generated.py\", line 9050, in goto\n",
            "    self._sync(\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/_impl/_sync_base.py\", line 115, in _sync\n",
            "    return task.result()\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/_impl/_page.py\", line 552, in goto\n",
            "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/_impl/_frame.py\", line 153, in goto\n",
            "    await self._channel.send(\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/_impl/_connection.py\", line 69, in send\n",
            "    return await self._connection.wrap_api_call(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/aryaman/LlmProjects/llm_engineering/.venv/lib/python3.12/site-packages/playwright/_impl/_connection.py\", line 559, in wrap_api_call\n",
            "    raise rewrite_error(error, f\"{parsed_st['apiName']}: {error}\") from None\n",
            "playwright._impl._errors.TimeoutError: Page.goto: Timeout 60000ms exceeded.\n",
            "Call log:\n",
            "  - navigating to \"https://openai.com/\", waiting until \"networkidle\"\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch method: unknown\n",
            "\n",
            "Title: \n",
            "\n",
            "Content preview (first 500 chars):\n",
            "\n",
            "\n",
            "Number of links found: 0\n"
          ]
        }
      ],
      "source": [
        "# Scrape a JavaScript-rendered website (like OpenAI)\n",
        "# Note: This requires Playwright to be installed\n",
        "try:\n",
        "    website = AdvancedWebsite(\"https://openai.com\", use_js=True, timeout=60)\n",
        "    \n",
        "    print(f\"Fetch method: {website.get_fetch_method()}\")\n",
        "    print(f\"\\nTitle: {website.title}\")\n",
        "    print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
        "    print(f\"\\nNumber of links found: {len(website.get_links())}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure Playwright is installed: pip install playwright && playwright install\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Automatic Mode (Smart Fallback)\n",
        "\n",
        "The scraper can automatically detect the best method. It tries requests first (faster), and falls back to Playwright if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch method used: requests\n",
            "\n",
            "Title: Streamlit • A faster way to build and share data apps\n",
            "\n",
            "Content preview (first 500 chars):\n",
            "Turn your data scripts into shareable web apps in minutes.\n",
            "All in pure Python. No front‑end experience required.\n",
            "Get started\n",
            "Try the live playground!\n",
            "Learn more with the\n",
            "Streamlit crash course on YouTube\n",
            "Trusted by\n",
            "over 90% of Fortune 50\n",
            "companies\n",
            "Get started in under a minute\n",
            "Streamlit is an\n",
            "open-source\n",
            "app framework that is a breeze to get started with.\n",
            "Just install it like any other Python library:\n",
            "pip install streamlit\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "streamlit hello\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "A\n"
          ]
        }
      ],
      "source": [
        "# Let the scraper decide the best method using the configured URL\n",
        "website = AdvancedWebsite(WEBSITE_URL)\n",
        "\n",
        "print(f\"Fetch method used: {website.get_fetch_method()}\")\n",
        "print(f\"\\nTitle: {website.title}\")\n",
        "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Getting Formatted Contents\n",
        "\n",
        "Use the `get_contents()` method to get a formatted string with title and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Webpage Title:\n",
            "Streamlit • A faster way to build and share data apps\n",
            "\n",
            "Webpage Contents:\n",
            "Turn your data scripts into shareable web apps in minutes.\n",
            "All in pure Python. No front‑end experience required.\n",
            "Get started\n",
            "Try the live playground!\n",
            "Learn more with the\n",
            "Streamlit crash course on YouTube\n",
            "Trusted by\n",
            "over 90% of Fortune 50\n",
            "companies\n",
            "Get started in under a minute\n",
            "Streamlit is an\n",
            "open-source\n",
            "app framework that is a breeze to get started with.\n",
            "Just install it like any other Python library:\n",
            "pip install streamlit\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "streamlit hello\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "And that's it! Next, check out our\n",
            "documentation\n",
            "and\n",
            "forums\n",
            "for more.\n",
            "Or you can skip local installation altogether:\n",
            "→\n",
            "Try a live playground in your browser\n",
            "The easiest way to try Streamlit before you install.\n",
            "→\n",
            "Build in public with Streamlit Community Cloud\n",
            "Public apps only. Totally free. You just need a GitHub account.\n",
            "→\n",
            "Build like a pro on Snowflake\n",
            "Unlimited private apps. Enterprise-grade reliability and s\n",
            "\n",
            "==================================================\n",
            "Limited to 500 characters:\n",
            "Webpage Title:\n",
            "Streamlit • A faster way to build and share data apps\n",
            "\n",
            "Webpage Contents:\n",
            "Turn your data scripts into shareable web apps in minutes.\n",
            "All in pure Python. No front‑end experience required.\n",
            "Get started\n",
            "Try the live playground!\n",
            "Learn more with the\n",
            "Streamlit crash course on YouTube\n",
            "Trusted by\n",
            "over 90% of Fortune 50\n",
            "companies\n",
            "Get started in under a minute\n",
            "Streamlit is an\n",
            "open-source\n",
            "app framework that is a breeze to get started with.\n",
            "Just install it like any other Python library:\n",
            "pip install streamlit\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "streamlit hello\n",
            "← Copy to clipboard\n",
            "Copied!\n",
            "A...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(WEBSITE_URL)\n",
        "\n",
        "# Get full contents\n",
        "contents = website.get_contents()\n",
        "print(contents[:1000])  # Print first 1000 characters\n",
        "\n",
        "# Get contents with length limit\n",
        "limited_contents = website.get_contents(max_length=500)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Limited to 500 characters:\")\n",
        "print(limited_contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Extracting Links\n",
        "\n",
        "Get all links found on the page, with automatic validation and normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 99 links:\n",
            "1. https://techcrunch.com/\n",
            "2. https://techcrunch.com/category/security/\n",
            "3. https://techcrunch.com/2025/12/17/cisco-says-chinese-hackers-are-exploiting-its-customers-with-a-new-zero-day/\n",
            "4. https://techcrunch.com/author/lorenzo-franceschi-bicchierai/\n",
            "5. https://techcrunch.com/category/media-entertainment/\n",
            "6. https://techcrunch.com/2025/12/17/youtube-will-stream-the-oscars-exclusively-beginning-in-2029/\n",
            "7. https://techcrunch.com/author/aisha-malik/\n",
            "8. https://techcrunch.com/category/transportation/\n",
            "9. https://techcrunch.com/2025/12/17/rad-power-bikes-files-for-bankruptcy-and-is-looking-to-sell-the-business/\n",
            "10. https://techcrunch.com/author/sean-okane/\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(\"https://techcrunch.com/\")\n",
        "\n",
        "links = website.get_links()\n",
        "print(f\"Found {len(links)} links:\")\n",
        "for i, link in enumerate(links[:10], 1):  # Show first 10 links\n",
        "    print(f\"{i}. {link}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Extracting Metadata\n",
        "\n",
        "Extract metadata like description, keywords, Open Graph tags, and Twitter Card tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Metadata:\n",
            "==================================================\n",
            "description: We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.\n",
            "open_graph: {'title': 'OpenAI', 'description': 'We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.', 'locale': 'en-US', 'image': 'https://images.ctfassets.net/kftzwdyauwt9/3KGOHkSXu53naMuSFNaiwv/cdb0e2f899f524abb71314ab20e09c9c/OAI-white-on-black.png?w=1600&h=900&fit=fill', 'image:width': '1600', 'image:height': '900', 'image:alt': 'OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.', 'type': 'website'}\n",
            "twitter_card: {'card': 'summary_large_image', 'site': '@OpenAI', 'title': 'OpenAI', 'description': 'We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.', 'image': 'https://images.ctfassets.net/kftzwdyauwt9/3KGOHkSXu53naMuSFNaiwv/cdb0e2f899f524abb71314ab20e09c9c/OAI-white-on-black.png?w=1600&h=900&fit=fill', 'image:width': '1600', 'image:height': '900', 'image:alt': 'OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.'}\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(\"https://openai.com\")\n",
        "\n",
        "metadata = website.get_metadata()\n",
        "print(\"Extracted Metadata:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in metadata.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 7: Waiting for Dynamic Content\n",
        "\n",
        "If a page loads content dynamically, you can wait for a specific selector before extracting content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Wait for a specific element to load\n",
        "# This is useful for pages that load content via JavaScript\n",
        "# website = AdvancedWebsite(\n",
        "#     WEBSITE_URL,\n",
        "#     use_js=True,\n",
        "#     wait_for_selector=\"main-content\"  # Wait for element with this ID or class\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic scraper:\n",
            "Title: OpenAI\n",
            "Text length: 3506\n",
            "\n",
            "==================================================\n",
            "Advanced scraper:\n",
            "Title: OpenAI\n",
            "Text length: 107\n",
            "Links found: 3\n",
            "Metadata keys: ['description', 'open_graph', 'twitter_card']\n"
          ]
        }
      ],
      "source": [
        "# Compare the two approaches\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Basic approach (from the original scraper)\n",
        "def basic_scrape(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    title = soup.title.string if soup.title else \"No title found\"\n",
        "    if soup.body:\n",
        "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            irrelevant.decompose()\n",
        "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        text = \"\"\n",
        "    return title, text\n",
        "\n",
        "# Advanced approach\n",
        "url = \"https://openai.com\"\n",
        "print(\"Basic scraper:\")\n",
        "basic_title, basic_text = basic_scrape(url)\n",
        "print(f\"Title: {basic_title}\")\n",
        "print(f\"Text length: {len(basic_text)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Advanced scraper:\")\n",
        "advanced = AdvancedWebsite(url, use_js=False)\n",
        "print(f\"Title: {advanced.title}\")\n",
        "print(f\"Text length: {len(advanced.text)}\")\n",
        "print(f\"Links found: {len(advanced.get_links())}\")\n",
        "print(f\"Metadata keys: {list(advanced.get_metadata().keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Website Summarization with Ollama\n",
        "\n",
        "The `AdvancedWebsite` class includes built-in summarization using Ollama via the OpenAI-compatible API. This allows you to automatically summarize any scraped website using local LLM models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites for Summarization\n",
        "\n",
        "Before using summarization, make sure:\n",
        "1. Ollama is installed and running: `ollama serve`\n",
        "2. A model is available: `ollama pull llama3.1` (or llama3.2, etc.)\n",
        "3. OpenAI library is installed: `pip install openai`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 8: Basic Website Summarization\n",
        "\n",
        "Scrape a website and summarize it using Ollama.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Website Title: Example Domain\n",
            "\n",
            "==================================================\n",
            "Original Content (first 300 chars):\n",
            "Example Domain\n",
            "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
            "Learn more\n",
            "\n",
            "==================================================\n",
            "Summary using Ollama:\n",
            "--------------------------------------------------\n",
            "Here is a summary of the website content:\n",
            "\n",
            "**Summary:** This website provides an example domain that can be used in documentation examples, but it should not be used in actual operations.\n"
          ]
        }
      ],
      "source": [
        "# Scrape and summarize a website\n",
        "# Note: Some sites like openai.com have bot protection (403 errors)\n",
        "# Use example.com or other sites for testing\n",
        "\n",
        "try:\n",
        "    # Use a simple site for testing (openai.com has bot protection)\n",
        "    website = AdvancedWebsite(\"https://example.com\", use_js=False)\n",
        "    \n",
        "    print(\"Website Title:\", website.title)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Original Content (first 300 chars):\")\n",
        "    print(website.text[:300])\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Summary using Ollama:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Check if summarize method exists\n",
        "    if hasattr(website, 'summarize'):\n",
        "        # Summarize using default settings\n",
        "        summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"ERROR: summarize method not found!\")\n",
        "        print(\"\\nSOLUTION: Please re-run the import cell (Cell 2) above to reload the module.\")\n",
        "        print(\"Or restart the kernel: Kernel -> Restart Kernel\")\n",
        "        print(f\"\\nAvailable methods: {[m for m in dir(website) if not m.startswith('_')]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"Error: {error_msg}\")\n",
        "    \n",
        "    if \"'AdvancedWebsite' object has no attribute 'summarize'\" in error_msg:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SOLUTION: The module needs to be reloaded!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. Go back to Cell 2 (the import cell)\")\n",
        "        print(\"2. Re-run that cell (Shift+Enter)\")\n",
        "        print(\"3. Then come back and run this cell again\")\n",
        "        print(\"\\nOR restart the kernel:\")\n",
        "        print(\"   Kernel -> Restart Kernel -> Restart\")\n",
        "    else:\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Make sure Ollama is running: ollama serve\")\n",
        "        print(\"2. Make sure model is available: ollama pull llama3.1\")\n",
        "        print(\"3. Make sure OpenAI library is installed: pip install openai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 9: Advanced Summarization with Custom Settings\n",
        "\n",
        "Use custom prompts, temperature, and other parameters for summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed Summary:\n",
            "==================================================\n",
            "Here's a comprehensive summary of the website content:\n",
            "\n",
            "**Main Topic:** Streamlit, an open-source app framework that allows users to build and share data apps in minutes using pure Python.\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "1. Streamlit is a fast and easy-to-use tool for building web applications.\n",
            "2. It requires no front-end experience and can be installed like any other Python library.\n",
            "3. Streamlit builds upon three simple principles: embracing scripting, weaving in interaction, and deploying instantly.\n",
            "4. The framework allows users to add widgets, create interactive apps, and deploy them instantly.\n",
            "\n",
            "**Important Details:**\n",
            "\n",
            "1. Streamlit is trusted by over 90% of Fortune 50 companies.\n",
            "2. It has a large community of users who share their experiences and projects on social media.\n",
            "3. Streamlit is compatible with various libraries and tools, including Plotly, Pandas, and NumPy.\n",
            "4. The framework offers three deployment options: Streamlit Community Cloud (public apps), Snowflake (enterprise-grade reliability and security), or self-hosted.\n",
            "\n",
            "**Actionable Insights:**\n",
            "\n",
            "1. Streamlit can help data scientists build and share web applications quickly and easily, without requiring extensive front-end experience.\n",
            "2. The framework's simplicity and flexibility make it an ideal choice for prototyping and deploying data-driven web apps.\n",
            "3. Streamlit's community cloud option provides a convenient way to deploy public apps, while Snowflake offers enterprise-grade reliability and security.\n",
            "4. Users can leverage Streamlit's compatibility with various libraries and tools to create powerful and interactive apps.\n",
            "\n",
            "Overall, Streamlit is an innovative tool that enables users to build and share data-driven web applications quickly and easily, making it an attractive choice for data scientists and developers.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "    \n",
        "    # Custom system prompt for more detailed summary\n",
        "    custom_prompt = (\n",
        "        \"You are an expert content analyst. Provide a comprehensive summary \"\n",
        "        \"that includes: 1) Main topic, 2) Key points, 3) Important details, \"\n",
        "        \"4) Any actionable insights. Format your response clearly.\"\n",
        "    )\n",
        "    \n",
        "    summary = website.summarize_with_ollama(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.3,  # Slightly more creative\n",
        "        max_tokens=500,   # Limit response length\n",
        "        system_prompt=custom_prompt\n",
        "    )\n",
        "    \n",
        "    print(\"Detailed Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(summary)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 10: Summarizing JavaScript-Rendered Websites\n",
        "\n",
        "Combine advanced scraping with summarization for JavaScript-heavy sites.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Playwright not available. Falling back to requests.\n",
            "Scraped using: requests\n",
            "Title: Streamlit • A faster way to build and share data apps\n",
            "Content length: 14478 characters\n",
            "\n",
            "==================================================\n",
            "Summary:\n",
            "--------------------------------------------------\n",
            "Here's a summary of the website content:\n",
            "\n",
            "**What is Streamlit?**\n",
            "\n",
            "Streamlit is an open-source app framework that allows users to build and share data apps in minutes, without requiring front-end experience. It's built on pure Python and can be installed using pip.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Easy to use**: Streamlit has a simple API that makes it easy to build apps with just a few lines of code.\n",
            "2. **Interactive**: Apps can include interactive widgets, making it easy to add user input and feedback.\n",
            "3. **Instant deployment**: Apps can be deployed instantly, either publicly or privately.\n",
            "4. **Flexible**: Streamlit supports various deployment options, including public cloud, private cloud, and on-premises.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Fast development**: Streamlit allows users to build apps quickly, making it ideal for prototyping and testing ideas.\n",
            "2. **Easy sharing**: Apps can be shared publicly or privately, making it easy to collaborate with others.\n",
            "3. **No front-end experience required**: Streamlit's simple API makes it accessible to users without front-end experience.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "1. **Data science**: Streamlit is used by data scientists to build and share interactive dashboards and visualizations.\n",
            "2. **Machine learning**: Streamlit can be used to deploy machine learning models as web apps.\n",
            "3. **Prototyping**: Streamlit's fast development capabilities make it ideal for prototyping and testing ideas.\n",
            "\n",
            "**Testimonials:**\n",
            "\n",
            "Streamlit has been praised by users from top companies, including Google, Uber, and Yelp, who have found it to be a game-changer in building data-driven web apps.\n"
          ]
        }
      ],
      "source": [
        "# Scrape a JavaScript-rendered site and summarize it\n",
        "try:\n",
        "    # Note: This requires Playwright to be installed\n",
        "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "    \n",
        "    print(f\"Scraped using: {website.get_fetch_method()}\")\n",
        "    print(f\"Title: {website.title}\")\n",
        "    print(f\"Content length: {len(website.text)} characters\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
        "    print(summary)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nIf you see Playwright errors, you can:\")\n",
        "    print(\"1. Install Playwright: pip install playwright && playwright install\")\n",
        "    print(\"2. Or use use_js=False for static sites\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 11: Complete Workflow - Scrape, Extract, and Summarize\n",
        "\n",
        "A complete example showing the full workflow from scraping to summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://streamlit.io...\n",
            "Warning: Playwright not available. Falling back to requests.\n",
            "✓ Scraped using: requests\n",
            "✓ Title: Streamlit • A faster way to build and share data apps\n",
            "✓ Content length: 14478 characters\n",
            "✓ Links found: 66\n",
            "✓ Metadata keys: ['description', 'open_graph', 'twitter_card']\n",
            "\n",
            "Generating summary using llama3.1...\n",
            "✓ Summary generated\n",
            "\n",
            "==================================================\n",
            "SUMMARY:\n",
            "==================================================\n",
            "Here's a summary of the website content:\n",
            "\n",
            "**What is Streamlit?**\n",
            "\n",
            "Streamlit is an open-source app framework that allows users to build and share data apps in minutes, without requiring front-end experience. It's built on pure Python and can be installed using pip.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Easy to use**: Streamlit has a simple API that makes it easy to build apps with just a few lines of code.\n",
            "2. **Interactive**: Apps can include interactive widgets, making it easy to add user input and feedback.\n",
            "3. **Instant deployment**: Apps can be deployed instantly, either publicly or privately.\n",
            "4. **Flexible**: Streamlit supports various deployment options, including public cloud, private cloud, and on-premises.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Fast development**: Streamlit allows users to build apps quickly, making it ideal for prototyping and testing ideas.\n",
            "2. **Easy sharing**: Apps can be shared publicly or privately, making it easy to collaborate with others.\n",
            "3. **No front-end experience required**: Streamlit's simple API makes it accessible to users without front-end experience.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "1. **Data science**: Streamlit is used by data scientists to build and share interactive dashboards and visualizations.\n",
            "2. **Machine learning**: Streamlit can be used to deploy machine learning models as web apps.\n",
            "3. **Prototyping**: Streamlit's fast development capabilities make it ideal for prototyping and testing ideas.\n",
            "\n",
            "**Testimonials:**\n",
            "\n",
            "Streamlit has been praised by users from top companies, including Google, Uber, and Yelp, who have found it to be a game-changer in building data-driven web apps.\n"
          ]
        }
      ],
      "source": [
        "def scrape_and_summarize(url, model, use_js=False):\n",
        "    \"\"\"\n",
        "    Complete workflow: Scrape a website and summarize it.\n",
        "    \n",
        "    Args:\n",
        "        url: URL to scrape\n",
        "        model: Ollama model to use for summarization\n",
        "        use_js: Whether to use JavaScript rendering\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with scraped data and summary\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Scrape the website\n",
        "        print(f\"Scraping {url}...\")\n",
        "        website = AdvancedWebsite(url, use_js=use_js)\n",
        "        print(f\"✓ Scraped using: {website.get_fetch_method()}\")\n",
        "        \n",
        "        # Step 2: Extract information\n",
        "        print(f\"✓ Title: {website.title}\")\n",
        "        print(f\"✓ Content length: {len(website.text)} characters\")\n",
        "        print(f\"✓ Links found: {len(website.get_links())}\")\n",
        "        print(f\"✓ Metadata keys: {list(website.get_metadata().keys())}\")\n",
        "        \n",
        "        # Step 3: Summarize\n",
        "        print(f\"\\nGenerating summary using {model}...\")\n",
        "        summary = website.summarize(model=model, temperature=0)\n",
        "        print(\"✓ Summary generated\")\n",
        "        \n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": website.title,\n",
        "            \"content_length\": len(website.text),\n",
        "            \"links_count\": len(website.get_links()),\n",
        "            \"metadata\": website.get_metadata(),\n",
        "            \"summary\": summary,\n",
        "            \"fetch_method\": website.get_fetch_method()\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage with configured variables\n",
        "result = scrape_and_summarize(WEBSITE_URL, model=MODEL_NAME, use_js=USE_JS)\n",
        "\n",
        "if result:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY:\")\n",
        "    print(\"=\"*50)\n",
        "    print(result[\"summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The `AdvancedWebsite` class now provides a complete solution for web scraping and summarization:\n",
        "\n",
        "1. **Advanced Scraping**: Handles both static and JavaScript-rendered pages\n",
        "2. **Smart Fallback**: Automatically chooses the best method\n",
        "3. **Enhanced Extraction**: Better text cleaning, link validation, metadata extraction\n",
        "4. **Built-in Summarization**: Summarize any scraped website using Ollama\n",
        "5. **Flexible Configuration**: Customize all aspects of scraping and summarization\n",
        "\n",
        "### Key Methods:\n",
        "- `AdvancedWebsite(url)` - Scrape a website\n",
        "- `website.summarize()` - Quick summarization with defaults\n",
        "- `website.summarize_with_ollama()` - Advanced summarization with custom settings\n",
        "- `website.get_contents()` - Get formatted content\n",
        "- `website.get_links()` - Get all links\n",
        "- `website.get_metadata()` - Get page metadata\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
