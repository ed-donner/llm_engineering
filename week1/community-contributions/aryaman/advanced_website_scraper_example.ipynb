{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Website Scraper - Usage Examples\n",
        "\n",
        "This notebook demonstrates how to use the `AdvancedWebsite` class for web scraping.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Handles JavaScript-rendered pages** (React, Vue, etc.) using Playwright\n",
        "- **Automatic fallback** to BeautifulSoup for static sites (faster)\n",
        "- **Better error handling** with retry logic\n",
        "- **Enhanced text extraction** with improved cleaning\n",
        "- **Metadata extraction** (description, keywords, Open Graph tags)\n",
        "- **Improved link extraction** with validation and normalization\n",
        "- **Built-in summarization** using Ollama via OpenAI-compatible API\n",
        "\n",
        "## Important Note\n",
        "\n",
        "If you update `advanced_website_scraper.py` and the changes don't appear, **re-run Cell 2** (the import cell) to reload the module. The import cell includes automatic reloading, but you may need to re-run it after making changes to the Python file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "If you want to use Playwright for JavaScript-rendered pages, you'll need to install it:\n",
        "\n",
        "```bash\n",
        "pip install playwright\n",
        "playwright install\n",
        "```\n",
        "\n",
        "Note: Playwright is optional. The scraper will fall back to requests + BeautifulSoup if Playwright is not available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ AdvancedWebsite class imported successfully\n",
            "✓ Available methods: ['get_contents', 'get_fetch_method', 'get_links', 'get_metadata', 'summarize', 'summarize_with_ollama']\n"
          ]
        }
      ],
      "source": [
        "# Import the AdvancedWebsite class\n",
        "# Note: This assumes advanced_website_scraper.py is in the same directory\n",
        "# If running from a different location, you may need to adjust the import path\n",
        "import sys\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "# Add current directory to path if needed\n",
        "current_dir = os.path.dirname(os.path.abspath(''))\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.insert(0, current_dir)\n",
        "\n",
        "# Import the module\n",
        "import advanced_website_scraper\n",
        "\n",
        "# Reload the module to pick up any changes (useful during development)\n",
        "importlib.reload(advanced_website_scraper)\n",
        "\n",
        "# Import the class\n",
        "from advanced_website_scraper import AdvancedWebsite\n",
        "\n",
        "print(\"✓ AdvancedWebsite class imported successfully\")\n",
        "print(f\"✓ Available methods: {[m for m in dir(AdvancedWebsite) if not m.startswith('_')]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your website URL and model name here. Change these variables to test different websites and models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Website URL: https://streamlit.io\n",
            "  Model Name: llama3.1\n",
            "  Use JavaScript: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables - Change these to test different websites and models\n",
        "WEBSITE_URL = \"https://streamlit.io\"  # Change this to any website URL\n",
        "MODEL_NAME = \"llama3.1\"  # Change this to your preferred Ollama model (e.g., \"llama3.2\", \"llama3.1\", etc.)\n",
        "USE_JS = True  # Set to True to force JavaScript rendering (requires Playwright)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Website URL: {WEBSITE_URL}\")\n",
        "print(f\"  Model Name: {MODEL_NAME}\")\n",
        "print(f\"  Use JavaScript: {USE_JS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Scraping a Static Website (Fast)\n",
        "\n",
        "For static websites, the scraper will use requests + BeautifulSoup, which is faster and lighter.\n",
        "\n",
        "**Note:** This example uses the `WEBSITE_URL` variable defined in the configuration cell above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch method: requests\n",
            "\n",
            "Title: OpenAI\n",
            "\n",
            "Content preview (first 500 chars):\n",
            "Switch to\n",
            "ChatGPT\n",
            "(opens in a new window)\n",
            "Sora\n",
            "(opens in a new window)\n",
            "API Platform\n",
            "(opens in a new window)\n",
            "\n",
            "Number of links found: 3\n"
          ]
        }
      ],
      "source": [
        "# Scrape a static website using the configured URL\n",
        "website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "\n",
        "print(f\"Fetch method: {website.get_fetch_method()}\")\n",
        "print(f\"\\nTitle: {website.title}\")\n",
        "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
        "print(f\"\\nNumber of links found: {len(website.get_links())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Scraping a JavaScript-Rendered Website\n",
        "\n",
        "For websites that use JavaScript to render content (like React, Vue, etc.), use Playwright mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Playwright not available. Falling back to requests.\n",
            "Fetch method: requests\n",
            "\n",
            "Title: OpenAI\n",
            "\n",
            "Content preview (first 500 chars):\n",
            "Switch to\n",
            "ChatGPT\n",
            "(opens in a new window)\n",
            "Sora\n",
            "(opens in a new window)\n",
            "API Platform\n",
            "(opens in a new window)\n",
            "\n",
            "Number of links found: 3\n"
          ]
        }
      ],
      "source": [
        "# Scrape a JavaScript-rendered website (like OpenAI)\n",
        "# Note: This requires Playwright to be installed\n",
        "try:\n",
        "    website = AdvancedWebsite(\"https://openai.com\", use_js=True, timeout=30)\n",
        "    \n",
        "    print(f\"Fetch method: {website.get_fetch_method()}\")\n",
        "    print(f\"\\nTitle: {website.title}\")\n",
        "    print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
        "    print(f\"\\nNumber of links found: {len(website.get_links())}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure Playwright is installed: pip install playwright && playwright install\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Automatic Mode (Smart Fallback)\n",
        "\n",
        "The scraper can automatically detect the best method. It tries requests first (faster), and falls back to Playwright if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetch method used: requests\n",
            "\n",
            "Title: OpenAI\n",
            "\n",
            "Content preview (first 500 chars):\n",
            "Switch to\n",
            "ChatGPT\n",
            "(opens in a new window)\n",
            "Sora\n",
            "(opens in a new window)\n",
            "API Platform\n",
            "(opens in a new window)\n"
          ]
        }
      ],
      "source": [
        "# Let the scraper decide the best method using the configured URL\n",
        "website = AdvancedWebsite(WEBSITE_URL)\n",
        "\n",
        "print(f\"Fetch method used: {website.get_fetch_method()}\")\n",
        "print(f\"\\nTitle: {website.title}\")\n",
        "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Getting Formatted Contents\n",
        "\n",
        "Use the `get_contents()` method to get a formatted string with title and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 1 failed: 403 Client Error: Forbidden for url: https://openai.com/. Retrying...\n",
            "Webpage Title:\n",
            "OpenAI\n",
            "\n",
            "Webpage Contents:\n",
            "Switch to\n",
            "ChatGPT\n",
            "(opens in a new window)\n",
            "Sora\n",
            "(opens in a new window)\n",
            "API Platform\n",
            "(opens in a new window)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Limited to 500 characters:\n",
            "Webpage Title:\n",
            "OpenAI\n",
            "\n",
            "Webpage Contents:\n",
            "Switch to\n",
            "ChatGPT\n",
            "(opens in a new window)\n",
            "Sora\n",
            "(opens in a new window)\n",
            "API Platform\n",
            "(opens in a new window)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(WEBSITE_URL)\n",
        "\n",
        "# Get full contents\n",
        "contents = website.get_contents()\n",
        "print(contents[:1000])  # Print first 1000 characters\n",
        "\n",
        "# Get contents with length limit\n",
        "limited_contents = website.get_contents(max_length=500)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Limited to 500 characters:\")\n",
        "print(limited_contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Extracting Links\n",
        "\n",
        "Get all links found on the page, with automatic validation and normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 links:\n",
            "1. https://chatgpt.com/?openaicom-did=cd616bdf-0a61-463e-b967-f1477da3ba5f&openaicom_referred=true\n",
            "2. https://sora.com/\n",
            "3. https://platform.openai.com/\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(\"https://openai.com\")\n",
        "\n",
        "links = website.get_links()\n",
        "print(f\"Found {len(links)} links:\")\n",
        "for i, link in enumerate(links[:10], 1):  # Show first 10 links\n",
        "    print(f\"{i}. {link}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Extracting Metadata\n",
        "\n",
        "Extract metadata like description, keywords, Open Graph tags, and Twitter Card tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 1 failed: 403 Client Error: Forbidden for url: https://openai.com/. Retrying...\n",
            "Extracted Metadata:\n",
            "==================================================\n",
            "description: We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.\n",
            "open_graph: {'title': 'OpenAI', 'description': 'We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.', 'locale': 'en-US', 'image': 'https://images.ctfassets.net/kftzwdyauwt9/3KGOHkSXu53naMuSFNaiwv/cdb0e2f899f524abb71314ab20e09c9c/OAI-white-on-black.png?w=1600&h=900&fit=fill', 'image:width': '1600', 'image:height': '900', 'image:alt': 'OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.', 'type': 'website'}\n",
            "twitter_card: {'card': 'summary_large_image', 'site': '@OpenAI', 'title': 'OpenAI', 'description': 'We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.', 'image': 'https://images.ctfassets.net/kftzwdyauwt9/3KGOHkSXu53naMuSFNaiwv/cdb0e2f899f524abb71314ab20e09c9c/OAI-white-on-black.png?w=1600&h=900&fit=fill', 'image:width': '1600', 'image:height': '900', 'image:alt': 'OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.'}\n"
          ]
        }
      ],
      "source": [
        "website = AdvancedWebsite(\"https://openai.com\")\n",
        "\n",
        "metadata = website.get_metadata()\n",
        "print(\"Extracted Metadata:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in metadata.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 7: Waiting for Dynamic Content\n",
        "\n",
        "If a page loads content dynamically, you can wait for a specific selector before extracting content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Wait for a specific element to load\n",
        "# This is useful for pages that load content via JavaScript\n",
        "# website = AdvancedWebsite(\n",
        "#     WEBSITE_URL,\n",
        "#     use_js=True,\n",
        "#     wait_for_selector=\"main-content\"  # Wait for element with this ID or class\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic scraper:\n",
            "Title: OpenAI\n",
            "Text length: 3506\n",
            "\n",
            "==================================================\n",
            "Advanced scraper:\n",
            "Title: OpenAI\n",
            "Text length: 107\n",
            "Links found: 3\n",
            "Metadata keys: ['description', 'open_graph', 'twitter_card']\n"
          ]
        }
      ],
      "source": [
        "# Compare the two approaches\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Basic approach (from the original scraper)\n",
        "def basic_scrape(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    title = soup.title.string if soup.title else \"No title found\"\n",
        "    if soup.body:\n",
        "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            irrelevant.decompose()\n",
        "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        text = \"\"\n",
        "    return title, text\n",
        "\n",
        "# Advanced approach\n",
        "url = \"https://openai.com\"\n",
        "print(\"Basic scraper:\")\n",
        "basic_title, basic_text = basic_scrape(url)\n",
        "print(f\"Title: {basic_title}\")\n",
        "print(f\"Text length: {len(basic_text)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Advanced scraper:\")\n",
        "advanced = AdvancedWebsite(url, use_js=False)\n",
        "print(f\"Title: {advanced.title}\")\n",
        "print(f\"Text length: {len(advanced.text)}\")\n",
        "print(f\"Links found: {len(advanced.get_links())}\")\n",
        "print(f\"Metadata keys: {list(advanced.get_metadata().keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Website Summarization with Ollama\n",
        "\n",
        "The `AdvancedWebsite` class includes built-in summarization using Ollama via the OpenAI-compatible API. This allows you to automatically summarize any scraped website using local LLM models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites for Summarization\n",
        "\n",
        "Before using summarization, make sure:\n",
        "1. Ollama is installed and running: `ollama serve`\n",
        "2. A model is available: `ollama pull llama3.1` (or llama3.2, etc.)\n",
        "3. OpenAI library is installed: `pip install openai`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 8: Basic Website Summarization\n",
        "\n",
        "Scrape a website and summarize it using Ollama.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Website Title: Example Domain\n",
            "\n",
            "==================================================\n",
            "Original Content (first 300 chars):\n",
            "Example Domain\n",
            "This domain is for use in documentation examples without needing permission. Avoid use in operations.\n",
            "Learn more\n",
            "\n",
            "==================================================\n",
            "Summary using Ollama:\n",
            "--------------------------------------------------\n",
            "Here is a summary of the website content:\n",
            "\n",
            "**Summary:** This website provides an example domain that can be used in documentation examples, but it should not be used in actual operations.\n"
          ]
        }
      ],
      "source": [
        "# Scrape and summarize a website\n",
        "# Note: Some sites like openai.com have bot protection (403 errors)\n",
        "# Use example.com or other sites for testing\n",
        "\n",
        "try:\n",
        "    # Use a simple site for testing (openai.com has bot protection)\n",
        "    website = AdvancedWebsite(\"https://example.com\", use_js=False)\n",
        "    \n",
        "    print(\"Website Title:\", website.title)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Original Content (first 300 chars):\")\n",
        "    print(website.text[:300])\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Summary using Ollama:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Check if summarize method exists\n",
        "    if hasattr(website, 'summarize'):\n",
        "        # Summarize using default settings\n",
        "        summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"ERROR: summarize method not found!\")\n",
        "        print(\"\\nSOLUTION: Please re-run the import cell (Cell 2) above to reload the module.\")\n",
        "        print(\"Or restart the kernel: Kernel -> Restart Kernel\")\n",
        "        print(f\"\\nAvailable methods: {[m for m in dir(website) if not m.startswith('_')]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"Error: {error_msg}\")\n",
        "    \n",
        "    if \"'AdvancedWebsite' object has no attribute 'summarize'\" in error_msg:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SOLUTION: The module needs to be reloaded!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. Go back to Cell 2 (the import cell)\")\n",
        "        print(\"2. Re-run that cell (Shift+Enter)\")\n",
        "        print(\"3. Then come back and run this cell again\")\n",
        "        print(\"\\nOR restart the kernel:\")\n",
        "        print(\"   Kernel -> Restart Kernel -> Restart\")\n",
        "    else:\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Make sure Ollama is running: ollama serve\")\n",
        "        print(\"2. Make sure model is available: ollama pull llama3.1\")\n",
        "        print(\"3. Make sure OpenAI library is installed: pip install openai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 9: Advanced Summarization with Custom Settings\n",
        "\n",
        "Use custom prompts, temperature, and other parameters for summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Playwright not available. Falling back to requests.\n",
            "Detailed Summary:\n",
            "==================================================\n",
            "Here is a comprehensive summary of the website content:\n",
            "\n",
            "**Main Topic:** Jamtara – Sabka Number Ayega, an Indian crime drama television series created and directed by Soumendra Padhi.\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "1. The series revolves around social engineering operations in the Jamtara district of Jharkhand.\n",
            "2. It was released on Netflix on January 10, 2020, and a second season premiered on September 23, 2022.\n",
            "3. The story follows a group of young men who operate a successful phishing racket, but their business is disrupted by a corrupt politician and a newly appointed police superintendent.\n",
            "4. The series has received positive reviews from critics, with many praising its engaging storyline and realistic portrayal of the characters.\n",
            "\n",
            "**Important Details:**\n",
            "\n",
            "1. The character of female SP Dolly Sahu was based on Jamtara's Superintendent Jaya Roy.\n",
            "2. The plot of the series is \"tightly-written\" and disseminates information seamlessly, according to Kirubhakar Purushothaman of The New Indian Express.\n",
            "3. Udita Jhunjunwala of Scroll.in called it \"extremely binge-able\" and compared its characterisation and dynamics to those of Anurag Kashyap and Tigmanshu Dhulia's films.\n",
            "\n",
            "**Actionable Insights:**\n",
            "\n",
            "1. If you are interested in crime dramas or social engineering, Jamtara – Sabka Number Ayega is a series worth watching.\n",
            "2. The show's realistic portrayal of the characters and their motivations may provide insights into the world of phishing and cybercrime.\n",
            "3. The series' engaging storyline and tight writing make it an excellent choice for binge-watching.\n",
            "\n",
            "**Additional Information:**\n",
            "\n",
            "1. Jamtara – Sabka Number Ayega is listed as a Netflix original current series, along with other popular shows such as Stranger Things and Queer Eye.\n",
            "2. The show has received several reviews from critics, including those from Hindustan Times, Scroll.in, and The Quint.\n",
            "3. A second season of the series premiered on September 23, 2022, which suggests that the show is a successful and popular series among audiences.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "    \n",
        "    # Custom system prompt for more detailed summary\n",
        "    custom_prompt = (\n",
        "        \"You are an expert content analyst. Provide a comprehensive summary \"\n",
        "        \"that includes: 1) Main topic, 2) Key points, 3) Important details, \"\n",
        "        \"4) Any actionable insights. Format your response clearly.\"\n",
        "    )\n",
        "    \n",
        "    summary = website.summarize_with_ollama(\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.3,  # Slightly more creative\n",
        "        max_tokens=500,   # Limit response length\n",
        "        system_prompt=custom_prompt\n",
        "    )\n",
        "    \n",
        "    print(\"Detailed Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(summary)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 10: Summarizing JavaScript-Rendered Websites\n",
        "\n",
        "Combine advanced scraping with summarization for JavaScript-heavy sites.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Playwright not available. Falling back to requests.\n",
            "Scraped using: requests\n",
            "Title: Streamlit • A faster way to build and share data apps\n",
            "Content length: 14478 characters\n",
            "\n",
            "==================================================\n",
            "Summary:\n",
            "--------------------------------------------------\n",
            "Here's a summary of the website content:\n",
            "\n",
            "**What is Streamlit?**\n",
            "\n",
            "Streamlit is an open-source app framework that allows users to build and share data apps in minutes, without requiring front-end experience. It's built on pure Python and can be installed using pip.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Easy to use**: Streamlit has a simple API that makes it easy to build apps with just a few lines of code.\n",
            "2. **Interactive**: Apps can include interactive widgets, making it easy to add user input and feedback.\n",
            "3. **Instant deployment**: Apps can be deployed instantly, either publicly or privately.\n",
            "4. **Flexible**: Streamlit supports various deployment options, including public cloud, private cloud, and on-premises.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Fast development**: Streamlit allows users to build apps quickly, making it ideal for prototyping and testing ideas.\n",
            "2. **Easy sharing**: Apps can be shared publicly or privately, making it easy to collaborate with others.\n",
            "3. **No front-end experience required**: Streamlit's simple API makes it accessible to users without front-end experience.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "1. **Data science**: Streamlit is used by data scientists to build and share interactive dashboards and visualizations.\n",
            "2. **Machine learning**: Streamlit can be used to deploy machine learning models as web apps.\n",
            "3. **Prototyping**: Streamlit's fast development capabilities make it ideal for prototyping and testing ideas.\n",
            "\n",
            "**Testimonials:**\n",
            "\n",
            "Streamlit has been praised by users from top companies, including Google, Uber, and Yelp, who have found it to be a game-changer in building data-driven web apps.\n"
          ]
        }
      ],
      "source": [
        "# Scrape a JavaScript-rendered site and summarize it\n",
        "try:\n",
        "    # Note: This requires Playwright to be installed\n",
        "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
        "    \n",
        "    print(f\"Scraped using: {website.get_fetch_method()}\")\n",
        "    print(f\"Title: {website.title}\")\n",
        "    print(f\"Content length: {len(website.text)} characters\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
        "    print(summary)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nIf you see Playwright errors, you can:\")\n",
        "    print(\"1. Install Playwright: pip install playwright && playwright install\")\n",
        "    print(\"2. Or use use_js=False for static sites\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 11: Complete Workflow - Scrape, Extract, and Summarize\n",
        "\n",
        "A complete example showing the full workflow from scraping to summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://streamlit.io...\n",
            "Warning: Playwright not available. Falling back to requests.\n",
            "✓ Scraped using: requests\n",
            "✓ Title: Streamlit • A faster way to build and share data apps\n",
            "✓ Content length: 14478 characters\n",
            "✓ Links found: 66\n",
            "✓ Metadata keys: ['description', 'open_graph', 'twitter_card']\n",
            "\n",
            "Generating summary using llama3.1...\n",
            "✓ Summary generated\n",
            "\n",
            "==================================================\n",
            "SUMMARY:\n",
            "==================================================\n",
            "Here's a summary of the website content:\n",
            "\n",
            "**What is Streamlit?**\n",
            "\n",
            "Streamlit is an open-source app framework that allows users to build and share data apps in minutes, without requiring front-end experience. It's built on pure Python and can be installed using pip.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Easy to use**: Streamlit has a simple API that makes it easy to build apps with just a few lines of code.\n",
            "2. **Interactive**: Apps can include interactive widgets, making it easy to add user input and feedback.\n",
            "3. **Instant deployment**: Apps can be deployed instantly, either publicly or privately.\n",
            "4. **Flexible**: Streamlit supports various deployment options, including public cloud, private cloud, and on-premises.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Fast development**: Streamlit allows users to build apps quickly, making it ideal for prototyping and testing ideas.\n",
            "2. **Easy sharing**: Apps can be shared publicly or privately, making it easy to collaborate with others.\n",
            "3. **No front-end experience required**: Streamlit's simple API makes it accessible to users without front-end experience.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "1. **Data science**: Streamlit is used by data scientists to build and share interactive dashboards and visualizations.\n",
            "2. **Machine learning**: Streamlit can be used to deploy machine learning models as web apps.\n",
            "3. **Prototyping**: Streamlit's fast development capabilities make it ideal for prototyping and testing ideas.\n",
            "\n",
            "**Testimonials:**\n",
            "\n",
            "Streamlit has been praised by users from top companies, including Google, Uber, and Yelp, who have found it to be a game-changer in building data-driven web apps.\n"
          ]
        }
      ],
      "source": [
        "def scrape_and_summarize(url, model, use_js=False):\n",
        "    \"\"\"\n",
        "    Complete workflow: Scrape a website and summarize it.\n",
        "    \n",
        "    Args:\n",
        "        url: URL to scrape\n",
        "        model: Ollama model to use for summarization\n",
        "        use_js: Whether to use JavaScript rendering\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with scraped data and summary\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Scrape the website\n",
        "        print(f\"Scraping {url}...\")\n",
        "        website = AdvancedWebsite(url, use_js=use_js)\n",
        "        print(f\"✓ Scraped using: {website.get_fetch_method()}\")\n",
        "        \n",
        "        # Step 2: Extract information\n",
        "        print(f\"✓ Title: {website.title}\")\n",
        "        print(f\"✓ Content length: {len(website.text)} characters\")\n",
        "        print(f\"✓ Links found: {len(website.get_links())}\")\n",
        "        print(f\"✓ Metadata keys: {list(website.get_metadata().keys())}\")\n",
        "        \n",
        "        # Step 3: Summarize\n",
        "        print(f\"\\nGenerating summary using {model}...\")\n",
        "        summary = website.summarize(model=model, temperature=0)\n",
        "        print(\"✓ Summary generated\")\n",
        "        \n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": website.title,\n",
        "            \"content_length\": len(website.text),\n",
        "            \"links_count\": len(website.get_links()),\n",
        "            \"metadata\": website.get_metadata(),\n",
        "            \"summary\": summary,\n",
        "            \"fetch_method\": website.get_fetch_method()\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage with configured variables\n",
        "result = scrape_and_summarize(WEBSITE_URL, model=MODEL_NAME, use_js=USE_JS)\n",
        "\n",
        "if result:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY:\")\n",
        "    print(\"=\"*50)\n",
        "    print(result[\"summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The `AdvancedWebsite` class now provides a complete solution for web scraping and summarization:\n",
        "\n",
        "1. **Advanced Scraping**: Handles both static and JavaScript-rendered pages\n",
        "2. **Smart Fallback**: Automatically chooses the best method\n",
        "3. **Enhanced Extraction**: Better text cleaning, link validation, metadata extraction\n",
        "4. **Built-in Summarization**: Summarize any scraped website using Ollama\n",
        "5. **Flexible Configuration**: Customize all aspects of scraping and summarization\n",
        "\n",
        "### Key Methods:\n",
        "- `AdvancedWebsite(url)` - Scrape a website\n",
        "- `website.summarize()` - Quick summarization with defaults\n",
        "- `website.summarize_with_ollama()` - Advanced summarization with custom settings\n",
        "- `website.get_contents()` - Get formatted content\n",
        "- `website.get_links()` - Get all links\n",
        "- `website.get_metadata()` - Get page metadata\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
