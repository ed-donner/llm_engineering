{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech & Finance Sentiment Analyzer with Selenium\n",
    "\n",
    "This notebook demonstrates how to combine **Selenium web scraping** with **OpenAI's GPT** to analyze sentiment from tech and finance news websites.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Using Selenium to scrape JavaScript-rendered websites\n",
    "- Extracting clean text from HTML with BeautifulSoup\n",
    "- Using GPT to perform structured sentiment analysis\n",
    "- Aggregating insights from multiple news sources\n",
    "\n",
    "**Why Selenium?**\n",
    "Many modern news sites use JavaScript to load content dynamically. Simple HTTP requests won't capture this content - Selenium runs a real browser that executes JavaScript first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once to install dependencies\n",
    "# !pip install selenium webdriver-manager openai python-dotenv beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env file\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key found - please set OPENAI_API_KEY in your .env file\")\n",
    "elif not api_key.startswith(\"sk-\"):\n",
    "    print(\"API key found but doesn't look right - please check your .env file\")\n",
    "else:\n",
    "    print(\"API key loaded successfully!\")\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Selenium Web Scraper\n",
    "\n",
    "This class handles all the web scraping logic:\n",
    "- Creates a headless Chrome browser\n",
    "- Navigates to URLs and waits for JavaScript to render\n",
    "- Extracts clean text from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumScraper:\n",
    "    \"\"\"\n",
    "    A web scraper that uses Selenium to handle JavaScript-rendered pages.\n",
    "    \n",
    "    Args:\n",
    "        headless: If True, runs browser without visible window (faster)\n",
    "        wait_time: Seconds to wait for JavaScript to render\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, wait_time=3):\n",
    "        self.headless = headless\n",
    "        self.wait_time = wait_time\n",
    "    \n",
    "    def _create_driver(self):\n",
    "        \"\"\"Create and configure a Chrome WebDriver instance.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Standard options for stability\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Use a realistic user agent\n",
    "        options.add_argument(\n",
    "            'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '\n",
    "            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        # webdriver-manager automatically downloads the correct ChromeDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    def _clean_html(self, html):\n",
    "        \"\"\"Extract clean text from HTML, removing navigation and scripts.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Get the title\n",
    "        title = soup.title.string.strip() if soup.title else \"No title\"\n",
    "        \n",
    "        # Remove elements that don't contain useful content\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header', \n",
    "                         'aside', 'img', 'input', 'button', 'form', 'iframe']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Get text with proper spacing\n",
    "        text = soup.body.get_text(separator=\"\\n\", strip=True) if soup.body else \"\"\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        clean_text = \"\\n\".join(lines)\n",
    "        \n",
    "        return title, clean_text\n",
    "    \n",
    "    def scrape(self, url, source_name=\"Website\"):\n",
    "        \"\"\"\n",
    "        Scrape a URL and return structured content.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            source_name: A friendly name for this source\n",
    "            \n",
    "        Returns:\n",
    "            dict with keys: url, source, title, text, success, error\n",
    "        \"\"\"\n",
    "        print(f\"Scraping: {source_name}...\")\n",
    "        \n",
    "        try:\n",
    "            driver = self._create_driver()\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for JavaScript to render\n",
    "            time.sleep(self.wait_time)\n",
    "            \n",
    "            # Scroll down to trigger lazy loading\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            driver.quit()\n",
    "            \n",
    "            title, text = self._clean_html(html)\n",
    "            \n",
    "            # Truncate very long content to stay within token limits\n",
    "            if len(text) > 12000:\n",
    "                text = text[:12000] + \"\\n[...content truncated...]\"\n",
    "            \n",
    "            print(f\"  Done! Extracted {len(text):,} characters\")\n",
    "            \n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"source\": source_name,\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "                \"success\": True,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"source\": source_name,\n",
    "                \"title\": None,\n",
    "                \"text\": None,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Sentiment Analyzer\n",
    "\n",
    "This class uses GPT to analyze the scraped content and extract:\n",
    "- Overall sentiment (bullish/bearish/neutral)\n",
    "- A sentiment score from -100 to +100\n",
    "- Key themes and topics\n",
    "- Companies mentioned\n",
    "- Actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Uses GPT to analyze sentiment in tech/finance news content.\n",
    "    Returns structured analysis with scores and insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"You are a financial analyst AI. Analyze the news content and respond with JSON:\n",
    "\n",
    "{\n",
    "    \"sentiment\": \"bullish\" or \"bearish\" or \"neutral\",\n",
    "    \"score\": <-100 to 100, where -100=very bearish, 100=very bullish>,\n",
    "    \"confidence\": <0-100>,\n",
    "    \"themes\": [\"theme1\", \"theme2\", \"theme3\"],\n",
    "    \"companies\": [\"company1\", \"company2\"],\n",
    "    \"summary\": \"2-3 sentence summary of key points\",\n",
    "    \"insight\": \"One actionable recommendation\"\n",
    "}\n",
    "\n",
    "Focus on tech industry and market implications.\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gpt-5-nano\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def analyze(self, scraped_data):\n",
    "        \"\"\"\n",
    "        Analyze scraped content for sentiment.\n",
    "        \n",
    "        Args:\n",
    "            scraped_data: dict from SeleniumScraper.scrape()\n",
    "            \n",
    "        Returns:\n",
    "            dict with sentiment analysis results\n",
    "        \"\"\"\n",
    "        if not scraped_data[\"success\"]:\n",
    "            return {\n",
    "                \"source\": scraped_data[\"source\"],\n",
    "                \"error\": scraped_data[\"error\"]\n",
    "            }\n",
    "        \n",
    "        user_prompt = f\"\"\"Analyze this news content:\n",
    "\n",
    "Source: {scraped_data['source']}\n",
    "Title: {scraped_data['title']}\n",
    "\n",
    "Content:\n",
    "{scraped_data['text'][:10000]}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            result[\"source\"] = scraped_data[\"source\"]\n",
    "            result[\"url\"] = scraped_data[\"url\"]\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": scraped_data[\"source\"],\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Report Formatting\n",
    "\n",
    "Helper functions to display the analysis results in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_badge(score):\n",
    "    \"\"\"Return a text badge based on sentiment score.\"\"\"\n",
    "    if score >= 50:\n",
    "        return \"[STRONGLY BULLISH]\"\n",
    "    elif score >= 20:\n",
    "        return \"[BULLISH]\"\n",
    "    elif score > -20:\n",
    "        return \"[NEUTRAL]\"\n",
    "    elif score > -50:\n",
    "        return \"[BEARISH]\"\n",
    "    else:\n",
    "        return \"[STRONGLY BEARISH]\"\n",
    "\n",
    "\n",
    "def format_analysis(analysis):\n",
    "    \"\"\"Format a single analysis result as markdown.\"\"\"\n",
    "    if \"error\" in analysis:\n",
    "        return f\"### {analysis.get('source', 'Unknown')}\\n\\nError: {analysis['error']}\\n\\n---\"\n",
    "    \n",
    "    score = analysis.get('score', 0)\n",
    "    badge = get_sentiment_badge(score)\n",
    "    \n",
    "    themes = \", \".join(analysis.get('themes', [])[:4])\n",
    "    companies = \", \".join(analysis.get('companies', [])[:5]) or \"None mentioned\"\n",
    "    \n",
    "    return f\"\"\"### {analysis['source']} {badge}\n",
    "\n",
    "**Sentiment Score:** {score}/100 (Confidence: {analysis.get('confidence', 'N/A')}%)\n",
    "\n",
    "**Summary:** {analysis.get('summary', 'N/A')}\n",
    "\n",
    "**Key Themes:** {themes}\n",
    "\n",
    "**Companies Mentioned:** {companies}\n",
    "\n",
    "**Actionable Insight:** {analysis.get('insight', 'N/A')}\n",
    "\n",
    "*Source: {analysis.get('url', 'N/A')}*\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def display_report(analyses):\n",
    "    \"\"\"Display a complete sentiment report.\"\"\"\n",
    "    # Calculate aggregate stats\n",
    "    valid = [a for a in analyses if \"error\" not in a]\n",
    "    \n",
    "    if valid:\n",
    "        avg_score = sum(a.get('score', 0) for a in valid) / len(valid)\n",
    "        avg_badge = get_sentiment_badge(avg_score)\n",
    "    else:\n",
    "        avg_score = 0\n",
    "        avg_badge = \"[NO DATA]\"\n",
    "    \n",
    "    # Build the report\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    report = f\"\"\"# Sentiment Analysis Report\n",
    "\n",
    "**Generated:** {timestamp}\n",
    "\n",
    "## Overall Market Sentiment {avg_badge}\n",
    "\n",
    "**Average Score:** {avg_score:.1f}/100 across {len(valid)} sources\n",
    "\n",
    "---\n",
    "\n",
    "## Individual Source Analysis\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for analysis in analyses:\n",
    "        report += format_analysis(analysis) + \"\\n\"\n",
    "    \n",
    "    display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Run the Analysis\n",
    "\n",
    "Now let's put it all together! Configure your news sources and run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the news sources you want to analyze\n",
    "# Format: {\"Friendly Name\": \"URL\"}\n",
    "\n",
    "NEWS_SOURCES = {\n",
    "    \"Hacker News\": \"https://news.ycombinator.com/\",\n",
    "    \"TechCrunch\": \"https://techcrunch.com/\",\n",
    "    \"The Verge Tech\": \"https://www.theverge.com/tech\",\n",
    "}\n",
    "\n",
    "# You can add more sources:\n",
    "# \"Ars Technica\": \"https://arstechnica.com/\",\n",
    "# \"Wired\": \"https://www.wired.com/\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our tools\n",
    "scraper = SeleniumScraper(headless=True, wait_time=4)\n",
    "analyzer = SentimentAnalyzer(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape all sources\n",
    "print(\"=\"*50)\n",
    "print(\"SCRAPING NEWS SOURCES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "scraped_data = []\n",
    "for name, url in NEWS_SOURCES.items():\n",
    "    result = scraper.scrape(url, name)\n",
    "    scraped_data.append(result)\n",
    "\n",
    "successful = sum(1 for s in scraped_data if s[\"success\"])\n",
    "print(f\"\\nScraped {successful}/{len(NEWS_SOURCES)} sources successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Analyze sentiment for each source\n",
    "print(\"=\"*50)\n",
    "print(\"ANALYZING SENTIMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "analyses = []\n",
    "for data in scraped_data:\n",
    "    print(f\"Analyzing: {data['source']}...\")\n",
    "    result = analyzer.analyze(data)\n",
    "    analyses.append(result)\n",
    "    print(f\"  Sentiment: {result.get('sentiment', 'error')}\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Display the report\n",
    "display_report(analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Quick Single-URL Analysis\n",
    "\n",
    "Use this function to quickly analyze any single URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_sentiment(url, name=\"Website\"):\n",
    "    \"\"\"One-liner to scrape and analyze any URL.\"\"\"\n",
    "    scraper = SeleniumScraper(headless=True)\n",
    "    analyzer = SentimentAnalyzer(model=\"gpt-5-nano\")\n",
    "    \n",
    "    data = scraper.scrape(url, name)\n",
    "    result = analyzer.analyze(data)\n",
    "    \n",
    "    display(Markdown(format_analysis(result)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it! Uncomment and run:\n",
    "quick_sentiment(\"https://news.ycombinator.com/\", \"Hacker News\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ideas for Extension\n",
    "\n",
    "- Add more news sources (Reuters, Bloomberg, etc.)\n",
    "- Track sentiment over time by saving results to a file\n",
    "- Add email alerts when sentiment shifts dramatically\n",
    "- Create charts with matplotlib to visualize trends\n",
    "- Use Ollama for local/free sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "*Week 1 Community Contribution - LLM Engineering Course*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
