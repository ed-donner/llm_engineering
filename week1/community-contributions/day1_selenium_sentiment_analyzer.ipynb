{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech & Finance Sentiment Analyzer with Selenium\n",
    "\n",
    "This notebook demonstrates how to combine **Selenium web scraping** with **OpenAI's GPT** to analyze sentiment from tech and finance news websites.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Using Selenium to scrape JavaScript-rendered websites\n",
    "- Extracting clean text from HTML with BeautifulSoup\n",
    "- Using GPT to perform structured sentiment analysis\n",
    "- Aggregating insights from multiple news sources\n",
    "\n",
    "**Why Selenium?**\n",
    "Many modern news sites use JavaScript to load content dynamically. Simple HTTP requests won't capture this content - Selenium runs a real browser that executes JavaScript first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once to install dependencies\n",
    "# !pip install selenium webdriver-manager openai python-dotenv beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load API key from .env file\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key found - please set OPENAI_API_KEY in your .env file\")\n",
    "elif not api_key.startswith(\"sk-\"):\n",
    "    print(\"API key found but doesn't look right - please check your .env file\")\n",
    "else:\n",
    "    print(\"API key loaded successfully!\")\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Selenium Web Scraper\n",
    "\n",
    "This class handles all the web scraping logic:\n",
    "- Creates a headless Chrome browser\n",
    "- Navigates to URLs and waits for JavaScript to render\n",
    "- Extracts clean text from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumScraper:\n",
    "    \"\"\"\n",
    "    A web scraper that uses Selenium to handle JavaScript-rendered pages.\n",
    "    \n",
    "    Args:\n",
    "        headless: If True, runs browser without visible window (faster)\n",
    "        wait_time: Seconds to wait for JavaScript to render\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True, wait_time=3):\n",
    "        self.headless = headless\n",
    "        self.wait_time = wait_time\n",
    "    \n",
    "    def _create_driver(self):\n",
    "        \"\"\"Create and configure a Chrome WebDriver instance.\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        \n",
    "        # Standard options for stability\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Use a realistic user agent\n",
    "        options.add_argument(\n",
    "            'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '\n",
    "            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        )\n",
    "        \n",
    "        # webdriver-manager automatically downloads the correct ChromeDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    def _clean_html(self, html):\n",
    "        \"\"\"Extract clean text from HTML, removing navigation and scripts.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Get the title\n",
    "        title = soup.title.string.strip() if soup.title else \"No title\"\n",
    "        \n",
    "        # Remove elements that don't contain useful content\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header', \n",
    "                         'aside', 'img', 'input', 'button', 'form', 'iframe']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Get text with proper spacing\n",
    "        text = soup.body.get_text(separator=\"\\n\", strip=True) if soup.body else \"\"\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        clean_text = \"\\n\".join(lines)\n",
    "        \n",
    "        return title, clean_text\n",
    "    \n",
    "    def scrape(self, url, source_name=\"Website\"):\n",
    "        \"\"\"\n",
    "        Scrape a URL and return structured content.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to scrape\n",
    "            source_name: A friendly name for this source\n",
    "            \n",
    "        Returns:\n",
    "            dict with keys: url, source, title, text, success, error\n",
    "        \"\"\"\n",
    "        print(f\"Scraping: {source_name}...\")\n",
    "        \n",
    "        try:\n",
    "            driver = self._create_driver()\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for JavaScript to render\n",
    "            time.sleep(self.wait_time)\n",
    "            \n",
    "            # Scroll down to trigger lazy loading\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 2);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            driver.quit()\n",
    "            \n",
    "            title, text = self._clean_html(html)\n",
    "            \n",
    "            # Truncate very long content to stay within token limits\n",
    "            if len(text) > 12000:\n",
    "                text = text[:12000] + \"\\n[...content truncated...]\"\n",
    "            \n",
    "            print(f\"  Done! Extracted {len(text):,} characters\")\n",
    "            \n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"source\": source_name,\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "                \"success\": True,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"source\": source_name,\n",
    "                \"title\": None,\n",
    "                \"text\": None,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Sentiment Analyzer\n",
    "\n",
    "This class uses GPT to analyze the scraped content and extract:\n",
    "- Overall sentiment (bullish/bearish/neutral)\n",
    "- A sentiment score from -100 to +100\n",
    "- Key themes and topics\n",
    "- Companies mentioned\n",
    "- Actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Uses GPT to analyze sentiment in tech/finance news content.\n",
    "    Returns structured analysis with scores and insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"You are a financial analyst AI. Analyze the news content and respond with JSON:\n",
    "\n",
    "{\n",
    "    \"sentiment\": \"bullish\" or \"bearish\" or \"neutral\",\n",
    "    \"score\": <-100 to 100, where -100=very bearish, 100=very bullish>,\n",
    "    \"confidence\": <0-100>,\n",
    "    \"themes\": [\"theme1\", \"theme2\", \"theme3\"],\n",
    "    \"companies\": [\"company1\", \"company2\"],\n",
    "    \"summary\": \"2-3 sentence summary of key points\",\n",
    "    \"insight\": \"One actionable recommendation\"\n",
    "}\n",
    "\n",
    "Focus on tech industry and market implications.\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def analyze(self, scraped_data):\n",
    "        \"\"\"\n",
    "        Analyze scraped content for sentiment.\n",
    "        \n",
    "        Args:\n",
    "            scraped_data: dict from SeleniumScraper.scrape()\n",
    "            \n",
    "        Returns:\n",
    "            dict with sentiment analysis results\n",
    "        \"\"\"\n",
    "        if not scraped_data[\"success\"]:\n",
    "            return {\n",
    "                \"source\": scraped_data[\"source\"],\n",
    "                \"error\": scraped_data[\"error\"]\n",
    "            }\n",
    "        \n",
    "        user_prompt = f\"\"\"Analyze this news content:\n",
    "\n",
    "Source: {scraped_data['source']}\n",
    "Title: {scraped_data['title']}\n",
    "\n",
    "Content:\n",
    "{scraped_data['text'][:10000]}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            result[\"source\"] = scraped_data[\"source\"]\n",
    "            result[\"url\"] = scraped_data[\"url\"]\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": scraped_data[\"source\"],\n",
    "                \"error\": str(e)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Report Formatting\n",
    "\n",
    "Helper functions to display the analysis results in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_badge(score):\n",
    "    \"\"\"Return a text badge based on sentiment score.\"\"\"\n",
    "    if score >= 50:\n",
    "        return \"[STRONGLY BULLISH]\"\n",
    "    elif score >= 20:\n",
    "        return \"[BULLISH]\"\n",
    "    elif score > -20:\n",
    "        return \"[NEUTRAL]\"\n",
    "    elif score > -50:\n",
    "        return \"[BEARISH]\"\n",
    "    else:\n",
    "        return \"[STRONGLY BEARISH]\"\n",
    "\n",
    "\n",
    "def format_analysis(analysis):\n",
    "    \"\"\"Format a single analysis result as markdown.\"\"\"\n",
    "    if \"error\" in analysis:\n",
    "        return f\"### {analysis.get('source', 'Unknown')}\\n\\nError: {analysis['error']}\\n\\n---\"\n",
    "    \n",
    "    score = analysis.get('score', 0)\n",
    "    badge = get_sentiment_badge(score)\n",
    "    \n",
    "    themes = \", \".join(analysis.get('themes', [])[:4])\n",
    "    companies = \", \".join(analysis.get('companies', [])[:5]) or \"None mentioned\"\n",
    "    \n",
    "    return f\"\"\"### {analysis['source']} {badge}\n",
    "\n",
    "**Sentiment Score:** {score}/100 (Confidence: {analysis.get('confidence', 'N/A')}%)\n",
    "\n",
    "**Summary:** {analysis.get('summary', 'N/A')}\n",
    "\n",
    "**Key Themes:** {themes}\n",
    "\n",
    "**Companies Mentioned:** {companies}\n",
    "\n",
    "**Actionable Insight:** {analysis.get('insight', 'N/A')}\n",
    "\n",
    "*Source: {analysis.get('url', 'N/A')}*\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def display_report(analyses):\n",
    "    \"\"\"Display a complete sentiment report.\"\"\"\n",
    "    # Calculate aggregate stats\n",
    "    valid = [a for a in analyses if \"error\" not in a]\n",
    "    \n",
    "    if valid:\n",
    "        avg_score = sum(a.get('score', 0) for a in valid) / len(valid)\n",
    "        avg_badge = get_sentiment_badge(avg_score)\n",
    "    else:\n",
    "        avg_score = 0\n",
    "        avg_badge = \"[NO DATA]\"\n",
    "    \n",
    "    # Build the report\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    report = f\"\"\"# Sentiment Analysis Report\n",
    "\n",
    "**Generated:** {timestamp}\n",
    "\n",
    "## Overall Market Sentiment {avg_badge}\n",
    "\n",
    "**Average Score:** {avg_score:.1f}/100 across {len(valid)} sources\n",
    "\n",
    "---\n",
    "\n",
    "## Individual Source Analysis\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for analysis in analyses:\n",
    "        report += format_analysis(analysis) + \"\\n\"\n",
    "    \n",
    "    display(Markdown(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Run the Analysis\n",
    "\n",
    "Now let's put it all together! Configure your news sources and run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the news sources you want to analyze\n",
    "# Format: {\"Friendly Name\": \"URL\"}\n",
    "\n",
    "NEWS_SOURCES = {\n",
    "    \"Hacker News\": \"https://news.ycombinator.com/\",\n",
    "    \"TechCrunch\": \"https://techcrunch.com/\",\n",
    "    \"The Verge Tech\": \"https://www.theverge.com/tech\",\n",
    "}\n",
    "\n",
    "# You can add more sources:\n",
    "# \"Ars Technica\": \"https://arstechnica.com/\",\n",
    "# \"Wired\": \"https://www.wired.com/\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our tools\n",
    "scraper = SeleniumScraper(headless=True, wait_time=4)\n",
    "analyzer = SentimentAnalyzer(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SCRAPING NEWS SOURCES\n",
      "==================================================\n",
      "Scraping: Hacker News...\n",
      "  Done! Extracted 3,940 characters\n",
      "Scraping: TechCrunch...\n",
      "  Done! Extracted 9,766 characters\n",
      "Scraping: The Verge Tech...\n",
      "  Done! Extracted 12,026 characters\n",
      "\n",
      "Scraped 3/3 sources successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scrape all sources\n",
    "print(\"=\"*50)\n",
    "print(\"SCRAPING NEWS SOURCES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "scraped_data = []\n",
    "for name, url in NEWS_SOURCES.items():\n",
    "    result = scraper.scrape(url, name)\n",
    "    scraped_data.append(result)\n",
    "\n",
    "successful = sum(1 for s in scraped_data if s[\"success\"])\n",
    "print(f\"\\nScraped {successful}/{len(NEWS_SOURCES)} sources successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANALYZING SENTIMENT\n",
      "==================================================\n",
      "Analyzing: Hacker News...\n",
      "  Sentiment: neutral\n",
      "Analyzing: TechCrunch...\n",
      "  Sentiment: neutral\n",
      "Analyzing: The Verge Tech...\n",
      "  Sentiment: neutral\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Analyze sentiment for each source\n",
    "print(\"=\"*50)\n",
    "print(\"ANALYZING SENTIMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "analyses = []\n",
    "for data in scraped_data:\n",
    "    print(f\"Analyzing: {data['source']}...\")\n",
    "    result = analyzer.analyze(data)\n",
    "    analyses.append(result)\n",
    "    print(f\"  Sentiment: {result.get('sentiment', 'error')}\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Sentiment Analysis Report\n",
       "\n",
       "**Generated:** 2026-01-14 19:35\n",
       "\n",
       "## Overall Market Sentiment [NEUTRAL]\n",
       "\n",
       "**Average Score:** 0.0/100 across 3 sources\n",
       "\n",
       "---\n",
       "\n",
       "## Individual Source Analysis\n",
       "\n",
       "### Hacker News [NEUTRAL]\n",
       "\n",
       "**Sentiment Score:** 0/100 (Confidence: 65%)\n",
       "\n",
       "**Summary:** The feed shows ongoing innovation in open-source software, AI tooling, and hardware, with Starlink expanding data plans and a vendor ecosystem including Adafruit and SparkFun undergoing changes. It also highlights regulatory shifts (UK digital ID) and security concerns (Linux malware, investigations) that could weigh on tech risk. Overall, tech progress persists alongside notable policy and security headwinds.\n",
       "\n",
       "**Key Themes:** open source, AI and tooling, security and regulation\n",
       "\n",
       "**Companies Mentioned:** Starlink, Adafruit, SparkFun, Redis, SolidQueue\n",
       "\n",
       "**Actionable Insight:** Monitor and selectively invest in OSS-enabled hardware and remote-connectivity enablers (Starlink, Adafruit/SparkFun ecosystems) to capitalize on edge computing and remote work trends, while maintaining risk hedges for cybersecurity and regulatory developments.\n",
       "\n",
       "*Source: https://news.ycombinator.com/*\n",
       "\n",
       "---\n",
       "\n",
       "### TechCrunch [NEUTRAL]\n",
       "\n",
       "**Sentiment Score:** 0/100 (Confidence: 65%)\n",
       "\n",
       "**Summary:** TechCrunch presents a mixed bag: ongoing AI and cloud infrastructure expansion alongside monetization shifts (Tesla FSD subscriptions, Apple Creator Studio), regulatory and workforce tensions (Meta layoffs), and notable security incidents. The collection underscores sustained capital expenditure in AI/cloud, even as security and market risks loom.\n",
       "\n",
       "**Key Themes:** AI/ML and AI infrastructure, Cloud computing and data centers, Monetization strategies and subscriptions, Security and data privacy\n",
       "\n",
       "**Companies Mentioned:** Tesla, Microsoft, Google, Apple, Meta\n",
       "\n",
       "**Actionable Insight:** Actionable: overweight AI/cloud infrastructure leaders (e.g., Microsoft and Google) to capture upside from data-center expansion and AI service rollouts, while actively monitoring cybersecurity and regulatory developments.\n",
       "\n",
       "*Source: https://techcrunch.com/*\n",
       "\n",
       "---\n",
       "\n",
       "### The Verge Tech [NEUTRAL]\n",
       "\n",
       "**Sentiment Score:** 0/100 (Confidence: 65%)\n",
       "\n",
       "**Summary:** The Verge’s tech roundup presents mixed signals: Nvidia debuts DLSS 4.5 and updates to its control panel, underscoring continued AI hardware demand; Meta is shrinking its metaverse footprint by closing VR studios; Airbnb recruits Meta’s AI leader to bolster its app, while Tesla shifts FSD to a subscription model. At the same time, memory dynamics weigh on the sector as Micron contemplates downsizing Crucial and memory costs rise, hinting at potential headwinds for consumer hardware costs. Overall, the news paints a balanced landscape with selective upside in AI/GPUs and ongoing costs/strategy pivots affecting hardware and platforms.\n",
       "\n",
       "**Key Themes:** AI & semiconductors, VR/metaverse strategy, Memory costs & supply dynamics\n",
       "\n",
       "**Companies Mentioned:** Nvidia, Meta, Airbnb, Tesla, Micron\n",
       "\n",
       "**Actionable Insight:** Actionable recommendation: overweight Nvidia and other AI-hardware beneficiaries given the DLSS 4.5 rollout and expanding AI workloads, while hedging memory-cycle risk by trimming Micron exposure until memory pricing and supply dynamics stabilize.\n",
       "\n",
       "*Source: https://www.theverge.com/tech*\n",
       "\n",
       "---\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Display the report\n",
    "display_report(analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Quick Single-URL Analysis\n",
    "\n",
    "Use this function to quickly analyze any single URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_sentiment(url, name=\"Website\"):\n",
    "    \"\"\"One-liner to scrape and analyze any URL.\"\"\"\n",
    "    scraper = SeleniumScraper(headless=True)\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    \n",
    "    data = scraper.scrape(url, name)\n",
    "    result = analyzer.analyze(data)\n",
    "    \n",
    "    display(Markdown(format_analysis(result)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Hacker News...\n",
      "  Done! Extracted 3,900 characters\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Hacker News [NEUTRAL]\n",
       "\n",
       "**Sentiment Score:** 0/100 (Confidence: 50%)\n",
       "\n",
       "**Summary:** The news content includes various discussions around technology topics ranging from updates on Starlink services to open-source projects. While there is no direct bullish or bearish momentum indicated for the tech sector, the active discourse on software development and emerging technologies suggests continued interest. The mention of GitHub reflects ongoing engagement within the developer community.\n",
       "\n",
       "**Key Themes:** technology, software development, open-source\n",
       "\n",
       "**Companies Mentioned:** GitHub, Starlink\n",
       "\n",
       "**Actionable Insight:** Invest in tech companies that are actively engaging in open-source projects or enhancing user experiences, as community-driven developments may provide growth opportunities.\n",
       "\n",
       "*Source: https://news.ycombinator.com/*\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neutral',\n",
       " 'score': 0,\n",
       " 'confidence': 50,\n",
       " 'themes': ['technology', 'software development', 'open-source'],\n",
       " 'companies': ['GitHub', 'Starlink'],\n",
       " 'summary': 'The news content includes various discussions around technology topics ranging from updates on Starlink services to open-source projects. While there is no direct bullish or bearish momentum indicated for the tech sector, the active discourse on software development and emerging technologies suggests continued interest. The mention of GitHub reflects ongoing engagement within the developer community.',\n",
       " 'insight': 'Invest in tech companies that are actively engaging in open-source projects or enhancing user experiences, as community-driven developments may provide growth opportunities.',\n",
       " 'source': 'Hacker News',\n",
       " 'url': 'https://news.ycombinator.com/'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try it! Uncomment and run:\n",
    "quick_sentiment(\"https://news.ycombinator.com/\", \"Hacker News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ideas for Extension\n",
    "\n",
    "- Add more news sources (Reuters, Bloomberg, etc.)\n",
    "- Track sentiment over time by saving results to a file\n",
    "- Add email alerts when sentiment shifts dramatically\n",
    "- Create charts with matplotlib to visualize trends\n",
    "- Use Ollama for local/free sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "*Week 1 Community Contribution - LLM Engineering Course*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
