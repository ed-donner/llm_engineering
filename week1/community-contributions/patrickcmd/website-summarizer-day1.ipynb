{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38187f7d",
      "metadata": {},
      "source": [
        "# Website Summarizer with Playwright & OpenAI\n",
        "\n",
        "This notebook demonstrates an end-to-end pipeline for **scraping** a website and **summarizing** its content using a large language model. It uses:\n",
        "\n",
        "- **Playwright** — a headless browser that fully renders JavaScript, making it suitable for modern single-page applications (React, Vue, Angular, etc.) where plain HTTP clients like `requests` would only see an empty shell.\n",
        "- **OpenAI GPT-4.1-mini** — to generate concise, readable summaries of the scraped content.\n",
        "\n",
        "## Installation and Setup\n",
        "\n",
        "```sh\n",
        "uv add playwright nest_asyncio openai\n",
        "playwright install\n",
        "```\n",
        "\n",
        "Since Jupyter already runs an event loop, we patch it with `nest_asyncio` so Playwright's async API can work inside notebook cells:\n",
        "\n",
        "```python\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf889a66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from typing import TypedDict\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()  # patches the running loop to allow nesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88af15ba",
      "metadata": {},
      "source": [
        "## Connecting to OpenAI\n",
        "\n",
        "The next cell is where we load in the environment variables in your `.env` file and connect to OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe9d530",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Check the key\n",
        "\n",
        "if not api_key:\n",
        "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
        "elif not api_key.startswith(\"sk-proj-\"):\n",
        "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
        "elif api_key.strip() != api_key:\n",
        "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350bf5a3",
      "metadata": {},
      "source": [
        "## Fetching Website Content with Playwright\n",
        "\n",
        "Below we define the core scraping logic. The `scrape_webpage` async function launches a headless Chromium browser, navigates to the target URL, waits for the page to finish loading, then extracts the visible text content — stripping out scripts, styles, and other non-content elements. A synchronous `scrape` wrapper is also provided for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e87a4f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PageData(TypedDict):\n",
        "    \"\"\"Structured result returned by the scraper.\"\"\"\n",
        "    url: str\n",
        "    title: str\n",
        "    content: str\n",
        "\n",
        "\n",
        "async def scrape_webpage(url: str, wait_for: str = \"networkidle\", timeout: int = 30000) -> PageData:\n",
        "    \"\"\"\n",
        "    Scrape the title and text content of a webpage using Playwright.\n",
        "\n",
        "    Playwright launches a real browser (Chromium by default), which fully executes\n",
        "    JavaScript — making it ideal for React, Vue, Angular, and other JS-heavy sites\n",
        "    that plain HTTP clients like `requests` cannot render.\n",
        "\n",
        "    Args:\n",
        "        url (str): The full URL of the webpage to scrape\n",
        "                   (e.g. \"https://example.com\").\n",
        "        wait_for (str): The condition to wait for before extracting content.\n",
        "                        Options:\n",
        "                          - \"networkidle\"      → waits until network has been idle\n",
        "                                                 for 500ms (best for SPAs). [default]\n",
        "                          - \"domcontentloaded\" → waits for the HTML to be parsed.\n",
        "                          - \"load\"             → waits for the load event.\n",
        "                          - \"commit\"           → waits until the response starts arriving.\n",
        "        timeout (int): Maximum time in milliseconds to wait for the page to load.\n",
        "                       Defaults to 30000 (30 seconds).\n",
        "\n",
        "    Returns:\n",
        "        PageData: A TypedDict containing:\n",
        "            - ``url``     (str): The URL that was scraped.\n",
        "            - ``title``   (str): The page's <title> tag value, or an empty string\n",
        "                                 if no title was found.\n",
        "            - ``content`` (str): The visible text content of the fully rendered\n",
        "                                 page, with whitespace normalized.\n",
        "\n",
        "    Raises:\n",
        "        PlaywrightTimeoutError: If the page does not load within the timeout period.\n",
        "        Exception: For any other browser or network related errors.\n",
        "\n",
        "    Example:\n",
        "        >>> import asyncio\n",
        "        >>> data = asyncio.run(scrape_webpage(\"https://news.ycombinator.com\"))\n",
        "        >>> print(data[\"title\"])\n",
        "        >>> print(data[\"content\"][:500])\n",
        "    \"\"\"\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        try:\n",
        "            context = await browser.new_context(\n",
        "                user_agent=(\n",
        "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                    \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            page = await context.new_page()\n",
        "            await page.goto(url, wait_until=wait_for, timeout=timeout)\n",
        "\n",
        "            # Extract both title and body text in a single evaluate call\n",
        "            # to avoid multiple round trips to the browser\n",
        "            result = await page.evaluate(\"\"\"\n",
        "                () => {\n",
        "                    const title = document.title ?? \"\";\n",
        "\n",
        "                    // Remove non-visible / non-content elements\n",
        "                    const tagsToRemove = ['script', 'style', 'noscript', 'svg', 'img'];\n",
        "                    tagsToRemove.forEach(tag => {\n",
        "                        document.querySelectorAll(tag).forEach(el => el.remove());\n",
        "                    });\n",
        "\n",
        "                    const content = document.body.innerText ?? \"\";\n",
        "\n",
        "                    return { title, content };\n",
        "                }\n",
        "            \"\"\")\n",
        "\n",
        "            # Normalize excessive whitespace and blank lines\n",
        "            lines = [line.strip() for line in result[\"content\"].splitlines()]\n",
        "            cleaned_content = \"\\n\".join(line for line in lines if line)\n",
        "\n",
        "            return PageData(\n",
        "                url=url,\n",
        "                title=result[\"title\"].strip(),\n",
        "                content=cleaned_content,\n",
        "            )\n",
        "\n",
        "        except PlaywrightTimeoutError:\n",
        "            raise PlaywrightTimeoutError(\n",
        "                f\"Page '{url}' did not finish loading within {timeout}ms. \"\n",
        "                \"Try increasing the timeout or using a different wait_for strategy.\"\n",
        "            )\n",
        "        finally:\n",
        "            await browser.close()\n",
        "\n",
        "\n",
        "def scrape(url: str, wait_for: str = \"networkidle\", timeout: int = 30000) -> PageData:\n",
        "    \"\"\"\n",
        "    Synchronous wrapper around :func:`scrape_webpage`.\n",
        "\n",
        "    Useful when you are not inside an async context and want a simple\n",
        "    one-liner call.\n",
        "\n",
        "    Args:\n",
        "        url (str): The full URL of the webpage to scrape.\n",
        "        wait_for (str): Load condition — see :func:`scrape_webpage` for options.\n",
        "        timeout (int): Timeout in milliseconds. Defaults to 30000.\n",
        "\n",
        "    Returns:\n",
        "        PageData: A TypedDict containing ``url``, ``title``, and ``content``.\n",
        "                  See :func:`scrape_webpage` for full field descriptions.\n",
        "\n",
        "    Example:\n",
        "        >>> from scraper import scrape\n",
        "        >>> data = scrape(\"https://news.ycombinator.com\")\n",
        "        >>> print(data[\"title\"])\n",
        "        >>> print(data[\"content\"][:300])\n",
        "    \"\"\"\n",
        "    return asyncio.run(scrape_webpage(url, wait_for=wait_for, timeout=timeout))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ec5246",
      "metadata": {},
      "source": [
        "### Test Run — Scraping a Page\n",
        "\n",
        "Let's try the scraper on an OpenAI blog post. We use `wait_for=\"domcontentloaded\"` and a generous timeout since some pages load heavy assets. The output shows the page title, URL, the first 5 000 characters of visible text, and the total character count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d25c519",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://openai.com/index/introducing-openai-frontier/\"\n",
        "# url = \"https://openai.com\"\n",
        "print(f\"Scraping: {url}\\n{'=' * 50}\")\n",
        "data = scrape(url, wait_for=\"domcontentloaded\", timeout=120000)  # Increase timeout for slower pages\n",
        "\n",
        "print(f\"Title   : {data['title']}\")\n",
        "print(f\"URL     : {data['url']}\")\n",
        "print(f\"{'=' * 50}\")\n",
        "print(data[\"content\"][:5000])\n",
        "print(f\"\\n{'=' * 50}\")\n",
        "print(f\"Total characters scraped: {len(data['content'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ec877f",
      "metadata": {},
      "source": [
        "## Summarizing Webpage Content with OpenAI (GPT-4.1-mini)\n",
        "\n",
        "Now that we can scrape any page, we need to feed that content to a language model. This requires two prompt templates:\n",
        "\n",
        "- A **system prompt** that tells the model to act as a website analyst and respond in Markdown.\n",
        "- A **user prompt** that injects the scraped content and asks for a summary.\n",
        "\n",
        "### Defining the Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85928da0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant that analyzes the contents of a website,\n",
        "and provides a short summary, ignoring text that might be navigation related.\n",
        "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Define our user prompt\n",
        "\n",
        "user_prompt_prefix = \"\"\"\n",
        "Here are the contents of a website.\n",
        "Provide a short summary of this website.\n",
        "If it includes news or announcements, then summarize these too.\n",
        "\n",
        "Website content:\n",
        "{content}\n",
        "\"\"\"\n",
        "\n",
        "# Define our messages\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt_prefix},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15959b78",
      "metadata": {},
      "source": [
        "### Building the Messages List\n",
        "\n",
        "The `build_messages` helper formats the system and user prompts into the message list that the OpenAI API expects, injecting the scraped content into the user prompt via string formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56dd0df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_messages(content: str) -> list[dict]:\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_prefix.format(content=content)},\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6ca46a",
      "metadata": {},
      "source": [
        "Preview the formatted messages to verify the prompt structure before sending them to the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b305e208",
      "metadata": {},
      "outputs": [],
      "source": [
        "build_messages(data[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b93379",
      "metadata": {},
      "source": [
        "### Putting It All Together\n",
        "\n",
        "The `summarize` function chains the entire pipeline: scrape a URL, build the prompt messages, call the OpenAI Responses API with GPT-4.1-mini, and return the summary text. The `display_summary` wrapper renders the result as formatted Markdown in the notebook output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0955ef5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# And now: call the OpenAI API using the responses api\n",
        "client = OpenAI()\n",
        "\n",
        "def summarize(url):\n",
        "    website_content = scrape(url, wait_for=\"domcontentloaded\", timeout=120000)\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        input=build_messages(website_content[\"content\"])\n",
        "    )\n",
        "    \n",
        "    return response.output_text\n",
        "\n",
        "# A function to display this nicely in the output, using markdown\n",
        "def display_summary(url):\n",
        "    summary = summarize(url)\n",
        "    display(Markdown(summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540abd9c",
      "metadata": {},
      "source": [
        "### Demo — Summarizing Different Websites\n",
        "\n",
        "Let's run the full pipeline (scrape → build messages → call OpenAI → display Markdown) on a few different sites to see how well the summarizer handles varying content types — a blog post, a company homepage, and an educational platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433a46b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://openai.com/index/introducing-openai-frontier/\"\n",
        "display_summary(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8c6810",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://anthropic.com\"\n",
        "display_summary(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2f19ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://www.codingforentrepreneurs.com/\"\n",
        "display_summary(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee25f4c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
