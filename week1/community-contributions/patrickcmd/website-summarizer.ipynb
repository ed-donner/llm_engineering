{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Website Summarizer with Playwright, OpenAI & Ollama\n",
        "\n",
        "This notebook demonstrates an end-to-end pipeline for **scraping** a website and **summarizing** its content using a large language model. It uses:\n",
        "\n",
        "- **Playwright** — a headless browser that fully renders JavaScript, making it suitable for modern single-page applications (React, Vue, Angular, etc.) where plain HTTP clients like `requests` would only see an empty shell.\n",
        "- **OpenAI GPT-4.1-mini** — a cloud-hosted model for high-quality summaries.\n",
        "- **Ollama (Llama 3.2)** — a locally-running open-source alternative that keeps your data on your machine at zero API cost.\n",
        "\n",
        "## Installation and Setup\n",
        "\n",
        "```sh\n",
        "uv add playwright nest_asyncio openai\n",
        "playwright install\n",
        "```\n",
        "\n",
        "Since Jupyter already runs an event loop, we patch it with `nest_asyncio` so Playwright's async API can work inside notebook cells:\n",
        "\n",
        "```python\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "```\n",
        "\n"
      ],
      "id": "38187f7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import asyncio\n",
        "from typing import TypedDict\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()  # patches the running loop to allow nesting\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cf889a66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connecting to OpenAI\n",
        "\n",
        "The next cell is where we load in the environment variables in your `.env` file and connect to OpenAI."
      ],
      "id": "88af15ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Check the key\n",
        "\n",
        "if not api_key:\n",
        "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
        "elif not api_key.startswith(\"sk-proj-\"):\n",
        "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
        "elif api_key.strip() != api_key:\n",
        "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0fe9d530"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetching Website Content with Playwright\n",
        "\n",
        "Below we define the core scraping logic. The `scrape_webpage` async function launches a headless Chromium browser, navigates to the target URL, waits for the page to finish loading, then extracts the visible text content — stripping out scripts, styles, and other non-content elements. A synchronous `scrape` wrapper is also provided for convenience."
      ],
      "id": "350bf5a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PageData(TypedDict):\n",
        "    \"\"\"Structured result returned by the scraper.\"\"\"\n",
        "    url: str\n",
        "    title: str\n",
        "    content: str\n",
        "\n",
        "\n",
        "async def scrape_webpage(url: str, wait_for: str = \"networkidle\", timeout: int = 30000) -> PageData:\n",
        "    \"\"\"\n",
        "    Scrape the title and text content of a webpage using Playwright.\n",
        "\n",
        "    Playwright launches a real browser (Chromium by default), which fully executes\n",
        "    JavaScript — making it ideal for React, Vue, Angular, and other JS-heavy sites\n",
        "    that plain HTTP clients like `requests` cannot render.\n",
        "\n",
        "    Args:\n",
        "        url (str): The full URL of the webpage to scrape\n",
        "                   (e.g. \"https://example.com\").\n",
        "        wait_for (str): The condition to wait for before extracting content.\n",
        "                        Options:\n",
        "                          - \"networkidle\"      → waits until network has been idle\n",
        "                                                 for 500ms (best for SPAs). [default]\n",
        "                          - \"domcontentloaded\" → waits for the HTML to be parsed.\n",
        "                          - \"load\"             → waits for the load event.\n",
        "                          - \"commit\"           → waits until the response starts arriving.\n",
        "        timeout (int): Maximum time in milliseconds to wait for the page to load.\n",
        "                       Defaults to 30000 (30 seconds).\n",
        "\n",
        "    Returns:\n",
        "        PageData: A TypedDict containing:\n",
        "            - ``url``     (str): The URL that was scraped.\n",
        "            - ``title``   (str): The page's <title> tag value, or an empty string\n",
        "                                 if no title was found.\n",
        "            - ``content`` (str): The visible text content of the fully rendered\n",
        "                                 page, with whitespace normalized.\n",
        "\n",
        "    Raises:\n",
        "        PlaywrightTimeoutError: If the page does not load within the timeout period.\n",
        "        Exception: For any other browser or network related errors.\n",
        "\n",
        "    Example:\n",
        "        >>> import asyncio\n",
        "        >>> data = asyncio.run(scrape_webpage(\"https://news.ycombinator.com\"))\n",
        "        >>> print(data[\"title\"])\n",
        "        >>> print(data[\"content\"][:500])\n",
        "    \"\"\"\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        try:\n",
        "            context = await browser.new_context(\n",
        "                user_agent=(\n",
        "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                    \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            page = await context.new_page()\n",
        "            await page.goto(url, wait_until=wait_for, timeout=timeout)\n",
        "\n",
        "            # Extract both title and body text in a single evaluate call\n",
        "            # to avoid multiple round trips to the browser\n",
        "            result = await page.evaluate(\"\"\"\n",
        "                () => {\n",
        "                    const title = document.title ?? \"\";\n",
        "\n",
        "                    // Remove non-visible / non-content elements\n",
        "                    const tagsToRemove = ['script', 'style', 'noscript', 'svg', 'img'];\n",
        "                    tagsToRemove.forEach(tag => {\n",
        "                        document.querySelectorAll(tag).forEach(el => el.remove());\n",
        "                    });\n",
        "\n",
        "                    const content = document.body.innerText ?? \"\";\n",
        "\n",
        "                    return { title, content };\n",
        "                }\n",
        "            \"\"\")\n",
        "\n",
        "            # Normalize excessive whitespace and blank lines\n",
        "            lines = [line.strip() for line in result[\"content\"].splitlines()]\n",
        "            cleaned_content = \"\\n\".join(line for line in lines if line)\n",
        "\n",
        "            return PageData(\n",
        "                url=url,\n",
        "                title=result[\"title\"].strip(),\n",
        "                content=cleaned_content,\n",
        "            )\n",
        "\n",
        "        except PlaywrightTimeoutError:\n",
        "            raise PlaywrightTimeoutError(\n",
        "                f\"Page '{url}' did not finish loading within {timeout}ms. \"\n",
        "                \"Try increasing the timeout or using a different wait_for strategy.\"\n",
        "            )\n",
        "        finally:\n",
        "            await browser.close()\n",
        "\n",
        "\n",
        "def scrape(url: str, wait_for: str = \"networkidle\", timeout: int = 30000) -> PageData:\n",
        "    \"\"\"\n",
        "    Synchronous wrapper around :func:`scrape_webpage`.\n",
        "\n",
        "    Useful when you are not inside an async context and want a simple\n",
        "    one-liner call.\n",
        "\n",
        "    Args:\n",
        "        url (str): The full URL of the webpage to scrape.\n",
        "        wait_for (str): Load condition — see :func:`scrape_webpage` for options.\n",
        "        timeout (int): Timeout in milliseconds. Defaults to 30000.\n",
        "\n",
        "    Returns:\n",
        "        PageData: A TypedDict containing ``url``, ``title``, and ``content``.\n",
        "                  See :func:`scrape_webpage` for full field descriptions.\n",
        "\n",
        "    Example:\n",
        "        >>> from scraper import scrape\n",
        "        >>> data = scrape(\"https://news.ycombinator.com\")\n",
        "        >>> print(data[\"title\"])\n",
        "        >>> print(data[\"content\"][:300])\n",
        "    \"\"\"\n",
        "    return asyncio.run(scrape_webpage(url, wait_for=wait_for, timeout=timeout))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0e87a4f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Run — Scraping a Page\n",
        "\n",
        "Let's try the scraper on an OpenAI blog post. We use `wait_for=\"domcontentloaded\"` and a generous timeout since some pages load heavy assets. The output shows the page title, URL, the first 5 000 characters of visible text, and the total character count."
      ],
      "id": "53ec5246"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://openai.com/index/introducing-openai-frontier/\"\n",
        "# url = \"https://openai.com\"\n",
        "print(f\"Scraping: {url}\\n{'=' * 50}\")\n",
        "data = scrape(url, wait_for=\"domcontentloaded\", timeout=120000)  # Increase timeout for slower pages\n",
        "\n",
        "print(f\"Title   : {data['title']}\")\n",
        "print(f\"URL     : {data['url']}\")\n",
        "print(f\"{'=' * 50}\")\n",
        "print(data[\"content\"][:5000])\n",
        "print(f\"\\n{'=' * 50}\")\n",
        "print(f\"Total characters scraped: {len(data['content'])}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2d25c519"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarizing Webpage Content with OpenAI (GPT-4.1-mini)\n",
        "\n",
        "Now that we can scrape any page, we need to feed that content to a language model. This requires two prompt templates:\n",
        "\n",
        "- A **system prompt** that tells the model to act as a website analyst and respond in Markdown.\n",
        "- A **user prompt** that injects the scraped content and asks for a summary.\n",
        "\n",
        "### Defining the Prompts"
      ],
      "id": "40ec877f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant that analyzes the contents of a website,\n",
        "and provides a short summary, ignoring text that might be navigation related.\n",
        "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Define our user prompt\n",
        "\n",
        "user_prompt_prefix = \"\"\"\n",
        "Here are the contents of a website.\n",
        "Provide a short summary of this website.\n",
        "If it includes news or announcements, then summarize these too.\n",
        "\n",
        "Website content:\n",
        "{content}\n",
        "\"\"\"\n",
        "\n",
        "# Define our messages\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt_prefix},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "85928da0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the Messages List\n",
        "\n",
        "The `build_messages` helper formats the system and user prompts into the message list that the OpenAI API expects, injecting the scraped content into the user prompt via string formatting."
      ],
      "id": "15959b78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_messages(content: str) -> list[dict]:\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_prefix.format(content=content)},\n",
        "    ]\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a56dd0df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preview the formatted messages to verify the prompt structure before sending them to the API."
      ],
      "id": "3b6ca46a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "build_messages(data[\"content\"])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b305e208"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Putting It All Together\n",
        "\n",
        "The `summarize` function chains the entire pipeline: scrape a URL, build the prompt messages, call the OpenAI Responses API with GPT-4.1-mini, and return the summary text. The `display_summary` wrapper renders the result as formatted Markdown in the notebook output."
      ],
      "id": "99b93379"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# And now: call the OpenAI API using the responses api\n",
        "client = OpenAI()\n",
        "\n",
        "def summarize(url):\n",
        "    website_content = scrape(url, wait_for=\"domcontentloaded\", timeout=120000)\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        input=build_messages(website_content[\"content\"])\n",
        "    )\n",
        "    \n",
        "    return response.output_text\n",
        "\n",
        "# A function to display this nicely in the output, using markdown\n",
        "def display_summary(url):\n",
        "    summary = summarize(url)\n",
        "    display(Markdown(summary))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0955ef5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo — Summarizing Different Websites\n",
        "\n",
        "Let's run the full pipeline (scrape → build messages → call OpenAI → display Markdown) on a few different sites to see how well the summarizer handles varying content types — a blog post, a company homepage, and an educational platform."
      ],
      "id": "540abd9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://openai.com/index/introducing-openai-frontier/\"\n",
        "display_summary(url)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "433a46b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://anthropic.com\"\n",
        "display_summary(url)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bb8c6810"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://www.codingforentrepreneurs.com/\"\n",
        "display_summary(url)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0f2f19ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "fee25f4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "1. No API charges - open-source\n",
        "2. Data doesn't leave your box\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "1. Significantly less power than Frontier Model\n",
        "\n",
        "### Recap on installation of Ollama\n",
        "\n",
        "Simply visit [ollama](https://ollama.com) and install!\n",
        "\n",
        "Once complete, the ollama server should already be running locally.\n",
        "If you visit:\n",
        "http://localhost:11434/\n",
        "\n",
        "You should see the message `Ollama is running`.\n",
        "\n",
        "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`\n",
        "\n",
        "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`\n",
        "\n",
        "Then try http://localhost:11434/ again.\n",
        "\n",
        "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b`\n",
        "\n",
        "from a Terminal or Powershell, and change the code from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
      ],
      "id": "b66fd615"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Redefining the Pipeline for Ollama\n",
        "\n",
        "Below we create a new `OpenAI` client that points at Ollama's local OpenAI-compatible endpoint (`localhost:11434/v1`). The `summarize` and `display_summary` functions are redefined to use `chat.completions.create` with the `llama3.2` model instead of the OpenAI Responses API."
      ],
      "id": "c40ed6a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# And now: call the OpenAI API using the responses api\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "MODEL_LLAMA = 'llama3.2'\n",
        "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
        "\n",
        "\n",
        "def summarize(url):\n",
        "    website_content = scrape(url, wait_for=\"domcontentloaded\", timeout=120000)\n",
        "    response = ollama.chat.completions.create(\n",
        "        model = MODEL_LLAMA,\n",
        "        messages = build_messages(website_content[\"content\"])\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# A function to display this nicely in the output, using markdown\n",
        "def display_summary(url):\n",
        "    summary = summarize(url)\n",
        "    display(Markdown(summary))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c7b57b8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo — Summarizing with Ollama\n",
        "\n",
        "The same scrape → summarize → display pipeline, now running entirely on your local machine via Ollama. Compare these results with the OpenAI summaries above to see how the open-source model stacks up."
      ],
      "id": "10e297ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://developer.hashicorp.com/terraform\"\n",
        "display_summary(url)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7366ef30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = \"https://www.codingforentrepreneurs.com/\"\n",
        "display_summary(url)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "655b07fc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}