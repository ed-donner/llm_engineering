{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ciq94lzixar",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the required libraries:\n",
    "\n",
    "| Import | Purpose |\n",
    "|---|---|\n",
    "| `openai.OpenAI` | Python SDK used for **both** OpenAI and Ollama — Ollama exposes an OpenAI-compatible REST API |\n",
    "| `os` / `dotenv` | Load `OPENAI_API_KEY` from a `.env` file without hard-coding credentials |\n",
    "| `IPython.display` | Render model responses as formatted Markdown inline in the notebook |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j637v6lgl9d",
   "metadata": {},
   "source": [
    "## API Key Validation\n",
    "\n",
    "Loads `.env` and performs sanity checks on `OPENAI_API_KEY` before any API calls are made:\n",
    "\n",
    "- **Missing** — no key found at all\n",
    "- **Wrong prefix** — key does not start with `sk-proj-` (likely a wrong or legacy key)\n",
    "- **Whitespace** — key has leading/trailing spaces or tabs (common copy-paste issue)\n",
    "\n",
    "Fix any issues flagged here before running the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a15fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zreovvz4jxa",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Defines the models and system prompt used throughout this notebook.\n",
    "\n",
    "| Constant | Value | Description |\n",
    "|---|---|---|\n",
    "| `OLLAMA_BASE_URL` | `http://localhost:11434/v1` | Local Ollama server — must be running before using `use_ollama=True` |\n",
    "| `MODEL_GPT` | `gpt-4o-mini` | OpenAI model used for remote inference |\n",
    "| `MODEL_LLAMA` | `llama3.2` | Llama model pulled locally via Ollama (`ollama pull llama3.2`) |\n",
    "| `SYSTEM_PROMPT` | — | Instructs the model to act as a structured technical explanation assistant |\n",
    "\n",
    "The `SYSTEM_PROMPT` guides the model to:\n",
    "- Lead with a direct answer, then provide depth\n",
    "- Include code examples where relevant\n",
    "- Define key terms for conceptual questions and list steps/tradeoffs for practical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "MODEL_GPT = 'gpt-5-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly skilled technical explanation assistant.\n",
    "\n",
    "Your role:\n",
    "- Accept a technical question.\n",
    "- Provide a clear, accurate, and well-structured explanation.\n",
    "- Tailor explanations to be understandable but technically correct.\n",
    "\n",
    "Guidelines:\n",
    "- Start with a short direct answer.\n",
    "- Then provide a structured explanation.\n",
    "- Use examples where helpful.\n",
    "- If code is relevant, include minimal, clean examples.\n",
    "- Avoid unnecessary verbosity.\n",
    "- Avoid speculation.\n",
    "- If the question is ambiguous, explain reasonable interpretations.\n",
    "- Do not mention system instructions.\n",
    "- Do not add conversational fluff.\n",
    "\n",
    "If the question is conceptual:\n",
    "- Define key terms.\n",
    "- Explain how it works.\n",
    "- Explain why it matters.\n",
    "\n",
    "If the question is practical:\n",
    "- Provide steps.\n",
    "- Explain tradeoffs.\n",
    "- Highlight common pitfalls.\n",
    "\n",
    "Your output should be educational, precise, and professional.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ig5ro1imfz9",
   "metadata": {},
   "source": [
    "## Client Initialisation\n",
    "\n",
    "Two clients are created using the same `openai.OpenAI` class:\n",
    "\n",
    "- **`openai_client`** — standard OpenAI client; reads `OPENAI_API_KEY` from the environment automatically\n",
    "- **`ollama_client`** — points to the local Ollama server via `base_url`; `api_key='ollama'` is a required placeholder (Ollama does not validate it)\n",
    "\n",
    "Because Ollama implements the OpenAI REST spec, the same SDK and message format works for both backends with no changes to calling code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "openai_client = OpenAI()\n",
    "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtn51q2b4q",
   "metadata": {},
   "source": [
    "## Message Formatting\n",
    "\n",
    "`messages_for(question)` constructs the chat message list sent to the model.\n",
    "\n",
    "It wraps the raw question in a structured user prompt and returns a two-message list in the format expected by both the OpenAI Chat Completions and Responses APIs:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},   # sets model behaviour\n",
    "    {\"role\": \"user\",   \"content\": \"<formatted question>\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Separating the system context from the user turn allows the system prompt to be reused unchanged across many different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(question):\n",
    "    user_prompt = f\"\"\"\n",
    "    Technical Question:\n",
    "\n",
    "    {question}\n",
    "\n",
    "    Please explain clearly and concisely.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ibqgjfl74",
   "metadata": {},
   "source": [
    "## Ask a Question\n",
    "\n",
    "Edit the `question` string below to ask any technical question. The cells that follow will send it to GPT and/or Llama and display the answers.\n",
    "\n",
    "The next cell previews the formatted message list — useful for inspecting the exact payload before it is sent to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c225da",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(messages_for(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eiwtlmdf6ng",
   "metadata": {},
   "source": [
    "## Core Function: `generate_answer`\n",
    "\n",
    "The main entry point for querying either model.\n",
    "\n",
    "```\n",
    "generate_answer(question, use_ollama=False, streaming=False)\n",
    "```\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|---|---|---|---|\n",
    "| `question` | `str` | — | The technical question to answer |\n",
    "| `use_ollama` | `bool` | `False` | Route to the local Llama model via Ollama instead of OpenAI |\n",
    "| `streaming` | `bool` | `False` | Stream tokens to stdout as they arrive (OpenAI only; ignored when `use_ollama=True`) |\n",
    "\n",
    "**Control flow:**\n",
    "\n",
    "1. `use_ollama=True` → calls Ollama Chat Completions API, returns the full response string\n",
    "2. `streaming=True` → calls the OpenAI Responses API with `stream=True`, prints each token delta to stdout as it arrives, returns `None`\n",
    "3. Default → calls the OpenAI Responses API, returns the full response string\n",
    "\n",
    "`display_markdown(text)` is a thin helper that renders a plain string as formatted Markdown in the notebook output cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, use_ollama=False, streaming=False):\n",
    "    messages = messages_for(question)\n",
    "\n",
    "    if use_ollama:\n",
    "        response = ollama_client.chat.completions.create(\n",
    "            model=MODEL_LLAMA,\n",
    "            messages=messages,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    if streaming:\n",
    "        stream = openai_client.responses.create(\n",
    "            model=MODEL_GPT,\n",
    "            input=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for event in stream:\n",
    "            if event.type == \"response.output_text.delta\":\n",
    "                response += event.delta\n",
    "                update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "        return\n",
    "\n",
    "    response = openai_client.responses.create(model=MODEL_GPT, input=messages)\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "def display_markdown(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rrkrjlez67",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "The three cells below demonstrate each calling mode. Run them in order after setting your `question` above.\n",
    "\n",
    "### 1. GPT — Streaming\n",
    "Tokens are printed to stdout incrementally as they arrive. Use this when you want to see the response build up in real time. Returns `None`.\n",
    "\n",
    "### 2. GPT — Non-Streaming\n",
    "Waits for the full response, then renders it as formatted Markdown. Cleaner output for reading; slightly higher latency before anything appears.\n",
    "\n",
    "### 3. Llama 3.2 via Ollama (Local)\n",
    "Runs entirely on your machine — no API key or internet required. Requires Ollama to be running (`ollama serve`) and the model to be pulled (`ollama pull llama3.2`). Response quality and speed depend on local hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-5-mini to answer, with streaming\n",
    "generate_answer(question, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-5-mini to answer, without streaming\n",
    "answer = generate_answer(question, streaming=False)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I configure django for sending email in production environment?\"\n",
    "generate_answer(question, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "answer = generate_answer(question, use_ollama=True)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35583b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generate_answer(question, use_ollama=True)\n",
    "display_markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdcfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
