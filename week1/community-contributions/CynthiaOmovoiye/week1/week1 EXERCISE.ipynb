{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {},
      "source": [
        "# End of week 1 exercise\n",
        "\n",
        "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
        "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c1070317-3ed9-4659-abe3-828943230e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "load_dotenv(override=True)\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_LLAMA = 'llama3.2'\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if not api_key:\n",
        "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
        "elif not api_key.startswith(\"AIz\"):\n",
        "    print(\"An API key was found, but it doesn't start AIz\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment\n",
        "\n",
        "ollama = OpenAI(\n",
        "    base_url=OLLAMA_BASE_URL, \n",
        "    api_key='ollama',\n",
        "    timeout=60\n",
        "    )\n",
        "openai_client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"http://localhost:8888\",\n",
        "        \"X-Title\": \"llm_engineering_course\"\n",
        "    })\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a precise book-summary assistant.\n",
        "\n",
        "Rules:\n",
        "- Use ONLY the provided public description, title, Date published and authors.\n",
        "- Do NOT add facts not present in the description.\n",
        "- Do NOT include release/promotional lines unless central to the book's ideas.\n",
        "- Output must follow the exact format below.\n",
        "- Keep total output under 170 words.\n",
        "\n",
        "Exact output format:\n",
        "What it's about: <one sentence>\n",
        "Author(s): <authors>\n",
        "Date published: <date>\n",
        "\n",
        "Key ideas:\n",
        "- <idea 1>\n",
        "- <idea 2>\n",
        "- <idea 3>\n",
        "- <idea 4>\n",
        "- <idea 5>\n",
        "\n",
        "Who it's for: <one line>\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_user_prompt(meta: dict) -> str:\n",
        "    authors = \", \".join(meta.get(\"authors\") or []) or \"Unknown\"\n",
        "    return f\"\"\"\n",
        "Title: {meta.get(\"title\", \"Unknown\")}\n",
        "Author(s): {authors},\n",
        "Date published: {meta.get(\"publishedDate\", \"Unknown\")}\n",
        "\n",
        "Public description:\n",
        "{meta.get(\"description\", \"\")}\n",
        "\n",
        "Now follow the exact output format from the system message.\n",
        "Do not write paragraphs beyond that format.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f239436",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define function to look up books\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lookup_book_google(title: str, author: str, max_results: int = 3):\n",
        "    \"\"\"\n",
        "    Look up a book on Google Books API using title + author,\n",
        "    then return normalized metadata for the best match.\n",
        "\n",
        "    Parameters:\n",
        "        title (str): Book title to search for.\n",
        "        author (str): Author name to search for.\n",
        "        max_results (int): How many results to request from API.\n",
        "\n",
        "    Returns:\n",
        "        dict | None:\n",
        "            Normalized metadata dict for best match, or None if no match found.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build Google Books query in the same style as:\n",
        "    # intitle:<title> inauthor:<author>\n",
        "    query = f\"intitle:{title} inauthor:{author}\"\n",
        "\n",
        "    # Endpoint and query params (requests handles URL encoding for us).\n",
        "    url = \"https://www.googleapis.com/books/v1/volumes\"\n",
        "    params = {\"q\": query, \"maxResults\": max_results}\n",
        "\n",
        "    # Make HTTP request and fail fast for HTTP errors.\n",
        "    response = requests.get(url, params=params, timeout=20)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse JSON payload from Google Books API.\n",
        "    data = response.json()\n",
        "\n",
        "    # If no results were found, return None.\n",
        "    items = data.get(\"items\", [])\n",
        "    if not items:\n",
        "        return None\n",
        "\n",
        "    # Pick the first result as \"best\" for now (will improve ranking later).\n",
        "    best = items[0].get(\"volumeInfo\", {})\n",
        "\n",
        "    # Return only the fields we care about, with safe defaults.\n",
        "    return {\n",
        "        \"title\": best.get(\"title\"),\n",
        "        \"authors\": best.get(\"authors\", []),\n",
        "        \"publishedDate\": best.get(\"publishedDate\"),\n",
        "        \"categories\": best.get(\"categories\", []),\n",
        "        \"description\": best.get(\"description\"),   # key text for summarization\n",
        "        \"pageCount\": best.get(\"pageCount\"),\n",
        "        \"previewLink\": best.get(\"previewLink\"),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "53e8463c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# llm call with openai\n",
        "\n",
        "def llm_with_gpt(user_prompt: str, stream=False) -> str:\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=MODEL_GPT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        stream=stream\n",
        "    )\n",
        "\n",
        "    if stream:\n",
        "        text = \"\"\n",
        "        display_handle = display(Markdown(\"\"), display_id=True)\n",
        "        for chunk in response:\n",
        "            text += chunk.choices[0].delta.content or \"\"\n",
        "            update_display(Markdown(text), display_id=display_handle.display_id)\n",
        "        return text\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "8ae1256b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#llm call with llama\n",
        "\n",
        "def llm_with_llama(user_prompt: str, stream=False) -> str:\n",
        "    response = ollama.chat.completions.create(\n",
        "        model=MODEL_LLAMA,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "        stream=stream\n",
        "    )\n",
        "\n",
        "    if stream:\n",
        "        text = \"\"\n",
        "        display_handle = display(Markdown(\"\"), display_id=True)\n",
        "        for chunk in response:\n",
        "            text += chunk.choices[0].delta.content or ''\n",
        "            update_display(Markdown(text), display_id=display_handle.display_id)\n",
        "        return text\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "12481fd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to summarize book\n",
        "def summarize_book(meta: dict, llm_callable, stream=False):\n",
        "    \"\"\"\n",
        "    Summarize a book using metadata from a public source.\n",
        "\n",
        "    Parameters:\n",
        "        meta (dict): Book metadata (title, authors, description, etc.)\n",
        "        llm_callable (callable): Function that takes a prompt string and returns model output.\n",
        "                                 Example signature: llm_callable(prompt: str) -> str\n",
        "\n",
        "    Returns:\n",
        "        str: Structured summary text from the LLM, or a fallback message.\n",
        "    \"\"\"\n",
        "\n",
        "    # If metadata is missing or no description is available, return a safe fallback.\n",
        "    if not meta or not meta.get(\"description\"):\n",
        "        return \"I couldn't find a public description for this book from the source used.\"\n",
        "\n",
        "\n",
        "    # Build the prompt and explicitly constrain the model to the provided description.\n",
        "    prompt = build_user_prompt(meta)\n",
        "\n",
        "    # Call the LLM wrapper function and return its output.\n",
        "    output = llm_callable(prompt, stream=stream)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542f84af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# lookup book with google books api\n",
        "meta = lookup_book_google(\"Atomic Habits\", \"James Clear\")\n",
        "print(meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get gpt-4o-mini to answer\n",
        "summary = summarize_book(meta, llm_with_gpt)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Llama 3.2 to answer\n",
        "summary = summarize_book(meta, llm_with_llama)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c524593",
      "metadata": {},
      "outputs": [],
      "source": [
        "#llm call with llama with stream\n",
        "summary = summarize_book(meta, llm_with_llama,True)\n",
        "# print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3ae29e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#llm call with gpt with stream\n",
        "summary = summarize_book(meta, llm_with_gpt,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f94fcd",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
