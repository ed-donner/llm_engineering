{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from IPython.display import display, Markdown, update_display\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "GPT_MODEL = \"gpt-5-nano\"\n",
    "OLLAMA_MODEL = \"llama3.2:1b\"\n",
    "\n",
    "# urls\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e34e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and startswith sk-pr/nWe are good to go\n"
     ]
    }
   ],
   "source": [
    "gpt_api = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not gpt_api:\n",
    "    print(\"OPENAI_API_KEY not found\")\n",
    "elif not gpt_api.startswith(\"sk-proj\"):\n",
    "    print(\"Incorrect API key format\")\n",
    "elif gpt_api:\n",
    "    print(f\"API key found and startswith {gpt_api[:5]}/nWe are good to go\")\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "# here is the question; type over this to ask something new\n",
    "\n",
    "system_prompt = \"\"\"You are an AI tutor for Altamash. You specialize in AI engineering, Python, LLMs, agents, automation, and practical project-building.\n",
    "\n",
    "Your goals:\n",
    "- Help Altamash truly understand concepts, not just memorize them.\n",
    "- Teach in simple, direct, digestible explanations.\n",
    "- Keep the tone calm, supportive, and professional.\n",
    "- Push him toward clarity, not confusion.\n",
    "\n",
    "How to respond:\n",
    "- Break down ideas step-by-step.\n",
    "- Use small, clean code examples with comments.\n",
    "- Avoid unnecessary theory unless it's needed.\n",
    "- If his question is vague, ask 1–2 clarifying questions.\n",
    "- Correct misunderstandings gently but honestly.\n",
    "- Encourage consistency and discourage overthinking.\n",
    "- End your explainations with a MCQs to test his understanding.\n",
    "\n",
    "Your priorities:\n",
    "1. Build Altamash’s confidence in coding and AI agents.\n",
    "2. Help him debug issues without taking shortcuts.\n",
    "3. Strengthen his ability to read, understand, and modify code.\n",
    "4. Keep explanations practical and project-focused.\n",
    "\n",
    "Stay focused on helping him become a capable AI engineer who can build real-world solutions.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def gpt_streaming(question):\n",
    "    print (f\"getting response from {GPT_MODEL}\")\n",
    "    response = openai.chat.completions.create(\n",
    "        model = GPT_MODEL,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\" : system_prompt},\n",
    "            {\"role\":\"user\", \"content\":question}\n",
    "            ],\n",
    "        stream = True\n",
    "    )\n",
    "    result = \"\"\n",
    "    display_handle = display(Markdown(result), display_id = True)\n",
    "    for chunk in response:\n",
    "        result += chunk.choices[0].delta.content\n",
    "        update_display(Markdown(result),display_id = display_handle.display_id)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_streaming(\"how can you help me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_gpt(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c2707ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED       \n",
      "llama3.2:1b         baf6a787fdff    1.3 GB    17 minutes ago    \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    4 days ago        \n"
     ]
    }
   ],
   "source": [
    "!ollama ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32270ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9227d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llama(question):\n",
    "    print(f\"getting response from {OLLAMA_MODEL}......\")\n",
    "    ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "    response = ollama.chat.completions.create(\n",
    "        model = OLLAMA_MODEL,\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\":system_prompt},\n",
    "            {\"role\":\"user\", \"content\":question}\n",
    "            ],\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    display_handle = display(Markdown(result), display_id=True)\n",
    "    for chunk in response:\n",
    "        result += chunk.choices[0].delta.content\n",
    "        update_display(Markdown(result), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45dc7ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting response from llama3.2:1b......\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This code snippet appears to be part of a logging system, likely for an application. Here's a breakdown of what it does:\n",
       "\n",
       "1. `response = \"\"`: Initializes an empty string variable `response` that will hold the output of the logging process.\n",
       "\n",
       "2. `display_handle = display(Markdown(\"\"), display_id=True)`: This line prints a Markdown-formatted string to a display element identified by `display_id=True`. I'm assuming this is a part of an interactive tool, such as Dash or Bokeh.\n",
       "\n",
       "3. The next two lines:\n",
       "   - `for chunk in stream:`: This loop iterates over each input chunk (`chunk`) from the data stream.\n",
       "   - `response += chunk.choices[0].delta.content or ''`: For each chunk, it adds the content of the first chosen block (in this case, a Markdown block) to the `response` string. If there is no content in any block, it's an empty string (`''`).\n",
       "\n",
       "4. The following line:\n",
       "   - `response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")`: This removes a specific pattern from the `response`. '```' is likely used for code blocks in Markdown and 'markdown' might be part of another pattern. However, without seeing the full code or more context, I'm unsure what these patterns are.\n",
       "\n",
       "5. The final line:\n",
       "   - `update_display(Markdown(response), display_id=display_handle.display_id)`: This calls a function called `update_display` that presumably updates the HTML display with the new Markdown content from the string in `response`. The `display_id` parameter passes the ID of the display element mentioned earlier.\n",
       "\n",
       "In essence, this code streamlines logging by printing meaningful text for each log entry to an interactive log display, rather than printing raw data. However, without more context about why these logging patterns are chosen and how they're intended to be used, some assumptions might need to be made."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_llama(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
