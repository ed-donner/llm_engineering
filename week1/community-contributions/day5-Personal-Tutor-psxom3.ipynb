{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaac8eb-b65c-42c5-abf6-efd9152d5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbba5c3-d7aa-45b4-9fc3-0ecc300a6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da61c277-e7df-48ed-9acc-a44af7539eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64764ad-1ec2-4788-98a1-69b7275fef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce33cd8c-f2a8-411b-a478-0788b7e2af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "\n",
    "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76605197-5df4-4893-b245-10256cb377a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e3577d-f73b-43eb-b3b3-5d089358fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided code snippet is a Python expression that uses the `yield from` statement along with a set comprehension to generate a sequence of authors from a collection of books. Let’s break down the components of this code to understand what it does and why it may be structured this way.\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   This part of the code is a set comprehension. A set is a collection of unique elements in Python. The expression iterates over a collection named `books`, which we can assume is likely a list or another iterable structure containing dictionaries (since the code uses `book.get(...)`).\n",
       "\n",
       "   - Inside the comprehension, it iterates over each `book` in `books`.\n",
       "   - The expression `book.get(\"author\")` tries to retrieve the value associated with the key \"author\" from each `book` dictionary.\n",
       "   - The condition `if book.get(\"author\")` ensures that only books with a non-None, non-empty author value are included in the resulting set. If a book does not have an author or if the author is falsy (like `None` or an empty string), it is excluded.\n",
       "\n",
       "   The result is a set containing unique author names from the provided `books` collection.\n",
       "\n",
       "2. **Yield from**: \n",
       "   python\n",
       "   yield from ...\n",
       "   \n",
       "   The `yield from` statement is used inside a generator function to yield all values from an iterable (in this case, the set created by the comprehension). When a generator function is called, it returns a generator object without starting execution immediately. The execution begins when the generator is iterated over, and each time a value is yielded, the generator can pause its state.\n",
       "\n",
       "   By using `yield from`, the generator will produce each author value sequentially. If the set comprehension yields a set of authors, `yield from` will iterate over that set and yield each author back to whoever is consuming the generator.\n",
       "\n",
       "### Why Use This Structure?\n",
       "\n",
       "1. **Functionality**: The main purpose of this code is to obtain and yield unique authors from a set of book items. The set comprehension ensures that there are no duplicate authors in the result.\n",
       "\n",
       "2. **Generator Behavior**: Using `yield from` makes the code concise and allows for a lazy evaluation of the authors. This is efficient, especially if the list of books is large, as authors are only generated when requested.\n",
       "\n",
       "3. **Set for Uniqueness**: Using a set directly means duplicate author names are automatically filtered out, which can be useful if `books` potentially contains multiple entries with the same author.\n",
       "\n",
       "### Final Thoughts\n",
       "\n",
       "Assuming this code is encapsulated inside a generator function, when you call this generator, it will produce a sequence of unique authors from the provided `books`. The use of both yield and set comprehension here is effective for cases where uniqueness of elements is desired, and where memory efficiency is a priority due to the potential size of data. \n",
       "\n",
       "Here's how you might see this in context:\n",
       "\n",
       "python\n",
       "def unique_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "In this context, `unique_authors` will yield each unique author found in `books`, and you can use it like so:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 4\"}  # No author\n",
       "]\n",
       "\n",
       "for author in unique_authors(books):\n",
       "    print(author)\n",
       "\n",
       "\n",
       "This would output:\n",
       "\n",
       "Author A\n",
       "Author B\n",
       "\n",
       "\n",
       "With the use of a generator, you can efficiently handle potentially large datasets without needing to store the entire result set in memory at once."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)\n",
    "    \n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92800fc-dcfd-4334-a61b-d53a325c9d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down this line of code.\n",
       "\n",
       "**What is this code doing?**\n",
       "\n",
       "This code is using a combination of Python features to extract a list of authors from a collection of books, while avoiding certain issues with iteration.\n",
       "\n",
       "Here's what's happening:\n",
       "\n",
       "1. `yield from`: This keyword is used in Python 3.3 and later versions. It's short for \"yield from\" or \"pass through,\" which means that it sends each item yielded by the inner generator expression to the outer generator.\n",
       "2. `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression, also known as a \"list comprehension with generators.\" It's equivalent to writing a regular list comprehension but uses generators instead of lists.\n",
       "\n",
       "Here's what this part does:\n",
       "\n",
       "* `for book in books`: Iterates over the `books` collection.\n",
       "* `if book.get(\"author\")`: Filters out any book that doesn't have an \"author\" key. If a book is missing this key, it will be skipped in the iteration.\n",
       "* `book.get(\"author\")`: Retrieves the value associated with the \"author\" key from each book dictionary.\n",
       "\n",
       "So, putting it all together, `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`:\n",
       "\n",
       "1. Iterates over each book in the `books` collection.\n",
       "2. Skips any book without an \"author\" key.\n",
       "3. Yields (i.e., sends) the author's name from each remaining book dictionary.\n",
       "\n",
       "**Why is this code necessary?**\n",
       "\n",
       "In Python, when you use a list comprehension or a for loop with a generator expression, it creates a temporary container (a list or iterator object) that stores the results of the expression. This can lead to memory issues if the collection being iterated over is very large.\n",
       "\n",
       "By using `yield from` and a generator expression, this code avoids creating an intermediate list or iterator object. Instead, it directly yields each author's name one by one, which is more memory-efficient for large collections.\n",
       "\n",
       "This technique is commonly used when working with large datasets, as it helps reduce memory usage and prevents potential out-of-memory errors.\n",
       "\n",
       "**Example Use Case**\n",
       "\n",
       "Suppose you have a collection of book dictionaries, like this:\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\"},  # missing author key\n",
       "    {\"title\": \"Book 4\", \"author\": \"Author D\"}\n",
       "]\n",
       "```\n",
       "Using the given code, you can extract a list of authors like this:\n",
       "```python\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: [\"Author A\", \"Author B\", \"Author D\"]\n",
       "```\n",
       "This will print out the names of all authors, without creating an intermediate list or iterator object."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "reply = response['message']['content']\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2623d9de-34dc-4f97-9726-5e95d26197e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "    user_prompt = f\" Please give a detailed explanation for this question: {question}\"\n",
    "\n",
    "    # messages\n",
    "\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":system_prompt},\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    "\n",
    "    print(\"Fetching details of GPT-4o\")\n",
    "    # Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages,stream=True)\n",
    "        \n",
    "    response_GPT = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response_GPT += chunk.choices[0].delta.content or ''\n",
    "        response_GPT = response_GPT.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response_GPT), display_id=display_handle.display_id)\n",
    "\n",
    "    print(\"Fetching response from Llama 3.2...\")\n",
    "    response_llama = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "    reply_llama = response_llama['message']['content']\n",
    "    display(Markdown(reply_llama))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52b1677-eddc-46a9-bd91-508c2fab2037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question:  what are LLms and its applications\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching details of GPT-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the concept of LLMs, which stands for \"Large Language Models.\"\n",
       "\n",
       "### What are LLMs?\n",
       "\n",
       "Large Language Models are a class of artificial intelligence (AI) systems that use machine learning techniques to understand and generate human-like text based on the data they were trained on. These models are built primarily using neural networks, particularly a type called transformers, which have become the state-of-the-art architecture for various natural language processing (NLP) tasks.\n",
       "\n",
       "#### Key Features of LLMs:\n",
       "\n",
       "1. **Scale**: LLMs are named for their size—the \"large\" in LLM refers to the number of parameters (weights) in the model, which can range from millions to hundreds of billions. Training these models requires vast amounts of text data and significant computational resources.\n",
       "\n",
       "2. **Contextual Understanding**: LLMs can capture contextual relationships between words and phrases, enabling them to understand nuances in language better than smaller models.\n",
       "\n",
       "3. **Pre-training and Fine-tuning**:\n",
       "   - **Pre-training**: LLMs are initially trained on a large corpus of text in an unsupervised manner, allowing them to learn grammar, facts, and the structure of the language without specific task instructions.\n",
       "   - **Fine-tuning**: After pre-training, these models can be fine-tuned on specific datasets to optimize their performance for particular applications.\n",
       "\n",
       "4. **Generative Capabilities**: Many LLMs can generate coherent and contextually relevant text, making them useful for a wide array of applications beyond mere text classification or sentiment analysis.\n",
       "\n",
       "### Applications of LLMs:\n",
       "\n",
       "LLMs have a broad range of applications across various domains. Here are some of the most notable ones:\n",
       "\n",
       "1. **Chatbots and Virtual Assistants**: LLMs can power conversational agents that assist users in customer service, FAQs, and more, providing human-like responses.\n",
       "\n",
       "2. **Content Creation**: They can generate articles, blogs, stories, and even poetry. This functionality can be utilized in marketing, social media, and entertainment industries.\n",
       "\n",
       "3. **Translation Services**: LLMs can help in translating text between languages, providing not just word-for-word translations but contextually relevant interpretations.\n",
       "\n",
       "4. **Text Summarization**: They can condense longer text documents into summaries, extracting key points while maintaining coherence.\n",
       "\n",
       "5. **Sentiment Analysis**: Businesses use LLMs for analyzing customer feedback, reviews, or social media sentiment to drive decision-making.\n",
       "\n",
       "6. **Code Generation**: Some LLMs are trained on programming languages and can assist in code completions or generate functional code snippets, aiding software developers.\n",
       "\n",
       "7. **Education**: LLMs can assist in generating educational material, answering student queries, grading essays, and providing personalized learning experiences.\n",
       "\n",
       "8. **Healthcare**: They can process medical literature, assist in diagnosing by analyzing patient records, and help healthcare providers communicate more effectively with patients.\n",
       "\n",
       "9. **Legal Services**: LLMs can assist lawyers by summarizing cases, drafting documents, and conducting legal research by analyzing vast amounts of legal texts.\n",
       "\n",
       "10. **Creative Writing**: In art and entertainment, LLMs can collaborate with writers and artists to brainstorm ideas, generate scripts, or create interactive storytelling experiences.\n",
       "\n",
       "### Challenges and Considerations:\n",
       "\n",
       "While LLMs have significant potential, they also pose challenges:\n",
       "\n",
       "- **Bias**: LLMs can inadvertently perpetuate biases present in their training data, leading to unfair or prejudiced outcomes.\n",
       "  \n",
       "- **Misinformation**: They might generate text that sounds plausible but is factually incorrect, necessitating rigorous fact-checking.\n",
       "\n",
       "- **Ethical Concerns**: The use of LLMs can raise questions about authorship, content ownership, and the implications of AI in society.\n",
       "\n",
       "- **Resource Intensive**: Training and running LLMs require substantial computational resources, raising environmental and accessibility concerns.\n",
       "\n",
       "In summary, LLMs represent a transformative advancement in AI and natural language processing, offering a range of applications that can significantly enhance human capabilities across various fields. As the technology continues to evolve, addressing its challenges will be crucial for its responsible and effective use."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching response from Llama 3.2...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**What is an LLM (Large Language Model)?**\n",
       "\n",
       "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is designed to process and understand human language. These models are trained on vast amounts of text data, which enables them to learn patterns, relationships, and nuances of language.\n",
       "\n",
       "LLMs are typically based on deep learning architectures, such as transformer models, which use self-attention mechanisms to weigh the importance of different words or tokens in a sentence. This allows LLMs to capture contextual information and generate human-like language outputs.\n",
       "\n",
       "**How do LLMs work?**\n",
       "\n",
       "The training process for an LLM involves the following steps:\n",
       "\n",
       "1. **Data collection**: A massive corpus of text data is collected, which can come from various sources such as books, articles, conversations, or even social media posts.\n",
       "2. **Preprocessing**: The text data is preprocessed to remove irrelevant information, such as punctuation, special characters, and stop words (common words like \"the\", \"and\", etc.).\n",
       "3. **Model training**: The LLM is trained on the preprocessed data using a supervised or unsupervised learning approach. The goal is to predict the next word in a sequence, given the context of the previous words.\n",
       "4. **Model evaluation**: The trained model is evaluated on a validation set to assess its performance and identify areas for improvement.\n",
       "\n",
       "**Applications of LLMs**\n",
       "\n",
       "LLMs have numerous applications across various industries:\n",
       "\n",
       "1. **Language Translation**: LLMs can be used for machine translation, where they can translate text from one language to another with high accuracy.\n",
       "2. **Text Summarization**: LLMs can summarize long documents or articles into concise summaries, highlighting the main points and key information.\n",
       "3. **Question Answering**: LLMs can answer questions based on the input text, providing relevant and accurate responses.\n",
       "4. **Sentiment Analysis**: LLMs can analyze text to determine its sentiment, emotional tone, or polarity (positive/negative).\n",
       "5. **Chatbots and Virtual Assistants**: LLMs can power chatbots and virtual assistants, enabling them to understand user queries and respond accordingly.\n",
       "6. **Content Generation**: LLMs can generate human-like content, such as news articles, product descriptions, or even entire books.\n",
       "7. **Natural Language Processing (NLP)**: LLMs are used in various NLP tasks, including text classification, entity recognition, and language modeling.\n",
       "8. **Speech Recognition**: LLMs can improve speech recognition systems by better understanding the context and nuances of human speech.\n",
       "\n",
       "**Real-world examples of LLMs**\n",
       "\n",
       "Some notable examples of LLMs include:\n",
       "\n",
       "1. **Google's BERT**: A pre-trained language model that has achieved state-of-the-art results in various NLP tasks.\n",
       "2. **Facebook's M** : A language model used for chatbots and virtual assistants on Facebook platforms.\n",
       "3. **Microsoft's Turing-NLG**: A natural language generation (NLG) model developed by Microsoft to generate human-like content.\n",
       "\n",
       "In conclusion, LLMs are powerful AI models that have revolutionized the field of NLP. Their applications are vast and diverse, ranging from simple text processing tasks to complex content generation and language translation.\n",
       "\n",
       "Do you have any specific questions about LLMs or their applications?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "your_question = input(\"Please enter your question: \")\n",
    "input_question(your_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39ac93-5536-422e-a21b-9eb89eb816cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
