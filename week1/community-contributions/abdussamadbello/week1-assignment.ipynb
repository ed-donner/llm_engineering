{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2edb6a7",
      "metadata": {},
      "source": [
        "# Week 1 Exercise â€“ LLM Engineering Tutor (abdussamadbello)\n",
        "\n",
        "This notebook implements a small technical tutor focused on our niche: helping developers design, debug, and deploy LLM-powered applications (prompt design, tools/agents, retrieval, and evaluation). It takes a technical LLM-engineering question and returns clear explanations from both a frontier model (OpenAI) and a local model (Ollama)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "MODEL_GPT = \"gpt-4o-mini\"\n",
        "MODEL_LLAMA = \"llama3.2\"  # adjust to your local Ollama tag if needed\n",
        "\n",
        "load_dotenv()\n",
        "openai = OpenAI()\n",
        "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a helpful technical tutor who answers questions about Python code, \"\n",
        "    \"software engineering, data science, and LLMs. Give clear, detailed explanations.\"\n",
        ")\n",
        "\n",
        "\n",
        "def messages_for_question(question: str):\n",
        "    \"\"\"Build chat messages for a technical question.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "\n",
        "def explain_with_gpt(question: str) -> str:\n",
        "    \"\"\"Ask the frontier model (gpt-4o-mini) and return the explanation.\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL_GPT,\n",
        "        messages=messages_for_question(question),\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def explain_with_llama(question: str) -> str:\n",
        "    \"\"\"Ask the local Llama 3.2 model via Ollama and return the explanation.\"\"\"\n",
        "    response = ollama.chat.completions.create(\n",
        "        model=MODEL_LLAMA,\n",
        "        messages=messages_for_question(question),\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e061c36",
      "metadata": {},
      "outputs": [],
      "source": [
        "example_question = \"\"\"\\\n",
        "I'm building a retrieval-augmented generation (RAG) system over internal company documents.\n",
        "How should I design my system and user prompts so the model always cites its sources\n",
        "and avoids hallucinating information that is not present in the retrieved chunks?\n",
        "\"\"\"\n",
        "\n",
        "gpt_answer = explain_with_gpt(example_question)\n",
        "llama_answer = explain_with_llama(example_question)\n",
        "\n",
        "print(\"--- GPT (gpt-4o-mini) answer ---\\n\")\n",
        "print(gpt_answer)\n",
        "print(\"\\n--- Llama (local) answer ---\\n\")\n",
        "print(llama_answer)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
