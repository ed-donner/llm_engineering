{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frank Asket's Week 1 Exercise\n",
    "\n",
    "[Frank Asket](https://github.com/frank-asket) — *Founder & CTO building Human-Centered AI infrastructure.*\n",
    "\n",
    "To demonstrate familiarity with the OpenAI API and Ollama, this notebook is a **technical Q&A tool**: you ask a question and get an explanation (GPT with streaming, then optionally Llama). A tool you can use throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "# Part 2 (website summarizer) needs scraper. Run from repo root or from week1/community-contributions/asket:\n",
    "for _path in ('week1', os.path.join(os.getcwd(), '..', '..', 'week1'), os.path.join(os.getcwd(), '..', '..')):\n",
    "    _p = os.path.abspath(_path)\n",
    "    if _p not in sys.path and os.path.isdir(_p):\n",
    "        sys.path.insert(0, _p)\n",
    "        break\n",
    "from scraper import fetch_website_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "MODEL_LLAMA = \"llama3.2:3b-instruct-q4_0\"  # or llama3.2:1b-instruct-q4_0; run 'ollama list' to see installed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openrouter_api_key:\n",
    "    openai = OpenAI(api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "elif openai_api_key:\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "else:\n",
    "    openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\\\"author\\\") for book in books if book.get(\\\"author\\\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "\n",
    "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs.\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The line of code you provided utilizes a combination of Python's generator functions and set comprehensions. Let's break it down step by step:\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   - This portion of the code is a set comprehension that creates a set of authors from a collection of `books`.\n",
       "   - **Iteration**: It iterates over each `book` in the iterable `books`.\n",
       "   - **Getting Author**: For each `book`, it attempts to retrieve the \"author\" using `book.get(\"author\")`. The `get` method of a dictionary returns the value associated with the specified key, in this case, \"author\". If the key doesn't exist, it returns `None`.\n",
       "   - **Conditional Filtering**: The expression includes a condition `if book.get(\"author\")`, meaning that it only includes the author's name in the set if it's not `None` or an empty string (falsy values).\n",
       "   - **Set Creation**: The resulting set will only contain unique authors, as sets do not allow duplicate values.\n",
       "\n",
       "2. **Yield from**:\n",
       "   python\n",
       "   yield from ...\n",
       "   \n",
       "   - The `yield from` statement is used to yield all values from an iterable (in this case, the set created from the comprehension) one by one.\n",
       "   - This means that the code will act like a generator, production one author at a time when iterated over.\n",
       "   - When you call the generator function that contains this line, it will yield each unique author found in `books`.\n",
       "\n",
       "### Purpose of the Code\n",
       "\n",
       "- **Unique Author Extraction**: The primary purpose of this line is to extract and yield unique authors from a list (or any iterable) of books. It allows consumers of this generator to retrieve authors lazily, meaning that the authors are generated on-the-fly and you don’t need to build the entire list of authors in memory at once.\n",
       "- **Efficiency**: Using `yield from` in combination with a set comprehension is efficient in terms of both time complexity (faster uniqueness management) and space complexity (not storing intermediate lists/updating for each author).\n",
       "\n",
       "### Use Cases\n",
       "\n",
       "- **Data Processing**: This code could be used in contexts where you want to collect authors for analysis, reports, or transformations, such as creating a summary of authors in a data processing pipeline.\n",
       "- **Memory Efficiency**: It is particularly beneficial when dealing with large datasets where storing all authors’ names at once may not be feasible, but processing them one at a time is manageable.\n",
       "\n",
       "### Example\n",
       "\n",
       "Here’s a simple example of how this could be used:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book A\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book B\", \"author\": \"Author 2\"},\n",
       "    {\"title\": \"Book C\"},  # No author\n",
       "    {\"title\": \"Book D\", \"author\": \"Author 1\"},  # Duplicate author\n",
       "]\n",
       "\n",
       "def unique_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Example usage\n",
       "for author in unique_authors(books):\n",
       "    print(author)\n",
       "\n",
       "# Output:\n",
       "# Author 1\n",
       "# Author 2\n",
       "\n",
       "\n",
       "In this example, the `unique_authors` generator function will produce only the unique authors of the books, omitting any duplicate entries and those books without an author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def strip_code_fence(text):\n",
    "    \"\"\"Remove only code-fence wrappers (e.g. ```markdown / ```) so prose containing 'markdown' is unchanged.\"\"\"\n",
    "    s = text\n",
    "    if s.startswith(\"```markdown\"):\n",
    "        i = s.find(\"\\n\")\n",
    "        s = s[i + 1:] if i != -1 else s[11:]\n",
    "    elif s.startswith(\"```\"):\n",
    "        i = s.find(\"\\n\")\n",
    "        s = s[i + 1:] if i != -1 else s[3:]\n",
    "    if s.rstrip().endswith(\"```\"):\n",
    "        s = s[:s.rstrip().rfind(\"```\")].rstrip()\n",
    "    return s\n",
    "\n",
    "stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "    response_clean = strip_code_fence(response)\n",
    "    update_display(Markdown(response_clean), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet is using the built-in `yield from` keyword in Python, which allows you to yield values from an iterable (such as a list or tuple) and returns a generator expression instead of creating multiple temporary variables.\n",
       "\n",
       "Here's what the code does:\n",
       "\n",
       "- It takes a list of books (`books`) and each book object (`book`).\n",
       "- For each book in `books`, it checks if the author is present in any of the books' author lists using another method (not shown in this snippet). This is done for each book in the `books` list.\n",
       "- If an author is found in a book's author list, that value is yielded from the generator expression (`yield from`). The values are directly returned by the function without creating temporary variables.\n",
       "\n",
       "This code essentially does the following:\n",
       "\n",
       "1. It fetches book metadata (author) for each book in the `books` list.\n",
       "2. For each book, it checks if an author is present in any of its own author lists. This means it might also return values from books that don't have a specific author listed.\n",
       "3. If an author is found, it yields the value directly.\n",
       "\n",
       "The purpose of this code could be to process or filter the data in some way without storing all values in memory at once. Here are a few possible scenarios:\n",
       "\n",
       "- Using the generated list of authors for analysis or further processing, such as filtering books based on specific authors.\n",
       "- Storing only unique authors and not duplicate values from different books.\n",
       "\n",
       "In general, using `yield from` here would allow you to process data without loading it all into memory at once. For example, you could write a function that processes each book's metadata (author, title, etc.) without having to load the entire dataset in memory.\n",
       "\n",
       "Here is an example with some added comments for clarity:\n",
       "\n",
       "```python\n",
       "def get_book_info():\n",
       "    # Fetch book metadata (author)\n",
       "    books = your_list_of_books  # Replace with your actual data structure\n",
       "    # Iterate over each book and check if its author list contains another book's author\n",
       "    for book in books:\n",
       "        author_in_other_book = False\n",
       "        \n",
       "        # Check if other book's author is present in current book's metadata\n",
       "        for existing_book in books:\n",
       "            if book.get(\"author\") == existing_book.get(\"author\"):\n",
       "                print(f\"Found a common author: {book['title']} by {existing_book['author']}\")\n",
       "                yield from [existing_book[\"author\"]]  # Yield the found author\n",
       "        \n",
       "        # If an author is found, return it directly\n",
       "        if book.get(\"author\"):\n",
       "            yield from [book.get(\"author\")]\n",
       "\n",
       "# Example usage:\n",
       "get_book_info()\n",
       "```\n",
       "\n",
       "In this example, `get_book_info()` function fetches metadata for a list of books and uses `yield from` to iterate over each book's author list. When an author is found in another book's author list, it yields the value directly; otherwise, it returns the author.\n",
       "\n",
       "Note: This snippet assumes you're using Python 3.7+ due to the use of `yield from` which was introduced in that version. If you're using an older version of Python or have issues with this syntax, consider upgrading your Python environment if necessary."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Optional] Get Llama 3.2 to answer (requires Ollama running locally)\n",
    "# If you see 'llama runner process has terminated: exit status 2' or Metal/MLX errors, upgrade Ollama from https://ollama.com/download (your 0.6.5 is old). The GPT part above is enough for the exercise.\n",
    "\n",
    "for model_tag in (\"llama3.2:1b-instruct-q4_0\", \"llama3.2:3b-instruct-q4_0\"):\n",
    "    try:\n",
    "        response = ollama.chat(model=model_tag, messages=messages)\n",
    "        reply = response[\"message\"][\"content\"]\n",
    "        display(Markdown(reply))\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if \"llama3.2:3b\" in model_tag:\n",
    "            print(\"Ollama failed for both models:\", e)\n",
    "            print(\"Fix: Install the latest Ollama from https://ollama.com/download (old versions can crash on macOS). You can skip this cell; the GPT answer above is enough for the exercise.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example: the kind of explanation this tool produces (Frank Asket)\n",
    "\n",
    "For the question *\"Explain: yield from {book.get(\\\"author\\\") for book in books if book.get(\\\"author\\\")}\"*, here's a breakdown:\n",
    "\n",
    "1. **`book.get(\\\"author\\\")`** — Retrieves the author for each book in `books` (assumed to be a list of dicts). Returns `None` if the key is missing.\n",
    "\n",
    "2. **`{ ... for book in books if book.get(\\\"author\\\") }`** — A **set comprehension** (not a generator): builds a set of unique author names, skipping books with no author.\n",
    "\n",
    "3. **`yield from`** — Delegates to that iterable and yields each item one by one. So the surrounding function is a **generator** that yields each unique author.\n",
    "\n",
    "**In short:** The line is a generator that yields every unique author name from a list of book dicts, ignoring missing authors. (Note: the expression uses a set `{ }`, so it's evaluated fully before `yield from`; a memory-lighter variant would use a generator expression in parentheses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Website summarizer (Ollama, bilingual: English + Guéré)\n",
    "\n",
    "Uses **Ollama only** (no API key). Fetches a URL (e.g. [Frank Asket's GitHub](https://github.com/frank-asket)) and produces a **description/summary in English** plus a version in **Guéré** (Guere), an Ivorian local language (bullet points, separated by `<hr>`). Run from repo root; ensure Ollama is installed and run `ollama serve` (and `ollama pull llama3.2` if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Ollama model (run once)\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ollama is running\n",
    "try:\n",
    "    requests.get(\"http://localhost:11434\", timeout=2)\n",
    "    print(\"Ollama is running.\")\n",
    "except Exception:\n",
    "    print(\"Ollama is not running. In a terminal run: ollama serve\")\n",
    "    print(\"Then: ollama pull llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama client (OpenAI-compatible API, local)\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**English Summary**\n",
       "\n",
       "Meet Franck Olivier Alex Asket, a talented developer with expertise in AI code creation. His GitHub profile showcases his contributions to various projects, including:\n",
       "\n",
       "* AI CODE CREATION: He's proficient in tools like GitHub Copilot, GitHub Spark, and GitHub Models.\n",
       "* DEVELOPER WORKFLOWS: He automates workflows using Actions and Codespaces.\n",
       "* APPLICATION SECURITY: He prioritizes security with features like GitHub Advanced Security and Secret protection.\n",
       "\n",
       "His interests lie at the intersection of software development, DevOps, and AI. With a background in healthcare, financial services, and manufacturing industries, Franck brings valuable experience to his coding endeavors.\n",
       "\n",
       "---\n",
       "\n",
       "**Summarize avec Guéré (English version will be provided alongside)**\n",
       "\n",
       "Asepé frank-asket: frannk Olivier Alex Askét ( Côte d'Ivoire )\n",
       "\n",
       "Bilangan èdey asezika:\n",
       "\n",
       "• Ayi nanò akonnan asekan nan nana na akokon\n",
       "• Akonnan à yon kounyè nan AI \n",
       "    nan akòn nou, copilot, spark an nan modeèles nan akèzè\n",
       "\n",
       "Bilangan projeks asepé frank-asket:\n",
       "\n",
       "* Développ' sa rèyon akonnan nan akòt nan projekts an nan\n",
       "• Akonnan nan akèske nan tseksè nan koutan \n",
       "    akwa akòn nan akòn nan n'anana nan oun\n",
       "\n",
       "Asepé nan kewòlan asepé frank-asket:\n",
       "\n",
       "* Akonnan  akòtin nou akèmnon nan koutou nan akòn\n",
       "* Nou akòn nan akèske nan ansa nan sekanan \n",
       "   nan oublò akwa nan akòn nan nan nana\n",
       "\n",
       "**hr>**\n",
       "\n",
       "---\n",
       "\n",
       "English summary remains the same"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure scraper is importable (run from repo root or week1/community-contributions/asket)\n",
    "import os, sys\n",
    "for _path in ('week1', os.path.join(os.getcwd(), '..', '..', 'week1'), os.path.join(os.getcwd(), '..', '..')):\n",
    "    _p = os.path.abspath(_path)\n",
    "    if _p not in sys.path and os.path.isdir(_p):\n",
    "        sys.path.insert(0, _p)\n",
    "        break\n",
    "from scraper import fetch_website_contents\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Ollama client (so this cell can run standalone)\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "# Prompts and summarizer (bilingual: English description + Guéré bullets, separated by <hr>)\n",
    "SUMMARY_SYSTEM = \"\"\"Your job is to analyze the content of a webpage and give a clear description or summary of the person or topic.\n",
    "Use bullet points and keep it clear. Do not wrap the output in a code block.\"\"\"\n",
    "SUMMARY_USER = \"\"\"Here are the contents of a webpage (e.g. a GitHub or profile page).\n",
    "Extract and summarize the key details or description about the person (e.g. name, role, bio, projects). Provide an English version as a short blog-style summary with an h1 title. Then provide a second version in Guéré (Guere), an Ivorian local language from Côte d'Ivoire, with bullet points.\n",
    "Separate the English and Guéré sections with an <hr> tag.\"\"\"\n",
    "\n",
    "def messages_for_site(website_text):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SUMMARY_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": SUMMARY_USER + \"\\n\\n\" + website_text}\n",
    "    ]\n",
    "\n",
    "def summarize_site(url):\n",
    "    web = fetch_website_contents(url)\n",
    "    # Use same tag as Part 1; run 'ollama list' to see your model name (e.g. llama3.2:3b-instruct-q4_0)\n",
    "    r = ollama_client.chat.completions.create(model=\"llama3.2:3b-instruct-q4_0\", messages=messages_for_site(web))\n",
    "    return r.choices[0].message.content\n",
    "\n",
    "def display_site_summary(url):\n",
    "    display(Markdown(summarize_site(url)))\n",
    "\n",
    "display_site_summary(\"https://github.com/frank-asket\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
