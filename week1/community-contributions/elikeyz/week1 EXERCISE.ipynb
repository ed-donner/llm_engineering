{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-5'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers technical questions including code explanations. You should explain the concepts clearly with practical examples, in a way that any layman would understand.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain how RAG AI systems work, with a simple example in Python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here’s the big idea in plain language:\n",
       "- A RAG system first looks up facts, then writes the answer. It doesn’t rely only on what the model “remembers.”\n",
       "- Steps:\n",
       "  1) Split your documents into small chunks and store them in a searchable index (often a vector database).\n",
       "  2) For a user question, retrieve the most relevant chunks.\n",
       "  3) Feed those chunks (the context) plus the question into a language model to generate a grounded answer.\n",
       "- Why it’s useful: answers can cite your data, stay up-to-date, and reduce hallucinations.\n",
       "\n",
       "Simple, self-contained Python example (no external packages). This shows the R and the “A” wiring. It uses a tiny TF‑IDF-like retriever for clarity and a toy generator; you can later swap in a real LLM.\n",
       "\n",
       "```python\n",
       "import re\n",
       "import math\n",
       "from collections import Counter, defaultdict\n",
       "\n",
       "# -----------------------------\n",
       "# 1) Toy knowledge base\n",
       "# -----------------------------\n",
       "docs = [\n",
       "    {\n",
       "        \"id\": \"company-overview\",\n",
       "        \"text\": (\n",
       "            \"Acme Solar builds portable solar chargers and home energy kits. \"\n",
       "            \"Our mission is to make clean energy easy to use. \"\n",
       "            \"Customer support is available 9am-6pm PST, Monday to Friday.\"\n",
       "        ),\n",
       "    },\n",
       "    {\n",
       "        \"id\": \"product-solar-charger\",\n",
       "        \"text\": (\n",
       "            \"The Acme Pocket Charger is a 20W portable solar panel with USB-C output. \"\n",
       "            \"It charges phones and small devices in direct sunlight. \"\n",
       "            \"For best results, angle the panel toward the sun and avoid shade.\"\n",
       "        ),\n",
       "    },\n",
       "    {\n",
       "        \"id\": \"refund-policy\",\n",
       "        \"text\": (\n",
       "            \"Refunds: We offer refunds within 30 days of delivery for damaged or defective products. \"\n",
       "            \"Contact support with your order number and photos of the issue. \"\n",
       "            \"Accessories and cables are covered under the same policy.\"\n",
       "        ),\n",
       "    },\n",
       "    {\n",
       "        \"id\": \"installation-guide\",\n",
       "        \"text\": (\n",
       "            \"To install a home energy kit, mount the panel securely and connect the charge controller. \"\n",
       "            \"Follow the wiring diagram and use weatherproof connectors. \"\n",
       "            \"If unsure, consult a licensed electrician.\"\n",
       "        ),\n",
       "    },\n",
       "]\n",
       "\n",
       "# -----------------------------\n",
       "# 2) Chunking (simple sentence split)\n",
       "# -----------------------------\n",
       "def split_sentences(text):\n",
       "    # naive split; good enough for demo\n",
       "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
       "    return [p.strip() for p in parts if p.strip()]\n",
       "\n",
       "chunks = []\n",
       "for d in docs:\n",
       "    for i, sent in enumerate(split_sentences(d[\"text\"])):\n",
       "        chunks.append({\n",
       "            \"chunk_id\": f'{d[\"id\"]}#{i}',\n",
       "            \"doc_id\": d[\"id\"],\n",
       "            \"text\": sent\n",
       "        })\n",
       "\n",
       "# -----------------------------\n",
       "# 3) Tiny TF-IDF-like retriever\n",
       "# -----------------------------\n",
       "def tokenize(text):\n",
       "    return re.findall(r\"[a-z0-9']+\", text.lower())\n",
       "\n",
       "# Build vocabulary and IDF from chunks\n",
       "vocab = {}                # token -> index\n",
       "df = defaultdict(int)     # document frequency per token\n",
       "N = len(chunks)\n",
       "\n",
       "for ch in chunks:\n",
       "    tokens = set(tokenize(ch[\"text\"]))\n",
       "    for tok in tokens:\n",
       "        if tok not in vocab:\n",
       "            vocab[tok] = len(vocab)\n",
       "        df[tok] += 1\n",
       "\n",
       "# IDF with smoothing\n",
       "idf = [0.0] * len(vocab)\n",
       "for tok, idx in vocab.items():\n",
       "    idf[idx] = math.log((1 + N) / (1 + df[tok])) + 1.0\n",
       "\n",
       "def tfidf_vector(text):\n",
       "    tokens = tokenize(text)\n",
       "    counts = Counter(tokens)\n",
       "    vec = [0.0] * len(vocab)\n",
       "    # term frequency (raw count) * idf\n",
       "    for tok, c in counts.items():\n",
       "        if tok in vocab:\n",
       "            vec[vocab[tok]] = c * idf[vocab[tok]]\n",
       "    # L2 normalize\n",
       "    norm = math.sqrt(sum(x*x for x in vec)) or 1.0\n",
       "    return [x / norm for x in vec]\n",
       "\n",
       "# Precompute embeddings for chunks\n",
       "chunk_vectors = [tfidf_vector(ch[\"text\"]) for ch in chunks]\n",
       "\n",
       "def cosine(a, b):\n",
       "    return sum(x*y for x, y in zip(a, b))\n",
       "\n",
       "def retrieve(query, k=3):\n",
       "    qv = tfidf_vector(query)\n",
       "    sims = [(cosine(qv, cv), i) for i, cv in enumerate(chunk_vectors)]\n",
       "    sims.sort(reverse=True)\n",
       "    top = [chunks[i] for _, i in sims[:k]]\n",
       "    return top\n",
       "\n",
       "# -----------------------------\n",
       "# 4) Prompt assembly and a toy \"generator\"\n",
       "# -----------------------------\n",
       "def build_context(top_chunks):\n",
       "    lines = []\n",
       "    for ch in top_chunks:\n",
       "        lines.append(f\"[{ch['doc_id']}] {ch['text']}\")\n",
       "    return \"\\n\".join(lines)\n",
       "\n",
       "def fake_llm_generate(question, context):\n",
       "    # This simulates what you'd ask a real LLM.\n",
       "    # For the demo, we extract an answer from the most relevant chunk(s).\n",
       "    # In production, you would call an actual LLM with the prompt below.\n",
       "    if \"refund\" in question.lower() or \"return\" in question.lower():\n",
       "        # try to find a number of days in the context\n",
       "        m = re.search(r'(\\d+)\\s*days', context.lower())\n",
       "        days = m.group(1) if m else None\n",
       "        base = \"We offer refunds for damaged or defective products\"\n",
       "        if days:\n",
       "            base += f\" within {days} days of delivery\"\n",
       "        return base + \". Please contact support with your order number and photos.\"\n",
       "    # Fallback: provide a concise sentence drawn from the top context\n",
       "    first_line = context.splitlines()[0] if context else \"\"\n",
       "    return \"Based on our docs: \" + re.sub(r'^\\[[^\\]]+\\]\\s*', '', first_line)\n",
       "\n",
       "def rag_answer(question, k=3):\n",
       "    top = retrieve(question, k=k)\n",
       "    context = build_context(top)\n",
       "    prompt = (\n",
       "        \"You are a helpful assistant. Use ONLY the information in Context to answer.\\n\"\n",
       "        f\"Question: {question}\\n\\n\"\n",
       "        f\"Context:\\n{context}\\n\\n\"\n",
       "        \"If the answer is not in the context, say you don't know.\"\n",
       "    )\n",
       "    answer = fake_llm_generate(question, context)\n",
       "    return answer, top, prompt\n",
       "\n",
       "# -----------------------------\n",
       "# 5) Try it\n",
       "# -----------------------------\n",
       "if __name__ == \"__main__\":\n",
       "    user_question = \"Do you offer refunds on broken solar chargers? How long do I have?\"\n",
       "    answer, sources, prompt = rag_answer(user_question, k=3)\n",
       "\n",
       "    print(\"Answer:\")\n",
       "    print(answer)\n",
       "    print(\"\\nSources used:\")\n",
       "    for s in sources:\n",
       "        print(f\"- {s['doc_id']} -> {s['text']}\")\n",
       "    print(\"\\n(Example prompt you would send to a real LLM):\")\n",
       "    print(prompt)\n",
       "```\n",
       "\n",
       "What this code shows\n",
       "- Retrieval: retrieve(question) finds the top-k relevant chunks by cosine similarity over a simple TF-IDF embedding.\n",
       "- Augmentation: build_context(...) stitches those chunks into a context block with lightweight citations.\n",
       "- Generation: fake_llm_generate(...) is a placeholder that mimics how an LLM would answer using only the provided context.\n",
       "- In practice, you replace fake_llm_generate with a real model call.\n",
       "\n",
       "How to swap in a real LLM (optional)\n",
       "- Replace fake_llm_generate with a function that calls your model. For example, with an API:\n",
       "  - Prepare the same prompt string that includes the Context and Question.\n",
       "  - Send it to your LLM of choice (OpenAI, local model via transformers, etc.).\n",
       "  - Return the model’s text output as the answer.\n",
       "\n",
       "Tips to make this production-ready\n",
       "- Use good embeddings (e.g., sentence-transformers) and a vector index (e.g., FAISS) instead of the toy TF-IDF.\n",
       "- Chunk documents with sensible sizes and overlaps (e.g., ~200–500 tokens, 10–50 token overlap).\n",
       "- Store metadata (source, URL, title) and return citations with the answer.\n",
       "- Add a re-ranking step to improve retrieval quality before generation.\n",
       "- Instruct the LLM to cite sources and to say “I don’t know” if the context is insufficient."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def chat(question, model):\n",
    "  OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "  openai = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\") if model == MODEL_LLAMA else OpenAI()\n",
    "\n",
    "  stream = openai.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True\n",
    "  )\n",
    "\n",
    "  return stream\n",
    "\n",
    "def display_stream(stream):\n",
    "  response=\"\"\n",
    "  display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "  for chunk in stream:\n",
    "      response += chunk.choices[0].delta.content or \"\"\n",
    "      update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "stream = chat(question, MODEL_GPT)\n",
    "display_stream(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is RAG (Relational Attention Graph) AI?**\n",
       "\n",
       "RAG is an artificial intelligence framework developed by Facebook's AI Research Lab. It's primarily used for natural language processing (NLP) tasks like language translation, question answering, and text classification.\n",
       "\n",
       "The core idea behind RAG is to replace traditional recurrent neural networks (RNNs) with graph-structured attention mechanisms. This allows the model to consider both sequential dependencies and global contextual relationships in a sentence or document.\n",
       "\n",
       "**How does RAG work?**\n",
       "\n",
       "In essence, RAG works by:\n",
       "\n",
       "1. Representing input text as a graph, where each node represents an element of the input sequence (e.g., word tokens).\n",
       "2. Assigning attention weights to each node, indicating its importance.\n",
       "3. Computing the weighted sum of all nodes' representations to generate the contextualized embeddings.\n",
       "4. Using these embeddings as inputs for downstream tasks.\n",
       "\n",
       "**A simple RAG example in Python**\n",
       "\n",
       "To demonstrate this concept, we can create a simplified RAG model using the Keras library and attention mechanisms:\n",
       "```python\n",
       "import numpy as np\n",
       "from keras.layers import Input, Dense, Embedding, Flatten, Attention\n",
       "from keras.models import Model\n",
       "\n",
       "# Sample input text (a sentence)\n",
       "sentences = [\"This is a sample query.\"]\n",
       "\n",
       "# Convert sentences to numerical labels\n",
       "tokenizer = Tokenizer()\n",
       "tokenizer.fit_on_texts(sentences)\n",
       "sequences = tokenizer.texts_to_sequences(sentences)\n",
       "\n",
       "# Create attention-based RAG model\n",
       "attention_input = Input(shape=(100,), name=\"Input\")\n",
       "x = Embedding(input_dim=100, output_dim=128)(attention_input)\n",
       "x = Flatten()(x)  # Flatten embedding\n",
       "\n",
       "# Compute attention layer\n",
       "attention_weights = Attention().apply(x)\n",
       "att_weights = Dense(1, kernel_initializer='zero')(attention_weights)\n",
       "\n",
       "# weighted sum of node representations\n",
       "node_representations = att_weights * x\n",
       "\n",
       "# Generate contextualized embeddings\n",
       "context_embedding = Dense(128, activation='relu')(node_representations)\n",
       "\n",
       "# Create RAG model with input and output layers\n",
       "model = Model(inputs=attention_input, outputs=context_embedding)\n",
       "```\n",
       "In this example:\n",
       "\n",
       "*   We create an input layer representing the tokens obtained from a sentence.\n",
       "*   The embedding layer maps these tokens to dense vectors of size 128.\n",
       "*   The attention layer computes weighted sums for each token, giving us attention scores att_weights and node representations x.\n",
       "*   Finally, we compute contextualized embeddings by integrating these node representations using the generated weights.\n",
       "\n",
       "Note: This simplified example is not optimized for accuracy or practicality and serves mostly to illustrate the fundamental concept of RAG.\n",
       "\n",
       "To go a step further in implementation of this model you may want to check the **RAG** documentation on this [GitHub website](https://github.com/fact-checking/rags).\n",
       "\n",
       "Hope that helps!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "stream = chat(question, MODEL_LLAMA)\n",
    "display_stream(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
