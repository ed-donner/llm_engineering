{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-5-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers technical questions including code explanations. You should explain the concepts clearly with practical examples, in a way that any layman would understand.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain how RAG AI systems work, with a simple example in Python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer\n",
       "- RAG (Retrieval-Augmented Generation) combines a retriever (searches a document collection) with a generator (a large language model) so the model answers using retrieved documents as context. That typically makes answers more factual and up-to-date than a closed-book LLM alone.\n",
       "\n",
       "How it works (plain language)\n",
       "1. Index documents: break your knowledge source (manuals, webpages, emails) into chunks and embed each chunk into a vector space.\n",
       "2. Retrieve: when a user asks a question, embed the question and find the most similar chunks using nearest-neighbor search.\n",
       "3. Form the prompt: assemble those retrieved chunks (plus any metadata) into a prompt or “context” for the LLM.\n",
       "4. Generate: send the prompt+question to a text generation model. The model conditions on the retrieved facts and writes the answer.\n",
       "5. (Optional) Post-process: verify facts, re-rank candidate answers, cite sources, or run a truth-checker.\n",
       "\n",
       "Why it’s useful\n",
       "- Keeps the generator “grounded” in a knowledge base (reduces hallucinations).\n",
       "- Makes answers up-to-date without retraining the model (just update the index).\n",
       "- Allows smaller LMs to produce accurate answers using external knowledge.\n",
       "\n",
       "Simple Python example\n",
       "This example uses:\n",
       "- sentence-transformers to create embeddings,\n",
       "- FAISS for a vector index (dense similarity search),\n",
       "- OpenAI’s chat completions for generation (you can replace the generation call with any LLM).\n",
       "\n",
       "Install prerequisites:\n",
       "- pip install sentence-transformers faiss-cpu openai\n",
       "\n",
       "Code:\n",
       "```\n",
       "# RAG minimal example\n",
       "# Requires: sentence-transformers, faiss-cpu, openai\n",
       "from sentence_transformers import SentenceTransformer\n",
       "import faiss\n",
       "import numpy as np\n",
       "import openai   # replace with your LLM client if needed\n",
       "\n",
       "# ---------- 1) Prepare documents ----------\n",
       "documents = [\n",
       "    \"RAG stands for Retrieval-Augmented Generation. It combines search with a language model.\",\n",
       "    \"FAISS is a library for fast similarity search over vectors.\",\n",
       "    \"Sentence-Transformers are pre-trained models that map text to vectors (embeddings).\",\n",
       "    \"Chunking long docs into smaller pieces helps retrieval and fits model context windows.\"\n",
       "]\n",
       "\n",
       "# (Optional) Chunking function — keep short chunks in practice\n",
       "def chunk_texts(docs, max_chars=500):\n",
       "    chunks = []\n",
       "    for d in docs:\n",
       "        if len(d) <= max_chars:\n",
       "            chunks.append(d)\n",
       "        else:\n",
       "            # very simple split; use smarter splitting for real data\n",
       "            for i in range(0, len(d), max_chars):\n",
       "                chunks.append(d[i:i+max_chars])\n",
       "    return chunks\n",
       "\n",
       "chunks = chunk_texts(documents)\n",
       "\n",
       "# ---------- 2) Compute embeddings and build FAISS index ----------\n",
       "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast\n",
       "embeddings = embed_model.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
       "dim = embeddings.shape[1]\n",
       "\n",
       "index = faiss.IndexFlatL2(dim)   # exact L2 index; switch to IVF/HNSW for large collections\n",
       "index.add(embeddings)           # add all chunk embeddings\n",
       "\n",
       "# Keep a mapping so we can show sources\n",
       "id_to_chunk = {i: chunks[i] for i in range(len(chunks))}\n",
       "\n",
       "# ---------- 3) Query, retrieve top-k ----------\n",
       "def retrieve(query, k=3):\n",
       "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
       "    distances, indices = index.search(q_emb, k)\n",
       "    results = []\n",
       "    for idx in indices[0]:\n",
       "        if idx == -1:\n",
       "            continue\n",
       "        results.append(id_to_chunk[idx])\n",
       "    return results\n",
       "\n",
       "# ---------- 4) Build the prompt (context) and call the LLM ----------\n",
       "def build_prompt(retrieved_chunks, user_question):\n",
       "    context = \"\\n\\n---\\n\".join(retrieved_chunks)\n",
       "    prompt = (\n",
       "        \"You are an assistant. Use ONLY the information in the context to answer the question. \"\n",
       "        \"If the answer isn't in the context, say you don't know.\\n\\n\"\n",
       "        f\"Context:\\n{context}\\n\\nQuestion: {user_question}\\nAnswer:\"\n",
       "    )\n",
       "    return prompt\n",
       "\n",
       "# Example run\n",
       "user_question = \"What does RAG mean?\"\n",
       "retrieved = retrieve(user_question, k=3)\n",
       "prompt = build_prompt(retrieved, user_question)\n",
       "\n",
       "# Use OpenAI Chat Completion (replace with your preferred LLM)\n",
       "openai.api_key = \"YOUR_API_KEY\"  # set via env variable in real code\n",
       "resp = openai.ChatCompletion.create(\n",
       "    model=\"gpt-4o-mini\",  # example — pick a suitable model\n",
       "    messages=[\n",
       "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
       "        {\"role\":\"user\",\"content\": prompt}\n",
       "    ],\n",
       "    max_tokens=200,\n",
       "    temperature=0.0\n",
       ")\n",
       "answer = resp['choices'][0]['message']['content'].strip()\n",
       "print(\"Retrieved chunks:\")\n",
       "for i, r in enumerate(retrieved, 1):\n",
       "    print(f\"{i}. {r}\")\n",
       "print(\"\\nLLM answer:\\n\", answer)\n",
       "```\n",
       "\n",
       "Notes about this example\n",
       "- Embeddings: sentence-transformers is an easy, local option. In production you may use a provider’s embedding model for better alignment with your LLM.\n",
       "- FAISS: for a few documents, IndexFlatL2 is fine. For millions of vectors, use IVF/HNSW indices and approximate search for speed.\n",
       "- Prompting: put a clear instruction to the LLM to rely on the provided context and to cite/decline if unsure.\n",
       "- Temperature: set low (e.g., 0–0.2) to reduce hallucination; but that affects creativity.\n",
       "- Citation: store and return chunk metadata (source URL, doc id, offset) so you can cite sources in answers.\n",
       "\n",
       "Practical improvements and variants\n",
       "- Hybrid retrieval: combine sparse (BM25) + dense (embeddings) retrieval and merge results.\n",
       "- Reranking: after retrieving, use a cross-encoder or another model to re-score candidate chunks for better relevance.\n",
       "- Chain-of-thought / step-by-step: if the task needs reasoning, you can ask the LLM to show its chain-of-thought, or use a verification step.\n",
       "- Retrieval during generation: “retrieve-then-generate” vs. “retrieve-and-augment-while-generating” (some systems call external search mid-generation).\n",
       "- Up-to-date data: refresh the index when documents change.\n",
       "\n",
       "Limitations and risks\n",
       "- Garbage in → garbage out: if documents are wrong, the LLM will propagate incorrect info.\n",
       "- Hallucination: LLMs can still hallucinate; use grounded prompts, verification, and fact-checking.\n",
       "- Cost & latency: embeddings + search + generation adds cost and time per query.\n",
       "- Privacy: be careful about what private data you index and which model you send data to.\n",
       "\n",
       "If you want, I can:\n",
       "- Provide a version that uses only open-source LLMs for generation (no API key),\n",
       "- Show how to include source citations automatically,\n",
       "- Show chunking strategies and how to scale FAISS for millions of docs.\n",
       "\n",
       "Which would you like next?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def chat(question, model):\n",
    "  OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "  openai = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\") if model == MODEL_LLAMA else OpenAI()\n",
    "\n",
    "  stream = openai.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True\n",
    "  )\n",
    "\n",
    "  return stream\n",
    "\n",
    "def display_stream(stream):\n",
    "  response=\"\"\n",
    "  display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "  for chunk in stream:\n",
    "      response += chunk.choices[0].delta.content or \"\"\n",
    "      update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "stream = chat(question, MODEL_GPT)\n",
    "display_stream(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is RAG (Relational Attention Graph) AI?**\n",
       "\n",
       "RAG is an artificial intelligence framework developed by Facebook's AI Research Lab. It's primarily used for natural language processing (NLP) tasks like language translation, question answering, and text classification.\n",
       "\n",
       "The core idea behind RAG is to replace traditional recurrent neural networks (RNNs) with graph-structured attention mechanisms. This allows the model to consider both sequential dependencies and global contextual relationships in a sentence or document.\n",
       "\n",
       "**How does RAG work?**\n",
       "\n",
       "In essence, RAG works by:\n",
       "\n",
       "1. Representing input text as a graph, where each node represents an element of the input sequence (e.g., word tokens).\n",
       "2. Assigning attention weights to each node, indicating its importance.\n",
       "3. Computing the weighted sum of all nodes' representations to generate the contextualized embeddings.\n",
       "4. Using these embeddings as inputs for downstream tasks.\n",
       "\n",
       "**A simple RAG example in Python**\n",
       "\n",
       "To demonstrate this concept, we can create a simplified RAG model using the Keras library and attention mechanisms:\n",
       "```python\n",
       "import numpy as np\n",
       "from keras.layers import Input, Dense, Embedding, Flatten, Attention\n",
       "from keras.models import Model\n",
       "\n",
       "# Sample input text (a sentence)\n",
       "sentences = [\"This is a sample query.\"]\n",
       "\n",
       "# Convert sentences to numerical labels\n",
       "tokenizer = Tokenizer()\n",
       "tokenizer.fit_on_texts(sentences)\n",
       "sequences = tokenizer.texts_to_sequences(sentences)\n",
       "\n",
       "# Create attention-based RAG model\n",
       "attention_input = Input(shape=(100,), name=\"Input\")\n",
       "x = Embedding(input_dim=100, output_dim=128)(attention_input)\n",
       "x = Flatten()(x)  # Flatten embedding\n",
       "\n",
       "# Compute attention layer\n",
       "attention_weights = Attention().apply(x)\n",
       "att_weights = Dense(1, kernel_initializer='zero')(attention_weights)\n",
       "\n",
       "# weighted sum of node representations\n",
       "node_representations = att_weights * x\n",
       "\n",
       "# Generate contextualized embeddings\n",
       "context_embedding = Dense(128, activation='relu')(node_representations)\n",
       "\n",
       "# Create RAG model with input and output layers\n",
       "model = Model(inputs=attention_input, outputs=context_embedding)\n",
       "```\n",
       "In this example:\n",
       "\n",
       "*   We create an input layer representing the tokens obtained from a sentence.\n",
       "*   The embedding layer maps these tokens to dense vectors of size 128.\n",
       "*   The attention layer computes weighted sums for each token, giving us attention scores att_weights and node representations x.\n",
       "*   Finally, we compute contextualized embeddings by integrating these node representations using the generated weights.\n",
       "\n",
       "Note: This simplified example is not optimized for accuracy or practicality and serves mostly to illustrate the fundamental concept of RAG.\n",
       "\n",
       "To go a step further in implementation of this model you may want to check the **RAG** documentation on this [GitHub website](https://github.com/fact-checking/rags).\n",
       "\n",
       "Hope that helps!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "stream = chat(question, MODEL_LLAMA)\n",
    "display_stream(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
