{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Homework - Week 1 Day 2: Resumidor de Websites com Ollama\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Usar um modelo open-source rodando localmente via **Ollama** para resumir o conteudo de um website em portugues brasileiro.\n",
    "\n",
    "## Como funciona\n",
    "\n",
    "1. **Scraping do website** - A funcao `fetch_website_contents` (do modulo `scraper.py`) faz uma requisicao HTTP ao site, extrai o texto do `<body>` removendo scripts, estilos e imagens, e retorna o titulo + conteudo truncado em 2.000 caracteres.\n",
    "\n",
    "2. **Ollama como backend local** - O Ollama expoe um endpoint compativel com a API da OpenAI em `http://localhost:11434/v1`. Isso permite usar o pacote `openai` do Python como client, apenas apontando o `base_url` para o servidor local. Nenhum dado sai da sua maquina.\n",
    "\n",
    "3. **Prompts em PT-BR** - O system prompt instrui o modelo a responder em portugues brasileiro, com um tom sarcastico e humoristico. O user prompt envia o conteudo extraido do site e pede um resumo.\n",
    "\n",
    "4. **Exibicao em Markdown** - A resposta do modelo e renderizada diretamente como Markdown no notebook.\n",
    "\n",
    "## Pre-requisitos\n",
    "\n",
    "- Ollama instalado e rodando (`ollama serve`)\n",
    "- Modelo baixado (ex: `ollama pull phi3` ou `ollama pull llama3.2`)\n",
    "- Pacotes Python: `openai`, `beautifulsoup4`, `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente Ollama configurado com modelo: phi4-mini:latest\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scraper import fetch_website_contents\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "MODEL = \"phi4-mini:latest\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "print(f\"Cliente Ollama configurado com modelo: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "kpglm8iymq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo phi4-mini:latest ja esta instalado.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Verifica se o modelo ja esta instalado, senao faz o pull\n",
    "installed = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "if MODEL not in installed.stdout:\n",
    "    print(f\"Modelo {MODEL} nao encontrado. Baixando...\")\n",
    "    subprocess.run([\"ollama\", \"pull\", MODEL])\n",
    "    print(f\"Modelo {MODEL} instalado com sucesso!\")\n",
    "else:\n",
    "    print(f\"Modelo {MODEL} ja esta instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "summarizer-cell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Resumo Curto e Sarc√°stico:\n",
       "\n",
       "**üåå Anthropic üöÄ:** Onde a privacidade n√£o descansa em salada.\n",
       "\n",
       "- **Noticias Nerds (Febr√£o de Feb!):**\n",
       "    - üéì *Opus 4.6*: N√£o se preocupem com as tarefas banais, temos agora um super-her√≥i do c√≥digo para essas infernais: Claude Opis dos Coding Machines.\n",
       "    \n",
       "**Nota Sarc√°stica:** Vamos esperar at√© que possamos mandar o AIA a fazer nossas pipocas e pizzas! üè†üë®‚ÄçüíªüçΩÔ∏è\n",
       "\n",
       "*Sem contar as \"features\" fancy, mas estamos na bra√ßada de um futuro no qual as IA podem se lembrar do seu nome (embora preferisse n√£o)? #FutureIsWhatWeMakeOf*\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Voce e um assistente sarcastico que analisa o conteudo de um website\n",
    "e fornece um resumo curto, sarcastico e bem-humorado, ignorando texto de navegacao.\n",
    "Responda sempre em portugues brasileiro.\n",
    "Responda em markdown. Nao coloque o markdown dentro de um bloco de codigo - responda direto com o markdown.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Aqui esta o conteudo de um website.\n",
    "Faca um resumo curto deste site.\n",
    "Se houver noticias ou anuncios, resuma-os tambem.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + website}\n",
    "    ]\n",
    "\n",
    "\n",
    "def summarize(url):\n",
    "    website = fetch_website_contents(url)\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "\n",
    "display_summary(\"https://anthropic.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
