# üìÑ Document Summarization Pipeline

A hierarchical document summarization system using:

- Cloud model: `gpt-4o-mini`
- Local fallback: `llama3.2:3b` via Ollama
- Multi-format document ingestion
- Recursive summarization (chunk ‚Üí batch ‚Üí executive summary)
- Automatic dynamic output naming

---

## üöÄ Features

- Supports `.txt`, `.pdf`, `.docx`
- Smart sentence-aware chunking
- Cloud-first architecture with automatic local fallback
- Executive-level structured summary output
- Automatic output filename generation
- Timestamp versioning to prevent overwrite
- Structured logging

---

## üèó Architecture Overview

```
Document ‚Üí Text Extraction ‚Üí Chunking ‚Üí 
Chunk Summaries ‚Üí Batch Reduction ‚Üí 
Final Executive Summary ‚Üí DOCX Export
```

### Execution Strategy

1. Attempt summarization using OpenAI (cloud)
2. If unavailable ‚Üí fallback to Ollama (local model)
3. If both fail ‚Üí raise runtime error

---

## üì¶ Requirements

### Python Version
Python 3.9+

### Dependencies

Install via:

```bash
pip install openai python-docx PyPDF2 docx2txt
```

If using local fallback:

```bash
ollama pull llama3.2:3b
```

Ollama must be running locally:

```bash
ollama serve
```

---

## üîë Environment Configuration

Set your OpenAI API key (for cloud mode):

### Windows (PowerShell)

```powershell
setx OPENAI_API_KEY "your_key_here"
```

### macOS / Linux

```bash
export OPENAI_API_KEY="your_key_here"
```

If no key is found, the system automatically switches to local mode.

---

## ‚ñ∂Ô∏è Usage

Edit inside `main`:

```python
input_file = r"path_to_your_file.docx"
```

Run:

```bash
python summarizer.py
```

Output will be saved automatically as:

```
<original_filename>_summary_<timestamp>.docx
```

Example:

```
report.docx
‚Üí report_summary_20260226_014501.docx
```

---

## üìÇ Supported Input Formats

| Format | Supported | Notes |
|--------|----------|-------|
| .txt   | ‚úÖ | UTF-8 only |
| .pdf   | ‚úÖ | Text-based PDFs only |
| .docx  | ‚úÖ | Extracted via docx2txt |
| .doc   | ‚ùå | Not supported |
| Scanned PDF | ‚ùå | No OCR support |

---

# ‚ö†Ô∏è Program Limitations

## 1Ô∏è‚É£ No OCR Support
- Image-based PDFs are not supported.
- Only text-based PDFs work.
- No OCR integration (e.g., Tesseract).

---

## 2Ô∏è‚É£ Token / Context Limits
- Chunking is character-based, not token-aware.
- Very large documents may:
  - Increase runtime
  - Increase API cost (cloud mode)
  - Reduce coherence across chunks

---

## 3Ô∏è‚É£ Sequential Processing
- Chunk summarization runs sequentially.
- No multiprocessing or async implementation.
- Large documents will take longer.

---

## 4Ô∏è‚É£ No Cost Control
- No token usage tracking.
- No spending cap.
- Large documents may generate unexpected API costs.

---

## 5Ô∏è‚É£ Model Dependency

Cloud Mode:
- Requires valid OpenAI API key.
- Depends on model availability.

Local Mode:
- Requires Ollama installed.
- Requires `llama3.2:3b` downloaded.
- Requires local server running.

If Ollama is not running, fallback fails.

---

## 6Ô∏è‚É£ Basic Sentence Splitting
Chunk splitting uses:

```python
text.rfind(".", 0, chunk_size)
```

Limitations:
- Not language-aware.
- May split incorrectly on abbreviations.
- Not NLP-optimized.

---

## 7Ô∏è‚É£ No Structured Output Validation
- No JSON schema enforcement.
- No format validation.
- Model output saved as-is.

---

## 8Ô∏è‚É£ Hardcoded Paths
- Input and output paths are defined in code.
- No CLI argument support.
- No configuration file support.

---

## 9Ô∏è‚É£ No Retry / Backoff Strategy
- If a request fails:
  - Switches model
  - No exponential retry logic
- Network instability may cause full failure.

---

## üîí Security Considerations
- Cloud mode sends documents to OpenAI API.
- Sensitive documents should use local mode only.
- No encryption-at-rest mechanism implemented.

---

## üìà Performance Expectations

| Document Size | Expected Behavior |
|--------------|------------------|
| < 20 pages | Fast |
| 20‚Äì100 pages | Moderate |
| 100+ pages | Slower |
| 300+ pages | High runtime |

Local 3B model will be slower than cloud.

---

## üõ† Suggested Future Improvements

- Token-aware chunking
- Async parallel processing
- CLI support with argparse
- JSON schema enforcement
- Cost tracking
- OCR integration
- Docker containerization
- REST API wrapper
- Caching mechanism
- Progress bar
- Streaming summarization

---

## üìä Design Pattern

Recursive Map-Reduce LLM Summarization  
With Resilient Dual-Execution (Cloud + Local Fallback)

Suitable for:
- Executive briefings
- Policy analysis
- Strategic planning documents
- Research synthesis

---