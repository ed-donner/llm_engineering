{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dcbf82",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d177a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0715f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in .env file\")\n",
    "\n",
    "\n",
    "# OpenAI client\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Ollama client (OpenAI-compatible endpoint)\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert technical tutor.\n",
    "\n",
    "Explain concepts clearly, concisely, and accurately.\n",
    "Use simple language first, then add technical depth if helpful.\n",
    "Provide practical examples when appropriate.\n",
    "Break complex ideas into step-by-step explanations.\n",
    "Avoid unnecessary jargon, but use correct terminology.\n",
    "Assume the learner is smart but unfamiliar with the topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(question, client, model, system_prompt):\n",
    "    \"\"\"Stream a response from any OpenAI-compatible backend and display live.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content\n",
    "        if delta:\n",
    "            response += delta\n",
    "            update_display(\n",
    "                Markdown(response),\n",
    "                display_id=display_handle.display_id,\n",
    "            )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is an AI agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## GPT-4o-mini Response...\\n\")\n",
    "\n",
    "user_input = input(\"Enter your question: \").strip()\n",
    "\n",
    "user_question = user_input if user_input else question\n",
    "try:\n",
    "    gpt_response = ask_llm(user_question, openai_client, MODEL_GPT, system_prompt)\n",
    "except Exception as e:\n",
    "    print(f\"GPT error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd634a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Llama Response...\\n\")\n",
    "\n",
    "user_input = input(\"Enter your question: \").strip()\n",
    "\n",
    "user_question = user_input if user_input else question\n",
    "try:\n",
    "    llama_response = ask_llm(user_question, openai_client, MODEL_LLAMA, system_prompt)\n",
    "except Exception as e:\n",
    "    print(f\"Llama error: {e}. Make sure Ollama is running and the model is available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
