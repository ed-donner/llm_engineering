{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a365369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =\"\"\"You are a helpful assistant that helps answer technical questions with plain English anwers\n",
    "your answers should be concise and to the point, avoiding unnecessary jargon or complexity.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "# question = \"\"\"\n",
    "# Please explain what this code does and why:\n",
    "# yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why, do it concisely:\n",
    "def stream_brochure(company_name, url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4ab5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc1c56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.chat.completions.create(\n",
    "#     model=MODEL_GPT,\n",
    "#     messages=messages,\n",
    "#     max_tokens=500,\n",
    "#     temperature=0.2\n",
    "# )\n",
    "# def display_markdown(text):\n",
    "#     \"\"\"Display text as Markdown in Jupyter Notebook.\"\"\"\n",
    "#     display(Markdown(text))\n",
    "# display_markdown(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee0df8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code defines a function called `stream_brochure` that generates content for a company's brochure using OpenAI's API.\n",
       "\n",
       "1. **Parameters**: It takes `company_name` and `url` as inputs.\n",
       "2. **API Call**: It sends a request to the OpenAI API to create a chat completion using a specified model. It includes a system message (likely to set the context) and a user message constructed by the function `get_brochure_user_prompt` using the `company_name` and `url`.\n",
       "3. **Streaming Response**: The response is streamed, meaning it is received in chunks rather than all at once.\n",
       "4. **Response Handling**:\n",
       "   - An empty string `response` is initialized to accumulate the content.\n",
       "   - A display handle is created to show the output in a  format.\n",
       "   - As each chunk of the response is received, it is added to `response`, and certain characters (like  and ) are removed for formatting.\n",
       "   - The display is updated with the current content of `response`.\n",
       "\n",
       "Overall, this function interacts with the OpenAI API to generate and display a brochure in real-time, presenting the data as it comes in."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "# stream = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages,\n",
    "#                                         stream=True)\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "#  stream = openai.chat.completions.create(\n",
    "#         model=MODEL,\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": system_prompt},\n",
    "#             {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "#           ],\n",
    "#         stream=True\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93fddeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant that helps answer technical questions with plain English anwers\\nyour answers should be concise and to the point, avoiding unnecessary jargon or complexity.\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nPlease explain what this code does and why, do it concisely:\\ndef stream_brochure(company_name, url):\\n    stream = openai.chat.completions.create(\\n        model=MODEL,\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_prompt},\\n            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\\n          ],\\n        stream=True\\n    )\\n\\nresponse = \"\"\\ndisplay_handle = display(Markdown(\"\"), display_id=True)\\nfor chunk in stream:\\n    response += chunk.choices[0].delta.content or \\'\\'\\n    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\\n    update_display(Markdown(response), display_id=display_handle.display_id)\\n\\n'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "\n",
       "This code generates a brochure using OpenAI's ChatGPT API and displays it in a GUI.\n",
       "\n",
       "Here's what it does:\n",
       "\n",
       "1. Creates a chat session with ChatGPT, where the user is the system.\n",
       "2. Sends two messages:\n",
       "\t* A system prompt that sets up the context for generating text.\n",
       "\t* A user prompt that asks ChatGPT to generate a brochure based on the provided company name and URL.\n",
       "3. Listens to the response from ChatGPT in chunks (small pieces of text).\n",
       "4. Asks ChatGPT for its choices (possible completions) for each chunk.\n",
       "5. Displays each chunk as Markdown in a GUI, with updates.\n",
       "\n",
       "**Why**\n",
       "\n",
       "The code uses this approach to generate a brochure by:\n",
       "\n",
       "* Utilizing AI to produce high-quality content (the brochure text).\n",
       "* Displaying the result in real-time to update the user's view of the generated content.\n",
       "\n",
       "This allows users to interactively explore and modify the brochure before it's fully generated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "import ollama\n",
    "stream = ollama.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    # response += chunk.choices[0].delta.content or ''\n",
    "    response += chunk['message']['content'] or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b776b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(role='assistant', content=\"**Code Explanation**\\n\\nThis code generates a brochure for a company using an AI model. Here's what it does:\\n\\n1. Creates a chat session with an OpenAI model.\\n2. Sends two prompts: one from the system (a fixed prompt) and one from the user (generated based on `company_name` and `url`).\\n3. Iterates through chunks of responses received from the model, adding each response to a string (`response`).\\n4. Removes markdown formatting from the response string.\\n5. Updates a display with the updated response string.\\n\\n**Purpose**: The goal is to generate a brochure by prompting an AI model to fill in text about the company based on its name and URL.\", thinking=None, images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# pprint(response)\n",
    "\n",
    "# pprint(vars(response['message']))\n",
    "\n",
    "pprint(response['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfba255e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"**Code Explanation**\\n\\nThis code generates a brochure for a company using an AI model. Here's what it does:\\n\\n1. Creates a chat session with an OpenAI model.\\n2. Sends two prompts: one from the system (a fixed prompt) and one from the user (generated based on `company_name` and `url`).\\n3. Iterates through chunks of responses received from the model, adding each response to a string (`response`).\\n4. Removes markdown formatting from the response string.\\n5. Updates a display with the updated response string.\\n\\n**Purpose**: The goal is to generate a brochure by prompting an AI model to fill in text about the company based on its name and URL.\",\n",
       " 'thinking': None,\n",
       " 'images': None,\n",
       " 'tool_calls': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a15d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "stream = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7864a993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Breakdown**\n",
       "\n",
       "This code generates a Markdown-formatted brochure for a given company.\n",
       "\n",
       "Here's what it does:\n",
       "\n",
       "1. It uses the OpenAI API to create a chat model response.\n",
       "2. The system asks questions in one \"system\" message, while the user provides information about the company through an input (via `company_name` and `url`).\n",
       "3. The generated text is displayed in real-time using `Markdown` formatting.\n",
       "\n",
       "**Why**\n",
       "\n",
       "The purpose of this code is to automatically generate a brochure-like webpage with information about a specific company. The use of AI-powered chat model response allows for the quick generation of high-quality content with minimal human input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    # print(chunk.choices[0].delta.content)\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be843d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
