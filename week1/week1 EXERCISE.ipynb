{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {},
      "source": [
        "# End of week 1 exercise\n",
        "\n",
        "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
        "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1070317-3ed9-4659-abe3-828943230e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found\n",
            "OpenAI client instance created\n",
            "Ollama client instance created\n"
          ]
        }
      ],
      "source": [
        "#Define the models to use\n",
        "MODEL_GPT = 'gpt-4o-mini'\n",
        "MODEL_LLAMA = 'llama3.2'\n",
        "\n",
        "#Load the API key from the environment variable\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "#Check if the API key is valid\n",
        "if api_key and api_key.startswith('sk-') and len(api_key) > 10:\n",
        "    print(\"API key found\")\n",
        "else:\n",
        "    print(\"No API key found or is not valid\")\n",
        "\n",
        "# Create an instance of the OpenAI client\n",
        "openai_client = OpenAI(api_key=api_key)\n",
        "print(\"OpenAI client instance created\")\n",
        "\n",
        "# Create an instance of the Ollama client\n",
        "# ollama_client = OpenAI(base_url = OLLAMA_BASE_URL, api_key='ollama')\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
        "print(\"Ollama client instance created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e8a533",
      "metadata": {},
      "outputs": [],
      "source": [
        "# requests.get(\"http://localhost:11434\").content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fe3461",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !ollama pull llama3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up environment\n",
        "question_system_prompt = \"\"\"\n",
        "You are a helpful assistant that can explain technical concepts in a way that is easy to understand even for non-technical people.\n",
        "With the help of the user, you will be able to explain the concept in a way that is easy to understand.\n",
        "\"\"\"\n",
        "question_user_prompt = \"\"\"\n",
        "Please explain what this code does and why:\n",
        "\n",
        "def fetch_website_contents(url):\n",
        "    \n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    title = soup.title.string if soup.title else \"No title found\"\n",
        "    if soup.body:\n",
        "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            irrelevant.decompose()\n",
        "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "        text = \"\"\n",
        "    return (title + \"\\n\\n\" + text)[:2_000]\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# question_user_prompt = \"\"\"\n",
        "# Please explain what the solution to this math question and also the reasoning behind it:\n",
        "\n",
        "# You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " This piece of code is a Python function named `fetch_website_contents`. Let's break down what it does step-by-step and explain the reasoning behind each part.\n",
              "\n",
              "### What the Code Does:\n",
              "\n",
              "1. **Function Definition**: \n",
              "   The function `fetch_website_contents` accepts a single argument called `url`, which is expected to be a string representing the web address of a website.\n",
              "\n",
              "2. **Sending a Request**:\n",
              "   ```python\n",
              "   response = requests.get(url, headers=headers)\n",
              "   ```\n",
              "   Here, the code sends a GET request to the specified URL using the `requests` library. This means it asks the server hosting that webpage to send back the page's content. The `headers=headers` part allows you to specify additional information (like a user-agent) with the request, potentially helping you avoid being blocked or to mimic a regular browser.\n",
              "\n",
              "3. **Parsing HTML**:\n",
              "   ```python\n",
              "   soup = BeautifulSoup(response.content, \"html.parser\")\n",
              "   ```\n",
              "   The response content (the HTML of the webpage) is then parsed using `BeautifulSoup` from the `bs4` library. BeautifulSoup makes it easier to work with HTML documents in Python.\n",
              "\n",
              "4. **Extracting the Title**:\n",
              "   ```python\n",
              "   title = soup.title.string if soup.title else \"No title found\"\n",
              "   ```\n",
              "   The code attempts to get the title of the webpage from the `<title>` tag in the HTML. If the title is found, it takes the text inside it; otherwise, it returns \"No title found.\" This is a common part of a webpage, making it easy for users to identify what the page is about in browser tabs.\n",
              "\n",
              "5. **Cleaning Up the Text**:\n",
              "   ```python\n",
              "   if soup.body:\n",
              "       for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
              "           irrelevant.decompose()\n",
              "       text = soup.body.get_text(separator=\"\n",
              "\", strip=True)\n",
              "   else:\n",
              "       text = \"\"\n",
              "   ```\n",
              "   In this part:\n",
              "   - The function checks if the `<body>` of the webpage exists (which contains the main content).\n",
              "   - It looks for certain types of tags that are often unnecessary for the basic reading of content (like `<script>`, `<style>`, `<img>`, and `<input>`) and removes them using `decompose()`. This helps in cleaning up the text to ensure only relevant content is kept.\n",
              "   - After that, it extracts the remaining text from the body with `get_text()`, which gathers all the text within the body and can optionally format it (e.g., adding new lines).\n",
              "\n",
              "6. **Returning the Combined Result**:\n",
              "   ```python\n",
              "   return (title + \"\n",
              "\n",
              "\" + text)[:2_000]\n",
              "   ```\n",
              "   Finally, the function combines the title and the cleaned text, adds a newline in between for separation, and returns the first 2,000 characters. This is useful for ensuring that the output is not excessively long and remains readable.\n",
              "\n",
              "### Why This Code is Useful:\n",
              "\n",
              "- **Website Scraping**: This function is useful for scraping or gathering the content from web pages, which can be valuable for data analysis, research, or creating summaries of web articles.\n",
              "- **Content Cleaning**: By removing scripts, styles, images, and other irrelevant tags, it ensures that the output is focused on the actual readable content for better usability.\n",
              "- **Basic Error Handling**: The code includes basic checks (like if a title exists or if the body is present), making it more robust against various types of web pages.\n",
              "\n",
              "In summary, this function efficiently retrieves and cleans the textual content from a specified web page, making it suitable for further analysis or presentation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get gpt-4o-mini to answer, with streaming\n",
        "stream = openai_client.chat.completions.create(\n",
        "    model = MODEL_GPT,\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": question_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question_user_prompt}\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "response = \" \"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "for chunk in stream:\n",
        "    response += chunk.choices[0].delta.content or ''\n",
        "    update_display(Markdown(response), display_id=display_handle.display_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's break down what this code does.\n",
              "\n",
              "**Functionality**\n",
              "\n",
              "This is a Python function called `fetch_website_contents` that takes a single argument: the URL of a website. The function retrieves the content of the webpage, parses it, and then extracts specific information from the parsed HTML.\n",
              "\n",
              "**Here's what happens step by step:**\n",
              "\n",
              "1. **Send an HTTP request**: The function uses the `requests` library to send a GET request to the specified URL.\n",
              "2. **Get the HTML response**: The response from the server is stored in the `response` variable.\n",
              "3. **Parse the HTML content with BeautifulSoup**: The code uses the `BeautifulSoup` library to parse the HTML content of the webpage. This creates a tree-like structure that can be easily navigated and searched.\n",
              "4. **Extract the title**: The function extracts the title element from the parsed HTML and assigns it to the `title` variable. If no title is found, an empty string is assigned instead.\n",
              "5. **Remove unwanted elements**: The code removes any script, style, or image elements from the HTML content using the `decompose()` method. This is done to prevent these elements from affecting the content extraction process.\n",
              "6. **Get the text content**: The function extracts the text content of the HTML content using the `get_text()` method. This method includes the separator between lines of text, which can be useful later.\n",
              "7. **Remove extra spaces and return limited content**: Finally, the function combines the title and extracted text content into a single string and limits it to 2,000 characters.\n",
              "\n",
              "**Why do these specific things?**\n",
              "\n",
              "The code is designed to extract specific information from a webpage:\n",
              "\n",
              "* The title element provides basic metadata about the page.\n",
              "* By removing unwanted elements (script, style, image) and extracting only the text content, we can get rid of any non-intrusive elements that might make it harder to understand what's on the page.\n",
              "\n",
              "The code is also designed to preserve a small portion of the webpage's text content. The limited return value (`[:2_000]`) allows us to capture a reasonable amount of text without getting caught up in too much detail or overhead.\n",
              "\n",
              "**Security note**, this function doesn't include any check for the existence of the website, or if it's not accessible or running properly.\n",
              "\n",
              "Let me know if you have any more questions about what code does, and especially how does it works"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get Llama 3.2 to answer (use same client as in constants: ollama)\n",
        "response = ollama.chat.completions.create(\n",
        "    model = MODEL_LLAMA,\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": question_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question_user_prompt}\n",
        "    ]\n",
        ")\n",
        "result = response.choices[0].message.content\n",
        "display(Markdown(result))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
