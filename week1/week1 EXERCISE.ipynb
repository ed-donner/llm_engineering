{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "system_prompt = \"\"\"\n",
    "You are a technical expert of AI and LLMs.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Provide deep explanations of the provided text.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Explain the provided text.\n",
    "\"\"\"\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Ollama does have an OpenAI compatible endpoint, but Gemini doesn't?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand the statement regarding Ollama and Gemini, we need to delve into several key components: what an OpenAI-compatible endpoint is, the functionalities and roles of Ollama and Gemini in the realm of AI, and some implications of their compatibility or lack thereof.\n",
      "\n",
      "### OpenAI-Compatible Endpoint\n",
      "\n",
      "An **OpenAI-compatible endpoint** refers to an interface or service provided by an application or platform that allows users to interact with OpenAI's models in a manner that mirrors the original specifications or protocols set by OpenAI. This means that developers can send requests (such as prompts) to the endpoint and receive responses in a format that adheres to OpenAI's API standards. Such compatibility is important for developers who want to integrate AI functionalities without having to significantly alter their existing codebase or integration strategies.\n",
      "\n",
      "### Ollama\n",
      "\n",
      "**Ollama** is likely a platform or service that provides access to various AI models, including those compatible with OpenAI's framework. If Ollama has an OpenAI-compatible endpoint, it suggests that it allows users to easily utilize OpenAI's language models through a familiar interface. This means:\n",
      "\n",
      "1. **Ease of Integration**: Developers using Ollama can directly employ OpenAI’s capabilities in their applications without needing to learn new integration mechanics.\n",
      "  \n",
      "2. **Consistency**: By offering an OpenAI-compatible interface, Ollama ensures users can expect a similar response format, error handling, and interaction models as those provided by OpenAI.\n",
      "\n",
      "3. **Flexibility**: Having an OpenAI-compatible endpoint gives developers the freedom to choose between different AI services while retaining a consistent experience.\n",
      "\n",
      "### Gemini\n",
      "\n",
      "On the other hand, **Gemini** is likely referring to another AI framework or service that lacks an OpenAI-compatible endpoint. The implications of this can be significant:\n",
      "\n",
      "1. **Integration Challenges**: If Gemini doesn’t have an OpenAI-compatible endpoint, developers may face difficulties in integrating it into their existing workflows. They might need to adapt their applications to accommodate Gemini’s own API structure, which could involve more overhead for ensuring functionality.\n",
      "\n",
      "2. **User Experience**: The user experience could also differ significantly. If the API and response structures differ from what users are accustomed to with OpenAI, there might be a learning curve and potential for errors during implementation.\n",
      "\n",
      "3. **Functional Limitations**: It could also suggest that Gemini may not support all the same features or functionalities as offered by OpenAI's models, which could affect how developers choose between the two systems based on their specific needs.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "In summary, the statement that \"Ollama does have an OpenAI compatible endpoint, but Gemini doesn't\" highlights the differences between two AI services regarding their compatibility with OpenAI's API framework. This distinction is crucial for developers, as it affects how easily they can adopt these tools into their projects, the workflows they build, and ultimately, their ability to harness AI for various applications. The presence of an OpenAI-compatible endpoint in Ollama signifies accessibility and usability, while Gemini’s lack of this feature may pose challenges for integration and potentially limit its appeal depending on developers' existing infrastructures and needs."
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def messages_for(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + question}\n",
    "    ]\n",
    "\n",
    "def run_model_streaming(model_name, question):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages_for(question),\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "\n",
    "run_model_streaming(MODEL_GPT, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide a detailed explanation, let's break down the statement into its components.\n",
      "\n",
      "**OpenAI Compatibility:**\n",
      "\n",
      "The question essentially compares two language models: Ollama and Gemini. To understand their compatibility with OpenAI, we need to consider what OpenAI refers to in this context. OpenAI is likely assuming that when you hear about compatibility, it means the model's API is compatible with the OpenAI Platform or has an equivalent endpoint that allows users to interact with it directly.\n",
      "\n",
      "An OpenAI-compatible endpoint would imply that the model:\n",
      "\n",
      "1. **Understands and supports the OpenAI API syntax**: The model is trained using data accessible through the OpenAI API or has been fine-tuned from a similar dataset.\n",
      "2. **Is deployable on the OpenAI Platform**: This includes support for any necessary infrastructure, such as hosting services, API endpoints, and distribution platforms.\n",
      "\n",
      "**Gemini vs Ollama:**\n",
      "\n",
      "To answer whether Gemini is compatible with this standard and Ollama isn't directly asks us to consider:\n",
      "- Whether Gemini (or its developers) claim compatibility through documentation or explicit statements.\n",
      "- To check if there's an official OpenAI-compatible endpoint for Gemini.\n",
      "- It may also hint at understanding the underlying differences between the models Gemini, Gemini, and Ollama might be training on different datasets which in turn could mean they may not necessarily support all the same endpoints of the open ai model even though its the same basic paradigm.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Without specific details about each AI or their OpenAI compatibility or lack thereof, several possibilities exist:\n",
      "\n",
      "- **Gemini Not Listed**: If Gemini developers haven’t explicitly stated openness to the API and standard OpenAI is in use then possibly it’s not supported (if not listed doesn’t mean its unsupported).\n",
      "\n",
      "- **Model Differences**: It could be a case of differences in data set training or deployment specifics meaning support for an endpoint depends on several model-dependent criteria.\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "# imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# set up environment\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\", \"http://localhost:11434/v1\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"ollama\")\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a technical expert of AI and LLMs.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Provide deep explanations of the provided text.\n",
    "\"\"\"\n",
    "\n",
    "# question\n",
    "question = \"\"\"\n",
    "Ollama does have an OpenAI compatible endpoint, but Gemini doesn't?\n",
    "\"\"\"\n",
    "\n",
    "# message\n",
    "def messages_for(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + question}\n",
    "    ]\n",
    "\n",
    "# response\n",
    "def run_model(model_name, question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages_for(question)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# run and print result\n",
    "print(run_model(MODEL_LLAMA, question))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
