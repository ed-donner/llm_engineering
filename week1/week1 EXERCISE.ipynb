{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--backend {openai,ollama}] [--model MODEL]\n",
      "                             [--stream] [--temp TEMP] [--extra EXTRA]\n",
      "                             question\n",
      "ipykernel_launcher.py: error: the following arguments are required: question\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tech Explainer Tool\n",
    "-------------------\n",
    "A single-file Python tool that answers technical questions with clear explanations,\n",
    "using either the OpenAI API or a local Ollama server (OpenAI-compatible endpoint).\n",
    "\n",
    "Features\n",
    "- Unified interface for both backends (OpenAI & Ollama).\n",
    "- Sensible system prompt for didactic, structured explanations.\n",
    "- Optional streaming output.\n",
    "- CLI + importable functions.\n",
    "- Safe handling of DeepSeek-R1 style <think> blocks (auto-stripping).\n",
    "\n",
    "Requirements\n",
    "- Python 3.9+\n",
    "- pip install openai python-dotenv (optional but recommended)\n",
    "\n",
    "Environment Variables\n",
    "- OPENAI_API_KEY: Required if backend=openai.\n",
    "- OLLAMA_BASE_URL: Optional, default http://localhost:11434/v1 if backend=ollama.\n",
    "\n",
    "Examples\n",
    "- CLI (OpenAI):\n",
    "    python tech_explainer_tool.py \"What is a vector database?\"\n",
    "- CLI (Ollama):\n",
    "    python tech_explainer_tool.py \"Explain RAG\" --backend ollama --model deepseek-r1:1.5b\n",
    "- As a library:\n",
    "    from tech_explainer_tool import answer\n",
    "    text = answer(\"Explain transformers vs RNNs\", backend=\"openai\", model=\"gpt-4.1-mini\")\n",
    "    print(text)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv  # optional\n",
    "    load_dotenv(override=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# ---------- Config ----------\n",
    "DEFAULT_OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4.1-mini\")\n",
    "DEFAULT_OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"deepseek-r1:1.5b\")\n",
    "DEFAULT_OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434/v1\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a patient, practical technical tutor.\\n\"\n",
    "    \"Explain with plain language first, then dive deeper.\\n\"\n",
    "    \"Use markdown with clear sections, short paragraphs, and code blocks where helpful.\\n\"\n",
    "    \"When appropriate, show a tiny working example, pitfalls, and a short checklist.\\n\"\n",
    "    \"Prefer correctness and reproducibility over flair.\\n\"\n",
    ")\n",
    "\n",
    "THINK_TAGS_RE = re.compile(r\"<think>.*?</think>\\s*\", flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def strip_think(text: str) -> str:\n",
    "    \"\"\"Remove reasoning tags like <think>...</think> if present.\"\"\"\n",
    "    return THINK_TAGS_RE.sub(\"\", text).strip()\n",
    "\n",
    "\n",
    "def build_messages(question: str, extra_instructions: str | None = None) -> List[Dict[str, str]]:\n",
    "    system = SYSTEM_PROMPT if not extra_instructions else SYSTEM_PROMPT + \"\\n\" + extra_instructions\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": question.strip()},\n",
    "    ]\n",
    "\n",
    "\n",
    "def _make_client(backend: str) -> OpenAI:\n",
    "    backend = backend.lower()\n",
    "    if backend == \"openai\":\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"OPENAI_API_KEY is required for backend=openai\")\n",
    "        return OpenAI(api_key=api_key)  # default base_url\n",
    "    elif backend == \"ollama\":\n",
    "        # Use OpenAI-compatible server exposed by Ollama\n",
    "        base_url = DEFAULT_OLLAMA_BASE_URL\n",
    "        return OpenAI(base_url=base_url, api_key=os.getenv(\"OLLAMA_API_KEY\", \"ollama\"))\n",
    "    else:\n",
    "        raise ValueError(\"backend must be 'openai' or 'ollama'\")\n",
    "\n",
    "\n",
    "def answer(\n",
    "    question: str,\n",
    "    *,\n",
    "    backend: str = \"openai\",\n",
    "    model: str | None = None,\n",
    "    stream: bool = False,\n",
    "    temperature: float = 0.2,\n",
    "    extra_instructions: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Return a markdown explanation for a technical question using the selected backend.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The technical question to answer.\n",
    "    backend : {'openai','ollama'}\n",
    "        Which backend to use.\n",
    "    model : str | None\n",
    "        Model name. Defaults per-backend if omitted.\n",
    "    stream : bool\n",
    "        If True, prints tokens incrementally to stdout and returns the final text.\n",
    "    temperature : float\n",
    "        Sampling temperature.\n",
    "    extra_instructions : str | None\n",
    "        Additional system guidance.\n",
    "    \"\"\"\n",
    "    client = _make_client(backend)\n",
    "    model = model or (DEFAULT_OPENAI_MODEL if backend == \"openai\" else DEFAULT_OLLAMA_MODEL)\n",
    "\n",
    "    msgs = build_messages(question, extra_instructions)\n",
    "\n",
    "    if stream:\n",
    "        chunks = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=msgs,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        collected: List[str] = []\n",
    "        try:\n",
    "            for chunk in chunks:  # type: ignore[union-attr]\n",
    "                delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "                if delta:\n",
    "                    sys.stdout.write(delta)\n",
    "                    sys.stdout.flush()\n",
    "                    collected.append(delta)\n",
    "        finally:\n",
    "            print()  # newline after stream\n",
    "        text = strip_think(\"\".join(collected))\n",
    "        return text\n",
    "\n",
    "    # non-streaming\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=msgs,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    text = resp.choices[0].message.content or \"\"\n",
    "    return strip_think(text)\n",
    "\n",
    "\n",
    "# ---------- CLI ----------\n",
    "\n",
    "def _parse_args(argv: List[str] | None = None) -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Answer technical questions using OpenAI or Ollama.\")\n",
    "    p.add_argument(\"question\", help=\"Technical question in quotes\")\n",
    "    p.add_argument(\"--backend\", choices=[\"openai\", \"ollama\"], default=\"openai\")\n",
    "    p.add_argument(\"--model\", help=\"Model name (optional)\")\n",
    "    p.add_argument(\"--stream\", action=\"store_true\", help=\"Stream the answer to stdout\")\n",
    "    p.add_argument(\"--temp\", type=float, default=0.2, help=\"Sampling temperature\")\n",
    "    p.add_argument(\"--extra\", help=\"Extra system instructions\")\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "\n",
    "def main(argv: List[str] | None = None) -> int:\n",
    "    args = _parse_args(argv)\n",
    "    try:\n",
    "        text = answer(\n",
    "            args.question,\n",
    "            backend=args.backend,\n",
    "            model=args.model,\n",
    "            stream=args.stream,\n",
    "            temperature=args.temp,\n",
    "            extra_instructions=args.extra,\n",
    "        )\n",
    "        if not args.stream:\n",
    "            print(text)\n",
    "        return 0\n",
    "    except KeyboardInterrupt:\n",
    "        return 130\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raise SystemExit(main())\n",
    "\n",
    "\n",
    "# --- Example: calling the tool with a prepared question variable (OpenAI only) ---\n",
    "if __name__ == \"__main__\" and False:\n",
    "    # Flip to True to run this demo when executing the file directly\n",
    "    question = (\n",
    "        \"\"\"\n",
    "        Please explain what this code does and why:\n",
    "        yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"[OpenAI] Answer:\")\n",
    "    print(\n",
    "        answer(\n",
    "            question,\n",
    "            backend=\"openai\",\n",
    "            model=os.getenv(\"OPENAI_MODEL\", \"gpt-4.1-mini\"),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tech_explainer_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtech_explainer_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m answer\n\u001b[32m      3\u001b[39m question = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mPlease explain what this code does and why:\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33myield from \u001b[39m\u001b[33m{\u001b[39m\u001b[33mbook.get(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) for book in books if book.get(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)}\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer(question, backend=\u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-mini\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tech_explainer_tool'"
     ]
    }
   ],
   "source": [
    "from tech_explainer_tool import answer\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "print(answer(question, backend=\"openai\", model=\"gpt-4.1-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
