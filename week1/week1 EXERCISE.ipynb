{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def stream_response(stream: Stream) -> None:\n",
    "    \"\"\"\n",
    "    Processes a Stream object and updates a Markdown display dynamically with content\n",
    "    received from the stream chunks.\n",
    "\n",
    "    Args:\n",
    "        stream (Stream): An object that produces chunks of responses with choices.\n",
    "    \"\"\"\n",
    "    response_parts: List[str] = []\n",
    "\n",
    "    # Initialize the display with Markdown formatting\n",
    "    handle_display = display(Markdown, display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        # Ensure we extract content safely\n",
    "        if chunk.choices and hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content'):\n",
    "            content = chunk.choices[0].delta.content or ''\n",
    "            response_parts.append(content)\n",
    "        else:\n",
    "            logging.warning(\"Chunk has no choices or invalid structure.\")\n",
    "\n",
    "    # Update the response display once at the end\n",
    "    full_response = ''.join(response_parts)\n",
    "    update_display(Markdown(full_response), display_id=handle_display.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "code = \"\"\"\n",
    "def stream_response(stream: Stream) -> None:\n",
    "    \"/\"/\"/\n",
    "    Processes a Stream object and updates a Markdown display dynamically with content\n",
    "    received from the stream chunks.\n",
    "\n",
    "    Args:\n",
    "        stream (Stream): An object that produces chunks of responses with choices.\n",
    "    \"/\"/\"/\n",
    "    response_parts: List = []\n",
    "\n",
    "    # Initialize the display with Markdown formatting\n",
    "    handle_display = display(Markdown, display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        # Ensure we extract content safely\n",
    "        try:\n",
    "            content = chunk.choices[0].delta.content or ''\n",
    "            response_parts.append(content)\n",
    "        except (IndexError, AttributeError) as error:\n",
    "            print(f\"Error processing chunk: {/error}/\")\n",
    "\n",
    "    # Update the response display once at the end\n",
    "    full_response = ''.join(response_parts)\n",
    "    update_display(Markdown(full_response), display_id=handle_display.display_id)\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "\n",
    "\"\"\"+code\n",
    "\n",
    "system_prompt=\"\"\"You are a helpful assistant that explains code.\n",
    " You want to explain the code in a way that is best practice for a developer and easy to learn.\n",
    " You use best practices for code readability and maintainability and also judge the code on readability and maintainability and clean code/architecture and give better code suggestions.\n",
    " Use only most recent knowledge and information.\n",
    "\"\"\"\n",
    "\n",
    "# construct the messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=MODEL,\n",
    "    stream=True,\n",
    "\n",
    ")\n",
    "\n",
    "stream_response(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url: 'http://localhost:11434/v1'\n",
      "model: llama3.2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Code Review and Explanation**\n",
       "\n",
       "The provided code defines a function `stream_response` that processes a stream of chunks, extracts relevant content from each chunk, and updates a Markdown display dynamically.\n",
       "\n",
       "### Purpose:\n",
       "\n",
       "This function appears to be part of a larger application, likely related to web development or interactive tools, where the user interacts with a Stream-like object to receive responses. The goal is to capture these responses and update a Markdown-based display in real-time.\n",
       "\n",
       "**Best Practices**\n",
       "\n",
       "Here are some observations about the code's readability, maintainability, and overall quality:\n",
       "\n",
       "*   **Docstrings**: The docstring provides sufficient context for users who might need to understand this function. However, it would be ideal to include more detailed information about the expected input formats (e.g., `stream` type), error handling specifics, or any specific requirements related to display updates.\n",
       "*   **Variable naming**: Variable names like `response_parts`, `handle_display`, and `display_id` could be improved with more descriptive names. Consider using a consistent naming convention throughout the function, such as following PEP 8's guidance for variable names in Python.\n",
       "*   **Error handling**: While the code catches and handles the two types of exceptions that might occur during chunk processing (`IndexError` and `AttributeError`), it simply prints an error message to the console. Consider logging or providing more informative error messages, potentially allowing for additional debugging information if needed.\n",
       "*   **Code organization**: This function performs both data extraction and display updates. Break down this logic into separate functions to improve readability and maintainability.\n",
       "\n",
       "### Suggested Improvements and Alternative Implementations\n",
       "\n",
       "Here are some code improvements:\n",
       "\n",
       "*   **Process chunk extraction more robustly**\n",
       "\n",
       "    ```python\n",
       "def process_chunk(chunk):\n",
       "    \"\"\"Extracts content from a stream chunk\"\"\"\n",
       "    return chunk.choices[0].delta.content or ''\n",
       "```\n",
       "\n",
       "*   **Create separate functions for setup and display updates**\n",
       "\n",
       "    ```python\n",
       "def prepare_response_display():\n",
       "    \"\"\"Sets up the Markdown response display.\"\"\"\n",
       "    handle_display = display(Markdown, display_id=True)\n",
       "    return handle_display\n",
       "```\n",
       "\n",
       "    ```python\n",
       "def update_markdown_display(text, display_id):\n",
       "    \"\"\"Updates the Markdown display with given text.\"\"\"\n",
       "    # Update logic goes here\n",
       "    pass\n",
       "```\n",
       "\n",
       "*   **Reconsider the approach for displaying updates**\n",
       "\n",
       "    Instead of directly creating and updating a Markdown display in-place, consider using HTML elements to render dynamic content, and update those elements as needed.\n",
       "\n",
       "Here's an updated, refactored version of your code with these suggestions integrated:\n",
       "\n",
       "```python\n",
       "# Setup function\n",
       "def prepare_response_display():\n",
       "    \"\"\"Sets up the Markdown response display.\"\"\"\n",
       "    handle_display = display(Markdown, display_id=True)\n",
       "    return handle_display\n",
       "\n",
       "def stream_response(stream):\n",
       "    \"\"\"Processes a Stream object and updates a Markdown display dynamically.\"\"\"\n",
       "    \n",
       "    response_parts: List[str] = []\n",
       "\n",
       "    for chunk in stream:\n",
       "        try:\n",
       "            content = process_chunk(chunk)\n",
       "            response_parts.append(content)\n",
       "        except (IndexError, AttributeError) as error:\n",
       "            print(f\"Error processing chunk: {str(error)}\")\n",
       "\n",
       "    # Update the display\n",
       "    full_response_text = ''.join(response_parts)\n",
       "    update_markdown_display(full_response_text, handle_display.display_id)\n",
       "\n",
       "def process_chunk(chunk):\n",
       "    \"\"\"Extracts content from a stream chunk\"\"\"\n",
       "    return chunk.choices[0].delta.content or ''\n",
       "\n",
       "def update_markdown_display(text: str, display_id):\n",
       "    # Update logic goes here\n",
       "    pass\n",
       "```\n",
       "\n",
       "Note that the `display` function and `update_display` functions are hypothetical since they were not included in your original code snippet. The refactored version assumes their existence, but you should implement these functions according to your application's specific requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OLLAMA_API_KEY')\n",
    "base_url= os.getenv('OLLAMA_BASE_URL')\n",
    "print(\"base_url:\", repr(base_url))\n",
    "print(\"model:\", MODEL_LLAMA)\n",
    "openai = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "stream_response(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
