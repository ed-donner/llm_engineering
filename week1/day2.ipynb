{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to the Day 2 Lab!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get started --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffe36f",
   "metadata": {},
   "source": [
    "## First - let's talk about the Chat Completions API\n",
    "\n",
    "1. The simplest way to call an LLM\n",
    "2. It's called Chat Completions because it's saying: \"here is a conversation, please predict what should come next\"\n",
    "3. The Chat Completions API was invented by OpenAI, but it's so popular that everybody uses it!\n",
    "\n",
    "### We will start by calling OpenAI again - but don't worry non-OpenAI people, your time is coming!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38f17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846274",
   "metadata": {},
   "source": [
    "## Do you know what an Endpoint is?\n",
    "\n",
    "If not, please review the Technical Foundations guide in the guides folder\n",
    "\n",
    "And, here is an endpoint that might interest you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af5c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-5-nano',\n",
       " 'messages': [{'role': 'user', 'content': 'Tell me a fun fact'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact\"}]\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0ab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Ctkqt7vzMXJzJiQfVrcxQdY0sUhSO',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1767404175,\n",
       " 'model': 'gpt-5-nano-2025-08-07',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Fun fact: Bananas are berries, but strawberries aren’t. Botanically, a berry comes from a single ovary; bananas fit that, while strawberries are made from multiple ovaries of a single flower. Want another fun fact?',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 11,\n",
       "  'completion_tokens': 1080,\n",
       "  'total_tokens': 1091,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 1024,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb11a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: Bananas are berries, but strawberries aren’t. Botanically, a berry comes from a single ovary; bananas fit that, while strawberries are made from multiple ovaries of a single flower. Want another fun fact?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3026a",
   "metadata": {},
   "source": [
    "# What is the openai package?\n",
    "\n",
    "It's known as a Python Client Library.\n",
    "\n",
    "It's nothing more than a wrapper around making this exact call to the http endpoint.\n",
    "\n",
    "It just allows you to work with nice Python code instead of messing around with janky json objects.\n",
    "\n",
    "But that's it. It's open-source and lightweight. Some people think it contains OpenAI model code - it doesn't!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490fdf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: Bananas are technically berries. In botany, a berry is a fruit from a single ovary with seeds embedded in the flesh. Bananas fit that, but strawberries aren’t berries at all—they’re aggregate fruits made from many tiny fruits (achenes) on the outside of a single center.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create OpenAI client\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7739cda",
   "metadata": {},
   "source": [
    "## And then this great thing happened:\n",
    "\n",
    "OpenAI's Chat Completions API was so popular, that the other model providers created endpoints that are identical.\n",
    "\n",
    "They are known as the \"OpenAI Compatible Endpoints\".\n",
    "\n",
    "For example, google made one here: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "\n",
    "And OpenAI decided to be kind: they said, hey, you can just use the same client library that we made for GPT. We'll allow you to specify a different endpoint URL and a different key, to use another provider.\n",
    "\n",
    "So you can use:\n",
    "\n",
    "```python\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=\"AIz....\")\n",
    "gemini.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "And to be clear - even though OpenAI is in the code, we're only using this lightweight python client library to call the endpoint - there's no OpenAI model involved here.\n",
    "\n",
    "If you're confused, please review Guide 9 in the Guides folder!\n",
    "\n",
    "And now let's try it!\n",
    "\n",
    "## THIS IS OPTIONAL - but if you wish to try out Google Gemini, please visit:\n",
    "\n",
    "https://aistudio.google.com/\n",
    "\n",
    "And set up your API key at\n",
    "\n",
    "https://aistudio.google.com/api-keys\n",
    "\n",
    "And then add your key to the `.env` file, being sure to Save the .env file after you change it:\n",
    "\n",
    "`GOOGLE_API_KEY=AIz...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74293bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\n"
     ]
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272432",
   "metadata": {},
   "source": [
    "## And Ollama also gives an OpenAI compatible endpoint\n",
    "\n",
    "...and it's on your local machine!\n",
    "\n",
    "If the next cell doesn't print \"Ollama is running\" then please open a terminal and run `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06280ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3807",
   "metadata": {},
   "source": [
    "### Download llama3.2 from meta\n",
    "\n",
    "Change this to llama3.2:1b if your computer is smaller.\n",
    "\n",
    "Don't use llama3.3 or llama4! They are too big for your computer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e633481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9419762",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2456cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s one:\\n\\nDid you know that there is a species of jellyfish that is immortal? The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" is a type of jellyfish that can transform its body into a younger state through a process called transdifferentiation. This means it can essentially revert back to its polyp stage and start its life cycle all over again!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a fun fact\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try deepseek-r1:1.5b - this is DeepSeek \"distilled\" into Qwen from Alibaba Cloud\n",
    "\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25002f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87012b",
   "metadata": {},
   "source": [
    "Scrape the website and put it in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import fetch_website_contents \n",
    "from IPython.display import Markdown, display\n",
    "website = fetch_website_contents(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4882b0",
   "metadata": {},
   "source": [
    "Option 1: Request a new Summary straight from the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa717050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:**\n",
      "\n",
      "The website belongs to Edward Donner, an entrepreneur and developer who is passionate about artificial intelligence (AI). As the co-founder and CTO of Nebula.io, he aims to apply AI in a significant way by helping people discover their potential. The company has created a platform that uses proprietary Large Language Models (LLMs) for talent management, which has received praise and press coverage. Edward shares his interests and expertise through various online channels, including writing code, DJing, and discussing AI-related topics on platforms like Hacker News.\n",
      "\n",
      "**Translation to Spanish (Latin American):**\n",
      "\n",
      "**Bienvenidos a mi hogar**\n",
      "\n",
      "Esto es\n",
      "\n",
      "*Conecta Cuatro*\n",
      "*Sorpréndete*\n",
      "\n",
      "Un espacio donde las inteligencias artificiales se enfrentan entre sí en un combate de diplomacia y astucia\n",
      "\n",
      "**Sobre mí**\n",
      "\n",
      "Hola, soy Ed. Me gusta escribir código y experimentar con LLMs, espero que tú también te gusten. También disfruto mezclando música electrónica (pero estoy muy fuera de práctica) y perdíneas en [Hacker News](hacker news), asintiendo con la cabeza hacia las cosas que solo entiendo a media.\n",
      "\n",
      "Soy el co-fundador y CTO de [Nebula.io](https://nebula.io/). Estamos aplicando el AI a un campo donde puede tener un impacto positivo masivo: ayudar a las personas a descubrir su potencial y perseguir su razón de ser. Reclutadoras utilizan nuestro producto hoy para encontrar, comprender, interactuar con y gestionar talentos.\n",
      "\n",
      "Soy previamente el fundador y CEO de la startup AI [Untapt](https://untapt.io/), que fue adquirida en 2021.\n",
      "\n",
      "Trabajamos con LLMs revolucionarios, verticalizados para la Talentación, hemos patenteado nuestro modelo de clasificación y tenemos una plataforma ganadora del premio y una gran cobertura de prensa.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Summarize the following website and translate it to spanish (Latin American): \" + website}])\n",
    "\n",
    "print(response.choices[0].message.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b9c9a",
   "metadata": {},
   "source": [
    "Option 2: Define System and User Prompts, and create a function to call the prompts, and use markdown for the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c59aeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Resumen del sitio web de Edward Donner**\n",
       "\n",
       "El sitio web se centra en la aplicación de la Inteligencia Artificial (IA) para ayudar a las personas a descubrir su potencial y perseguir su razón de ser. Ed Donner es el creador y CEO de Nebula.io, una startup que utiliza LLMs (Modelos de Lenguaje Basados en Máquina) para ayudar a los reclutadores a encontrar y gestionar talento.\n",
       "\n",
       "El sitio ofrece diferentes recursos y contenido relacionado con la IA, como artículos de blog, newsletters y información sobre eventos y conferencias. También está presente inmediatamente en algunos canales sociales: LinkedIn, Twitter y Facebook.\n",
       "\n",
       "* **Misión:** Aplicar la IA para ayudar a las personas a descubrir su potencial y perseguir su razón de ser.\n",
       "* **Productos/ Servicios:** Nebula.io, una plataforma que utiliza LLMs para ayudar a los reclutadores a encontrar y gestionar talento.\n",
       "* **Noticias Recientes:**\n",
       " + Fecha de lanzamiento del AI Curriculum (18 de mayo de 2025).\n",
       " + Participación en el AI Engineering MLOps Track – Deploy AI to Production el 28 de mayo de 2025.\n",
       " + El evento más único de un evento vivo de IA, The Unique Energy of an AI Live Event, el 11 de noviembre de 2025.\n",
       "\n",
       "Este sitio web destaca la importancia de aplicar la IA para mejorar vidas y alcanzar objetivos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that summarizes and translates websites to spanish (Latin American).\n",
    "Always respond in markdown. \n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Summarize the following website: \n",
    "\"\"\"\n",
    "def summarize_website(website):\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt + website}])\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "summary = summarize_website(website)\n",
    "display(Markdown(summary))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
