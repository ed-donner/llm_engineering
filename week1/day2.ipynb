{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to the Day 2 Lab!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get started --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffe36f",
   "metadata": {},
   "source": [
    "## First - let's talk about the Chat Completions API\n",
    "\n",
    "1. The simplest way to call an LLM\n",
    "2. It's called Chat Completions because it's saying: \"here is a conversation, please predict what should come next\"\n",
    "3. The Chat Completions API was invented by OpenAI, but it's so popular that everybody uses it!\n",
    "\n",
    "### We will start by calling OpenAI again - but don't worry non-OpenAI people, your time is coming!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
<<<<<<< HEAD
    "api_key = os.getenv('OPENAI_API_KEY')\n",
=======
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
>>>>>>> parent of 2b087bc9 (Revert "Merge branch 'intel-env' into community-contributions-week4")
    "\n",
    "\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-or-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846274",
   "metadata": {},
   "source": [
    "## Do you know what an Endpoint is?\n",
    "\n",
    "If not, please review the Technical Foundations guide in the guides folder\n",
    "\n",
    "And, here is an endpoint that might interest you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af5c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'openai/gpt-5-nano',\n",
       " 'messages': [{'role': 'user', 'content': 'Tell me a fun fact'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"openai/gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact\"}]\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0ab242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'gen-1771834690-jsasLrWyGkApuyQrKy6y',\n",
       " 'provider': 'OpenAI',\n",
       " 'model': 'openai/gpt-5-nano',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1771834690,\n",
       " 'choices': [{'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'native_finish_reason': 'completed',\n",
       "   'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Fun fact: Octopuses have three hearts. Two pump blood through the gills, while the third pumps it to the rest of the body. When an octopus swims, the systemic heart activity drops, which is why they often prefer crawling. Want another fun fact on a different topic?',\n",
       "    'refusal': None,\n",
       "    'reasoning': '**Providing fun facts**\\n\\nI need to respond to the user with a fun fact since they asked for one. It‚Äôs a chance to share something light and interesting! I‚Äôm thinking of a few intriguing options, like ‚ÄúA group of flamingos is called a \\'flamboyance,\\'‚Äù or ‚ÄúDid you know honey never spoils?‚Äù I could even mention that ‚ÄúBananas are berries, but strawberries aren\\'t.‚Äù Since they want a single fact, I might give one or three choices and see what they prefer!**Sharing a fun fact**\\n\\nI should give the user a fun fact and maybe offer more if they want different topics. I think \"Octopuses have three hearts\" is a great choice! To add a little context, I could explain that two of the hearts pump to the gills, while one pumps to the rest of the body. It‚Äôs interesting that when an octopus swims, the heart sending blood to the body stops, which is why they prefer to crawl. I‚Äôm double-checking for accuracy!**Clarifying octopus heart function**\\n\\nThere‚Äôs a common claim that when an octopus swims, the systemic heart stops beating, while the two branchial hearts keep pumping. I recall that it\\'s actually the heart circulating blood to the body that stops, leading to exhaustion. The two branchial hearts pump blood to the gills, while the systemic heart sends it to the rest of the body. It\\'s important to note that when swimming, the systemic heart reduces its activity, making swimming energy-intensive. I want to ensure this information is accurate!**Finalizing the octopus fun fact**\\n\\nI realize the claim about the octopus heart might be an oversimplification, but it works as a fun fact. There\\'s also the option of saying, ‚ÄúBananas are berries, but strawberries aren\\'t,‚Äù which is a neat one-liner. However, I\\'ll go with the octopus fact, presented clearly: ‚ÄúDid you know octopuses have three hearts? Two pump blood through the gills, and one pumps it to the rest of the body. When swimming, the systemic heart\\'s activity drops, causing them to tire quickly and prefer crawling.‚Äù That sounds good!**Inviting topic preferences**\\n\\nI should offer the user more options by saying, ‚ÄúIf you\\'d like a fact about a different topic, like space, nature, or tech, just let me know!‚Äù I want to make sure to include that invite clearly and concisely at the end of my response. It‚Äôs a friendly way to encourage them to engage further. Now, I\\'ll prepare my final answer, keeping it succinct while inviting their preferences!',\n",
       "    'reasoning_details': [{'format': 'openai-responses-v1',\n",
       "      'index': 0,\n",
       "      'type': 'reasoning.summary',\n",
       "      'summary': '**Providing fun facts**\\n\\nI need to respond to the user with a fun fact since they asked for one. It‚Äôs a chance to share something light and interesting! I‚Äôm thinking of a few intriguing options, like ‚ÄúA group of flamingos is called a \\'flamboyance,\\'‚Äù or ‚ÄúDid you know honey never spoils?‚Äù I could even mention that ‚ÄúBananas are berries, but strawberries aren\\'t.‚Äù Since they want a single fact, I might give one or three choices and see what they prefer!**Sharing a fun fact**\\n\\nI should give the user a fun fact and maybe offer more if they want different topics. I think \"Octopuses have three hearts\" is a great choice! To add a little context, I could explain that two of the hearts pump to the gills, while one pumps to the rest of the body. It‚Äôs interesting that when an octopus swims, the heart sending blood to the body stops, which is why they prefer to crawl. I‚Äôm double-checking for accuracy!**Clarifying octopus heart function**\\n\\nThere‚Äôs a common claim that when an octopus swims, the systemic heart stops beating, while the two branchial hearts keep pumping. I recall that it\\'s actually the heart circulating blood to the body that stops, leading to exhaustion. The two branchial hearts pump blood to the gills, while the systemic heart sends it to the rest of the body. It\\'s important to note that when swimming, the systemic heart reduces its activity, making swimming energy-intensive. I want to ensure this information is accurate!**Finalizing the octopus fun fact**\\n\\nI realize the claim about the octopus heart might be an oversimplification, but it works as a fun fact. There\\'s also the option of saying, ‚ÄúBananas are berries, but strawberries aren\\'t,‚Äù which is a neat one-liner. However, I\\'ll go with the octopus fact, presented clearly: ‚ÄúDid you know octopuses have three hearts? Two pump blood through the gills, and one pumps it to the rest of the body. When swimming, the systemic heart\\'s activity drops, causing them to tire quickly and prefer crawling.‚Äù That sounds good!**Inviting topic preferences**\\n\\nI should offer the user more options by saying, ‚ÄúIf you\\'d like a fact about a different topic, like space, nature, or tech, just let me know!‚Äù I want to make sure to include that invite clearly and concisely at the end of my response. It‚Äôs a friendly way to encourage them to engage further. Now, I\\'ll prepare my final answer, keeping it succinct while inviting their preferences!'},\n",
       "     {'id': 'rs_0ab8bc7dee3c6fa301699c0d43180881958bb5f7e294b4d249',\n",
       "      'format': 'openai-responses-v1',\n",
       "      'index': 0,\n",
       "      'type': 'reasoning.encrypted',\n",
       "      'data': 'gAAAAABpnA1QWKTe2NswkpXLyTtuUpUMsGgtH3F52DJFZezfYxj8lhPK50mDfByqoIHJ6fOxTEx6qU0dBbokhM-q-WCutFejH0hZiaGwNneKNHrqoH-fR9rRYOnzVk_4WpmSZiJehiK1Y81-Gsr7OZleDt4SkQ0eqEwcwuMaQ-6tAkSHWEsYjo4D_hpMkez6CYQINWzb3GmN5e9pgeuQxgE0oItVFpW62IsxOYU3Nop3_v7eMJOUJLpWIg1GsXsjZoMrWLB_k61GLfsp1LeSLCBcQ-jUnj-Y3cstcJUZZA5ucKru1eliWdX5fM7DunzwLaqUSLNHjATyyiLE0Mjoe2myYc0Yc4KA09QzJQ9r_WLAbs_-WydGVzF_UnPl9ZGsOg9ytHLpeTOrGwYWtc8tZuOEEwEGmM_frCVZQNIsNVjHLWhdsN2SqiFBb-xPKFfgpfr3hAoEXwBHCte3romzaTskXUBDY1uRq-_kJ9lOGbf-5MivyWIbM5eMHxDsp5aiQOP9_LP53sAgdg-Nr0mlwOEBpVxQQWmcCUIaNWWId1wHf5YT5ihP6SBYxIJt5KnQqZSswwl9YHDoMKvL7x6KLzcYsjBevlaVLN0lGbJlCGzDSqrj5B_woqwZV8eXFnpc3LUBZIyWTBMT64FoNdxxGB1JUJFat177M9ujlOIDE0g9ZWr5_2w5N86Rv8W0F9vckIePJKDwalDfr_5962rqKN2gvV9BB0SMhDgvlGzuuvSEtQx9z_lPcvPD2RQ73U2JhVkx4yZlnhQRddD2f3hgPiFMmC6_LwaZL8QPdS9xUz0Vrqg0QfWyM9mbaXt0vLdF686eAMRwUkYy8A2py8EgJMJg4j5CwYEZ8Ale8tHZyQ-TZqjRLqPV_CnpS_xWZjupEIy46zbuV_T4PbPpQcatjBB7vvPpV9IRfxv0rOeyct1S1gZ4G0xwFauJGHaL4n81JCnnBREl5Vjw6GaaN1iPSk_y9JwHgRuJHeA7Zpq-nNDRZ2-vo0hWNaa4-Y9suSBLPZuUCrHm5N3Oyu7tBYCt0zxqyIRQVIEPqSKF5457-kCEUbi2rS_4-AHTM-RD9uXkKn4kqJ8FRO8TeMg98yePhew2HvMXog4PMErmPLtD_Ca4yGAtRyJ0fHIqhmBNR0EBOQnqOJoWcuLKE-oLzWGF-kutVMEGPArlAVkHdwAfQITM_Odk3OmvpCE5w9fSydaUIYM_nTBFu4CVqgSRs9akAiP5Te7VGZKQ2MuC5EWTvpC1u4NB2Z96jPug2tGNNAFCb2FOgqoFV7EOK7AgItDeXDkOj0IWXH3KvLpdZ5QOHCoUDF6XpbbJ8q-n_ru3KRC_CEG3fvQgTRZKqwtHBoHpoXtcXfo4EQ9NZUYbK7MngW8OqueRqLKZh9X1A9MVWwPGiq8vPdd_TL_IBh47KTu_UKEGq0y8Bsle9N-bpov4bXenPtLmJ_ofN6EwSBoii8dSj4fJPGfB0RyaTc5Ai8hY7YIMKujIYPx3XTAFKAxgb05xS-lFb-8zFZQF56R0sNk4MJ_Qeh6kl35XqGQjH52w5N33YKsnRUYRvBXzU1Sj79UsOD6OzaA4m0wjudMoq-ypsfmFw1PN5FMx6UA9dh7LCNB8JP3JqzyM08zFsZvUhzizvkxYcQlVEPrlIgMIuOsSflaqaCdBmA_TUfWRwv0LeW0fVSouqPkd4Ky3eg99PUE3NwvJvS9vgDxQTfXqQAorVSuWJZi7UCj997WuES1SaVqXFzFmNlk-_0V2dvwQXrIzWdjcaT9pUNPGNlKN9HYeqzsL13ka5KHNTLt79Uk0o1Z45ND25y4SQv02LTb6yzGG5fnjf2ylfVDwi-tRe2EuKSAZH0Los7S4JGSlEsLHUaix_sn4B9q_Blgt64F0Xt2TgmgDjEj31-QWNQvSKKrekzIXmmpzV8HLOL5QQCLp06Tcu7q1vyiaD-fMkJFaVC0XN6kXqjxXyqOn0rrkMp0FhRgQNjy0zOQvCc08cCaWJcNfyEblKHoQSpshjt9jgkxQEjSRbycZkPbpwNbN7tIwkk4Vg-iQ0xEXh6mfjhlLnp_9AgjPY8ZPYH7tMOfVXpsvBYz8WIa0UUSo92ddmpkUF_Nk7FVK6iMXzlnHT3YJlyR4IL1KNMorAQLZGOHvzmbfYy6eQbkW5DhI1RIukWkIj1SoZOcWMnnaMHLprO1FqbNcsN5XP5QHU7ymlUMH5L5XpexJcb_wVzp4adIWeLajh7Uq6v0kVANRYpG61no9W6UarT8TitQn8TuJcB7v4ZMTEEgZDpye5yKXDD68a527gOPd5tcKvbWVP92krbUOrWSj4lt9kG98fFrtrl_2Jd_U33fz3gWraKfpy36Bzwjnoo2_t_jgBWhu3A0yvUbGjWw69v2FB9xmCeq1EfWQpmS_MuprP14xqxEM_KBDN5rCshXBWoTiCNxZqfO44gMnsbOiZ_adBPP1al3Ms6F883rWtDqTASOGppqn-_-0Ht5jIXgktw1WtfJCO_1XYyQKoiB_CKCKmrAJpUeP485R-g6TVSlo5xgbFp7g5lgp-44e40FoNz0SsqyDoHcyPUdIuOZetecg6fctXWlZjsxEt9reFeJ_RfAliqo7abxfJeeRaSkVE8hT480QR9xtXwEe66uh5fYPH3zHrdAV8xnNI0wufgxhaSbr9o-jJYbf17CIJVhy3FJZ0e9M4LUJDS-ki6IweKXlNG9Kd7YG1Qq1jchFityFr2HuCzwYWR_pjQKT58IWrj4tXX_fcpHOA-wD0GHkBKJx9XRyXau9Onz4rIefrYW4c4vn_T3b95_LWUUGj9vaIiqrOth0K7pD3Nyn5etGhag-0u4OL7GdgNVHJQ3AcoQo_wBBczBdxrGa1VDR8C-Civa19J0GZWdzoLEp5GxTKJz-u4VDA95V_tmVkCZyzUDc-hdUpGfbF5BzLgdwt0bmL56svsK5HWjXpGlnxl9389OGyu2YlFBl5tNS01tgvaYnDHOFlrnVMFImbsyqDb76_d8svO550ioUVzv0uqNSegO2Iv7cXRCifuI657T_2SGhRIXLppt5_WFk_vFNIgPoO1vCOtM0dPQvsWP_8C5GAyjisJfbYgVqUHW62QvWQaMH4zSoNkXwab169PHbik_OPPl905GXu3MlcpZaGCGg3vS_arXGDVrt7L1hPT0LK0oigG_P7F6tk1nMLXXtsMpahvCDBIpU_mbq5KWL7ueNN__xnkrW4NqyRUoo1X4Zl7eIxQZ8ExXESujGeorbJXs5mXKRzONe5OCYQLmlRAqiAyYlSJ9gLqc2Akr93d7WZ6h7KgsT5LpBYNQ7KczoB0RkptDM_iAZoWsEmHrP0Ohmw1D-qj3yFD810qO6Q1A8NgqYA7915auxUQrupkxhICzRu76-AvGg1d4VmvByp5BkJ74g-wTRS80OaRnjQ9CN4fGTeBaxZNPG8v_BII5dyDX1zijBWNPhCn3wzWSzqESFl_0ginqhSPY9o4W-5lhSw7AM_QOfs_r4jOgqxsIYYLsI3tMISID_j-fa5ha6jiSIaVPZNK8lZ2pGQhnwvaBs2LlXN0yHH5i_xFW-di1d7Jlj7SeeVtm4-vzdznXX1E-1hMuWvphA1o_XxGVV1rE35LAYG5GwhCmeveir-iYKQhQesIZ8rHI2CXxa3rVFd7BUB7tH8z5Ne8YVvHNxwjO9hoQBowUmEgNEJBe1tYOiO0egFwviG6spaEfKtSz4KFHP8c4vf3oUT_AR8Ib_EhBrQWwj-9Nti_XmPLbnVDVL0oxmjSFOHsBnQKbzKfkQsyAl06ugfnV3tnAQ-L_x_hDboNNNmQ7rUiT0ZADhYKvNSgqZc2bNZZJXQtu0_IdkuYhyAk_coAhRAmoZm22ZmnkPm8Nf3BFIgp41m0FRe6YJdscMC9fJ-F3rcw0WEFYPkTcypI07UWbHrDHNxklSequBOzDblDgCCQuH-0Y8A3onMEZ4Q_hjIycCYJxJz9c7SNIoJNzRX2K0jnJXutagrESrgoHZQJ5-T6AW6a9orQRfkrZUXrFu078TzFZ3DJJNbTY4nwjFzFZTINpPx__oHOcAVMkZaX12oo-_UVIv1UFDv4hu_ZP966W7nr3892ZbxRZRr0PzpthqaM1vQXpYDSdGmrQF0CQIjQwyP5_0M5eHk7qCNZvIcR3UWbHgGptxvEvlqHTTz7nUvufTeAYmE0cFUpqrfIOshAHzJym1lrQXLUy0TgCIKE2-QTrmCBNAne6UVxMvBa1Z9NR3cFEK6X7yzQxk1UPEuZ7Dq-e2KgsBNrUYqFbf99QxQ7WmCK7BZgvEY44lXcDrH5Sjr4bVDXrA0YZG3MsEYZQoG2uBdQGyLty2kLzHjiQWwxad9t9cj_Re1Ub411c7cRvR_OcSSsj_H8XgGluT7K_J_RYafFJeFyJj1LvF45GBCJiMGT_NOi4Visz-NA7UIDC3jmPAvLHw_JF4bvapZZtiw0TdHhbA1HpfU1SZndiKInBL_dD__WiKsG69Y11l-ymmzdCCyYFmSyCe3C96EJOa_B8lR8VXkSn6TaV_ppib49hiSVomk4lHJfC9qU4uLPPYHc0HuoyARBfVA8zTCyJaqqswYtee46QKklJgPqjm5dyrpL86pIue0_ZFz4hx57ivqhENVNsrcDCfvoI1EbmeiXuVm8rrhp8Lujd91BfqfHUU9eS6fXwzd5pPiAEop8xWToxP2bPlPn46IR_IHlF71ew6pfkU4VXkd6da8dmZqUxNY06Uz6glv8NWH9OlE6xW8da9N_Wvf3M3mBXO_MQ-ri3EkcWwSTBCdJneE8zGtgJM-AuFVYZ_58rw6rAwfFoUb6Op8eGM__YP0cJ22tOJO6SzAR5_Hvbk9cDc03FvtHjzhjU7RRj9_yiuTjKi6v0605b1gYNiOALU8pj5yRnRnCWkfNc4OZYP2KtbkgdIelunt484yZPRSdmGe235t_Jb3FchiY1CqSKwzbAXBbHBdsx5IsAW0AKXREpEJRFaMamm-ni3Os6NTl0zz6enta-NyHfHTpJMn1XcsYjbDbjxNAnINyKXusb4L-i9OyEzazz4AgayhzRbvtg6yNp3Xt4Tjr8mBUb44ljpLyVf5EWyC-O6-L6795l97pUhppNqyMh1ePozfRmtcOveRcXTZooZzK9DbvL-9vWqEWtItvljv8YVQbOeZbU2uJnIKwb9GRgipN5cSBW4f63004RsX2GuxyClCsaQxuf7inUfb1E7psj-IRWQ06tTJmKBEu2XAeTfhmajn4rt15Cr4sZ4B5YgLqT8MhUGNQ4u0B3osHxjwEhDUitgHkECc4yVooYQmt4PEARJY5fpvbO6vCaEz1dbF3E-k1xNnBTRKVxBXX79xGcjU21mIPzsf-1gtB8ug9Gpk-NKdubakpL0-jNCJMt9-Lm9qVUArrloI8HPkcCDa4JmS-QVtOdpOEv3-SiynxM94W0Bj5Xnr_M07cajkwYOtSb0T38MjAvjA60G5CVul8Vo6FHVLDbwHdD4R8Vei8jyknUDBkPvReod7chCtZdwJdFmXK7am6p9lbrZd9sY0p44qf07H2exRlBJxRswAGok40eMrkb9D5EwBBdPUTuzGCbPRRx1xLoxD6C9gJ-miPsLU_L0DV_i2gkkVDp-'}]}}],\n",
       " 'usage': {'prompt_tokens': 11,\n",
       "  'completion_tokens': 853,\n",
       "  'total_tokens': 864,\n",
       "  'cost': 0.00034175,\n",
       "  'is_byok': False,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0},\n",
       "  'cost_details': {'upstream_inference_cost': 0.00034175,\n",
       "   'upstream_inference_prompt_cost': 5.5e-07,\n",
       "   'upstream_inference_completions_cost': 0.0003412},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 768, 'image_tokens': 0}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb11a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: Octopuses have three hearts. Two pump blood through the gills, while the third pumps it to the rest of the body. When an octopus swims, the systemic heart activity drops, which is why they often prefer crawling. Want another fun fact on a different topic?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3026a",
   "metadata": {},
   "source": [
    "# What is the openai package?\n",
    "\n",
    "It's known as a Python Client Library.\n",
    "\n",
    "It's nothing more than a wrapper around making this exact call to the http endpoint.\n",
    "\n",
    "It just allows you to work with nice Python code instead of messing around with janky json objects.\n",
    "\n",
    "But that's it. It's open-source and lightweight. Some people think it contains OpenAI model code - it doesn't!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490fdf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fun fact: A group of flamingos is called a flamboyance. If you want another fun fact, I can share more!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create OpenAI client\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI(\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = api_key\n",
    ")\n",
    "\n",
    "response = openai.chat.completions.create(model=\"openai/gpt-5-nano\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7739cda",
   "metadata": {},
   "source": [
    "## And then this great thing happened:\n",
    "\n",
    "OpenAI's Chat Completions API was so popular, that the other model providers created endpoints that are identical.\n",
    "\n",
    "They are known as the \"OpenAI Compatible Endpoints\".\n",
    "\n",
    "For example, google made one here: https://generativelanguage.googleapis.com/v1beta/openai/\n",
    "\n",
    "And OpenAI decided to be kind: they said, hey, you can just use the same client library that we made for GPT. We'll allow you to specify a different endpoint URL and a different key, to use another provider.\n",
    "\n",
    "So you can use:\n",
    "\n",
    "```python\n",
    "gemini = OpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\", api_key=\"AIz....\")\n",
    "gemini.chat.completions.create(...)\n",
    "```\n",
    "\n",
    "And to be clear - even though OpenAI is in the code, we're only using this lightweight python client library to call the endpoint - there's no OpenAI model involved here.\n",
    "\n",
    "If you're confused, please review Guide 9 in the Guides folder!\n",
    "\n",
    "And now let's try it!\n",
    "\n",
    "## THIS IS OPTIONAL - but if you wish to try out Google Gemini, please visit:\n",
    "\n",
    "https://aistudio.google.com/\n",
    "\n",
    "And set up your API key at\n",
    "\n",
    "https://aistudio.google.com/api-keys\n",
    "\n",
    "And then add your key to the `.env` file, being sure to Save the .env file after you change it:\n",
    "\n",
    "`GOOGLE_API_KEY=AIz...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74293bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d060f484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did you know that a group of pugs is called a **\"grumble\"**?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272432",
   "metadata": {},
   "source": [
    "## And Ollama also gives an OpenAI compatible endpoint\n",
    "\n",
    "...and it's on your local machine!\n",
    "\n",
    "If the next cell doesn't print \"Ollama is running\" then please open a terminal and run `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06280ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef3807",
   "metadata": {},
   "source": [
    "### Download llama3.2 from meta\n",
    "\n",
    "Change this to llama3.2:1b if your computer is smaller.\n",
    "\n",
    "Don't use llama3.3 or llama4! They are too big for your computer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e633481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feb 16 2026 22:18:12 - ERROR - generated.c:2199 - CHECK failed: mlx_array_item_float16_\n",
      "2026/02/23 11:34:44 ERROR Failed to load MLX dynamic library symbols path=/Applications/Ollama.app/Contents/Resources/libmlxc.dylib\n",
      "2026/02/23 11:34:44 WARN MLX dynamic library not available error=\"failed to load MLX dynamic library (searched: [/Applications/Ollama.app/Contents/Resources /Users/johnmboga/Documents/Applications/Andela AI Bootcamp/llm_engineering/week1/build/lib/ollama])\"\n",
      "\u001b]11;?\u001b\\\u001b[6n\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9419762",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2456cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s one:\\n\\nDid you know that there is a species of jellyfish that is immortal?! The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can transform its body into a younger state through a process called transdifferentiation. This means it can essentially revert back to its polyp stage and grow back into an adult again, allowing it to live indefinitely!\\n\\nIsn\\'t that just mind-blowingly cool?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a fun fact\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try deepseek-r1:1.5b - this is DeepSeek \"distilled\" into Qwen from Alibaba Cloud\n",
    "\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25002f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's fun about reading?! üòä You could just hear the rustling of leaves or imagine discovering something extraordinary on another planet! üåã‚ú®\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import fetch_website_content\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = \"https://www.cdc.gov/sickle-cell/about/index.html\"\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e7bed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a resourceful assistant that analyzes the contents of a website,\\nand provides a clear and easy to follow summary, ignoring text that might be navigation related.\\nRespond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a resourceful assistant that analyzes the contents of a website,\n",
    "and provides a clear and easy to follow summary, ignoring text that might be navigation related.\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccd0269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere are the contents of a website.\\nProvide a short summary of this website.\\nIf it includes news or announcements, then summarize these too.\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our user prompt\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "\n",
    "\"\"\"\n",
    "user_prompt_prefix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a57c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + website}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27396d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nYou are a resourceful assistant that analyzes the contents of a website,\\nand provides a clear and easy to follow summary, ignoring text that might be navigation related.\\nRespond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nHere are the contents of a website.\\nProvide a short summary of this website.\\nIf it includes news or announcements, then summarize these too.\\n\\nhttps://medium.com/medium-handbook/how-this-data-analyst-uses-medium-to-build-his-writing-portfolio-9cbbc710d314'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2959163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = fetch_website_content(url)\n",
    "    print(website)\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6630da3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Snarky Summary**\\nThis is an official US government website, because you need to trust the government with your search queries. Looks like they published a article about sickle cell disease, which sounds super fun (said no one ever). The author has some credentials that seem legit, and it's all written in a pretty academic tone. Probably not super exciting, but hey, if you're into medical research, this might be for you...\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a58be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d4af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
