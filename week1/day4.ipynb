{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e61417",
   "metadata": {},
   "source": [
    "# Day 4\n",
    "\n",
    "## Tokenizing with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc1c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\st\\documents\\kurs_przyszły_programista\\llm_engineering\\llm_engineering\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "tokens = encoding.encode(\"I looooove youuuuuu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7632966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1445, 61341, 1048, 481, 9084, 176972]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce0c188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 = I\n",
      "1445 =  lo\n",
      "61341 = ooo\n",
      "1048 = ove\n",
      "481 =  you\n",
      "9084 = uu\n",
      "176972 = uuu\n"
     ]
    }
   ],
   "source": [
    "for token_id in tokens:\n",
    "    token_text = encoding.decode([token_id])\n",
    "    print(f\"{token_id} = {token_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e3bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "538efe61",
   "metadata": {},
   "source": [
    "# And another topic!\n",
    "\n",
    "### The Illusion of \"memory\"\n",
    "\n",
    "Many of you will know this already. But for those that don't -- this might be an \"AHA\" moment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a4b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618859b",
   "metadata": {},
   "source": [
    "### You should be very comfortable with what the next cell is doing!\n",
    "\n",
    "_I'm creating a new instance of the OpenAI Python Client library, a lightweight wrapper around making HTTP calls to an endpoint for calling the GPT LLM, or other LLM providers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b959be3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m openai = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\kurs_Przyszły_Programista\\LLM_Engineering\\llm_engineering\\venv\\Lib\\site-packages\\openai\\_client.py:137\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    135\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa889e80",
   "metadata": {},
   "source": [
    "### A message to OpenAI is a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97298fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Natalia\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3475a36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Natalia! It's lovely to meet you. How can I assist you today? Do you have any questions or topics you'd like to discuss? I'm all ears!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f45ed8",
   "metadata": {},
   "source": [
    "### OK let's now ask a follow-up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bce2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404462f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your identity, but I can try to help you figure it out. Can you please tell me something about yourself, like where you're from or what you're interested in? That might give me some clues about who you are!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098237ef",
   "metadata": {},
   "source": [
    "### Wait, wha??\n",
    "\n",
    "We just told you!\n",
    "\n",
    "What's going on??\n",
    "\n",
    "Here's the thing: every call to an LLM is completely STATELESS. It's a totally new call, every single time. As AI engineers, it's OUR JOB to devise techniques to give the impression that the LLM has a \"memory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d43f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Natalia!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Natalia! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7ac742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Natalia. (I think it might not be the case, though - we just started our conversation!)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c49557",
   "metadata": {},
   "source": [
    "## To recap\n",
    "\n",
    "With apologies if this is obvious to you - but it's still good to reinforce:\n",
    "\n",
    "1. Every call to an LLM is stateless\n",
    "2. We pass in the entire conversation so far in the input prompt, every time\n",
    "3. This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a trick; it's a by-product of providing the entire conversation, every time\n",
    "5. An LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Ed\" and later \"What's my name?\" then it will predict.. Ed!\n",
    "\n",
    "The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in.\n",
    "\n",
    "\"Does that mean we have to pay extra each time for all the conversation so far\"\n",
    "\n",
    "For sure it does. And that's what we WANT. We want the LLM to predict the next tokens in the sequence, looking back on the entire conversation. We want that compute to happen, so we need to pay the electricity bill for it!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
