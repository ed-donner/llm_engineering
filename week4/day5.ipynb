{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: OPTIONAL to execute C++ code or Rust code</h2>\n",
    "            <span style=\"color:#f71;\">As an alternative, you can run it on the website given yesterday</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use high end models GPT 5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Grok 4, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please pick lower cost models like gpt-5-nano.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
    "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-5\", \"claude-sonnet-4-5-20250929\", \"gemini-2.5-pro\", \"glm-4.7:cloud\", \"deepseek-v3.2:cloud\"]\n",
    "\n",
    "clients = {\"gpt-5\": openai, \"claude-sonnet-4-5-20250929\": anthropic, \"gemini-2.5-pro\": gemini, \"glm-4.7:cloud\": ollama, \"deepseek-v3.2:cloud\": ollama}\n",
    "\n",
    "# Want to keep costs ultra-low? Replace this with models of your choice, using the examples from yesterday\n",
    "\n",
    "# Test DeepSeek V3.2 Cloud connection (optional - uncomment to test)\n",
    "test_response = ollama.chat.completions.create(\n",
    "    model=\"deepseek-v3.2:cloud\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n",
    "    max_tokens=50\n",
    ")\n",
    "print(\"\u2705 DeepSeek V3.2 Cloud is working!\")\n",
    "print(test_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_info import retrieve_system_info, rust_toolchain_info\n",
    "import os\n",
    "\n",
    "# Add Rust to PATH so rust_toolchain_info() can find it\n",
    "cargo_bin = os.path.expanduser(\"~/.cargo/bin\")\n",
    "if cargo_bin not in os.environ.get(\"PATH\", \"\"):\n",
    "    os.environ[\"PATH\"] = f\"{cargo_bin}:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "rust_info = rust_toolchain_info()\n",
    "rust_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bd44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Rust compiler to compile a single rust file called main.rs and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install a Rust toolchain to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Rust code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "Have the maximum possible runtime performance in mind; compile time can be slow. Fastest possible runtime performance for this platform is key.\n",
    "Reply with the commands in markdown.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\n",
    "Rust toolchain information:\n",
    "{rust_info}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(model=models[0], messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92c12",
   "metadata": {},
   "source": [
    "## For C++, overwrite this with the commands from yesterday, or for Rust, use the new commands\n",
    "\n",
    "Or just use the website like yesterday:\n",
    "\n",
    " https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d734a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rust compile command for maximum runtime performance\n",
    "# Note: Ensure Rust is in PATH by running: source \"$HOME/.cargo/env\" in your shell\n",
    "# Or use the full path: os.path.expanduser(\"~/.cargo/bin/rustc\")\n",
    "compile_command = [\n",
    "    \"rustc\",\n",
    "    \"-C\", \"opt-level=3\",\n",
    "    \"-C\", \"lto=fat\",\n",
    "    \"-C\", \"codegen-units=1\",\n",
    "    \"-C\", \"target-cpu=native\",\n",
    "    \"-C\", \"panic=abort\",\n",
    "    \"-C\", \"strip=symbols\",\n",
    "    \"-o\", \"main\",\n",
    "    \"main.rs\"\n",
    "]\n",
    "\n",
    "run_command = [\"./main\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a437",
   "metadata": {},
   "source": [
    "## And now, on with the main task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Rust\" # or \"C++\"\n",
    "extension = \"rs\" if language == \"Rust\" else \"cpp\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "Your task is to convert Python code into high performance {language} code.\n",
    "Respond only with {language} code. Do not provide any explanation other than occasional comments.\n",
    "The {language} response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to {language} with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.{language} and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with {language} code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(code):\n",
    "    with open(f\"main.{extension}\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model, python):\n",
    "    client = clients[model]\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```rust','').replace('```','')\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer\n",
    "\n",
    "    try:\n",
    "        exec(code, globals_dict)\n",
    "        output = buffer.getvalue()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commands from GPT 5\n",
    "\n",
    "def compile_and_run(code):\n",
    "    write_output(code)\n",
    "    \n",
    "    # Ensure Rust is in PATH (needed for notebook environments)\n",
    "    import os\n",
    "    env = os.environ.copy()\n",
    "    cargo_bin = os.path.expanduser(\"~/.cargo/bin\")\n",
    "    if cargo_bin not in env.get(\"PATH\", \"\"):\n",
    "        env[\"PATH\"] = f\"{cargo_bin}:{env.get('PATH', '')}\"\n",
    "    \n",
    "    try:\n",
    "        # Compile the Rust code\n",
    "        compile_result = subprocess.run(\n",
    "            compile_command, \n",
    "            check=True, \n",
    "            text=True, \n",
    "            capture_output=True, \n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        # Run the compiled binary\n",
    "        run_result = subprocess.run(\n",
    "            run_command, \n",
    "            check=True, \n",
    "            text=True, \n",
    "            capture_output=True, \n",
    "            env=env\n",
    "        )\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Return both stdout and stderr for better error messages\n",
    "        error_msg = f\"An error occurred:\\n\"\n",
    "        if e.stderr:\n",
    "            error_msg += f\"STDERR: {e.stderr}\\n\"\n",
    "        if e.stdout:\n",
    "            error_msg += f\"STDOUT: {e.stdout}\\n\"\n",
    "        return error_msg\n",
    "    except FileNotFoundError as e:\n",
    "        return f\"Error: Rust compiler not found. Please ensure Rust is installed.\\n{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large numbers\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "465d6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles import CSS\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=gr.themes.Monochrome(), title=f\"Port from Python to {language}\") as ui:\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python = gr.Code(\n",
    "                label=\"Python (original)\",\n",
    "                value=python_hard,\n",
    "                language=\"python\",\n",
    "                lines=26\n",
    "            )\n",
    "        with gr.Column(scale=6):\n",
    "            cpp = gr.Code(\n",
    "                label=f\"{language} (generated)\",\n",
    "                value=\"\",\n",
    "                language=\"cpp\",\n",
    "                lines=26\n",
    "            )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"controls\"]):\n",
    "        python_run = gr.Button(\"Run Python\", elem_classes=[\"run-btn\", \"py\"])\n",
    "        model = gr.Dropdown(models, value=models[0], show_label=False)\n",
    "        convert = gr.Button(f\"Port to {language}\", elem_classes=[\"convert-btn\"])\n",
    "        cpp_run = gr.Button(f\"Run {language}\", elem_classes=[\"run-btn\", \"cpp\"])\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python_out = gr.TextArea(label=\"Python result\", lines=8, elem_classes=[\"py-out\"])\n",
    "        with gr.Column(scale=6):\n",
    "            cpp_out = gr.TextArea(label=f\"{language} result\", lines=8, elem_classes=[\"cpp-out\"])\n",
    "\n",
    "    convert.click(fn=port, inputs=[model, python], outputs=[cpp])\n",
    "    python_run.click(fn=run_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(fn=compile_and_run, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311ada8",
   "metadata": {},
   "source": [
    "## RESULTS!\n",
    "\n",
    "Qwen 2.5 Coder: FAIL  \n",
    "Gemini 2.5 Pro: FAIL  \n",
    "DeepSeek Coder v2: FAIL  \n",
    "Qwen3 Coder 30B: FAIL  \n",
    "Claude Sonnet 4.5: FAIL    \n",
    "GPT-5: FAIL    \n",
    "\n",
    "3rd place: GPT-oss-20B: 0.000341  \n",
    "2nd place: Grok 4: 0.000317  \n",
    "**1st place: OpenAI GPT-OSS 120B: 0.000304**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b51dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In Ed's experimenet, the GPT-OSS 120B model outcome is {33.755209/0.000304:,.0f} times faster than the Python code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc84332",
   "metadata": {},
   "source": [
    "### Quick Check: What's Available in Ollama?\n",
    "\n",
    "### Check Your Current Models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a8b98",
   "metadata": {},
   "source": [
    "### Test Your Current Setup\n",
    "\n",
    "Your notebook is already configured to use `deepseek-v3.2:cloud`. You can test it right now with your existing code! The cloud version works perfectly for most use cases and doesn't require any local hardware.\n",
    "\n",
    "**If you want to try a smaller LOCAL DeepSeek model**, you could use:\n",
    "- `deepseek-coder-v2:latest` (8.9 GB - already installed!)\n",
    "- `deepseek-r1:latest` (4.7 GB - already installed!)\n",
    "\n",
    "These run locally but are smaller models with different capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522c7ca",
   "metadata": {},
   "source": [
    "## \u2705 Pricing & Configuration Status\n",
    "\n",
    "### Is the Cloud Version FREE?\n",
    "\n",
    "**YES!** The `deepseek-v3.2:cloud` model is **FREE** to use through Ollama's cloud service:\n",
    "\n",
    "- **Free Tier:** Unlimited access to cloud models (with reasonable usage limits)\n",
    "- **No Credit Card Needed:** Completely free to use\n",
    "- **Pro/Max Plans:** Available at $20/month and $100/month for higher limits, but free tier is sufficient for most use cases\n",
    "\n",
    "### \u26a0\ufe0f Authentication Required for Cloud Models\n",
    "\n",
    "**IMPORTANT:** Cloud models require you to sign in to Ollama (it's free, just need an account).\n",
    "\n",
    "**To fix authentication errors:**\n",
    "\n",
    "1. **Run this command in your terminal:**\n",
    "   ```bash\n",
    "   ollama signin\n",
    "   ```\n",
    "\n",
    "2. **Visit the URL it provides** (it will look like `https://ollama.com/connect?name=...&key=...`)\n",
    "\n",
    "3. **Sign in or create a free account** on Ollama's website\n",
    "\n",
    "4. **Verify it worked** - try using the model again in your notebook\n",
    "\n",
    "**Alternative:** If you don't want to sign in, you can use local models instead:\n",
    "- `deepseek-coder-v2:latest` (8.9 GB - already installed, runs locally)\n",
    "- `deepseek-r1:latest` (4.7 GB - already installed, runs locally)\n",
    "\n",
    "### Do You Have the Right Configuration?\n",
    "\n",
    "**YES!** Your configuration is **perfectly set up**:\n",
    "\n",
    "\u2705 **Ollama is installed** (version 0.13.5)  \n",
    "\u2705 **Ollama is running** (confirmed via API check)  \n",
    "\u2705 **DeepSeek V3.2 Cloud is installed** (`deepseek-v3.2:cloud`)  \n",
    "\u2705 **Client is configured correctly:**\n",
    "   - Base URL: `http://localhost:11434/v1`\n",
    "   - Model name: `deepseek-v3.2:cloud`\n",
    "   - Client mapping: `{\"deepseek-v3.2:cloud\": ollama}`\n",
    "\n",
    "**Once you sign in, you're all set!** \ud83c\udf89\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb237a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your Ollama sign-in link\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run([\"ollama\", \"signin\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcdd INSTRUCTIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Copy the URL above (starts with https://ollama.com/connect)\")\n",
    "print(\"2. Open it in your browser\")\n",
    "print(\"3. Sign in or create a free Ollama account\")\n",
    "print(\"4. Come back and try using deepseek-v3.2:cloud again!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148725a",
   "metadata": {},
   "source": [
    "## RESULTS - Rust Code Execution Times\n",
    "\n",
    "Ranked from fastest to slowest:\n",
    "\n",
    "**\ud83e\udd47 1st place: Gemini 2.5 Pro** - 0.000684 seconds  \n",
    "**\ud83e\udd48 2nd place: GLM 4.7** - 0.000701 seconds  \n",
    "**\ud83e\udd49 3rd place: Claude Sonnet 4.5** - 0.000910 seconds  \n",
    "**4th place: GPT-5** - 0.001007 seconds  \n",
    "**5th place: DeepSeek V3.2** - 0.525210 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fd342",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}