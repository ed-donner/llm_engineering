{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: OPTIONAL to execute C++ code or Rust code</h2>\n",
    "            <span style=\"color:#f71;\">As an alternative, you can run it on the website given yesterday</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use high end models GPT 5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Grok 4, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please pick lower cost models like gpt-5-nano.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "Grok API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n",
      "OpenRouter API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "import requests\n",
    "requests.get(\"http://localhost:11434\").content\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-5-nano\", \"claude-sonnet-4-5-20250929\", \"grok-4\", \"gemini-2.5-flash-lite\", \"qwen2.5\", \"deepseek\", \"llama3.2\", \"qwen/qwen3-coder-30b-a3b-instruct\", \"openai/gpt-oss-120b\", ]\n",
    "\n",
    "clients = {\"gpt-5-nano\": openai, \"claude-sonnet-4-5-20250929\": anthropic, \"grok-4\": grok, \"gemini-2.5-flash-lite\": gemini, \"openai/gpt-oss-120b\": groq, \"qwen2.5\": ollama, \"deepseek\": ollama, \"llama3.2\": ollama, \"qwen/qwen3-coder-30b-a3b-instruct\": openrouter}\n",
    "\n",
    "# Want to keep costs ultra-low? Replace this with models of your choice, using the examples from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c1f1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'installed': True,\n",
       " 'rustc': {'path': 'C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rustc.EXE',\n",
       "  'version': 'rustc 1.93.0 (254b59607 2026-01-19)',\n",
       "  'host_triple': 'x86_64-pc-windows-msvc',\n",
       "  'release': '1.93.0',\n",
       "  'commit_hash': '254b59607d4417e9dffbc307138ae5c86280fe4c'},\n",
       " 'cargo': {'path': 'C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\cargo.EXE',\n",
       "  'version': 'cargo 1.93.0 (083ac5135 2025-12-15)'},\n",
       " 'rustup': {'path': 'C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rustup.EXE',\n",
       "  'version': 'rustup 1.28.2 (e4f3ad6f8 2025-04-28)',\n",
       "  'active_toolchain': 'stable-x86_64-pc-windows-msvc (default)',\n",
       "  'default_toolchain': '',\n",
       "  'toolchains': ['stable-x86_64-pc-windows-msvc (active, default)'],\n",
       "  'targets_installed': ['x86_64-pc-windows-msvc']},\n",
       " 'rust_analyzer': {'path': 'C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rust-analyzer.EXE'},\n",
       " 'env': {'CARGO_HOME': 'C:\\\\Users\\\\Lenovo\\\\.cargo',\n",
       "  'RUSTUP_HOME': 'C:\\\\Users\\\\Lenovo\\\\.rustup',\n",
       "  'RUSTFLAGS': '',\n",
       "  'CARGO_BUILD_TARGET': ''},\n",
       " 'execution_examples': ['\"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\cargo.EXE\" build',\n",
       "  '\"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\cargo.EXE\" run',\n",
       "  '\"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\cargo.EXE\" test',\n",
       "  '\"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rustc.EXE\" hello.rs -o hello']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from system_info import retrieve_system_info, rust_toolchain_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "rust_info = rust_toolchain_info()\n",
    "rust_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bd44f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer:\n",
       "- You do not need to install anything new. Rust toolchain (rustc, cargo, rustup) is already installed and in use on your system.\n",
       "\n",
       "What you currently have (as you shared):\n",
       "- rustc.EXE at C:\\Users\\Lenovo\\.cargo\\bin\\rustc.EXE\n",
       "- cargo.EXE at C:\\Users\\Lenovo\\.cargo\\bin\\cargo.EXE\n",
       "- Active toolchain: stable-x86_64-pc-windows-msvc (default)\n",
       "\n",
       "Given that, here is the simplest, fastest way to compile a single Rust file (main.rs) and run it, with the highest runtime performance in mind.\n",
       "\n",
       "Step-by-step (manual commands you can run in PowerShell or CMD)\n",
       "- Compile command (aimed at maximum runtime performance; uses native CPU optimizations, single codegen unit, and panic=abort):\n",
       "- Run command (execute the resulting binary)\n",
       "\n",
       "Python equivalents (exact commands to put into your script)\n",
       "\n",
       "- Compile command:\n",
       "```python\n",
       "compile_command = [\n",
       "    \"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rustc.EXE\",\n",
       "    \"main.rs\",\n",
       "    \"-O\",\n",
       "    \"-C\", \"codegen-units=1\",\n",
       "    \"-C\", \"target-cpu=native\",\n",
       "    \"-C\", \"lto=fat\",\n",
       "    \"-C\", \"panic=abort\",\n",
       "    \"-o\", \"main.exe\"\n",
       "]\n",
       "```\n",
       "\n",
       "- Run command:\n",
       "```python\n",
       "run_command = [\"main.exe\"]\n",
       "```\n",
       "\n",
       "Notes and tips\n",
       "- Run the compile command in the directory that contains main.rs. The produced executable will be main.exe in the same directory.\n",
       "- On Windows, you execute the binary with either main.exe (CMD) or .\\main.exe (PowerShell). In Python, using [\"main.exe\"] assumes the current working directory is the one containing main.exe.\n",
       "- If you prefer the simplest setup (and youâ€™re okay with cargo handling), you can put main.rs in a Cargo project and build with cargo build --release, then run the binary at target\\release\\YourProjectName.exe. This is often convenient for larger projects, but for a single file, rustc as shown above is the simplest fastest route to maximum performance.\n",
       "- If you later add dependencies, consider switching to a Cargo-based workflow and use cargo build --release to ensure consistent optimizations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Rust compiler to compile a single rust file called main.rs and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install a Rust toolchain to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Rust code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "Have the maximum possible runtime performance in mind; compile time can be slow. Fastest possible runtime performance for this platform is key.\n",
    "Reply with the commands in markdown.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\n",
    "Rust toolchain information:\n",
    "{rust_info}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(model=models[0], messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92c12",
   "metadata": {},
   "source": [
    "## For C++, overwrite this with the commands from yesterday, or for Rust, use the new commands\n",
    "\n",
    "Or just use the website like yesterday:\n",
    "\n",
    " https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d734a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_command = [\n",
    "    \"C:\\\\Users\\\\Lenovo\\\\.cargo\\\\bin\\\\rustc.EXE\",\n",
    "    \"main.rs\",\n",
    "    \"-O\",\n",
    "    \"-C\", \"codegen-units=1\",\n",
    "    \"-C\", \"target-cpu=native\",\n",
    "    \"-C\", \"lto=fat\",\n",
    "    \"-C\", \"panic=abort\",\n",
    "    \"-o\", \"main.exe\"\n",
    "]\n",
    "run_command = [\"main.exe\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a437",
   "metadata": {},
   "source": [
    "## And now, on with the main task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Rust\" # or \"C++\"\n",
    "extension = \"rs\" if language == \"Rust\" else \"cpp\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "Your task is to convert Python code into high performance {language} code.\n",
    "Respond only with {language} code. Do not provide any explanation other than occasional comments.\n",
    "The {language} response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to {language} with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.{language} and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with {language} code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(code):\n",
    "    with open(f\"main.{extension}\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model, python):\n",
    "    client = clients[model]\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```rust','').replace('```','')\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals_dict = {\"__builtins__\": __builtins__}\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer\n",
    "\n",
    "    try:\n",
    "        exec(code, globals_dict)\n",
    "        output = buffer.getvalue()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commands from GPT 5\n",
    "\n",
    "def compile_and_run(code):\n",
    "    write_output(code)\n",
    "    try:\n",
    "        subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "        run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large numbers\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465d6cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19236\\3949588770.py:3: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme, css. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(css=CSS, theme=gr.themes.Monochrome(), title=f\"Port from Python to {language}\") as ui:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from styles import CSS\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=gr.themes.Monochrome(), title=f\"Port from Python to {language}\") as ui:\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python = gr.Code(\n",
    "                label=\"Python (original)\",\n",
    "                value=python_hard,\n",
    "                language=\"python\",\n",
    "                lines=26\n",
    "            )\n",
    "        with gr.Column(scale=6):\n",
    "            cpp = gr.Code(\n",
    "                label=f\"{language} (generated)\",\n",
    "                value=\"\",\n",
    "                language=\"cpp\",\n",
    "                lines=26\n",
    "            )\n",
    "\n",
    "    with gr.Row(elem_classes=[\"controls\"]):\n",
    "        python_run = gr.Button(\"Run Python\", elem_classes=[\"run-btn\", \"py\"])\n",
    "        model = gr.Dropdown(models, value=models[0], show_label=False)\n",
    "        convert = gr.Button(f\"Port to {language}\", elem_classes=[\"convert-btn\"])\n",
    "        cpp_run = gr.Button(f\"Run {language}\", elem_classes=[\"run-btn\", \"cpp\"])\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=6):\n",
    "            python_out = gr.TextArea(label=\"Python result\", lines=8, elem_classes=[\"py-out\"])\n",
    "        with gr.Column(scale=6):\n",
    "            cpp_out = gr.TextArea(label=f\"{language} result\", lines=8, elem_classes=[\"cpp-out\"])\n",
    "\n",
    "    convert.click(fn=port, inputs=[model, python], outputs=[cpp])\n",
    "    python_run.click(fn=run_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(fn=compile_and_run, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b609be-2cdc-47c6-9ece-55180e8ca97c",
   "metadata": {},
   "source": [
    "## Rust code by gpt\n",
    "```rust\n",
    "use std::time::Instant;\n",
    "\n",
    "const A: u32 = 1664525;\n",
    "const C: u32 = 1013904223;\n",
    "\n",
    "fn max_subarray_sum(n: usize, seed: u32, min_val: i32, max_val: i32) -> i64 {\n",
    "    let range = (max_val - min_val + 1) as u64; // e.g., 21\n",
    "    let mut value: u32 = seed;\n",
    "    let mut best: i64 = i64::MIN;\n",
    "    let mut current: i64 = 0;\n",
    "\n",
    "    for _ in 0..n {\n",
    "        value = value.wrapping_mul(A).wrapping_add(C);\n",
    "        let offset = (value as u64) % range;\n",
    "        let num = (offset as i64) + (min_val as i64);\n",
    "\n",
    "        current += num;\n",
    "        if current > best {\n",
    "            best = current;\n",
    "        }\n",
    "        if current < 0 {\n",
    "            current = 0;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best\n",
    "}\n",
    "\n",
    "fn total_max_subarray_sum(n: usize, initial_seed: u32, min_val: i32, max_val: i32) -> i64 {\n",
    "    let mut value: u32 = initial_seed;\n",
    "    let mut total_sum: i64 = 0;\n",
    "\n",
    "    for _ in 0..20 {\n",
    "        value = value.wrapping_mul(A).wrapping_add(C);\n",
    "        total_sum += max_subarray_sum(n, value, min_val, max_val);\n",
    "    }\n",
    "\n",
    "    total_sum\n",
    "}\n",
    "\n",
    "fn main() {\n",
    "    // Parameters matching the Python snippet\n",
    "    let n: usize = 10_000;\n",
    "    let initial_seed: u32 = 42;\n",
    "    let min_val: i32 = -10;\n",
    "    let max_val: i32 = 10;\n",
    "\n",
    "    let start = Instant::now();\n",
    "    let result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
    "    let duration = start.elapsed();\n",
    "\n",
    "    println!(\"Total Maximum Subarray Sum (20 runs): {}\", result);\n",
    "    println!(\"Execution Time: {:.6} seconds\", duration.as_secs_f64());\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99328367-bcee-47aa-8b46-d4fafc726dc3",
   "metadata": {},
   "source": [
    "## **Results**\n",
    "\n",
    "## Rust\n",
    "Total Maximum Subarray Sum (20 runs): 10980\n",
    "\n",
    "Execution Time: 0.002737 seconds\n",
    "\n",
    "\n",
    "## Python Outputs and time\n",
    "Total Maximum Subarray Sum (20 runs): 10980\n",
    "\n",
    "Execution Time: 321.343096 seconds\n",
    "\n",
    "## C++ outputs and time\n",
    "Total Maximum Subarray Sum (20 runs): 10980\n",
    "\n",
    "Execution Time: 0.000912 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b51dc7",
   "metadata": {},
   "source": [
    "## C++\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <chrono>\n",
    "#include <cstdint>\n",
    "#include <limits>\n",
    "\n",
    "const uint32_t A = 1664525;\n",
    "const uint32_t C = 1013904223;\n",
    "\n",
    "int64_t max_subarray_sum(size_t n, uint32_t seed, int32_t min_val, int32_t max_val) {\n",
    "    uint64_t range = static_cast<uint64_t>(max_val - min_val + 1);\n",
    "    uint32_t value = seed;\n",
    "    int64_t best = std::numeric_limits<int64_t>::min();\n",
    "    int64_t current = 0;\n",
    "    \n",
    "    for (size_t i = 0; i < n; ++i) {\n",
    "        value = value * A + C;\n",
    "        uint64_t offset = static_cast<uint64_t>(value) % range;\n",
    "        int64_t num = static_cast<int64_t>(offset) + static_cast<int64_t>(min_val);\n",
    "        \n",
    "        current += num;\n",
    "        if (current > best) {\n",
    "            best = current;\n",
    "        }\n",
    "        if (current < 0) {\n",
    "            current = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return best;\n",
    "}\n",
    "\n",
    "int64_t total_max_subarray_sum(size_t n, uint32_t initial_seed, int32_t min_val, int32_t max_val) {\n",
    "    uint32_t value = initial_seed;\n",
    "    int64_t total_sum = 0;\n",
    "    \n",
    "    for (int i = 0; i < 20; ++i) {\n",
    "        value = value * A + C;\n",
    "        total_sum += max_subarray_sum(n, value, min_val, max_val);\n",
    "    }\n",
    "    \n",
    "    return total_sum;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Parameters matching the Rust code\n",
    "    size_t n = 10000;\n",
    "    uint32_t initial_seed = 42;\n",
    "    int32_t min_val = -10;\n",
    "    int32_t max_val = 10;\n",
    "    \n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    int64_t result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    std::chrono::duration<double> duration = end - start;\n",
    "    \n",
    "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
    "    std::cout << \"Execution Time: \" << std::fixed << duration.count() << \" seconds\" << std::endl;\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
