{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: fetch latest code</h2>\n",
    "            <span style=\"color:#f71;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in a Cursor Terminal, run:<br/>\n",
    "            <code>uv sync</code><br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use high end models GPT 5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Grok 4, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please pick lower cost models like gpt-5-nano.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "Grok API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_MODEL = \"gpt-5\"\n",
    "# CLAUDE_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "# GROK_MODEL = \"grok-4\"\n",
    "# GEMINI_MODEL = \"gemini-2.5-pro\"\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "\n",
    "OPENAI_MODEL = \"gpt-5-nano\"\n",
    "CLAUDE_MODEL = \"claude-3-5-haiku-latest\"\n",
    "GROK_MODEL = \"grok-4-fast-non-reasoning\"\n",
    "GEMINI_MODEL = \"gemini-2.5-flash-lite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab38a7",
   "metadata": {},
   "source": [
    "## PLEASE NOTE:\n",
    "\n",
    "We will be writing a solution to convert Python into efficient, optimized C++ code for your machine, which can be compiled to native machine code and executed.\n",
    "\n",
    "It is not necessary for you to execute the code yourself - that's not the point of the exercise!\n",
    "\n",
    "But if you would like to (because it's satisfying!) then I'm including the steps here. Very optional!\n",
    "\n",
    "As an alternative, I'll also show you a website where you can run the C++ code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2fbb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': {'system': 'Windows',\n",
       "  'arch': 'AMD64',\n",
       "  'release': '10',\n",
       "  'version': '10.0.19045',\n",
       "  'kernel': '10',\n",
       "  'distro': None,\n",
       "  'wsl': False,\n",
       "  'rosetta2_translated': False,\n",
       "  'target_triple': 'x86_64-w64-mingw32'},\n",
       " 'package_managers': ['winget'],\n",
       " 'cpu': {'brand': 'Intel(R) Core(TM) i5-1035G1 CPU @ 1.00GHz',\n",
       "  'cores_logical': 8,\n",
       "  'cores_physical': 4,\n",
       "  'simd': []},\n",
       " 'toolchain': {'compilers': {'gcc': 'gcc.EXE (Rev2, Built by MSYS2 project) 14.2.0',\n",
       "   'g++': 'g++.EXE (Rev2, Built by MSYS2 project) 14.2.0',\n",
       "   'clang': '',\n",
       "   'msvc_cl': ''},\n",
       "  'build_tools': {'cmake': '', 'ninja': '', 'make': ''},\n",
       "  'linkers': {'ld_lld': ''}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from system_info import retrieve_system_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "system_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d29a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer:\n",
       "- You don’t need to install a compiler. Your system already has GCC (g++.EXE) from MSYS2, which is a working C++ compiler on Windows.\n",
       "\n",
       "What to do right now (simplest path)\n",
       "\n",
       "Option A: Compile and run manually (works best in an MSYS2/MINGW64 shell or any Windows terminal where g++ is in PATH)\n",
       "- Open a terminal that has g++ in PATH (e.g., MSYS2 MinGW 64-bit shell, or a Windows Command Prompt/PowerShell if MSYS2’s bin is in PATH).\n",
       "- Make sure main.cpp is in your current directory.\n",
       "- Compile with optimized settings (fastest runtime):\n",
       "  - g++ -O3 -std=c++17 main.cpp -o main.exe\n",
       "  - If you want to push performance further and your CPU supports it, you can add -march=native -mtune=native:\n",
       "    - g++ -O3 -march=native -mtune=native -std=c++17 main.cpp -o main.exe\n",
       "- Run:\n",
       "  - In MSYS2 shell: ./main.exe\n",
       "  - In Windows CMD/PowerShell: main.exe\n",
       "\n",
       "Option B: Python snippet (specifically for you, with fastest practical optimization)\n",
       "- Use these exact command lists:\n",
       "\n",
       "compile_command = [\"g++\", \"-O3\", \"-std=c++17\", \"main.cpp\", \"-o\", \"main.exe\"]\n",
       "# If you want the extra aggressive tuning (may not matter for short runs or some CPUs):\n",
       "# compile_command = [\"g++\", \"-O3\", \"-march=native\", \"-mtune=native\", \"-std=c++17\", \"main.cpp\", \"-o\", \"main.exe\"]\n",
       "\n",
       "run_command = [\"main.exe\"]\n",
       "\n",
       "- Then in Python:\n",
       "  - compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
       "  - run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
       "  - output = run_result.stdout\n",
       "\n",
       "Notes and quick checks\n",
       "- Your system info shows: gcc.EXE and g++.EXE (MSYS2 project) 14.2.0 are installed. That means you already have a working GCC toolchain.\n",
       "- If g++ isn’t found in your PATH in a plain Windows terminal, run from the MSYS2 MinGW 64-bit shell or add the appropriate MSYS2 bin directory to your PATH (typically something like C:\\msys64\\mingw64\\bin).\n",
       "- If your code uses newer C++ features, -std=c++17 is included above; you can switch to -std=c++20 if needed.\n",
       "\n",
       "If you ever want to install a compiler from scratch (only if you don’t have one)\n",
       "- In case g++ is not available, the simplest route on Windows is:\n",
       "  - Install MSYS2 (msys2.org)\n",
       "  - Open the MSYS2 MinGW 64-bit console and run:\n",
       "    - pacman -Syu\n",
       "    - pacman -S --needed base-devel mingw-w64-x86_64-toolchain\n",
       "  - Ensure C:\\msys64\\mingw64\\bin is on your PATH (or use the MSYS2 shell to compile).\n",
       "  - Then use the same compile_command and run_command as above.\n",
       "\n",
       "If you’d like, tell me whether you’re running in a pure Windows CMD/PowerShell environment or inside the MSYS2 shell, and I’ll tailor the exact run_command you should use."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a C++ compiler to compile a single C++ file called main.cpp and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install any C++ compiler to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile C++ code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "```python\n",
    "compile_command = # something here - to achieve the fastest possible runtime performance\n",
    "compile_result = subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "run_command = # something here\n",
    "run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
    "return run_result.stdout\n",
    "```\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "\n",
    "System information:\n",
    "{system_info}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92c12",
   "metadata": {},
   "source": [
    "## If you need to install something\n",
    "\n",
    "If you would like to, please follow GPTs instructions! Then rerun the analysis afterwards (you might need to Restart the notebook) to confirm you're set.\n",
    "\n",
    "You should now be equipped with the command to compile the code, and the command to run it!\n",
    "\n",
    "Enter that in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d734a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_command = [\"clang++\", \"-std=c++17\", \"-Ofast\", \"-mcpu=native\", \"-flto=thin\", \"-fvisibility=hidden\", \"-DNDEBUG\", \"main.cpp\", \"-o\", \"main\"]\n",
    "run_command = [\"./main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a437",
   "metadata": {},
   "source": [
    "## And now, on with the main task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance C++ code.\n",
    "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
    "The C++ response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
    "The system information is:\n",
    "{system_info}\n",
    "Your response will be written to a file called main.cpp and then compiled and executed; the compilation command is:\n",
    "{compile_command}\n",
    "Respond only with C++ code.\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(cpp):\n",
    "    with open(\"main.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(client, model, python):\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```','')\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(200_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    "    globals = {\"__builtins__\": __builtins__}\n",
    "    exec(code, globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7faa90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592656089\n",
      "Execution Time: 45.561339 seconds\n"
     ]
    }
   ],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "105db6f9-343c-491d-8e44-3a5328b81719",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(openai, OPENAI_MODEL, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda894b2-3033-4235-a331-4c79417f6eef",
   "metadata": {},
   "source": [
    "## CODE\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "#include <chrono>\n",
    "\n",
    "int main() {\n",
    "    const long long iterations = 200000000LL;\n",
    "    const double param1 = 4.0;\n",
    "    const double param2 = 1.0;\n",
    "\n",
    "    double result = 1.0;\n",
    "    double i_times_p1 = param1; // equals 1*param1 initially\n",
    "\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    for (long long i = 0; i < iterations; ++i) {\n",
    "        double denom1 = i_times_p1 - param2;\n",
    "        result -= 1.0 / denom1;\n",
    "\n",
    "        double denom2 = i_times_p1 + param2;\n",
    "        result += 1.0 / denom2;\n",
    "\n",
    "        i_times_p1 += param1;\n",
    "    }\n",
    "\n",
    "    double final_result = result * 4.0;\n",
    "\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    std::chrono::duration<double> elapsed = end_time - start_time;\n",
    "\n",
    "    std::cout.setf(std::ios::fixed);\n",
    "    std::cout << \"Result: \" << std::setprecision(12) << final_result << \"\\n\";\n",
    "    std::cout << \"Execution Time: \" << std::setprecision(6) << elapsed.count() << \" seconds\" << \"\\n\";\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "## execution\n",
    "Result: 3.141592656089\n",
    "\n",
    "Execution Time: 1.531273 seconds\n",
    "\n",
    "\n",
    "## **=== Code Execution Successful ===**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d",
   "metadata": {},
   "source": [
    "# Compiling C++ and executing\n",
    "\n",
    "This next cell contains the command to compile a C++ file based on the instructions from GPT.\n",
    "\n",
    "Again, it's not crucial to do this step if you don't wish to!\n",
    "\n",
    "OR alternatively: student Sandeep K.G. points out that you can run Python and C++ code online to test it out that way. Thank you Sandeep!  \n",
    "> Not an exact comparison but you can still get the idea of performance difference.  \n",
    "> For example here: https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commands from GPT 5\n",
    "\n",
    "def compile_and_run():\n",
    "    subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
    "    print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24ebe7-1450-46fe-8e55-7114f53fcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "19.178207/0.082168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b8ef9",
   "metadata": {},
   "source": [
    "## OK let's try the other contenders!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a11fe-e24d-4c65-8269-9802c5ef3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(anthropic, CLAUDE_MODEL, pi)\n",
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(grok, GROK_MODEL, pi)\n",
    "compile_and_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfa2484a-0067-4f49-96ca-111644113c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output2(cpp):\n",
    "    with open(\"maing.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33b0148d-c2dd-4dc0-a5d2-2836de69b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port2(client, model, python):\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```','')\n",
    "    write_output2(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a0243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "port2(gemini, GEMINI_MODEL, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c33bd-e5bd-4e04-b592-919a92781123",
   "metadata": {},
   "source": [
    "#code\n",
    "```cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "#include <chrono>\n",
    "\n",
    "// Function to perform the calculation\n",
    "double calculate(int iterations, double param1, double param2) {\n",
    "    double result = 1.0;\n",
    "    // Using a for loop for iterations\n",
    "    for (int i = 1; i <= iterations; ++i) {\n",
    "        // Calculate j for subtraction\n",
    "        double j_sub = static_cast<double>(i) * param1 - param2;\n",
    "        // Calculate j for addition\n",
    "        double j_add = static_cast<double>(i) * param1 + param2;\n",
    "        \n",
    "        // Avoid division by zero, although with the given parameters it's unlikely\n",
    "        if (j_sub != 0) {\n",
    "            result -= (1.0 / j_sub);\n",
    "        }\n",
    "        if (j_add != 0) {\n",
    "            result += (1.0 / j_add);\n",
    "        }\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Record start time\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    // Define parameters\n",
    "    int iterations = 200000000;\n",
    "    double param1 = 4.0;\n",
    "    double param2 = 1.0;\n",
    "\n",
    "    // Perform calculation and multiply by 4\n",
    "    double final_result = calculate(iterations, param1, param2) * 4.0;\n",
    "\n",
    "    // Record end time\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    // Calculate duration\n",
    "    std::chrono::duration<double> elapsed_time = end_time - start_time;\n",
    "\n",
    "    // Print the result with specified precision\n",
    "    std::cout << std::fixed << std::setprecision(12) << \"Result: \" << final_result << std::endl;\n",
    "    // Print the execution time with specified precision\n",
    "    std::cout << std::fixed << std::setprecision(6) << \"Execution Time: \" << elapsed_time.count() << \" seconds\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "## Execution\n",
    "Result: 3.141592656089\n",
    "\n",
    "\n",
    "Execution Time: 2.186025 seconds\n",
    "\n",
    "\n",
    "## **=== Code Execution Successful ===**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ffb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "In Ed's experiments, the performance speedups were:\n",
    "\n",
    "4th place: Claude Sonnet 4.5: {19.178207/0.104241:.0f}X speedup\n",
    "3rd place: GPT-5: {19.178207/0.082168:.0f}X speedup\n",
    "2nd place: Grok 4: {19.178207/0.018092:.0f}X speedup\n",
    "1st place: Gemini 2.5 Pro: {19.178207/0.013314:.0f}X speedup\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58753b",
   "metadata": {},
   "source": [
    "# using local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7202e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get(\"http://localhost:11434\").content\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "from openai import OpenAI\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e8c54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output3(cpp):\n",
    "    with open(\"main_ollama.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ac35ecb-d72b-4a81-9bda-aee004ea3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port3(client, model, python):\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```','')\n",
    "    write_output3(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74bc7bf9-5528-48c8-ab01-5b84998602fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "port3(ollama, MODEL, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36fd8a-5a11-4fc1-a955-9d117269bf77",
   "metadata": {},
   "source": [
    "## llama3.2 code\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <unistd.h>\n",
    "\n",
    "const long double param1 = 4.0;\n",
    "const long double param2 = 1.0;\n",
    "\n",
    "long double calculate(long double iterations) {\n",
    "    long double result = 1.0;\n",
    "    for (int i = 1; i <= static_cast<int>(iterations); ++i) {\n",
    "        long double j = i * param1 - param2;\n",
    "        result -= (1.0 / j);\n",
    "        j = i * param1 + param2;\n",
    "        result += (1.0 / j);\n",
    "    }\n",
    "    return result * param1;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Avoid precision issues by using long double arithmetic\n",
    "    std::cout.precision(18);\n",
    "\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    long int iterations = 200000000;\n",
    "    auto result = calculate(iterations);\n",
    "\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time).count() / 1000.0;\n",
    "\n",
    "    // Print and display result in a human-readable format\n",
    "    std::cout << \"Result: \" << result << std::endl;\n",
    "    std::cout << \"Execution Time: \" << duration / 60.0 << \".00 seconds\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "# shows error\n",
    "ERROR!\n",
    "/tmp/M5427B7IXL/main.cpp: In function 'int main()':\n",
    "/tmp/M5427B7IXL/main.cpp:23:28: error: 'std::chrono' has not been declared\n",
    "   23 |     auto start_time = std::chrono::high_resolution_clock::now();\n",
    "      |                            ^~~~~~\n",
    "ERROR!\n",
    "/tmp/M5427B7IXL/main.cpp:28:26: error: 'std::chrono' has not been declared\n",
    "   28 |     auto end_time = std::chrono::high_resolution_clock::now();\n",
    "      |                          ^~~~~~\n",
    "/tmp/M5427B7IXL/main.cpp:29:26: error: 'std::chrono' has not been declared\n",
    "   29 |     auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time).count() / 1000.0;\n",
    "      |                          ^~~~~~\n",
    "/tmp/M5427B7IXL/main.cpp:29:53: error: 'std::chrono' has not been declared\n",
    "   29 |     auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time).count() / 1000.0;\n",
    "      |                                                     ^~~~~~\n",
    "\n",
    "\n",
    "## === Code Exited With Errors ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bebdd50c-29f7-4f7e-9877-1ee0a46cba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using qwen2.5\n",
    "def write_output4(cpp):\n",
    "    with open(\"main_qwen2.5.cpp\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cpp)\n",
    "\n",
    "\n",
    "def port4(client, model, python):\n",
    "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
    "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
    "    reply = response.choices[0].message.content\n",
    "    reply = reply.replace('```cpp','').replace('```','')\n",
    "    write_output4(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9148e64f-ce92-4ff5-b118-18c49eb8ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "port4(ollama, \"qwen2.5\", pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c944f-c675-4194-a55b-41c9dc7ef268",
   "metadata": {},
   "source": [
    "code qwen2.5\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <chrono>\n",
    "\n",
    "double calculate(int iterations, int param1, int param2) {\n",
    "    double result = 1.0;\n",
    "    for (int i = 1; i <= iterations; ++i) {\n",
    "        int j = i * param1 - param2;\n",
    "        result -= 1.0 / j;\n",
    "        j = i * param1 + param2;\n",
    "        result += 1.0 / j;\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "    double result = calculate(200_000_000, 4, 1) * 4;\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    std::cout << \"Result: \" << std::setprecision(12) << result << std::endl;\n",
    "    std::cout << \"Execution Time: \" \n",
    "              << std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time).count() / 1'000'000.0\n",
    "              << \" seconds\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "/tmp/nGs2LTLMjw/main.cpp: In function 'int main()':\n",
    "/tmp/nGs2LTLMjw/main.cpp:17:31: error: unable to find numeric literal operator 'operator\"\"_000_000'\n",
    "   17 |     double result = calculate(200_000_000, 4, 1) * 4;\n",
    "      |                               ^~~~~~~~~~~\n",
    "/tmp/nGs2LTLMjw/main.cpp:20:37: error: 'setprecision' is not a member of 'std'\n",
    "   20 |     std::cout << \"Result: \" << std::setprecision(12) << result << std::endl;\n",
    "      |                                     ^~~~~~~~~~~~\n",
    "/tmp/nGs2LTLMjw/main.cpp:3:1: note: 'std::setprecision' is defined in header '<iomanip>'; this is probably fixable by adding '#include <iomanip>'\n",
    "    2 | #include <chrono>\n",
    "  +++ |+#include <iomanip>\n",
    "    3 | \n",
    "\n",
    "\n",
    "=== Code Exited With Errors ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2fc43-db43-4ca8-903a-d715820d7477",
   "metadata": {},
   "source": [
    "# local model\n",
    "#llama error\n",
    "#starcoder2 failed no code some sign of c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa00bb-3842-4f4c-991a-05dbdcc104f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
