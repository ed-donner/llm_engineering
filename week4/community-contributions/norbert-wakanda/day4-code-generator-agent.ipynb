{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0789c3c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from system_info import retrieve_system_info\n",
        "\n",
        "# Fix: gradio 6.x uses click.Choice[str] subscript at import-time, which\n",
        "# click 8.x doesn't support on Python 3.13.  Patch before importing gradio.\n",
        "import click\n",
        "if not hasattr(click.Choice, \"__class_getitem__\"):\n",
        "    click.Choice.__class_getitem__ = classmethod(lambda cls, _: cls)\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# Single OpenAI-compatible client pointing at OpenRouter (same as day3)\n",
        "llm_client = OpenAI(\n",
        "    api_key=openrouter_api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")\n",
        "\n",
        "OPENAI_MODEL = \"gpt-5.2\"\n",
        "CLAUDE_MODEL = \"anthropic/claude-haiku-4-5\"\n",
        "GEMINI_MODEL = \"google/gemini-2.5-flash-lite\"\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "de55d314",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── System prompt used internally when converting Python → C++ ────────────────\n",
        "_CPP_SYSTEM_PROMPT = \"\"\"\n",
        "Your task is to convert Python code into high performance C++ code.\n",
        "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
        "The C++ response needs to produce an identical output in the fastest possible time.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ── Tool 1: convert Python to C++ via a dedicated LLM call ───────────────────\n",
        "\n",
        "def convert_to_cpp(python_code: str) -> str:\n",
        "    \"\"\"Call the LLM to port Python code to high-performance C++.\n",
        "\n",
        "    Mirrors the port() + user_prompt_for() pattern from day3.\n",
        "    Returns the raw C++ source as a string (markdown fences stripped).\n",
        "    \"\"\"\n",
        "    # Embed current system info so the LLM picks the right flags/features\n",
        "    system_info = retrieve_system_info()\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
        "The system information is:\n",
        "{system_info}\n",
        "\n",
        "Your response will be written to a file called main.cpp and then compiled and executed.\n",
        "Respond ONLY with C++ code — no markdown, no explanation.\n",
        "\n",
        "Python code to port:\n",
        "```python\n",
        "{python_code.strip()}\n",
        "```\n",
        "\"\"\"\n",
        "    # Use reasoning_effort=\"high\" for GPT models (same as port() in day3)\n",
        "    reasoning_effort = \"high\" if \"gpt\" in OPENAI_MODEL else None\n",
        "    try:\n",
        "        resp = llm_client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": _CPP_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\",   \"content\": user_prompt},\n",
        "            ],\n",
        "            reasoning_effort=reasoning_effort,\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Some OpenRouter backends reject unknown kwargs — fall back gracefully\n",
        "        resp = llm_client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": _CPP_SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\",   \"content\": user_prompt},\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    cpp = resp.choices[0].message.content or \"\"\n",
        "    # Strip markdown code fences the model may wrap the output in\n",
        "    cpp = re.sub(r\"^```(?:cpp|c\\+\\+)?\\s*\\n\", \"\", cpp, flags=re.IGNORECASE)\n",
        "    cpp = re.sub(r\"\\n```\\s*$\", \"\", cpp)\n",
        "    return cpp.strip()\n",
        "\n",
        "\n",
        "# ── Tool 2: save C++ source code to a file ────────────────────────────────────\n",
        "\n",
        "def save_cpp_to_file(cpp_code: str, filename: str = \"main.cpp\") -> str:\n",
        "    \"\"\"Write C++ source code to disk.\n",
        "\n",
        "    Mirrors write_output() from day3.\n",
        "    Returns the absolute path of the saved file so the agent can report it.\n",
        "    \"\"\"\n",
        "    path = Path(filename).resolve()\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    path.write_text(cpp_code, encoding=\"utf-8\")\n",
        "    return str(path)\n",
        "\n",
        "\n",
        "# ── Tool 3: gather system info to derive compile/run commands ─────────────────\n",
        "\n",
        "def get_system_info() -> str:\n",
        "    \"\"\"Return system environment details as a JSON string.\n",
        "\n",
        "    The agent uses this to decide the right g++ flags and run command\n",
        "    for the current machine (OS, CPU SIMD support, compiler version, etc.).\n",
        "    Calls retrieve_system_info() which is already imported from system_info.py.\n",
        "    \"\"\"\n",
        "    info = retrieve_system_info()\n",
        "    return json.dumps(info, indent=2)\n",
        "\n",
        "\n",
        "# ── Tool 4: compile the C++ file and run it ───────────────────────────────────\n",
        "\n",
        "def compile_and_run(compile_command: str, run_command: str) -> str:\n",
        "    \"\"\"Compile and execute a C++ source file.\n",
        "\n",
        "    compile_command : JSON array string, e.g.\n",
        "        '[\"g++\", \"-std=c++20\", \"-Ofast\", \"-march=native\", \"-flto\", \"-DNDEBUG\", \"main.cpp\", \"-o\", \"main\"]'\n",
        "    run_command     : JSON array string, e.g.  '[\"./main\"]'\n",
        "\n",
        "    Mirrors compile_command / run_command from day3, but accepts them as\n",
        "    JSON strings so the LLM can pass whatever the system requires.\n",
        "    Returns the program's stdout, or a descriptive error if it fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        compile_cmd = json.loads(compile_command)\n",
        "        run_cmd     = json.loads(run_command)\n",
        "    except json.JSONDecodeError as exc:\n",
        "        return f\"Could not parse command JSON: {exc}\"\n",
        "\n",
        "    # ── compile ──────────────────────────────────────────────────────────────\n",
        "    compile_result = subprocess.run(compile_cmd, text=True, capture_output=True)\n",
        "    if compile_result.returncode != 0:\n",
        "        return (\n",
        "            \"Compilation failed.\\n\"\n",
        "            f\"stderr:\\n{compile_result.stderr or compile_result.stdout}\"\n",
        "        )\n",
        "\n",
        "    # ── run ──────────────────────────────────────────────────────────────────\n",
        "    run_result = subprocess.run(run_cmd, text=True, capture_output=True)\n",
        "    if run_result.returncode != 0:\n",
        "        return (\n",
        "            f\"Compilation succeeded but execution failed (exit {run_result.returncode}).\\n\"\n",
        "            f\"stderr:\\n{run_result.stderr}\"\n",
        "        )\n",
        "\n",
        "    output = run_result.stdout.strip()\n",
        "    return output if output else \"Program ran successfully and produced no output.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "20198f57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Tool schemas (OpenAI function-calling format) ─────────────────────────────\n",
        "\n",
        "convert_to_cpp_function = {\n",
        "    \"name\": \"convert_to_cpp\",\n",
        "    \"description\": (\n",
        "        \"Convert Python source code to high-performance C++. \"\n",
        "        \"Returns the full C++ source as a string.\"\n",
        "    ),\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"python_code\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The complete Python source code to convert.\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"python_code\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "save_cpp_to_file_function = {\n",
        "    \"name\": \"save_cpp_to_file\",\n",
        "    \"description\": (\n",
        "        \"Save C++ source code to a file on disk. \"\n",
        "        \"Returns the absolute file path where it was saved.\"\n",
        "    ),\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"cpp_code\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The C++ source code to write to disk.\",\n",
        "            },\n",
        "            \"filename\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Destination filename (default: main.cpp).\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"cpp_code\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "get_system_info_function = {\n",
        "    \"name\": \"get_system_info\",\n",
        "    \"description\": (\n",
        "        \"Retrieve OS, CPU, and compiler details for this machine. \"\n",
        "        \"Use the returned information to choose the correct g++ flags \"\n",
        "        \"and run command before compiling C++ code.\"\n",
        "    ),\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "compile_and_run_function = {\n",
        "    \"name\": \"compile_and_run\",\n",
        "    \"description\": (\n",
        "        \"Compile a C++ source file and execute the resulting binary. \"\n",
        "        \"Returns the program's stdout output, or an error message.\"\n",
        "    ),\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"compile_command\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"JSON array string for the compile step, e.g. '[\\\"g++\\\", \\\"-std=c++20\\\", \\\"-Ofast\\\", \\\"-march=native\\\", \\\"-flto\\\", \\\"-DNDEBUG\\\", \\\"main.cpp\\\", \\\"-o\\\", \\\"main\\\"]'\",\n",
        "            },\n",
        "            \"run_command\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"JSON array string for the run step, e.g. '[\\\"./main\\\"]'\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"compile_command\", \"run_command\"],\n",
        "        \"additionalProperties\": False,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Collect all tools in the same list structure as the example flow\n",
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": convert_to_cpp_function},\n",
        "    {\"type\": \"function\", \"function\": save_cpp_to_file_function},\n",
        "    {\"type\": \"function\", \"function\": get_system_info_function},\n",
        "    {\"type\": \"function\", \"function\": compile_and_run_function},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "240644a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Tool dispatcher (mirrors handle_tool_call from the example flow) ──────────\n",
        "\n",
        "def handle_tool_call(message) -> dict:\n",
        "    \"\"\"Route a single tool call from the LLM to the correct Python function\n",
        "    and return a 'tool' role message ready to append to the conversation.\"\"\"\n",
        "    tool_call  = message.tool_calls[0]\n",
        "    arguments  = json.loads(tool_call.function.arguments)\n",
        "    name       = tool_call.function.name\n",
        "\n",
        "    if name == \"convert_to_cpp\":\n",
        "        content = convert_to_cpp(arguments[\"python_code\"])\n",
        "\n",
        "    elif name == \"save_cpp_to_file\":\n",
        "        content = save_cpp_to_file(\n",
        "            arguments[\"cpp_code\"],\n",
        "            arguments.get(\"filename\", \"main.cpp\"),\n",
        "        )\n",
        "\n",
        "    elif name == \"get_system_info\":\n",
        "        content = get_system_info()\n",
        "\n",
        "    elif name == \"compile_and_run\":\n",
        "        content = compile_and_run(\n",
        "            arguments[\"compile_command\"],\n",
        "            arguments[\"run_command\"],\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        content = f\"Unknown tool: {name}\"\n",
        "\n",
        "    return {\n",
        "        \"role\":         \"tool\",\n",
        "        \"content\":      content,\n",
        "        \"tool_call_id\": tool_call.id,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b9da5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── System message ─────────────────────────────────────────────────────────────\n",
        "\n",
        "SYSTEM_MESSAGE = \"\"\"\n",
        "You are an expert Python-to-C++ conversion agent. When the user provides Python code, follow these steps in order — do not skip any:\n",
        "\n",
        "1. Call convert_to_cpp with the Python code to generate high-performance C++.\n",
        "2. Call save_cpp_to_file with the returned C++ code to persist it to main.cpp.\n",
        "3. Call get_system_info to learn about the machine's OS, CPU, and compiler.\n",
        "4. Call compile_and_run with the compile and run commands derived from the system info.\n",
        "   Use the fastest flags appropriate for the system (e.g., -Ofast -march=native -flto -DNDEBUG for g++).\n",
        "5. Report back to the user with:\n",
        "   - The generated C++ code (in a code block)\n",
        "   - The file path where it was saved\n",
        "   - The program's execution output\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# ── Main chat function (tool-call loop, same pattern as the example) ──────────\n",
        "\n",
        "def chat(message: str, history: list) -> str:\n",
        "    \"\"\"Orchestrate the full Python → C++ pipeline using chained tool calls.\n",
        "\n",
        "    Each tool's output feeds into the LLM's next decision, forming the chain:\n",
        "      convert_to_cpp → save_cpp_to_file → get_system_info → compile_and_run → reply\n",
        "    \"\"\"\n",
        "    # Convert Gradio history list to OpenAI message dicts\n",
        "    history_msgs = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n",
        "        + history_msgs\n",
        "        + [{\"role\": \"user\", \"content\": message}]\n",
        "    )\n",
        "\n",
        "    response = llm_client.chat.completions.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    )\n",
        "\n",
        "    # Loop: keep handling tool calls until the model is done\n",
        "    # (one tool output becomes context for the next tool decision)\n",
        "    while response.choices[0].finish_reason == \"tool_calls\":\n",
        "        tool_message   = response.choices[0].message\n",
        "        tool_response  = handle_tool_call(tool_message)\n",
        "\n",
        "        # Append both the assistant's tool-call request and the tool result\n",
        "        messages.append(tool_message)\n",
        "        messages.append(tool_response)\n",
        "\n",
        "        response = llm_client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "        )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# ── Gradio UI ──────────────────────────────────────────────────────────────────\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"Python → C++ Converter Agent\",\n",
        "    description=(\n",
        "        \"Paste Python code and the agent will:\\n\"\n",
        "        \"1. Convert it to high-performance C++\\n\"\n",
        "        \"2. Save it to `main.cpp`\\n\"\n",
        "        \"3. Detect your system's compiler\\n\"\n",
        "        \"4. Compile & run it — then report the output\"\n",
        "    ),\n",
        "   \n",
        ").launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
