{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# AI Backend Refactor Assistant  \n",
    "**Non-agentic LLM-powered tool to transform Python backend logic into production-ready artifacts**\n",
    "\n",
    "**Goal**  \n",
    "Convert any Python function (or existing FastAPI endpoint) into:\n",
    "\n",
    "- ü¶Ä High-performance Rust implementation  \n",
    "- üìù Enhanced Python version (with full docstrings + type hints)  \n",
    "- üß™ Comprehensive pytest unit tests  \n",
    "- üåê Automatic FastAPI wrapper (when input is a plain function)  \n",
    "- üìò Valid OpenAPI 3.0 specification (YAML)\n",
    "\n",
    "Built with Gradio UI ‚Ä¢ Uses frontier LLMs via OpenRouter/Groq/etc. ‚Ä¢ Sequential pipeline (no agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key not set\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "Grok API Key not set (and this is optional)\n",
      "Groq API Key exists and begins gsk_\n",
      "OpenRouter API Key exists and begins sk-or-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "# openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "openai = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "anthropic = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
    "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-5\", \"claude-sonnet-4-5-20250929\", \"grok-4\", \"gemini-2.5-pro\", \"qwen2.5-coder\", \"deepseek-coder-v2\", \"gpt-oss:20b\", \"qwen/qwen3-coder-30b-a3b-instruct\", \"openai/gpt-oss-120b\", ]\n",
    "\n",
    "clients = {\"gpt-5\": openai, \"claude-sonnet-4-5-20250929\": anthropic, \"grok-4\": grok, \"gemini-2.5-pro\": gemini, \"openai/gpt-oss-120b\": groq, \"qwen2.5-coder\": ollama, \"deepseek-coder-v2\": ollama, \"gpt-oss:20b\": ollama, \"qwen/qwen3-coder-30b-a3b-instruct\": openrouter}\n",
    "\n",
    "# Want to keep costs ultra-low? Replace this with models of your choice, using the examples from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large numbers\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a0d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rust_messages(python_code: str) -> list:\n",
    "    system = \"\"\"You are a world-class Rust performance engineer.\n",
    "Rewrite the Python backend logic as a complete, high-performance, zero-dependency `main.rs`.\n",
    "Rules:\n",
    "- Use ONLY the Rust standard library\n",
    "- Maximize runtime speed (opt-level 3 mindset: native CPU, minimal allocations, best algorithms)\n",
    "- Include a `fn main()` that runs the logic and prints the same output the Python code would produce\n",
    "- Add comprehensive `///` documentation\n",
    "- The code must compile with `rustc main.rs -C opt-level=3 -C target-cpu=native`\n",
    "Respond **exclusively** with the full Rust source code. No explanations.\"\"\"\n",
    "    user = f\"Python code to port:\\n```python\\n{python_code}\\n```\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "\n",
    "def get_enhanced_python_messages(python_code: str) -> list:\n",
    "    system = \"\"\"You are a senior Python engineer.\n",
    "Enhance the provided code WITHOUT changing any logic:\n",
    "- Add complete Google-style docstrings to every function and class\n",
    "- Add/fix all type hints (use typing when needed)\n",
    "- Add clear inline comments for non-obvious sections\n",
    "- Improve readability where possible\n",
    "Respond **exclusively** with the full enhanced Python code.\"\"\"\n",
    "    user = f\"Code to enhance:\\n```python\\n{python_code}\\n```\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "\n",
    "def get_tests_messages(python_code: str) -> list:\n",
    "    system = \"\"\"You are a pytest expert.\n",
    "Generate a complete, ready-to-run pytest test file for the provided Python code.\n",
    "Requirements:\n",
    "- Happy-path, edge-case, and error-case tests\n",
    "- Use `@pytest.mark.parametrize` wherever sensible\n",
    "- No external mocks unless absolutely required\n",
    "- Tests should be importable (assume the original code is in `backend.py`)\n",
    "Respond **exclusively** with the full pytest Python code.\"\"\"\n",
    "    user = f\"Generate tests for:\\n```python\\n{python_code}\\n```\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "\n",
    "def get_fastapi_wrapper_messages(python_code: str) -> list:\n",
    "    system = \"\"\"You are a FastAPI expert.\n",
    "If the input is a standalone Python function ‚Üí wrap it as a complete minimal FastAPI application.\n",
    "If the input is already a FastAPI endpoint/app ‚Üí return it unchanged (but guarantee it is a full runnable app with `app = FastAPI()` and uvicorn block).\n",
    "\n",
    "Rules for standalone function:\n",
    "- Use POST endpoint named after the function (or `/compute`)\n",
    "- Create Pydantic `BaseModel` for request body when parameters exist\n",
    "- Return JSON `{\"result\": ...}`\n",
    "- Include `if __name__ == \"__main__\": uvicorn.run(...)`\n",
    "- Keep the original computation logic untouched\n",
    "\n",
    "Respond **exclusively** with the full FastAPI Python code.\"\"\"\n",
    "    user = f\"Code:\\n```python\\n{python_code}\\n```\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "\n",
    "def get_openapi_messages(fastapi_code: str) -> list:\n",
    "    system = \"\"\"You are an OpenAPI 3.0 specialist.\n",
    "From the provided FastAPI code, generate a complete, valid OpenAPI 3.0 YAML specification.\n",
    "Include paths, requestBody schemas, response schemas, and status codes.\n",
    "Respond **exclusively** with the YAML content.\"\"\"\n",
    "    user = f\"FastAPI code:\\n```python\\n{fastapi_code}\\n```\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfb430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages: list, model: str) -> str:\n",
    "    client = clients.get(model, openai)          # fallback to OpenRouter client\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.1,\n",
    "            max_tokens=8192,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Extract first code block (most reliable for clean output)\n",
    "        code_match = re.search(r'```(?:\\w+)?\\s*(.*?)```', content, re.DOTALL)\n",
    "        if code_match:\n",
    "            content = code_match.group(1).strip()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå LLM Error ({model}): {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf9c840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_artifacts(python_code: str, selected_model: str):\n",
    "    if not python_code or not python_code.strip():\n",
    "        empty = \"‚ùå No input code provided.\"\n",
    "        return empty, empty, empty, empty, empty\n",
    "\n",
    "    # 1. Enhanced Python (always)\n",
    "    enhanced_python = call_llm(get_enhanced_python_messages(python_code), selected_model)\n",
    "\n",
    "    # 2. FastAPI Wrapper (LLM decides if input is already an endpoint)\n",
    "    fastapi_wrapper = call_llm(get_fastapi_wrapper_messages(python_code), selected_model)\n",
    "\n",
    "    # 3. High-performance Rust\n",
    "    rust_code = call_llm(get_rust_messages(python_code), selected_model)\n",
    "    rust_display = f\"```rust\\n{rust_code}\\n```\" if rust_code else \"No Rust code generated.\"\n",
    "\n",
    "    # 4. Pytest unit tests\n",
    "    unit_tests = call_llm(get_tests_messages(python_code), selected_model)\n",
    "\n",
    "    # 5. OpenAPI 3.0 YAML (based on the wrapper)\n",
    "    openapi_spec = call_llm(get_openapi_messages(fastapi_wrapper), selected_model)\n",
    "\n",
    "    \n",
    "    return rust_display, enhanced_python, unit_tests, fastapi_wrapper, openapi_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles import CSS\n",
    "\n",
    "with gr.Blocks(\n",
    "    css=CSS if \"CSS\" in globals() else None,\n",
    "    theme=gr.themes.Monochrome(),\n",
    "    title=\"AI Backend Refactor Assistant\"\n",
    ") as ui:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üß† AI Backend Refactor Assistant\n",
    "    **Non-agentic** ‚Ä¢ Production-ready artifacts from any Python backend function or FastAPI endpoint\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=5):\n",
    "            python_input = gr.Code(\n",
    "                label=\"üì• Python Input (standalone function or @app.get/post endpoint)\",\n",
    "                value=python_hard,\n",
    "                language=\"python\",\n",
    "                lines=22\n",
    "            )\n",
    "        with gr.Column(scale=1):\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=models,\n",
    "                value=models[0],\n",
    "                label=\"ü§ñ Model (used for ALL tasks)\",\n",
    "                info=\"All 5 artifacts are generated sequentially with this model\"\n",
    "            )\n",
    "            file_upload = gr.File(\n",
    "                label=\"üìÅ Upload .py file\",\n",
    "                file_types=[\".py\"],\n",
    "                height=100\n",
    "            )\n",
    "\n",
    "    generate_btn = gr.Button(\"üöÄ Generate All 5 Artifacts\", variant=\"primary\", size=\"large\")\n",
    "\n",
    "    # Collapsible accordions exactly as specified in the SRD\n",
    "    with gr.Column():\n",
    "        with gr.Accordion(\"ü¶Ä High-Performance Rust Implementation\", open=True):\n",
    "            rust_out = gr.Markdown(value=\"```rust\\n// Waiting for generation...\\n```\")\n",
    "\n",
    "        with gr.Accordion(\"üìù Enhanced Python (docstrings + types)\", open=False):\n",
    "            enhanced_out = gr.Code(label=\"\", language=\"python\", lines=25)\n",
    "\n",
    "        with gr.Accordion(\"üß™ Pytest Unit Tests\", open=False):\n",
    "            tests_out = gr.Code(label=\"\", language=\"python\", lines=25)\n",
    "\n",
    "        with gr.Accordion(\"üåê FastAPI Wrapper (auto-generated if standalone)\", open=False):\n",
    "            fastapi_out = gr.Code(label=\"\", language=\"python\", lines=25)\n",
    "\n",
    "        with gr.Accordion(\"üìò OpenAPI 3.0 Specification (YAML)\", open=False):\n",
    "            openapi_out = gr.Code(label=\"\", language=\"yaml\", lines=25)\n",
    "\n",
    "    # File upload handler\n",
    "    def load_uploaded_file(file_obj):\n",
    "        if file_obj is None:\n",
    "            return python_hard\n",
    "        try:\n",
    "            with open(file_obj.name, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {e}\"\n",
    "\n",
    "    file_upload.upload(fn=load_uploaded_file, inputs=file_upload, outputs=python_input)\n",
    "\n",
    "    # Pipeline trigger\n",
    "    generate_btn.click(\n",
    "        fn=generate_artifacts,\n",
    "        inputs=[python_input, model_dropdown],\n",
    "        outputs=[rust_out, enhanced_out, tests_out, fastapi_out, openapi_out]\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
