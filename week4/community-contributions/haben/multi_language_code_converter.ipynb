{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåê Multi-Language Code Converter\n",
        "\n",
        "**AI-Powered Code Translation Across 13+ Programming Languages**\n",
        "\n",
        "A comprehensive code conversion tool that translates code between multiple programming languages using state-of-the-art LLM models. Supports both frontier models (GPT-5, Claude 4.5, Gemini 2.5 Pro, Grok 4) and open-source models (Ollama, Groq, OpenRouter).\n",
        "\n",
        "## üéØ Key Features\n",
        "\n",
        "- **13+ Language Support**: Python, C++, Rust, Java, JavaScript, TypeScript, C, C#, Go, PHP, Swift, Ruby, Kotlin, Julia\n",
        "- **Multi-LLM Support**: OpenAI, Anthropic, Google, Grok, Ollama, Groq, OpenRouter\n",
        "- **Performance Optimization**: Generates high-performance code optimized for target language\n",
        "- **Interactive Interface**: Gradio-based web UI for easy code conversion\n",
        "- **Compilation Testing**: Automatic compilation and execution for compiled languages (C++, Rust, Go)\n",
        "- **Quality Analysis**: Code validation and performance metrics\n",
        "\n",
        "## üìã Supported Languages\n",
        "\n",
        "**Source/Target Languages:**\n",
        "- Python ‚Ä¢ C++ ‚Ä¢ Rust ‚Ä¢ Java ‚Ä¢ JavaScript ‚Ä¢ TypeScript ‚Ä¢ C ‚Ä¢ C# ‚Ä¢ Go ‚Ä¢ PHP ‚Ä¢ Swift ‚Ä¢ Ruby ‚Ä¢ Kotlin ‚Ä¢ Julia\n",
        "\n",
        "## ü§ñ Supported LLM Models\n",
        "\n",
        "**Frontier Models:**\n",
        "- GPT-5 (OpenAI)\n",
        "- Claude Sonnet 4.5 (Anthropic)\n",
        "- Gemini 2.5 Pro (Google)\n",
        "- Grok 4 (xAI)\n",
        "\n",
        "**Open-Source Models:**\n",
        "- Qwen2.5 Coder (Ollama)\n",
        "- DeepSeek Coder v2 (Ollama)\n",
        "- GPT-OSS 20B (Ollama)\n",
        "- GPT-OSS 120B (Groq)\n",
        "- Qwen3 Coder 30B (OpenRouter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Multi-Language Code Converter - Imports and Environment Setup\n",
        "\n",
        "This cell imports required libraries and loads environment variables from the root .env file.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Tuple, List, Any\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def find_root_directory(target_name: str = \"llm_engineering\") -> Path:\n",
        "    \"\"\"\n",
        "    Find the root directory by traversing up the directory tree.\n",
        "    \n",
        "    Args:\n",
        "        target_name: Name of the target directory to find\n",
        "        \n",
        "    Returns:\n",
        "        Path to the root directory, or current directory if not found\n",
        "    \"\"\"\n",
        "    current_dir = Path.cwd()\n",
        "    root_dir = current_dir\n",
        "    \n",
        "    while root_dir.name != target_name and root_dir.parent != root_dir:\n",
        "        root_dir = root_dir.parent\n",
        "    \n",
        "    return root_dir if root_dir.name == target_name else current_dir\n",
        "\n",
        "\n",
        "# Load environment variables from root .env file\n",
        "root_dir = find_root_directory(\"llm_engineering\")\n",
        "env_path = root_dir / '.env'\n",
        "\n",
        "if env_path.exists():\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"‚úÖ Loaded .env from: {env_path}\")\n",
        "else:\n",
        "    # Fallback to current directory\n",
        "    load_dotenv(override=True)\n",
        "    print(f\"‚ö†Ô∏è  Root .env not found at {env_path}, using current directory .env\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Setup and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Environment Setup and API Key Configuration\n",
        "\n",
        "Loads and validates API keys from environment variables.\n",
        "\"\"\"\n",
        "\n",
        "# API Key Configuration\n",
        "API_KEYS = {\n",
        "    'openai': os.getenv('OPENAI_API_KEY'),\n",
        "    'anthropic': os.getenv('ANTHROPIC_API_KEY'),\n",
        "    'google': os.getenv('GOOGLE_API_KEY'),\n",
        "    'grok': os.getenv('GROK_API_KEY'),\n",
        "    'groq': os.getenv('GROQ_API_KEY'),\n",
        "    'openrouter': os.getenv('OPENROUTER_API_KEY'),\n",
        "}\n",
        "\n",
        "# OpenRouter configuration\n",
        "OPENROUTER_BASE_URL = os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')\n",
        "\n",
        "# API endpoint URLs\n",
        "API_ENDPOINTS = {\n",
        "    'anthropic': \"https://api.anthropic.com/v1/\",\n",
        "    'gemini': \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    'grok': \"https://api.x.ai/v1\",\n",
        "    'groq': \"https://api.groq.com/openai/v1\",\n",
        "    'ollama': \"http://localhost:11434/v1\",\n",
        "}\n",
        "\n",
        "\n",
        "def check_api_key_status(key_name: str, key_value: Optional[str], prefix_length: int = 8) -> None:\n",
        "    \"\"\"\n",
        "    Check and print API key status.\n",
        "    \n",
        "    Args:\n",
        "        key_name: Name of the API key\n",
        "        key_value: The API key value\n",
        "        prefix_length: Number of characters to show in prefix\n",
        "    \"\"\"\n",
        "    if key_value:\n",
        "        prefix = key_value[:prefix_length]\n",
        "        print(f\"‚úÖ {key_name.capitalize()} API Key exists (begins with {prefix})\")\n",
        "    else:\n",
        "        status = \"‚ùå\" if key_name == 'openai' else \"‚ö†Ô∏è\"\n",
        "        optional = \"\" if key_name == 'openai' else \" (optional)\"\n",
        "        print(f\"{status} {key_name.capitalize()} API Key not set{optional}\")\n",
        "\n",
        "\n",
        "# Check API key status\n",
        "print(\"API Key Status:\")\n",
        "check_api_key_status('openai', API_KEYS['openai'])\n",
        "check_api_key_status('anthropic', API_KEYS['anthropic'], prefix_length=7)\n",
        "check_api_key_status('google', API_KEYS['google'], prefix_length=2)\n",
        "check_api_key_status('grok', API_KEYS['grok'], prefix_length=4)\n",
        "check_api_key_status('groq', API_KEYS['groq'], prefix_length=4)\n",
        "check_api_key_status('openrouter', API_KEYS['openrouter'], prefix_length=6)\n",
        "\n",
        "if API_KEYS['openrouter']:\n",
        "    print(f\"‚úÖ OpenRouter Base URL: {OPENROUTER_BASE_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize LLM Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize LLM Clients\n",
        "\n",
        "Creates OpenAI-compatible clients for various LLM providers.\n",
        "Priority for OpenAI: OpenRouter > Direct OpenAI API\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def initialize_openai_client() -> Optional[OpenAI]:\n",
        "    \"\"\"\n",
        "    Initialize OpenAI client with priority: OpenRouter > Direct OpenAI.\n",
        "    \n",
        "    Returns:\n",
        "        OpenAI client instance or None if no API key is available\n",
        "    \"\"\"\n",
        "    if API_KEYS['openrouter']:\n",
        "        client = OpenAI(api_key=API_KEYS['openrouter'], base_url=OPENROUTER_BASE_URL)\n",
        "        print(f\"‚úÖ Using OpenAI API via OpenRouter at {OPENROUTER_BASE_URL}\")\n",
        "        return client\n",
        "    elif API_KEYS['openai']:\n",
        "        client = OpenAI(api_key=API_KEYS['openai'])\n",
        "        print(\"‚úÖ Using direct OpenAI API\")\n",
        "        return client\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  OpenAI client not available (no API key)\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def initialize_llm_client(provider: str, api_key: Optional[str], base_url: str) -> Optional[OpenAI]:\n",
        "    \"\"\"\n",
        "    Initialize an LLM client for a specific provider.\n",
        "    \n",
        "    Args:\n",
        "        provider: Name of the provider\n",
        "        api_key: API key for the provider\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        OpenAI client instance or None if API key is not available\n",
        "    \"\"\"\n",
        "    if api_key:\n",
        "        return OpenAI(api_key=api_key, base_url=base_url)\n",
        "    return None\n",
        "\n",
        "\n",
        "# Initialize OpenAI client (with priority handling)\n",
        "openai_client = initialize_openai_client()\n",
        "\n",
        "# Initialize other LLM clients\n",
        "anthropic_client = initialize_llm_client('anthropic', API_KEYS['anthropic'], API_ENDPOINTS['anthropic'])\n",
        "gemini_client = initialize_llm_client('gemini', API_KEYS['google'], API_ENDPOINTS['gemini'])\n",
        "grok_client = initialize_llm_client('grok', API_KEYS['grok'], API_ENDPOINTS['grok'])\n",
        "groq_client = initialize_llm_client('groq', API_KEYS['groq'], API_ENDPOINTS['groq'])\n",
        "ollama_client = OpenAI(api_key=\"ollama\", base_url=API_ENDPOINTS['ollama'])  # Ollama doesn't require API key\n",
        "openrouter_client = initialize_llm_client('openrouter', API_KEYS['openrouter'], OPENROUTER_BASE_URL)\n",
        "\n",
        "print(\"‚úÖ LLM clients initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Available models\n",
        "FRONTIER_MODELS = {\n",
        "    \"gpt-5\": openai_client,\n",
        "    \"claude-sonnet-4-5-20250929\": anthropic_client,\n",
        "    \"gemini-2.5-pro\": gemini_client,\n",
        "    \"grok-4\": grok_client,\n",
        "}\n",
        "\n",
        "OPEN_SOURCE_MODELS = {\n",
        "    \"qwen2.5-coder\": ollama_client,\n",
        "    \"deepseek-coder-v2\": ollama_client,\n",
        "    \"gpt-oss:20b\": ollama_client,\n",
        "    \"openai/gpt-oss-120b\": groq_client,\n",
        "    \"qwen/qwen3-coder-30b-a3b-instruct\": openrouter_client,\n",
        "}\n",
        "\n",
        "# Filter out None clients\n",
        "FRONTIER_MODELS = {k: v for k, v in FRONTIER_MODELS.items() if v is not None}\n",
        "OPEN_SOURCE_MODELS = {k: v for k, v in OPEN_SOURCE_MODELS.items() if v is not None}\n",
        "\n",
        "ALL_MODELS = {**FRONTIER_MODELS, **OPEN_SOURCE_MODELS}\n",
        "\n",
        "print(f\"‚úÖ Available models: {list(ALL_MODELS.keys())}\")\n",
        "\n",
        "# Low-cost alternatives (uncomment to use)\n",
        "# FRONTIER_MODELS = {\n",
        "#     \"gpt-5-nano\": openai_client,\n",
        "#     \"claude-haiku-4-5\": anthropic_client,\n",
        "#     \"gemini-2.5-flash-lite\": gemini_client,\n",
        "#     \"grok-4-fast-non-reasoning\": grok_client,\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. System Information (for optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "System Information Retrieval\n",
        "\n",
        "Loads system information for code optimization.\n",
        "Attempts to use the system_info module from week4 directory, falls back to basic info if unavailable.\n",
        "\"\"\"\n",
        "\n",
        "import platform\n",
        "\n",
        "\n",
        "def get_basic_system_info() -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Get basic system information using platform module.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing basic system information\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"os\": platform.system(),\n",
        "        \"platform\": platform.platform(),\n",
        "        \"processor\": platform.processor(),\n",
        "    }\n",
        "\n",
        "\n",
        "def load_system_info() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load system information, attempting to use system_info module from week4.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing system information\n",
        "    \"\"\"\n",
        "    # Try to find and import system_info from week4 directory\n",
        "    week4_dir = find_root_directory(\"week4\")\n",
        "    \n",
        "    if week4_dir.name == 'week4':\n",
        "        sys.path.insert(0, str(week4_dir))\n",
        "        try:\n",
        "            from system_info import retrieve_system_info\n",
        "            system_info = retrieve_system_info()\n",
        "            print(\"‚úÖ Loaded detailed system information\")\n",
        "            return system_info\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  system_info module not found, using basic system info\")\n",
        "            return get_basic_system_info()\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  week4 directory not found, using basic system info\")\n",
        "        return get_basic_system_info()\n",
        "\n",
        "\n",
        "# Load system information\n",
        "system_info = load_system_info()\n",
        "\n",
        "# Display system information\n",
        "print(\"\\nSystem Information:\")\n",
        "if isinstance(system_info, dict) and len(system_info) > 3:\n",
        "    # Detailed system info\n",
        "    display(Markdown(str(system_info)))\n",
        "else:\n",
        "    # Basic system info\n",
        "    info_text = f\"**OS:** {system_info.get('os', 'Unknown')}\\n\"\n",
        "    info_text += f\"**Platform:** {system_info.get('platform', 'Unknown')}\\n\"\n",
        "    info_text += f\"**Processor:** {system_info.get('processor', 'Unknown')}\"\n",
        "    display(Markdown(info_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compilation Commands (for compiled languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Compilation Configuration\n",
        "\n",
        "Defines compilation and execution commands for compiled languages.\n",
        "\"\"\"\n",
        "\n",
        "# Compilation commands with optimization flags\n",
        "COMPILE_COMMANDS: Dict[str, List[str]] = {\n",
        "    \"cpp\": [\n",
        "        \"clang++\", \"-std=c++17\", \"-Ofast\", \"-mcpu=native\",\n",
        "        \"-flto=thin\", \"-fvisibility=hidden\", \"-DNDEBUG\",\n",
        "        \"main.cpp\", \"-o\", \"main\"\n",
        "    ],\n",
        "    \"rust\": [\"rustc\", \"-O\", \"main.rs\", \"-o\", \"main\"],\n",
        "    \"go\": [\"go\", \"build\", \"-o\", \"main\", \"main.go\"],\n",
        "    \"c\": [\n",
        "        \"clang\", \"-std=c11\", \"-Ofast\", \"-mcpu=native\",\n",
        "        \"-flto=thin\", \"-DNDEBUG\", \"main.c\", \"-o\", \"main\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Execution commands\n",
        "RUN_COMMANDS: Dict[str, List[str]] = {\n",
        "    \"cpp\": [\"./main\"],\n",
        "    \"rust\": [\"./main\"],\n",
        "    \"go\": [\"./main\"],\n",
        "    \"c\": [\"./main\"],\n",
        "}\n",
        "\n",
        "# File extensions mapping\n",
        "FILE_EXTENSIONS: Dict[str, Tuple[str, str]] = {\n",
        "    \"cpp\": (\"cpp\", \"main.cpp\"),\n",
        "    \"c++\": (\"cpp\", \"main.cpp\"),\n",
        "    \"rust\": (\"rs\", \"main.rs\"),\n",
        "    \"go\": (\"go\", \"main.go\"),\n",
        "    \"c\": (\"c\", \"main.c\"),\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Compilation commands configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Core Translation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Core Translation Functions\n",
        "\n",
        "Functions for generating prompts and creating message structures for LLM API calls.\n",
        "\"\"\"\n",
        "\n",
        "# Supported programming languages\n",
        "SUPPORTED_LANGUAGES: List[str] = [\n",
        "    \"Python\", \"C++\", \"Rust\", \"Java\", \"JavaScript\", \"TypeScript\",\n",
        "    \"C\", \"C#\", \"Go\", \"PHP\", \"Swift\", \"Ruby\", \"Kotlin\", \"Julia\"\n",
        "]\n",
        "\n",
        "\n",
        "def get_system_prompt(source_lang: str, target_lang: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate system prompt for code conversion.\n",
        "    \n",
        "    Args:\n",
        "        source_lang: Source programming language\n",
        "        target_lang: Target programming language\n",
        "        \n",
        "    Returns:\n",
        "        System prompt string\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are an expert code translator specializing in converting {source_lang} code to high-performance {target_lang} code.\n",
        "\n",
        "Your task is to convert {source_lang} code into optimized {target_lang} code.\n",
        "- Respond ONLY with {target_lang} code\n",
        "- Use comments sparingly and only when necessary\n",
        "- The {target_lang} code must produce identical output to the original {source_lang} code\n",
        "- Optimize for performance and follow {target_lang} best practices\n",
        "- Include all necessary imports/headers/packages\n",
        "- Ensure proper type handling to avoid overflows\n",
        "- Keep random number generators identical for reproducible results\"\"\"\n",
        "\n",
        "\n",
        "def get_user_prompt(\n",
        "    source_lang: str,\n",
        "    target_lang: str,\n",
        "    source_code: str,\n",
        "    additional_instructions: str = \"\",\n",
        "    system_info_dict: Optional[Dict[str, Any]] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate user prompt for code conversion.\n",
        "    \n",
        "    Args:\n",
        "        source_lang: Source programming language\n",
        "        target_lang: Target programming language\n",
        "        source_code: Source code to convert\n",
        "        additional_instructions: Optional additional instructions\n",
        "        system_info_dict: Optional system information dictionary\n",
        "        \n",
        "    Returns:\n",
        "        User prompt string\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Convert this {source_lang} code to {target_lang} with the fastest possible implementation that produces identical output.\n",
        "\n",
        "Requirements:\n",
        "- Respond ONLY with {target_lang} code\n",
        "- Do not provide explanations, only code with minimal comments\n",
        "- Pay attention to number types to ensure no integer overflows\n",
        "- Include all necessary {target_lang} packages/modules/headers\n",
        "- Optimize for performance while maintaining correctness\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    if system_info_dict:\n",
        "        prompt += f\"System information for optimization:\\n{system_info_dict}\\n\\n\"\n",
        "    \n",
        "    if additional_instructions:\n",
        "        prompt += f\"Additional instructions: {additional_instructions}\\n\\n\"\n",
        "    \n",
        "    prompt += f\"{source_lang} code to convert:\\n\\n```{source_lang.lower()}\\n{source_code}\\n```\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def create_messages(\n",
        "    source_lang: str,\n",
        "    target_lang: str,\n",
        "    source_code: str,\n",
        "    additional_instructions: str = \"\",\n",
        "    system_info_dict: Optional[Dict[str, Any]] = None\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Create messages structure for LLM API call.\n",
        "    \n",
        "    Args:\n",
        "        source_lang: Source programming language\n",
        "        target_lang: Target programming language\n",
        "        source_code: Source code to convert\n",
        "        additional_instructions: Optional additional instructions\n",
        "        system_info_dict: Optional system information dictionary\n",
        "        \n",
        "    Returns:\n",
        "        List of message dictionaries for API call\n",
        "    \"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": get_system_prompt(source_lang, target_lang)},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": get_user_prompt(\n",
        "                source_lang, target_lang, source_code,\n",
        "                additional_instructions, system_info_dict\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "print(\"‚úÖ Translation functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Code Conversion Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Code Conversion Function\n",
        "\n",
        "Main function for converting code between programming languages using LLM models.\n",
        "\"\"\"\n",
        "\n",
        "# Code block markers to remove from LLM responses\n",
        "CODE_BLOCK_MARKERS = [\n",
        "    '```cpp', '```rust', '```go', '```java', '```javascript',\n",
        "    '```typescript', '```python', '```c', '```csharp', '```php',\n",
        "    '```swift', '```ruby', '```kotlin', '```julia', '```'\n",
        "]\n",
        "\n",
        "\n",
        "def clean_code_response(code: str, target_lang: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean markdown code blocks from LLM response.\n",
        "    \n",
        "    Args:\n",
        "        code: Raw code response from LLM\n",
        "        target_lang: Target language name (for specific marker removal)\n",
        "        \n",
        "    Returns:\n",
        "        Cleaned code string\n",
        "    \"\"\"\n",
        "    cleaned = code\n",
        "    # Remove language-specific markers\n",
        "    target_lang_lower = target_lang.lower()\n",
        "    cleaned = cleaned.replace(f'```{target_lang_lower}', '')\n",
        "    \n",
        "    # Remove common code block markers\n",
        "    for marker in CODE_BLOCK_MARKERS:\n",
        "        cleaned = cleaned.replace(marker, '')\n",
        "    \n",
        "    return cleaned.strip()\n",
        "\n",
        "\n",
        "def convert_code(\n",
        "    model_name: str,\n",
        "    source_lang: str,\n",
        "    target_lang: str,\n",
        "    source_code: str,\n",
        "    additional_instructions: str = \"\",\n",
        "    system_info_dict: Optional[Dict[str, Any]] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Convert code from source language to target language using specified model.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the LLM model to use\n",
        "        source_lang: Source programming language\n",
        "        target_lang: Target programming language\n",
        "        source_code: Source code to convert\n",
        "        additional_instructions: Optional additional instructions\n",
        "        system_info_dict: Optional system information dictionary\n",
        "        \n",
        "    Returns:\n",
        "        Converted code string or error message\n",
        "    \"\"\"\n",
        "    if model_name not in ALL_MODELS:\n",
        "        available = list(ALL_MODELS.keys())\n",
        "        return f\"‚ùå Error: Model '{model_name}' is not available. Available models: {available}\"\n",
        "    \n",
        "    client = ALL_MODELS[model_name]\n",
        "    if client is None:\n",
        "        return f\"‚ùå Error: Client for model '{model_name}' is not initialized. Check API keys.\"\n",
        "    \n",
        "    messages = create_messages(\n",
        "        source_lang, target_lang, source_code,\n",
        "        additional_instructions, system_info_dict\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        # Use reasoning_effort for GPT models\n",
        "        reasoning_effort = \"high\" if \"gpt\" in model_name.lower() else None\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            reasoning_effort=reasoning_effort\n",
        "        )\n",
        "        \n",
        "        converted_code = response.choices[0].message.content\n",
        "        if not converted_code:\n",
        "            return \"‚ùå Error: Empty response from model\"\n",
        "        \n",
        "        # Clean up markdown code blocks\n",
        "        converted_code = clean_code_response(converted_code, target_lang)\n",
        "        \n",
        "        return converted_code\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        return f\"‚ùå Error during conversion: {error_msg}\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Code conversion function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compilation and Execution Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Compilation and Execution Helpers\n",
        "\n",
        "Functions for compiling and executing code in compiled languages.\n",
        "\"\"\"\n",
        "\n",
        "# Timeout constants\n",
        "COMPILE_TIMEOUT = 30  # seconds\n",
        "EXECUTE_TIMEOUT = 10  # seconds\n",
        "\n",
        "\n",
        "def compile_and_run(code: str, language: str) -> Tuple[str, bool]:\n",
        "    \"\"\"\n",
        "    Compile and run code for compiled languages.\n",
        "    \n",
        "    Args:\n",
        "        code: Code to compile and run\n",
        "        language: Programming language name\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (output, success) where output is stdout or error message,\n",
        "        and success is a boolean indicating if compilation/execution succeeded\n",
        "    \"\"\"\n",
        "    lang_lower = language.lower()\n",
        "    \n",
        "    # Check if compilation is supported\n",
        "    if lang_lower not in COMPILE_COMMANDS:\n",
        "        url = f\"https://www.programiz.com/{lang_lower}-programming/online-compiler/\"\n",
        "        return (\n",
        "            f\"‚ö†Ô∏è  Compilation not supported for {language}. \"\n",
        "            f\"Use an online compiler like {url}\",\n",
        "            False\n",
        "        )\n",
        "    \n",
        "    # Get file extension and filename\n",
        "    file_info = FILE_EXTENSIONS.get(lang_lower)\n",
        "    if not file_info:\n",
        "        return f\"‚ö†Ô∏è  Compilation not configured for {language}\", False\n",
        "    \n",
        "    _, filename = file_info\n",
        "    \n",
        "    try:\n",
        "        # Write code to file\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(code)\n",
        "        \n",
        "        # Compile\n",
        "        compile_cmd = COMPILE_COMMANDS[lang_lower]\n",
        "        compile_result = subprocess.run(\n",
        "            compile_cmd,\n",
        "            check=True,\n",
        "            text=True,\n",
        "            capture_output=True,\n",
        "            timeout=COMPILE_TIMEOUT\n",
        "        )\n",
        "        \n",
        "        # Run\n",
        "        run_cmd = RUN_COMMANDS[lang_lower]\n",
        "        run_result = subprocess.run(\n",
        "            run_cmd,\n",
        "            check=True,\n",
        "            text=True,\n",
        "            capture_output=True,\n",
        "            timeout=EXECUTE_TIMEOUT\n",
        "        )\n",
        "        \n",
        "        return run_result.stdout, True\n",
        "        \n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"‚ùå Error: Compilation or execution timed out\", False\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error_msg = e.stderr if e.stderr else str(e)\n",
        "        return f\"‚ùå Compilation/Execution Error: {error_msg}\", False\n",
        "    except OSError as e:\n",
        "        return f\"‚ùå System Error: {str(e)}\", False\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Unexpected Error: {str(e)}\", False\n",
        "\n",
        "\n",
        "print(\"‚úÖ Compilation helpers ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Example Python Code (for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Python code for testing\n",
        "EXAMPLE_PYTHON_CODE = \"\"\"import time\n",
        "\n",
        "def calculate_pi(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations + 1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1 / j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1 / j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate_pi(200_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example Python code loaded:\")\n",
        "print(EXAMPLE_PYTHON_CODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_with_ui(\n",
        "    model: str,\n",
        "    source_lang: str,\n",
        "    target_lang: str,\n",
        "    source_code: str,\n",
        "    additional_instructions: str,\n",
        "    compile_and_test: bool\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Convert code and optionally compile/test it.\n",
        "    \n",
        "    Args:\n",
        "        model: LLM model name\n",
        "        source_lang: Source programming language\n",
        "        target_lang: Target programming language\n",
        "        source_code: Source code to convert\n",
        "        additional_instructions: Optional additional instructions\n",
        "        compile_and_test: Whether to compile and test the converted code\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (converted_code, test_output)\n",
        "    \"\"\"\n",
        "    # Convert code (system_info is available in global scope)\n",
        "    converted_code = convert_code(\n",
        "        model, source_lang, target_lang, source_code,\n",
        "        additional_instructions, system_info\n",
        "    )\n",
        "    \n",
        "    # Compile and test if requested\n",
        "    test_output = \"\"\n",
        "    if compile_and_test and converted_code and not converted_code.startswith(\"‚ùå\"):\n",
        "        output, success = compile_and_run(converted_code, target_lang)\n",
        "        test_output = f\"**Compilation & Execution Result:**\\n\\n{output}\"\n",
        "        if not success:\n",
        "            test_output = f\"‚ö†Ô∏è {test_output}\"\n",
        "    \n",
        "    return converted_code, test_output\n",
        "\n",
        "\"\"\"\n",
        "Gradio Interface\n",
        "\n",
        "Creates an interactive web interface for code conversion.\n",
        "\"\"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Multi-Language Code Converter\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üåê Multi-Language Code Converter\n",
        "    \n",
        "    Convert code between 13+ programming languages using state-of-the-art LLM models.\n",
        "    \n",
        "    **Supported Languages:** Python, C++, Rust, Java, JavaScript, TypeScript, C, C#, Go, PHP, Swift, Ruby, Kotlin, Julia\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            model_dropdown = gr.Dropdown(\n",
        "                choices=list(ALL_MODELS.keys()),\n",
        "                value=list(ALL_MODELS.keys())[0] if ALL_MODELS else None,\n",
        "                label=\"ü§ñ Select LLM Model\",\n",
        "                info=\"Choose the model for code conversion\"\n",
        "            )\n",
        "            \n",
        "            source_lang_dropdown = gr.Dropdown(\n",
        "                choices=SUPPORTED_LANGUAGES,\n",
        "                value=\"Python\",\n",
        "                label=\"üì• Source Language\"\n",
        "            )\n",
        "            \n",
        "            target_lang_dropdown = gr.Dropdown(\n",
        "                choices=SUPPORTED_LANGUAGES,\n",
        "                value=\"C++\",\n",
        "                label=\"üì§ Target Language\"\n",
        "            )\n",
        "            \n",
        "            compile_checkbox = gr.Checkbox(\n",
        "                value=False,\n",
        "                label=\"üîß Compile & Test (for compiled languages)\",\n",
        "                info=\"Automatically compile and run the converted code\"\n",
        "            )\n",
        "            \n",
        "            additional_instructions = gr.Textbox(\n",
        "                label=\"üìù Additional Instructions (Optional)\",\n",
        "                placeholder=\"e.g., Use std::vector instead of arrays, optimize for speed...\",\n",
        "                lines=2\n",
        "            )\n",
        "        \n",
        "        with gr.Column(scale=2):\n",
        "            source_code = gr.Textbox(\n",
        "                label=\"üìù Source Code\",\n",
        "                value=EXAMPLE_PYTHON_CODE,\n",
        "                lines=20\n",
        "            )\n",
        "            \n",
        "            convert_btn = gr.Button(\"üöÄ Convert Code\", variant=\"primary\", size=\"lg\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        converted_code = gr.Textbox(\n",
        "            label=\"‚ú® Converted Code\",\n",
        "            lines=20\n",
        "        )\n",
        "    \n",
        "    with gr.Row():\n",
        "        test_output = gr.Markdown(\n",
        "            label=\"üß™ Compilation & Execution Results\",\n",
        "            visible=True\n",
        "        )\n",
        "    \n",
        "    # Event handlers\n",
        "    convert_btn.click(\n",
        "        fn=convert_with_ui,\n",
        "        inputs=[\n",
        "            model_dropdown,\n",
        "            source_lang_dropdown,\n",
        "            target_lang_dropdown,\n",
        "            source_code,\n",
        "            additional_instructions,\n",
        "            compile_checkbox\n",
        "        ],\n",
        "        outputs=[converted_code, test_output]\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Gradio interface created!\")\n",
        "print(\"Run: demo.launch(inbrowser=True) to start the interface\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface\n",
        "demo.launch(inbrowser=True, share=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
