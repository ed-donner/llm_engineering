{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35800e5e",
      "metadata": {},
      "source": [
        "# Week 4 Exercise — Bug Benchmark Arena\n",
        "\n",
        "**Building on the full pipeline** — Generate buggy code (Week 3) → Analyze with personas (Week 1) → Interactively debug (Week 2) — this exercise asks: **which models are actually best at injecting and detecting bugs?**\n",
        "\n",
        "### What This Does\n",
        "\n",
        "- **Benchmarks coding-focused LLMs** head-to-head on two tasks: **infesting** clean code with realistic bugs, and **detecting** those bugs in infested code\n",
        "- Uses **OpenRouter** to access frontier and open-weight coding models (Qwen Coder, Claude, Gemini, DeepSeek, GPT) through a single API\n",
        "- **Dual-panel Gradio UI**: left side for bug infesting, right side for independent validation — each with its own model selector\n",
        "- Configurable **bug type checkboxes** (SyntaxError, NameError, IndentationError, TypeError, IndexError, LogicError)\n",
        "- Structured **JSON output** from validation showing exactly which errors were found and whether they match what was injected\n",
        "- Enables direct comparison: which model is the best \"bug injector\" and which is the best \"bug detector\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20581063",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you get \"No module named pip\", run: python -m ensurepip --upgrade\n",
        "%pip install -q openai gradio python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc65bd83",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32901f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise EnvironmentError(\"OPENAI_API_KEY not found in .env — add your OpenRouter key there.\")\n",
        "\n",
        "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "client = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba510f98",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS = {\n",
        "    \"Qwen 2.5 Coder 32B\": \"qwen/qwen-2.5-coder-32b-instruct\",\n",
        "    \"DeepSeek Chat V3\": \"deepseek/deepseek-chat\",\n",
        "    \"GPT-4o Mini\": \"openai/gpt-4o-mini\",\n",
        "    \"Gemini 2.5 Flash\": \"google/gemini-2.5-flash\",\n",
        "    \"Claude 3.5 Haiku\": \"anthropic/claude-3.5-haiku\",\n",
        "    \"Llama 3.1 70B\": \"meta-llama/llama-3.1-70b-instruct\",\n",
        "}\n",
        "\n",
        "BUG_TYPES = [\n",
        "    \"SyntaxError\",\n",
        "    \"IndentationError\",\n",
        "    \"NameError\",\n",
        "    \"TypeError\",\n",
        "    \"IndexError\",\n",
        "    \"LogicError\",\n",
        "]\n",
        "\n",
        "CLEAN_SAMPLES = {\n",
        "    \"Binary Search\": (\n",
        "        \"def binary_search(arr, target):\\n\"\n",
        "        \"    low, high = 0, len(arr) - 1\\n\"\n",
        "        \"    while low <= high:\\n\"\n",
        "        \"        mid = (low + high) // 2\\n\"\n",
        "        \"        if arr[mid] == target:\\n\"\n",
        "        \"            return mid\\n\"\n",
        "        \"        elif arr[mid] < target:\\n\"\n",
        "        \"            low = mid + 1\\n\"\n",
        "        \"        else:\\n\"\n",
        "        \"            high = mid - 1\\n\"\n",
        "        \"    return -1\"\n",
        "    ),\n",
        "    \"Bubble Sort\": (\n",
        "        \"def bubble_sort(arr):\\n\"\n",
        "        \"    n = len(arr)\\n\"\n",
        "        \"    for i in range(n):\\n\"\n",
        "        \"        for j in range(n - i - 1):\\n\"\n",
        "        \"            if arr[j] > arr[j + 1]:\\n\"\n",
        "        \"                arr[j], arr[j + 1] = arr[j + 1], arr[j]\\n\"\n",
        "        \"    return arr\"\n",
        "    ),\n",
        "    \"Fibonacci\": (\n",
        "        \"def fibonacci(n):\\n\"\n",
        "        \"    if n <= 0:\\n\"\n",
        "        \"        return 0\\n\"\n",
        "        \"    elif n == 1:\\n\"\n",
        "        \"        return 1\\n\"\n",
        "        \"    a, b = 0, 1\\n\"\n",
        "        \"    for _ in range(2, n + 1):\\n\"\n",
        "        \"        a, b = b, a + b\\n\"\n",
        "        \"    return b\"\n",
        "    ),\n",
        "    \"Matrix Multiply\": (\n",
        "        \"def matrix_multiply(a, b):\\n\"\n",
        "        \"    rows_a, cols_a = len(a), len(a[0])\\n\"\n",
        "        \"    rows_b, cols_b = len(b), len(b[0])\\n\"\n",
        "        \"    if cols_a != rows_b:\\n\"\n",
        "        \"        raise ValueError('Incompatible dimensions')\\n\"\n",
        "        \"    result = [[0] * cols_b for _ in range(rows_a)]\\n\"\n",
        "        \"    for i in range(rows_a):\\n\"\n",
        "        \"        for j in range(cols_b):\\n\"\n",
        "        \"            for k in range(cols_a):\\n\"\n",
        "        \"                result[i][j] += a[i][k] * b[k][j]\\n\"\n",
        "        \"    return result\"\n",
        "    ),\n",
        "    \"LRU Cache\": (\n",
        "        \"class LRUCache:\\n\"\n",
        "        \"    def __init__(self, capacity):\\n\"\n",
        "        \"        self.capacity = capacity\\n\"\n",
        "        \"        self.cache = {}\\n\"\n",
        "        \"        self.order = []\\n\"\n",
        "        \"\\n\"\n",
        "        \"    def get(self, key):\\n\"\n",
        "        \"        if key in self.cache:\\n\"\n",
        "        \"            self.order.remove(key)\\n\"\n",
        "        \"            self.order.append(key)\\n\"\n",
        "        \"            return self.cache[key]\\n\"\n",
        "        \"        return -1\\n\"\n",
        "        \"\\n\"\n",
        "        \"    def put(self, key, value):\\n\"\n",
        "        \"        if key in self.cache:\\n\"\n",
        "        \"            self.order.remove(key)\\n\"\n",
        "        \"        elif len(self.cache) >= self.capacity:\\n\"\n",
        "        \"            oldest = self.order.pop(0)\\n\"\n",
        "        \"            del self.cache[oldest]\\n\"\n",
        "        \"        self.cache[key] = value\\n\"\n",
        "        \"        self.order.append(key)\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17e1f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "INFEST_PROMPT = \"\"\"\\\n",
        "You are an expert Python instructor creating buggy code for students to debug.\n",
        "\n",
        "Take the following CORRECT Python code and inject exactly {num_bugs} bug(s) into it.\n",
        "You MUST choose bugs from these types: {bug_types}.\n",
        "\n",
        "Rules:\n",
        "- The code should still look like a real programmer's mistake — not obviously garbled.\n",
        "- Typos in variable names count as NameError. Missing colons count as SyntaxError.\n",
        "- Wrong indentation counts as IndentationError. Off-by-one or wrong operators count as LogicError.\n",
        "- Do NOT add comments hinting at the bugs.\n",
        "\n",
        "CORRECT CODE:\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Respond with ONLY a valid JSON object — no markdown fences, no extra text:\n",
        "{{\n",
        "  \"buggy_code\": \"<the infested code with \\\\n for newlines>\",\n",
        "  \"injected_bugs\": [\n",
        "    {{\"type\": \"<error type>\", \"description\": \"<what you changed>\"}}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "VALIDATE_PROMPT = \"\"\"\\\n",
        "Analyze the following Python code for errors. Find ALL bugs — syntax errors, \\\n",
        "logic errors, naming issues, indentation problems, type errors, index errors, \\\n",
        "or anything else wrong.\n",
        "\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Respond with ONLY a valid JSON object — no markdown fences, no extra text:\n",
        "{{\n",
        "  \"bugs_found\": [\n",
        "    {{\"type\": \"<error type e.g. SyntaxError, NameError, LogicError>\", \"description\": \"<what is wrong>\", \"location\": \"<line or area>\"}}\n",
        "  ],\n",
        "  \"total_bugs\": <number>,\n",
        "  \"is_buggy\": true or false\n",
        "}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053cd75f",
      "metadata": {},
      "outputs": [],
      "source": [
        "_verbose = False\n",
        "_debug_log: list[str] = []\n",
        "\n",
        "\n",
        "def _log(msg: str):\n",
        "    if _verbose:\n",
        "        _debug_log.append(msg)\n",
        "\n",
        "\n",
        "def parse_llm_json(text: str):\n",
        "    \"\"\"Extract JSON from LLM output, handling markdown fences and stray text.\"\"\"\n",
        "    text = text.strip()\n",
        "    fence = re.search(r\"```(?:json)?\\s*\\n?([\\s\\S]*?)```\", text)\n",
        "    if fence:\n",
        "        text = fence.group(1).strip()\n",
        "    for start, end in [(\"{\", \"}\"), (\"[\", \"]\")]:\n",
        "        first = text.find(start)\n",
        "        last = text.rfind(end)\n",
        "        if first != -1 and last > first:\n",
        "            try:\n",
        "                return json.loads(text[first : last + 1])\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return json.loads(text)\n",
        "\n",
        "\n",
        "def call_model(model_id: str, prompt: str, max_tokens: int = 1500) -> str:\n",
        "    \"\"\"Call an OpenRouter model with retry logic.\"\"\"\n",
        "    _log(f\"── REQUEST to {model_id} ({len(prompt)} chars) ──\\n{prompt[:400]}{'...' if len(prompt) > 400 else ''}\\n\")\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            result = response.choices[0].message.content\n",
        "            _log(f\"── RESPONSE ({len(result)} chars) ──\\n{result[:500]}{'...' if len(result) > 500 else ''}\\n\")\n",
        "            return result\n",
        "        except Exception as exc:\n",
        "            if attempt == 2:\n",
        "                raise\n",
        "            wait = 3 * (2 ** attempt)\n",
        "            _log(f\"  RETRY {attempt+1} — {str(exc)[:120]}... waiting {wait}s\")\n",
        "            time.sleep(wait)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def infest_code(model_id: str, clean_code: str, bug_types: list[str], num_bugs: int = 2) -> dict:\n",
        "    \"\"\"Ask a model to inject bugs into clean code. Returns parsed JSON or error.\"\"\"\n",
        "    prompt = INFEST_PROMPT.format(\n",
        "        code=clean_code,\n",
        "        bug_types=\", \".join(bug_types),\n",
        "        num_bugs=num_bugs,\n",
        "    )\n",
        "    raw = call_model(model_id, prompt)\n",
        "    try:\n",
        "        result = parse_llm_json(raw)\n",
        "        if isinstance(result, dict) and \"buggy_code\" in result:\n",
        "            result.setdefault(\"injected_bugs\", [])\n",
        "            return result\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        pass\n",
        "    return {\"error\": \"Failed to parse infest response\", \"raw\": raw[:500]}\n",
        "\n",
        "\n",
        "def validate_code(model_id: str, code: str) -> dict:\n",
        "    \"\"\"Ask a model to find bugs in code (zero context). Returns parsed JSON or error.\"\"\"\n",
        "    prompt = VALIDATE_PROMPT.format(code=code)\n",
        "    raw = call_model(model_id, prompt)\n",
        "    try:\n",
        "        result = parse_llm_json(raw)\n",
        "        if isinstance(result, dict):\n",
        "            result.setdefault(\"bugs_found\", [])\n",
        "            result.setdefault(\"total_bugs\", len(result[\"bugs_found\"]))\n",
        "            return result\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        pass\n",
        "    return {\"error\": \"Failed to parse validate response\", \"raw\": raw[:500]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb1b95e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _freq_map(types: list[str]) -> dict[str, int]:\n",
        "    freq = {}\n",
        "    for t in types:\n",
        "        freq[t] = freq.get(t, 0) + 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def _freq_str(freq: dict[str, int]) -> str:\n",
        "    if not freq:\n",
        "        return \"none\"\n",
        "    return \", \".join(f\"{k} x{v}\" for k, v in sorted(freq.items(), key=lambda x: -x[1]))\n",
        "\n",
        "\n",
        "def compute_match_score(injected: list[dict], found: list[dict]) -> dict:\n",
        "    \"\"\"Compare injected bug types against detected bug types.\"\"\"\n",
        "    injected_types = [b.get(\"type\", \"\").strip() for b in injected]\n",
        "    found_types = [b.get(\"type\", \"\").strip() for b in found]\n",
        "\n",
        "    matched = 0\n",
        "    unmatched_found = list(found_types)\n",
        "    for t in injected_types:\n",
        "        if t in unmatched_found:\n",
        "            matched += 1\n",
        "            unmatched_found.remove(t)\n",
        "\n",
        "    precision = matched / len(found_types) if found_types else 0\n",
        "    recall = matched / len(injected_types) if injected_types else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"injected_freq\": _freq_str(_freq_map(injected_types)),\n",
        "        \"found_freq\": _freq_str(_freq_map(found_types)),\n",
        "        \"matched\": matched,\n",
        "        \"false_positives\": len(unmatched_found),\n",
        "        \"missed\": len(injected_types) - matched,\n",
        "        \"precision\": round(precision, 2),\n",
        "        \"recall\": round(recall, 2),\n",
        "        \"f1\": round(f1, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "def run_benchmark(\n",
        "    infest_model_id: str,\n",
        "    validate_model_id: str,\n",
        "    clean_code: str,\n",
        "    bug_types: list[str],\n",
        "    num_bugs: int,\n",
        "    progress_cb=None,\n",
        ") -> dict:\n",
        "    \"\"\"Run a single infest→validate cycle and return full results.\"\"\"\n",
        "    if progress_cb:\n",
        "        progress_cb(0.1, \"Infesting code...\")\n",
        "    infest_result = infest_code(infest_model_id, clean_code, bug_types, num_bugs)\n",
        "    if \"error\" in infest_result:\n",
        "        return {\"error\": f\"Infest failed: {infest_result['error']}\"}\n",
        "\n",
        "    buggy_code = infest_result.get(\"buggy_code\", \"\")\n",
        "    injected = infest_result.get(\"injected_bugs\", [])\n",
        "\n",
        "    if progress_cb:\n",
        "        progress_cb(0.5, \"Validating infested code...\")\n",
        "    validate_result = validate_code(validate_model_id, buggy_code)\n",
        "    if \"error\" in validate_result:\n",
        "        return {\"error\": f\"Validate failed: {validate_result['error']}\"}\n",
        "\n",
        "    found = validate_result.get(\"bugs_found\", [])\n",
        "    score = compute_match_score(injected, found)\n",
        "\n",
        "    if progress_cb:\n",
        "        progress_cb(1.0, \"Done\")\n",
        "\n",
        "    return {\n",
        "        \"infest_model\": infest_model_id,\n",
        "        \"validate_model\": validate_model_id,\n",
        "        \"clean_code\": clean_code,\n",
        "        \"buggy_code\": buggy_code,\n",
        "        \"injected_bugs\": injected,\n",
        "        \"detected_bugs\": found,\n",
        "        \"score\": score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36020ac3",
      "metadata": {},
      "outputs": [],
      "source": [
        "last_result: dict = {}\n",
        "\n",
        "def format_infest_md(result: dict) -> str:\n",
        "    if \"error\" in result:\n",
        "        return f\"**Error:** {result['error']}\"\n",
        "    code = result.get(\"buggy_code\", \"\")\n",
        "    bugs = result.get(\"injected_bugs\", [])\n",
        "    lines = [f\"### Infested Code\\n\\n```python\\n{code}\\n```\\n\", \"### Injected Bugs\\n\"]\n",
        "    for b in bugs:\n",
        "        lines.append(f\"- **{b.get('type', '?')}**: {b.get('description', '')}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_validate_md(result: dict) -> str:\n",
        "    if \"error\" in result:\n",
        "        return f\"**Error:** {result['error']}\"\n",
        "    found = result.get(\"detected_bugs\", [])\n",
        "    score = result.get(\"score\", {})\n",
        "    lines = [\"### Detected Bugs\\n\"]\n",
        "    for b in found:\n",
        "        lines.append(f\"- **{b.get('type', '?')}** at `{b.get('location', '?')}`: {b.get('description', '')}\")\n",
        "    lines.append(f\"\\n### Match Score\\n\")\n",
        "    lines.append(f\"| Metric | Value |\")\n",
        "    lines.append(f\"|--------|-------|\")\n",
        "    lines.append(f\"| Injected | {score.get('injected_freq', 'none')} |\")\n",
        "    lines.append(f\"| Found | {score.get('found_freq', 'none')} |\")\n",
        "    lines.append(f\"| Matched | {score.get('matched', 0)} |\")\n",
        "    lines.append(f\"| Missed | {score.get('missed', 0)} |\")\n",
        "    lines.append(f\"| False Positives | {score.get('false_positives', 0)} |\")\n",
        "    lines.append(f\"| **Precision** | **{score.get('precision', 0)}** |\")\n",
        "    lines.append(f\"| **Recall** | **{score.get('recall', 0)}** |\")\n",
        "    lines.append(f\"| **F1 Score** | **{score.get('f1', 0)}** |\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def on_run(infest_model, validate_model, sample_name, bug_types, num_bugs, verbose, progress=gr.Progress()):\n",
        "    global last_result, _verbose, _debug_log\n",
        "    _verbose = verbose\n",
        "    _debug_log = []\n",
        "\n",
        "    if not bug_types:\n",
        "        gr.Warning(\"Select at least one bug type.\")\n",
        "        return \"Select bug types.\", \"Select bug types.\", \"{}\", \"\"\n",
        "\n",
        "    clean_code = CLEAN_SAMPLES.get(sample_name, \"\")\n",
        "    if not clean_code:\n",
        "        return \"No sample selected.\", \"No sample selected.\", \"{}\", \"\"\n",
        "\n",
        "    infest_id = MODELS[infest_model]\n",
        "    validate_id = MODELS[validate_model]\n",
        "    _log(f\"CONFIG — infest={infest_id}, validate={validate_id}, bugs={bug_types}, n={int(num_bugs)}\")\n",
        "\n",
        "    def progress_cb(frac, msg):\n",
        "        progress(frac, desc=msg)\n",
        "\n",
        "    result = run_benchmark(infest_id, validate_id, clean_code, bug_types, int(num_bugs), progress_cb)\n",
        "    last_result = result\n",
        "\n",
        "    _log(f\"\\nBENCHMARK COMPLETE\")\n",
        "    log_text = \"\\n\".join(_debug_log) if _verbose else \"\"\n",
        "\n",
        "    if \"error\" in result:\n",
        "        err = f\"**Error:** {result['error']}\"\n",
        "        return err, err, json.dumps(result, indent=2), log_text\n",
        "\n",
        "    infest_md = format_infest_md(result)\n",
        "    validate_md = format_validate_md(result)\n",
        "    return infest_md, validate_md, json.dumps(result, indent=2), log_text\n",
        "\n",
        "\n",
        "def on_sample_select(sample_name):\n",
        "    return CLEAN_SAMPLES.get(sample_name, \"\")\n",
        "\n",
        "\n",
        "model_names = list(MODELS.keys())\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Bug Benchmark Arena\") as app:\n",
        "    gr.Markdown(\n",
        "        \"## Bug Benchmark Arena\\n\"\n",
        "        \"Pick an **Infest model** and a **Validate model**, select a clean code sample and bug types, \"\n",
        "        \"then hit **Run Benchmark** to see how well each model performs.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        sample_dd = gr.Dropdown(choices=list(CLEAN_SAMPLES.keys()), value=\"Binary Search\", label=\"Code Sample\")\n",
        "        num_bugs_sl = gr.Slider(1, 5, value=2, step=1, label=\"Bugs to inject\")\n",
        "\n",
        "    code_preview = gr.Code(value=CLEAN_SAMPLES[\"Binary Search\"], language=\"python\", label=\"Clean Code\", interactive=False)\n",
        "    bug_cb = gr.CheckboxGroup(choices=BUG_TYPES, value=BUG_TYPES[:3], label=\"Bug Types\")\n",
        "\n",
        "    with gr.Row():\n",
        "        infest_dd = gr.Dropdown(choices=model_names, value=model_names[0], label=\"Infest Model\")\n",
        "        validate_dd = gr.Dropdown(choices=model_names, value=model_names[1], label=\"Validate Model\")\n",
        "\n",
        "    verbose_ck = gr.Checkbox(value=False, label=\"Show debug logs (prompts & responses)\")\n",
        "\n",
        "    run_btn = gr.Button(\"Run Benchmark\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"#### Infest Results\")\n",
        "            infest_output = gr.Markdown(\"*Run a benchmark to see infest results.*\")\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"#### Validate Results\")\n",
        "            validate_output = gr.Markdown(\"*Run a benchmark to see validation results.*\")\n",
        "\n",
        "    with gr.Accordion(\"Raw JSON Output\", open=False):\n",
        "        json_output = gr.Textbox(lines=15, max_lines=15, interactive=False, show_label=False)\n",
        "\n",
        "    with gr.Accordion(\"Debug Logs\", open=False):\n",
        "        debug_box = gr.Textbox(lines=12, max_lines=12, interactive=False, show_label=False, placeholder=\"Enable the debug toggle and run a benchmark to see logs.\")\n",
        "\n",
        "    sample_dd.change(fn=on_sample_select, inputs=[sample_dd], outputs=[code_preview])\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=lambda: gr.update(interactive=False),\n",
        "        outputs=[run_btn],\n",
        "    ).then(\n",
        "        fn=on_run,\n",
        "        inputs=[infest_dd, validate_dd, sample_dd, bug_cb, num_bugs_sl, verbose_ck],\n",
        "        outputs=[infest_output, validate_output, json_output, debug_box],\n",
        "    ).then(\n",
        "        fn=lambda: gr.update(interactive=True),\n",
        "        outputs=[run_btn],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d37a34e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "app.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
