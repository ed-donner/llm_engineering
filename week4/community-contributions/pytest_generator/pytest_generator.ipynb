{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba193fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import uuid\n",
    "import json\n",
    "import textwrap\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Protocol, Tuple, Dict, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import BadRequestError as _OpenAIBadRequest\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# --- Provider base URLs (Gemini & Groq speak OpenAI-compatible API) ---\n",
    "GEMINI_BASE = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "GROQ_BASE   = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# --- API Keys (add these in your .env) ---\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")   # OpenAI\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")   # Gemini\n",
    "groq_api_key   = os.getenv(\"GROQ_API_KEY\")     # Groq\n",
    "\n",
    "# --- Clients ---\n",
    "openai_client = OpenAI()  # OpenAI default (reads OPENAI_API_KEY)\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=GEMINI_BASE) if google_api_key else None\n",
    "groq_client   = OpenAI(api_key=groq_api_key,   base_url=GROQ_BASE)   if groq_api_key   else None\n",
    "\n",
    "# --- Model registry: label -> { client, model } ---\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "def _register(label: str, client: Optional[OpenAI], model_id: str):\n",
    "    \"\"\"Add a model to the registry only if its client is configured.\"\"\"\n",
    "    if client is not None:\n",
    "        MODEL_REGISTRY[label] = {\"client\": client, \"model\": model_id}\n",
    "\n",
    "# OpenAI\n",
    "_register(\"OpenAI ‚Ä¢ GPT-5\",        openai_client, \"gpt-5\")\n",
    "_register(\"OpenAI ‚Ä¢ GPT-5 Nano\",   openai_client, \"gpt-5-nano\")\n",
    "_register(\"OpenAI ‚Ä¢ GPT-4o-mini\",  openai_client, \"gpt-4o-mini\")\n",
    "\n",
    "# Gemini (Google)\n",
    "_register(\"Gemini ‚Ä¢ 2.5 Pro\",      gemini_client, \"gemini-2.5-pro\")\n",
    "_register(\"Gemini ‚Ä¢ 2.5 Flash\",    gemini_client, \"gemini-2.5-flash\")\n",
    "\n",
    "# Groq\n",
    "_register(\"Groq ‚Ä¢ Llama 3.1 8B\",   groq_client,   \"llama-3.1-8b-instant\")\n",
    "_register(\"Groq ‚Ä¢ Llama 3.1 70B\",  groq_client,   \"llama-3.1-70b-versatile\")\n",
    "_register(\"Groq ‚Ä¢ GPT-OSS 20B\",    groq_client,   \"gpt-oss-20b\")\n",
    "_register(\"Groq ‚Ä¢ GPT-OSS 120B\",   groq_client,   \"gpt-oss-120b\")\n",
    "\n",
    "DEFAULT_MODEL = next(iter(MODEL_REGISTRY.keys()), None)\n",
    "\n",
    "print(f\"Providers configured ‚Üí OpenAI:{bool(openai_api_key)}  Gemini:{bool(google_api_key)}  Groq:{bool(groq_api_key)}\")\n",
    "print(\"Models available     ‚Üí\", \", \".join(MODEL_REGISTRY.keys()) or \"None (add API keys in .env)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionClient(Protocol):\n",
    "    \"\"\"Any LLM client provides a .complete() method using a registry label.\"\"\"\n",
    "    def complete(self, *, model_label: str, system: str, user: str) -> str: ...\n",
    "\n",
    "\n",
    "def _extract_code_or_text(s: str) -> str:\n",
    "    \"\"\"Prefer fenced python if present; otherwise return raw text.\"\"\"\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    return m.group(1).strip() if m else s.strip()\n",
    "\n",
    "\n",
    "class MultiModelChatClient:\n",
    "    \"\"\"Routes requests to the right provider/client based on model label.\"\"\"\n",
    "    def __init__(self, registry: Dict[str, Dict[str, object]]):\n",
    "        self._registry = registry\n",
    "\n",
    "    def _call(self, *, client: OpenAI, model_id: str, system: str, user: str) -> str:\n",
    "        params = {\n",
    "            \"model\": model_id,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\",   \"content\": user},\n",
    "            ],\n",
    "        }\n",
    "        resp = client.chat.completions.create(**params)  # do NOT send temperature for strict providers\n",
    "        text = (resp.choices[0].message.content or \"\").strip()\n",
    "        return _extract_code_or_text(text)\n",
    "\n",
    "    def complete(self, *, model_label: str, system: str, user: str) -> str:\n",
    "        if model_label not in self._registry:\n",
    "            raise ValueError(f\"Unknown model label: {model_label}\")\n",
    "        info   = self._registry[model_label]\n",
    "        client = info[\"client\"]\n",
    "        model  = info[\"model\"]\n",
    "        try:\n",
    "            return self._call(client=client, model_id=str(model), system=system, user=user)\n",
    "        except _OpenAIBadRequest as e:\n",
    "            # Providers may reject stray params; we don't send any, but retry anyway.\n",
    "            if \"temperature\" in str(e).lower():\n",
    "                return self._call(client=client, model_id=str(model), system=system, user=user)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31558bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_code_or_text(s: str) -> str:\n",
    "    \"\"\"Prefer fenced python if present; otherwise return raw text.\"\"\"\n",
    "    m = re.search(r\"```(?:python)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    return m.group(1).strip() if m else s.strip()\n",
    "\n",
    "\n",
    "def _ensure_header_and_import(code: str, module_name: str) -> str:\n",
    "    \"\"\"Make sure tests import the module and pytest; keep output minimal.\"\"\"\n",
    "    code = code.strip()\n",
    "    needs_pytest = \"import pytest\" not in code\n",
    "    needs_import = f\"import {module_name}\" not in code and f\"import {module_name} as mod\" not in code\n",
    "\n",
    "    header_lines = []\n",
    "    if needs_pytest:\n",
    "        header_lines.append(\"import pytest\")\n",
    "    if needs_import:\n",
    "        header_lines.append(f\"import {module_name} as mod\")\n",
    "\n",
    "    if header_lines:\n",
    "        code = \"\\n\".join(header_lines) + \"\\n\\n\" + code\n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeadedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SymbolInfo:\n",
    "    kind: str      # \"function\" | \"class\" | \"method\"\n",
    "    name: str\n",
    "    signature: str\n",
    "    lineno: int\n",
    "\n",
    "\n",
    "class PublicAPIExtractor:\n",
    "    \"\"\"Extract a small 'public API' summary from a Python module.\"\"\"\n",
    "    def extract(self, source: str) -> List[SymbolInfo]:\n",
    "        tree = ast.parse(source)\n",
    "        out: List[SymbolInfo] = []\n",
    "\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, ast.FunctionDef) and not node.name.startswith(\"_\"):\n",
    "                out.append(SymbolInfo(\"function\", node.name, self._sig(node), node.lineno))\n",
    "            elif isinstance(node, ast.ClassDef) and not node.name.startswith(\"_\"):\n",
    "                out.append(SymbolInfo(\"class\", node.name, node.name, node.lineno))\n",
    "                for sub in node.body:\n",
    "                    if isinstance(sub, ast.FunctionDef) and not sub.name.startswith(\"_\"):\n",
    "                        out.append(SymbolInfo(\"method\",\n",
    "                                              f\"{node.name}.{sub.name}\",\n",
    "                                              self._sig(sub),\n",
    "                                              sub.lineno))\n",
    "        return sorted(out, key=lambda s: (s.kind, s.name.lower(), s.lineno))\n",
    "\n",
    "    def _sig(self, fn: ast.FunctionDef) -> str:\n",
    "        args = [a.arg for a in fn.args.args]\n",
    "        if fn.args.vararg:\n",
    "            args.append(\"*\" + fn.args.vararg.arg)\n",
    "        args.extend(a.arg + \"=?\" for a in fn.args.kwonlyargs)\n",
    "        if fn.args.kwarg:\n",
    "            args.append(\"**\" + fn.args.kwarg.arg)\n",
    "        ret = \"\"\n",
    "        if fn.returns is not None:\n",
    "            try:\n",
    "                ret = f\" -> {ast.unparse(fn.returns)}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        return f\"def {fn.name}({', '.join(args)}){ret}:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ac5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Builds concise, deterministic prompts for pytest generation.\"\"\"\n",
    "    SYSTEM = (\n",
    "        \"You are a senior Python engineer. Produce a single, self-contained pytest file.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Output only Python test code (no prose, no markdown fences).\\n\"\n",
    "        \"- Use plain pytest tests (functions), no classes unless unavoidable.\\n\"\n",
    "        \"- Deterministic: avoid network/IO; seed randomness if used.\\n\"\n",
    "        \"- Import the target module by module name.\\n\"\n",
    "        \"- Create a minimal test covering every public function and method.\\n\"\n",
    "        \"- Prefer straightforward, fast assertions over exhaustive checks.\\n\"\n",
    "    )\n",
    "\n",
    "    def build_user(self, *, module_name: str, source: str, symbols: List[SymbolInfo]) -> str:\n",
    "        summary = \"\\n\".join(f\"- {s.kind:<6}  {s.signature}\" for s in symbols) or \"- (no public symbols)\"\n",
    "        return textwrap.dedent(f\"\"\"\n",
    "        Create pytest tests for module `{module_name}`.\n",
    "\n",
    "        Public API Summary:\n",
    "        {summary}\n",
    "\n",
    "        Constraints:\n",
    "        - Import as: `import {module_name} as mod`\n",
    "        - Keep tests tiny, fast, and deterministic.\n",
    "\n",
    "        Full module source (for reference):\n",
    "        # --- BEGIN SOURCE {module_name}.py ---\n",
    "        {source}\n",
    "        # --- END SOURCE ---\n",
    "        \"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGenerator:\n",
    "    \"\"\"Orchestrates extraction, prompt, model call, and final polish.\"\"\"\n",
    "    def __init__(self, llm: CompletionClient):\n",
    "        self._llm = llm\n",
    "        self._extractor = PublicAPIExtractor()\n",
    "        self._prompts = PromptBuilder()\n",
    "\n",
    "    def generate_tests(self, model_label: str, module_name: str, source: str) -> str:\n",
    "        symbols = self._extractor.extract(source)\n",
    "        user = self._prompts.build_user(module_name=module_name, source=source, symbols=symbols)\n",
    "        raw = self._llm.complete(model_label=model_label, system=self._prompts.SYSTEM, user=user)\n",
    "        return _ensure_header_and_import(raw, module_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = OpenAIChatClient(OPENAI_CLIENT)\n",
    "SERVICE = TestGenerator(LLM, TESTGEN_MODEL)\n",
    "\n",
    "def build_module_name_from_path(path: str) -> str:\n",
    "    p = Path(path)\n",
    "    return p.stem\n",
    "\n",
    "def generate_from_code(module_name: str, code: str, save: bool, out_dir: str) -> tuple[str, str]:\n",
    "    if not module_name.strip():\n",
    "        return \"\", \"‚ùå Please provide a module name.\"\n",
    "    if not code.strip():\n",
    "        return \"\", \"‚ùå Please paste some Python code.\"\n",
    "\n",
    "    tests_code = SERVICE.generate_tests(module_name=module_name.strip(), source=code)\n",
    "    saved = \"\"\n",
    "    if save:\n",
    "        out = Path(out_dir or \"tests\")\n",
    "        out.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = out / f\"test_{module_name}.py\"\n",
    "        out_path.write_text(tests_code, encoding=\"utf-8\")\n",
    "        saved = f\"‚úÖ Saved to {out_path}\"\n",
    "    return tests_code, saved\n",
    "\n",
    "\n",
    "def generate_from_file(file_obj, save: bool, out_dir: str) -> tuple[str, str]:\n",
    "    if file_obj is None:\n",
    "        return \"\", \"‚ùå Please upload a .py file.\"\n",
    "    code = file_obj.decode(\"utf-8\")\n",
    "    module_name = build_module_name_from_path(\"uploaded_module.py\")\n",
    "    return generate_from_code(module_name, code, save, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d240ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Simple PyTest Generator\") as ui:\n",
    "    gr.Markdown(\"## üß™ Simple PyTest Generator (Week 4 ‚Ä¢ Community Contribution)\\n\"\n",
    "                \"Generate **minimal, deterministic** pytest tests from a Python module using a Frontier model.\")\n",
    "\n",
    "    with gr.Tab(\"Paste Code\"):\n",
    "        with gr.Row():\n",
    "            module_name = gr.Textbox(label=\"Module name (used in `import <name> as mod`)\", value=\"mymodule\")\n",
    "        code_in = gr.Code(label=\"Python module code\", language=\"python\", lines=22)\n",
    "        with gr.Row():\n",
    "            save_cb = gr.Checkbox(label=\"Save to /tests\", value=True)\n",
    "            out_dir = gr.Textbox(label=\"Output folder\", value=\"tests\")\n",
    "        gen_btn = gr.Button(\"Generate tests\", variant=\"primary\")\n",
    "        with gr.Row():\n",
    "            tests_out = gr.Code(label=\"Generated tests (pytest)\", language=\"python\", lines=20)\n",
    "        status = gr.Markdown()\n",
    "\n",
    "        def _on_gen(name, code, save, outdir):\n",
    "            tests, msg = generate_from_code(name, code, save, outdir)\n",
    "            return tests, (msg or \"‚úÖ Done\")\n",
    "\n",
    "        gen_btn.click(_on_gen, inputs=[module_name, code_in, save_cb, out_dir], outputs=[tests_out, status])\n",
    "\n",
    "    with gr.Tab(\"Upload .py\"):\n",
    "        upload = gr.File(file_types=[\".py\"], label=\"Upload a Python module\")\n",
    "        with gr.Row():\n",
    "            save_cb2 = gr.Checkbox(label=\"Save to /tests\", value=True)\n",
    "            out_dir2 = gr.Textbox(label=\"Output folder\", value=\"tests\")\n",
    "        gen_btn2 = gr.Button(\"Generate tests from file\")\n",
    "        tests_out2 = gr.Code(label=\"Generated tests (pytest)\", language=\"python\", lines=20)\n",
    "        status2 = gr.Markdown()\n",
    "\n",
    "        def _on_gen_file(f, save, outdir):\n",
    "            tests, msg = generate_from_file(f.read() if f else None, save, outdir)\n",
    "            return tests, (msg or \"‚úÖ Done\")\n",
    "\n",
    "        gen_btn2.click(_on_gen_file, inputs=[upload, save_cb2, out_dir2], outputs=[tests_out2, status2])\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
