{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Assignment: Unit Test Codegen Tool\n",
        "\n",
        "Use a frontier model to generate unit tests for Python code. Paste your code, pick a model, generate tests, and run them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pytest found: /Users/collinewaitire/projects/python-p/llm_engineering/.venv/lib/python3.12/site-packages/pytest/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import re\n",
        "import contextlib\n",
        "import unittest\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# pytest is required to run generated tests\n",
        "try:\n",
        "    import pytest\n",
        "    print(\"pytest found:\", pytest.__file__)\n",
        "except ImportError:\n",
        "    print(\"WARNING: pytest not installed. Generated tests will fail to run.\")\n",
        "    print(\"From the project root (llm_engineering) run:  uv sync\")\n",
        "    print(\"Then restart the kernel and ensure this notebook uses that Python (e.g. select the '.venv' kernel).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Anthropic API Key not set (optional)\n",
            "Google API Key not set (optional)\n",
            "Grok API Key not set (optional)\n",
            "Groq API Key not set (optional)\n",
            "OpenRouter API Key not set (optional)\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "grok_api_key = os.getenv('GROK_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set (optional)\")\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set (optional)\")\n",
        "if grok_api_key:\n",
        "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Grok API Key not set (optional)\")\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
        "else:\n",
        "    print(\"Groq API Key not set (optional)\")\n",
        "if openrouter_api_key:\n",
        "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
        "else:\n",
        "    print(\"OpenRouter API Key not set (optional)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to client libraries (same as day5)\n",
        "\n",
        "openai = OpenAI()\n",
        "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
        "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "grok_url = \"https://api.x.ai/v1\"\n",
        "groq_url = \"https://api.groq.com/openai/v1\"\n",
        "ollama_url = \"http://localhost:11434/v1\"\n",
        "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
        "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
        "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
        "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
        "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
        "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
        "\n",
        "models = [\n",
        "    \"gpt-5\",\n",
        "    \"claude-sonnet-4-5-20250929\",\n",
        "    \"grok-4\",\n",
        "    \"gemini-2.5-pro\",\n",
        "    \"qwen2.5-coder\",\n",
        "    \"deepseek-coder-v2\",\n",
        "    \"gpt-oss:20b\",\n",
        "    \"qwen/qwen3-coder-30b-a3b-instruct\",\n",
        "    \"openai/gpt-oss-120b\",\n",
        "]\n",
        "\n",
        "clients = {\n",
        "    \"gpt-5\": openai,\n",
        "    \"claude-sonnet-4-5-20250929\": anthropic,\n",
        "    \"grok-4\": grok,\n",
        "    \"gemini-2.5-pro\": gemini,\n",
        "    \"openai/gpt-oss-120b\": groq,\n",
        "    \"qwen2.5-coder\": ollama,\n",
        "    \"deepseek-coder-v2\": ollama,\n",
        "    \"gpt-oss:20b\": ollama,\n",
        "    \"qwen/qwen3-coder-30b-a3b-instruct\": openrouter,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unit test generation prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an expert at writing unit tests for Python code.\n",
        "Your task is to generate comprehensive, runnable unit tests for the given Python code.\n",
        "\n",
        "Rules:\n",
        "- Use pytest. Use assert statements and pytest idioms (e.g. pytest.raises for exceptions).\n",
        "- Respond ONLY with Python code. No markdown, no explanation outside comments.\n",
        "- Include the original code in your response so the tests can run in one block (paste the source first, then the test code).\n",
        "- Cover normal cases, edge cases, and error cases where appropriate.\n",
        "- If the code has dependencies (e.g. requests), mock them in tests; use unittest.mock or pytest fixtures as needed.\n",
        "- Add a brief comment before each test describing what it checks.\n",
        "\"\"\"\n",
        "\n",
        "def user_prompt_for(python_code: str) -> str:\n",
        "    return f\"\"\"Generate pytest unit tests for this Python code.\n",
        "Include the original code in your response first, then the test code, so everything can be executed together in a single block.\n",
        "Respond only with Python code.\n",
        "\n",
        "Python code to test:\n",
        "\n",
        "```python\n",
        "{python_code}\n",
        "```\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def messages_for(python_code: str):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python_code)},\n",
        "    ]\n",
        "\n",
        "def extract_code(reply: str) -> str:\n",
        "    \"\"\"Strip markdown code fences from model output.\"\"\"\n",
        "    reply = reply.strip()\n",
        "    for pattern in (r'```python\\s*', r'```\\s*'):\n",
        "        reply = re.sub(f'^{pattern}', '', reply)\n",
        "        reply = re.sub(f'{pattern}$', '', reply)\n",
        "    return reply.strip()\n",
        "\n",
        "def generate_unit_tests(model: str, python_code: str) -> str:\n",
        "    \"\"\"Call the chosen model to generate unit tests for the given Python code.\"\"\"\n",
        "    if not python_code or not python_code.strip():\n",
        "        return \"Please paste some Python code first.\"\n",
        "    client = clients.get(model)\n",
        "    if not client:\n",
        "        return f\"Unknown model: {model}\"\n",
        "    try:\n",
        "        kwargs = {\"model\": model, \"messages\": messages_for(python_code)}\n",
        "        if \"gpt\" in model and \"groq\" not in model and \"openrouter\" not in model:\n",
        "            kwargs[\"reasoning_effort\"] = \"high\"\n",
        "        response = client.chat.completions.create(**kwargs)\n",
        "        reply = response.choices[0].message.content or \"\"\n",
        "        return extract_code(reply)\n",
        "    except Exception as e:\n",
        "        return f\"Error calling model: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run generated tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _strip_ansi(text: str) -> str:\n",
        "    \"\"\"Remove ANSI escape codes so output is readable in Gradio textbox.\"\"\"\n",
        "    return re.sub(r\"\\x1b\\[[0-9;]*m\", \"\", text)\n",
        "\n",
        "def _clean_pytest_output(text: str, temp_path: str = \"\") -> str:\n",
        "    \"\"\"Strip ANSI and shorten temp file paths in pytest output.\"\"\"\n",
        "    text = _strip_ansi(text)\n",
        "    if temp_path:\n",
        "        base = os.path.basename(temp_path)\n",
        "        text = text.replace(temp_path, \"generated_tests.py\")\n",
        "        # Replace full relative path (greedy): ../../../../path/tmpXXX.py -> generated_tests.py\n",
        "        text = re.sub(r\"(\\.\\./)+[^\\s]*\" + re.escape(base), \"generated_tests.py\", text)\n",
        "        # Remove any leftover ../ or .. before \"generated_tests.py\"\n",
        "        text = re.sub(r\"(\\.\\./)*(\\.\\.)?generated_tests\\.py\", \"generated_tests.py\", text)\n",
        "    return text\n",
        "\n",
        "def _output_to_html(plain_text: str) -> str:\n",
        "    \"\"\"Convert pytest output to HTML with green for PASSED, red for FAILED.\"\"\"\n",
        "    import html\n",
        "    lines = plain_text.splitlines()\n",
        "    out = []\n",
        "    for line in lines:\n",
        "        escaped = html.escape(line)\n",
        "        if \" PASSED \" in line or line.strip().endswith(\"PASSED\"):\n",
        "            escaped = escaped.replace(\"PASSED\", '<span style=\"color:#0a0;font-weight:600\">PASSED</span>')\n",
        "        if \" FAILED \" in line or line.strip().endswith(\"FAILED\"):\n",
        "            escaped = escaped.replace(\"FAILED\", '<span style=\"color:#c00;font-weight:600\">FAILED</span>')\n",
        "        if \"passed\" in line and \"warning\" in line.lower() and \"=\" in line:\n",
        "            escaped = f'<div style=\"margin-top:0.75em;font-weight:600;color:#0a0;\">{escaped}</div>'\n",
        "        elif \"failed\" in line and \"=\" in line:\n",
        "            escaped = f'<div style=\"margin-top:0.75em;font-weight:600;color:#c00;\">{escaped}</div>'\n",
        "        out.append(escaped)\n",
        "    return \"<pre style='margin:0;font-family:monospace;font-size:0.9em;line-height:1.4;'>\" + \"\\n\".join(out) + \"</pre>\"\n",
        "\n",
        "def run_unit_tests(code: str) -> str:\n",
        "    \"\"\"Execute the combined code (source + tests) and run tests.\"\"\"\n",
        "    if not code or not code.strip():\n",
        "        return \"No code to run.\"\n",
        "    buffer = io.StringIO()\n",
        "    temp_path = \"\"\n",
        "    try:\n",
        "        with contextlib.redirect_stdout(buffer), contextlib.redirect_stderr(buffer):\n",
        "            # Ensure pytest is available (generated code often does \"import pytest\" at top)\n",
        "            try:\n",
        "                import pytest\n",
        "            except ImportError:\n",
        "                buffer.write(\n",
        "                    \"pytest is not installed in this Python environment.\\n\\n\"\n",
        "                    \"Fix: From the project root folder (llm_engineering) run:\\n\"\n",
        "                    \"  uv sync\\n\\n\"\n",
        "                    \"Then restart the kernel and pick the kernel that uses this project's \"\n",
        "                    \"environment (in Cursor: click the kernel name top-right → Select Another Kernel \"\n",
        "                    \"→ Python Environments → choose the one that shows '.venv' or 'llm_engineering').\\n\"\n",
        "                )\n",
        "                return buffer.getvalue()\n",
        "            ns = {}\n",
        "            try:\n",
        "                exec(code, ns)\n",
        "            except ModuleNotFoundError as e:\n",
        "                if \"pytest\" in str(e):\n",
        "                    buffer.write(\n",
        "                        \"pytest not found in this environment.\\n\\n\"\n",
        "                        \"Use the project's Python: from the llm_engineering folder run 'uv sync', \"\n",
        "                        \"then restart the kernel and select the .venv / llm_engineering kernel.\\n\"\n",
        "                    )\n",
        "                    return buffer.getvalue()\n",
        "                raise\n",
        "            # Run unittest.TestCase subclasses if present\n",
        "            test_cases = [\n",
        "                obj for obj in ns.values()\n",
        "                if isinstance(obj, type) and issubclass(obj, unittest.TestCase)\n",
        "            ]\n",
        "            if test_cases:\n",
        "                suite = unittest.TestSuite()\n",
        "                for case in test_cases:\n",
        "                    suite.addTests(unittest.defaultTestLoader.loadTestsFromTestCase(case))\n",
        "                runner = unittest.TextTestRunner(stream=buffer, verbosity=2)\n",
        "                runner.run(suite)\n",
        "            else:\n",
        "                # Pytest-style: write to temp file and run pytest (no color for clean UI output)\n",
        "                import tempfile\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n",
        "                    f.write(code)\n",
        "                    temp_path = f.name\n",
        "                try:\n",
        "                    pytest.main([temp_path, \"-v\", \"--tb=short\", \"-x\"])\n",
        "                finally:\n",
        "                    os.unlink(temp_path)\n",
        "    except Exception as e:\n",
        "        buffer.write(f\"Error: {e}\")\n",
        "    out = buffer.getvalue()\n",
        "    return _clean_pytest_output(out, temp_path)\n",
        "\n",
        "def run_unit_tests_html(code: str) -> str:\n",
        "    \"\"\"Run tests and return HTML with green/red for Gradio.\"\"\"\n",
        "    plain = run_unit_tests(code)\n",
        "    return _output_to_html(plain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run_unit_tests is defined in the cell above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_code = \"\"\"def add(a: int, b: int) -> int:\n",
        "    return a + b\n",
        "\n",
        "def divide(a: float, b: float) -> float:\n",
        "    if b == 0:\n",
        "        raise ValueError(\"division by zero\")\n",
        "    return a / b\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(title=\"Unit Test Codegen\") as ui:\n",
        "    gr.Markdown(\"# Unit Test Codegen Tool\")\n",
        "    gr.Markdown(\"Paste Python code, choose a model, and generate pytest unit tests. Then run them.\")\n",
        "    with gr.Row():\n",
        "        code_in = gr.Code(\n",
        "            label=\"Python code to test\",\n",
        "            value=sample_code,\n",
        "            language=\"python\",\n",
        "            lines=12,\n",
        "        )\n",
        "        test_code = gr.Code(\n",
        "            label=\"Generated unit tests (source + tests)\",\n",
        "            value=\"\",\n",
        "            language=\"python\",\n",
        "            lines=20,\n",
        "        )\n",
        "    with gr.Row():\n",
        "        model = gr.Dropdown(choices=models, value=models[0], label=\"Model\")\n",
        "        generate_btn = gr.Button(\"Generate unit tests\")\n",
        "        run_btn = gr.Button(\"Run tests\")\n",
        "    test_output = gr.HTML(label=\"Test run output\", value=\"<pre style='margin:0;color:#666'>Run tests to see output. Passing tests will appear in green, failed in red.</pre>\")\n",
        "\n",
        "    generate_btn.click(fn=generate_unit_tests, inputs=[model, code_in], outputs=[test_code])\n",
        "    run_btn.click(fn=run_unit_tests_html, inputs=[test_code], outputs=[test_output])\n",
        "\n",
        "ui.launch(inbrowser=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
