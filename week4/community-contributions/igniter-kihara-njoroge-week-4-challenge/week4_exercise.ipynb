{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 Challenge\n",
        "\n",
        "Using different open source models to:\n",
        "- Detect bugs and security vulnerabilities\n",
        "- Suggest code improvements\n",
        "- Generate unit tests\n",
        "- Add docstrings and comments\n",
        "- Compare results across multiple models\n",
        "\n",
        "**Models used** (via OpenRouter): Qwen2.5-Coder 32B, Qwen3 Coder, DeepSeek R1, GPT-OSS-20B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "from typing import Any\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "openrouter_client: OpenAI | None = None\n",
        "if openrouter_api_key:\n",
        "    openrouter_client = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n",
        "    print(f\"OpenRouter connected (key: {openrouter_api_key[:6]}...)\")\n",
        "else:\n",
        "    print(\"OPENROUTER_API_KEY not set â€” add it to your .env file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Model registry â€” add or remove models here without touching any other code\n",
        "# ---------------------------------------------------------------------------\n",
        "MODELS: dict[str, dict[str, Any]] = {}\n",
        "\n",
        "if openrouter_client:\n",
        "    _openrouter_models = [\n",
        "        {\n",
        "            \"key\": \"qwen-2.5-coder-32b\",\n",
        "            \"name\": \"Qwen2.5-Coder 32B\",\n",
        "            \"model\": \"qwen/qwen-2.5-coder-32b-instruct\",\n",
        "            \"description\": \"Alibaba's top open-source code model\",\n",
        "        },\n",
        "        {\n",
        "            \"key\": \"qwen3-coder\",\n",
        "            \"name\": \"Qwen3 Coder 480B A35B\",\n",
        "            \"model\": \"qwen/qwen3-coder\",\n",
        "            \"description\": \"Latest Qwen3 coding model\",\n",
        "        },\n",
        "        {\n",
        "            \"key\": \"deepseek-r1\",\n",
        "            \"name\": \"DeepSeek R1\",\n",
        "            \"model\": \"deepseek/deepseek-r1\",\n",
        "            \"description\": \"DeepSeek reasoning model\",\n",
        "        },\n",
        "        {\n",
        "            \"key\": \"gpt-oss-20b\",\n",
        "            \"name\": \"GPT-OSS 20B\",\n",
        "            \"model\": \"openai/gpt-oss-20b\",\n",
        "            \"description\": \"OpenAI open-source 20B model\",\n",
        "        },\n",
        "    ]\n",
        "    for m in _openrouter_models:\n",
        "        MODELS[m[\"key\"]] = {**m, \"client\": openrouter_client}\n",
        "\n",
        "if MODELS:\n",
        "    print(f\"{len(MODELS)} model(s) ready:\")\n",
        "    for key, cfg in MODELS.items():\n",
        "        print(f\"   â€¢ {cfg['name']} ({key})\")\n",
        "else:\n",
        "    print(\"No models configured. Ensure OPENROUTER_API_KEY is set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. System Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "BUG_DETECTION_SYSTEM_PROMPT = \"\"\"You are an expert code reviewer specializing in bugs, security vulnerabilities, and logic errors.\n",
        "\n",
        "Analyze the provided Python code and return ONLY a valid JSON array. No preamble, no explanation, no markdown fences.\n",
        "\n",
        "Each element must follow this schema exactly:\n",
        "[\n",
        "  {\n",
        "    \"severity\": \"critical|high|medium|low\",\n",
        "    \"line\": <integer or null>,\n",
        "    \"issue\": \"concise description of the problem\",\n",
        "    \"suggestion\": \"actionable fix\"\n",
        "  }\n",
        "]\n",
        "\n",
        "Severity definitions:\n",
        "- critical: security vulnerabilities, data loss, crashes\n",
        "- high: logic bugs that produce wrong results\n",
        "- medium: edge-case failures, missing error handling\n",
        "- low: code smell, minor style issues\n",
        "\n",
        "Return [] if no issues are found.\"\"\"\n",
        "\n",
        "\n",
        "DOCSTRING_SYSTEM_PROMPT = \"\"\"You are a senior Python engineer specialising in clear, professional documentation.\n",
        "\n",
        "Task: enhance the given Python code by adding Google-style docstrings and inline comments.\n",
        "\n",
        "Rules:\n",
        "- Add module, class, and function docstrings following PEP 257\n",
        "- Use Google-style (Args / Returns / Raises sections)\n",
        "- Write inline comments only for non-obvious logic â€” never for obvious lines\n",
        "- Preserve ALL original logic unchanged\n",
        "- Return ONLY the updated Python code â€” no markdown fences, no explanations\"\"\"\n",
        "\n",
        "\n",
        "IMPROVEMENTS_SYSTEM_PROMPT = \"\"\"You are a senior software engineer focused on code quality.\n",
        "\n",
        "Analyse the Python code and suggest improvements. Return ONLY a valid JSON array. No preamble, no markdown.\n",
        "\n",
        "Schema:\n",
        "[\n",
        "  {\n",
        "    \"category\": \"readability|performance|style|error_handling|security\",\n",
        "    \"line\": <integer or null>,\n",
        "    \"current\": \"current code snippet (keep short)\",\n",
        "    \"improved\": \"improved code snippet (keep short)\",\n",
        "    \"explanation\": \"why this change is beneficial\"\n",
        "  }\n",
        "]\n",
        "\n",
        "Only include meaningful, actionable improvements. Return [] if the code is already well-written.\"\"\"\n",
        "\n",
        "\n",
        "TEST_GENERATION_SYSTEM_PROMPT = \"\"\"You are an expert in writing comprehensive pytest test suites.\n",
        "\n",
        "Generate pytest unit tests for the given Python code. Requirements:\n",
        "- Cover happy-path, edge cases, and error conditions\n",
        "- Write tests for any identified bugs\n",
        "- Use descriptive test function names (test_<function>_<scenario>)\n",
        "- Add a brief docstring to each test explaining what it checks\n",
        "- Group tests in a class per function under test\n",
        "- Include all required imports at the top\n",
        "\n",
        "Return ONLY valid Python code. No explanations, no markdown fences.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Core API Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _call_model(\n",
        "    model_key: str,\n",
        "    system_prompt: str,\n",
        "    user_prompt: str,\n",
        "    temperature: float = 0.1,\n",
        "    max_tokens: int = 4096,\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"Low-level wrapper around the chat completion API.\n",
        "\n",
        "    Args:\n",
        "        model_key: Key into the MODELS registry.\n",
        "        system_prompt: Instruction prompt for the model.\n",
        "        user_prompt: The user-facing input.\n",
        "        temperature: Sampling temperature (lower = more deterministic).\n",
        "        max_tokens: Maximum tokens in the response.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys ``success`` (bool), ``model`` (str),\n",
        "        ``content`` (str | None), and ``error`` (str | None).\n",
        "    \"\"\"\n",
        "    if model_key not in MODELS:\n",
        "        return {\"success\": False, \"model\": model_key, \"content\": None,\n",
        "                \"error\": f\"Unknown model key: '{model_key}'\"}\n",
        "\n",
        "    cfg = MODELS[model_key]\n",
        "    try:\n",
        "        response = cfg[\"client\"].chat.completions.create(\n",
        "            model=cfg[\"model\"],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "        )\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"model\": cfg[\"name\"],\n",
        "            \"content\": response.choices[0].message.content,\n",
        "            \"error\": None,\n",
        "        }\n",
        "    except Exception as exc:\n",
        "        logger.error(\"Model call failed for '%s': %s\", model_key, exc)\n",
        "        return {\"success\": False, \"model\": cfg[\"name\"], \"content\": None, \"error\": str(exc)}\n",
        "\n",
        "\n",
        "def _extract_json(text: str) -> list[dict]:\n",
        "    \"\"\"Robustly extract a JSON array from model output.\n",
        "\n",
        "    Handles optional markdown code fences and leading/trailing prose.\n",
        "\n",
        "    Args:\n",
        "        text: Raw model response text.\n",
        "\n",
        "    Returns:\n",
        "        Parsed list of dicts, or an empty list on failure.\n",
        "    \"\"\"\n",
        "    # Strip markdown fences\n",
        "    text = re.sub(r\"```(?:json)?\\n?\", \"\", text).strip()\n",
        "\n",
        "    # Try greedy JSON array match first, then full-text parse\n",
        "    for candidate in (re.search(r\"\\[\\s*\\{.*\\}\\s*\\]\", text, re.DOTALL), None):\n",
        "        snippet = candidate.group() if candidate else text\n",
        "        try:\n",
        "            parsed = json.loads(snippet)\n",
        "            if isinstance(parsed, list):\n",
        "                return parsed\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "    logger.warning(\"Could not parse JSON from model response; returning empty list.\")\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_bugs(code: str, model_key: str) -> dict[str, Any]:\n",
        "    \"\"\"Detect bugs and security issues in Python code.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to analyse.\n",
        "        model_key: Key of the model to use from MODELS.\n",
        "\n",
        "    Returns:\n",
        "        dict with ``model``, ``issues`` (list), ``success`` (bool),\n",
        "        and optionally ``error`` (str).\n",
        "    \"\"\"\n",
        "    result = _call_model(\n",
        "        model_key,\n",
        "        BUG_DETECTION_SYSTEM_PROMPT,\n",
        "        f\"Analyse this Python code for bugs and security issues:\\n\\n```python\\n{code}\\n```\",\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    issues = _extract_json(result[\"content\"]) if result[\"success\"] else []\n",
        "    return {**result, \"issues\": issues}\n",
        "\n",
        "\n",
        "def add_docstrings(code: str, model_key: str) -> dict[str, Any]:\n",
        "    \"\"\"Add Google-style docstrings and inline comments to Python code.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to document.\n",
        "        model_key: Key of the model to use from MODELS.\n",
        "\n",
        "    Returns:\n",
        "        dict with ``model``, ``documented_code`` (str), ``success`` (bool),\n",
        "        and optionally ``error`` (str).\n",
        "    \"\"\"\n",
        "    result = _call_model(\n",
        "        model_key,\n",
        "        DOCSTRING_SYSTEM_PROMPT,\n",
        "        f\"Add proper docstrings and inline comments to this Python code:\\n\\n```python\\n{code}\\n```\",\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    # Strip any residual markdown fences the model may have added\n",
        "    documented = re.sub(r\"```(?:python)?\\n?\", \"\", result.get(\"content\") or \"\").strip()\n",
        "    return {**result, \"documented_code\": documented}\n",
        "\n",
        "\n",
        "def suggest_improvements(code: str, model_key: str) -> dict[str, Any]:\n",
        "    \"\"\"Suggest readability, performance, and best-practice improvements.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to improve.\n",
        "        model_key: Key of the model to use from MODELS.\n",
        "\n",
        "    Returns:\n",
        "        dict with ``model``, ``improvements`` (list), ``success`` (bool),\n",
        "        and optionally ``error`` (str).\n",
        "    \"\"\"\n",
        "    result = _call_model(\n",
        "        model_key,\n",
        "        IMPROVEMENTS_SYSTEM_PROMPT,\n",
        "        f\"Suggest improvements for this Python code:\\n\\n```python\\n{code}\\n```\",\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    improvements = _extract_json(result[\"content\"]) if result[\"success\"] else []\n",
        "    return {**result, \"improvements\": improvements}\n",
        "\n",
        "\n",
        "def generate_tests(code: str, bugs: list[dict], model_key: str) -> dict[str, Any]:\n",
        "    \"\"\"Generate a pytest test suite for the given code.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to test.\n",
        "        bugs: List of bug dicts (from detect_bugs) to include regression tests for.\n",
        "        model_key: Key of the model to use from MODELS.\n",
        "\n",
        "    Returns:\n",
        "        dict with ``model``, ``test_code`` (str), ``success`` (bool),\n",
        "        and optionally ``error`` (str).\n",
        "    \"\"\"\n",
        "    bugs_section = \"\"\n",
        "    if bugs:\n",
        "        bug_lines = \"\\n\".join(\n",
        "            f\"  - Line {b.get('line', '?')} [{b.get('severity', '?').upper()}]: {b.get('issue', '')}\"\n",
        "            for b in bugs\n",
        "        )\n",
        "        bugs_section = f\"\\n\\nKnown bugs to cover with regression tests:\\n{bug_lines}\"\n",
        "\n",
        "    result = _call_model(\n",
        "        model_key,\n",
        "        TEST_GENERATION_SYSTEM_PROMPT,\n",
        "        f\"Generate pytest tests for this Python code:{bugs_section}\\n\\n```python\\n{code}\\n```\",\n",
        "        temperature=0.3,\n",
        "        max_tokens=6000,\n",
        "    )\n",
        "    test_code = re.sub(r\"```(?:python)?\\n?\", \"\", result.get(\"content\") or \"\").strip()\n",
        "    return {**result, \"test_code\": test_code}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Formatting Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "_SEVERITY_EMOJI = {\"CRITICAL\": \"ðŸ”´\", \"HIGH\": \"ðŸŸ \", \"MEDIUM\": \"ðŸŸ¡\", \"LOW\": \"ðŸ”µ\"}\n",
        "_SEVERITY_ORDER = {\"critical\": 0, \"high\": 1, \"medium\": 2, \"low\": 3}\n",
        "\n",
        "\n",
        "def _error_msg(result: dict) -> str:\n",
        "    \"\"\"Return a formatted error string for a failed model call.\"\"\"\n",
        "    return f\"**Error ({result.get('model', 'Unknown')}):** {result.get('error', 'Unknown error')}\"\n",
        "\n",
        "\n",
        "def format_bugs_output(result: dict) -> str:\n",
        "    \"\"\"Render bug-detection results as Markdown.\"\"\"\n",
        "    if not result.get(\"success\"):\n",
        "        return _error_msg(result)\n",
        "\n",
        "    issues = result.get(\"issues\", [])\n",
        "    if not issues:\n",
        "        return f\"**{result['model']}**: No issues found â€” code looks clean!\"\n",
        "\n",
        "    sorted_issues = sorted(issues, key=lambda x: _SEVERITY_ORDER.get(x.get(\"severity\", \"low\"), 3))\n",
        "    lines = [f\"### {result['model']} â€” {len(issues)} issue(s) found\\n\"]\n",
        "\n",
        "    for issue in sorted_issues:\n",
        "        sev = issue.get(\"severity\", \"unknown\").upper()\n",
        "        emoji = _SEVERITY_EMOJI.get(sev, \"âšª\")\n",
        "        line_ref = f\"Line {issue['line']}\" if issue.get(\"line\") else \"General\"\n",
        "        lines.append(f\"{emoji} **{sev}** ({line_ref}): {issue.get('issue', '')}\")\n",
        "        if fix := issue.get(\"suggestion\"):\n",
        "            lines.append(f\"*Fix:* {fix}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_docstring_output(result: dict) -> str:\n",
        "    \"\"\"Return the documented code (plain Python string for gr.Code).\"\"\"\n",
        "    if not result.get(\"success\"):\n",
        "        return _error_msg(result)\n",
        "    return result.get(\"documented_code\") or f\"**{result.get('model', '?')}**: No output generated.\"\n",
        "\n",
        "\n",
        "def format_improvements_output(result: dict) -> str:\n",
        "    \"\"\"Render improvement suggestions as Markdown.\"\"\"\n",
        "    if not result.get(\"success\"):\n",
        "        return _error_msg(result)\n",
        "\n",
        "    improvements = result.get(\"improvements\", [])\n",
        "    if not improvements:\n",
        "        return f\"**{result['model']}**: Code follows best practices â€” no major changes needed.\"\n",
        "\n",
        "    lines = [f\"### {result['model']} â€” {len(improvements)} suggestion(s)\\n\"]\n",
        "    for imp in improvements:\n",
        "        category = imp.get(\"category\", \"general\").replace(\"_\", \" \").title()\n",
        "        line_ref = f\"Line {imp['line']}\" if imp.get(\"line\") else \"General\"\n",
        "        lines.append(f\"\\n**{category}** ({line_ref})\")\n",
        "        current = imp.get(\"current\", \"\")\n",
        "        improved = imp.get(\"improved\", \"\")\n",
        "        if current and improved:\n",
        "            lines.append(f\"  - Before: `{current[:80]}{'â€¦' if len(current) > 80 else ''}`\")\n",
        "            lines.append(f\"  - After:  `{improved[:80]}{'â€¦' if len(improved) > 80 else ''}`\")\n",
        "        if explanation := imp.get(\"explanation\"):\n",
        "            lines.append(f\"{explanation}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_tests_output(result: dict) -> str:\n",
        "    \"\"\"Return generated test code (plain string for gr.Code).\"\"\"\n",
        "    if not result.get(\"success\"):\n",
        "        return _error_msg(result)\n",
        "    return result.get(\"test_code\") or f\"**{result.get('model', '?')}**: No tests generated.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Orchestration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def review_code(\n",
        "    code: str,\n",
        "    model_key: str,\n",
        "    include_tests: bool = True,\n",
        ") -> tuple[str, str, str, str]:\n",
        "    \"\"\"Run a complete code review: bugs â†’ improvements â†’ docstrings â†’ tests.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to review.\n",
        "        model_key: Model registry key to use for all steps.\n",
        "        include_tests: Whether to generate unit tests.\n",
        "\n",
        "    Returns:\n",
        "        4-tuple of (bugs_md, improvements_md, test_code, documented_code).\n",
        "    \"\"\"\n",
        "    if not code.strip():\n",
        "        empty = \"Please provide some Python code to review.\"\n",
        "        return empty, empty, \"\", \"\"\n",
        "\n",
        "    bugs_result = detect_bugs(code, model_key)\n",
        "    improvements_result = suggest_improvements(code, model_key)\n",
        "    docstring_result = add_docstrings(code, model_key)\n",
        "\n",
        "    test_code = \"\"\n",
        "    if include_tests:\n",
        "        tests_result = generate_tests(code, bugs_result.get(\"issues\", []), model_key)\n",
        "        test_code = format_tests_output(tests_result)\n",
        "\n",
        "    return (\n",
        "        format_bugs_output(bugs_result),\n",
        "        format_improvements_output(improvements_result),\n",
        "        test_code,\n",
        "        format_docstring_output(docstring_result),\n",
        "    )\n",
        "\n",
        "\n",
        "def compare_models(code: str, model_keys: list[str] | None = None) -> str:\n",
        "    \"\"\"Run bug detection across several models and summarise results.\n",
        "\n",
        "    Args:\n",
        "        code: Python source code to analyse.\n",
        "        model_keys: Keys of models to compare. Defaults to all configured models.\n",
        "\n",
        "    Returns:\n",
        "        Markdown-formatted comparison report.\n",
        "    \"\"\"\n",
        "    if not code.strip():\n",
        "        return \"Please provide code to review.\"\n",
        "\n",
        "    keys = model_keys or list(MODELS.keys())\n",
        "    results = [detect_bugs(code, k) for k in keys]\n",
        "\n",
        "    lines = [\"##Model Comparison\\n\"]\n",
        "\n",
        "    # Per-model summary\n",
        "    for res in results:\n",
        "        model_name = res[\"model\"]\n",
        "        if not res.get(\"success\"):\n",
        "            lines.append(f\"**{model_name}**: {res.get('error', 'Unknown error')}\")\n",
        "            continue\n",
        "\n",
        "        issues = res.get(\"issues\", [])\n",
        "        severity_counts: dict[str, int] = {}\n",
        "        for issue in issues:\n",
        "            sev = issue.get(\"severity\", \"low\")\n",
        "            severity_counts[sev] = severity_counts.get(sev, 0) + 1\n",
        "\n",
        "        breakdown = \", \".join(f\"{k}: {v}\" for k, v in sorted(severity_counts.items()))\n",
        "        summary = f\"({breakdown})\" if breakdown else \"(no issues)\"\n",
        "        lines.append(f\"**{model_name}**: {len(issues)} issue(s) {summary}\")\n",
        "\n",
        "    # Consensus issues â€” found by 2+ models on the same line\n",
        "    if len(results) > 1:\n",
        "        sig_to_models: dict[str, list[str]] = {}\n",
        "        for res in results:\n",
        "            if not res.get(\"success\"):\n",
        "                continue\n",
        "            for issue in res.get(\"issues\", []):\n",
        "                sig = f\"{issue.get('line')}-{issue.get('issue', '')[:50]}\"\n",
        "                sig_to_models.setdefault(sig, []).append(res[\"model\"])\n",
        "\n",
        "        consensus = [(sig, ms) for sig, ms in sig_to_models.items() if len(ms) > 1]\n",
        "        if consensus:\n",
        "            lines.append(f\"\\n###Consensus Issues ({len(consensus)} agreed by 2+ models)\")\n",
        "            for sig, ms in consensus:\n",
        "                lines.append(f\"  - `{sig}` â€” flagged by: {', '.join(ms)}\")\n",
        "        else:\n",
        "            lines.append(\"\\n*No consensus issues â€” models found different problems.*\")\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXAMPLE_CODE = '''\\\n",
        "def divide_numbers(a, b):\n",
        "    return a / b  # ZeroDivisionError if b == 0\n",
        "\n",
        "\n",
        "def process_user_data(user_input):\n",
        "    # SECURITY: eval executes arbitrary code â€” never use with untrusted input\n",
        "    result = eval(user_input)\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_user_by_id(user_id):\n",
        "    # SQL injection: user_id is inserted directly into the query string\n",
        "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
        "    return query\n",
        "\n",
        "\n",
        "def calculate_average(numbers):\n",
        "    total = sum(numbers)\n",
        "    return total / len(numbers)  # ZeroDivisionError if numbers is empty\n",
        "'''\n",
        "\n",
        "FIBONACCI_EXAMPLE = '''\\\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
        "'''\n",
        "\n",
        "CONFIG_EXAMPLE = '''\\\n",
        "def parse_config(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return eval(f.read())\n",
        "'''\n",
        "\n",
        "\n",
        "def create_ui() -> gr.Blocks:\n",
        "    \"\"\"Build and return the Gradio interface.\"\"\"\n",
        "    model_choices = [(f\"{cfg['name']} â€” {cfg['description']}\", key) for key, cfg in MODELS.items()]\n",
        "    default_model = list(MODELS.keys())[0] if MODELS else None\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"AI Code Review Assistant\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\".tab-nav { font-size: 1rem !important; }\",\n",
        "    ) as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "#AI-Powered Code Review Assistant\n",
        "Analyse Python code with open-source LLMs â€” detect bugs, suggest improvements, generate tests & docstrings.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Row(equal_height=False):\n",
        "            # â”€â”€ Left panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            with gr.Column(scale=2, min_width=360):\n",
        "                code_input = gr.Code(\n",
        "                    label=\"Python Code\",\n",
        "                    value=EXAMPLE_CODE,\n",
        "                    language=\"python\",\n",
        "                    lines=22,\n",
        "                )\n",
        "\n",
        "                model_selector = gr.Dropdown(\n",
        "                    choices=model_choices,\n",
        "                    value=default_model,\n",
        "                    label=\"Model\",\n",
        "                    info=\"Open-source model via OpenRouter\",\n",
        "                    interactive=bool(MODELS),\n",
        "                )\n",
        "\n",
        "                include_tests = gr.Checkbox(label=\"Generate unit tests\", value=True)\n",
        "\n",
        "                with gr.Row():\n",
        "                    review_btn = gr.Button(\"Review Code\", variant=\"primary\", scale=2)\n",
        "                    compare_btn = gr.Button(\"Compare All Models\", variant=\"secondary\", scale=1)\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[[EXAMPLE_CODE], [FIBONACCI_EXAMPLE], [CONFIG_EXAMPLE]],\n",
        "                    inputs=[code_input],\n",
        "                    label=\"Example snippets\",\n",
        "                )\n",
        "\n",
        "            # â”€â”€ Right panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            with gr.Column(scale=3, min_width=480):\n",
        "                with gr.Tabs():\n",
        "                    with gr.Tab(\"Bugs\"):\n",
        "                        bugs_output = gr.Markdown(\n",
        "                            value=\"*Select a model and click **Review Code** to begin.*\"\n",
        "                        )\n",
        "                    with gr.Tab(\"Improvements\"):\n",
        "                        improvements_output = gr.Markdown(\n",
        "                            value=\"*Improvement suggestions will appear here.*\"\n",
        "                        )\n",
        "                    with gr.Tab(\"Tests\"):\n",
        "                        tests_output = gr.Code(\n",
        "                            label=\"Generated pytest suite\",\n",
        "                            language=\"python\",\n",
        "                            lines=28,\n",
        "                        )\n",
        "                    with gr.Tab(\"Docstrings\"):\n",
        "                        docstring_output = gr.Code(\n",
        "                            label=\"Documented code\",\n",
        "                            language=\"python\",\n",
        "                            lines=28,\n",
        "                        )\n",
        "                    with gr.Tab(\"Comparison\"):\n",
        "                        comparison_output = gr.Markdown(\n",
        "                            value=\"*Click **Compare All Models** to see a side-by-side analysis.*\"\n",
        "                        )\n",
        "\n",
        "        review_btn.click(\n",
        "            fn=review_code,\n",
        "            inputs=[code_input, model_selector, include_tests],\n",
        "            outputs=[bugs_output, improvements_output, tests_output, docstring_output],\n",
        "        )\n",
        "\n",
        "        compare_btn.click(\n",
        "            fn=lambda code: compare_models(code),\n",
        "            inputs=[code_input],\n",
        "            outputs=[comparison_output],\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "demo = create_ui()\n",
        "demo.launch(inbrowser=True, share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quick Notebook Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity-check: run bug detection on a trivial snippet\n",
        "if MODELS:\n",
        "    _test_code = \"\"\"\\\n",
        "def divide(a, b):\n",
        "    return a / b\n",
        "\"\"\"\n",
        "    _result = detect_bugs(_test_code, list(MODELS.keys())[0])\n",
        "    print(format_bugs_output(_result))\n",
        "else:\n",
        "    print(\"No models configured â€” skipping test.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-engineering (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
