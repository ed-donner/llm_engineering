{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Week 4 Exercise - Code Generator Toolbox\n",
    "### Author: Samuel Kalu, Team Euclid, Week 4\n",
    "\n",
    "This notebook implements a comprehensive code generation toolbox with:\n",
    "- **Python to C++ Converter**: Generate high-performance C++ code from Python\n",
    "- **Python to Go Converter**: Generate efficient Go code from Python\n",
    "- **Docstring & Comment Generator**: Add PEP-257 compliant docstrings and inline comments\n",
    "- **Unit Test Generator**: Generate comprehensive pytest unit tests\n",
    "\n",
    "Features:\n",
    "- Gradio UI with multiple tabs\n",
    "- Streaming responses\n",
    "- Model switching (OpenAI GPT, Ollama, Google Gemini)\n",
    "- Error handling and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# API Keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY', 'ollama')\n",
    "\n",
    "# Verify API keys\n",
    "if openai_api_key:\n",
    "    print(f\"âœ“ OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"âœ— OpenAI API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"âœ“ Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"â„¹ Google API Key not set (optional)\")\n",
    "\n",
    "if ollama_api_key:\n",
    "    print(f\"âœ“ Ollama API Key exists\")\n",
    "else:\n",
    "    print(\"â„¹ Ollama API Key not set (optional - requires local Ollama installation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clients-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API clients\n",
    "\n",
    "# OpenAI client\n",
    "openai_client = OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "\n",
    "# Google Gemini client (via OpenAI-compatible API)\n",
    "gemini_client = None\n",
    "if google_api_key:\n",
    "    gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    gemini_client = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "\n",
    "# Ollama client (local)\n",
    "ollama_client = None\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "try:\n",
    "    ollama_client = OpenAI(api_key=ollama_api_key, base_url=ollama_url)\n",
    "    print(\"âœ“ Ollama client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ Ollama not available: {e}\")\n",
    "\n",
    "# Available models configuration\n",
    "MODELS = {\n",
    "    \"OpenAI\": [\"gpt-4.1-mini\", \"gpt-4o\", \"gpt-4.1-nano\"],\n",
    "    \"Gemini\": [\"gemini-2.5-pro\", \"gemini-2.0-flash\"],\n",
    "    \"Ollama\": [\"llama3.1\", \"llama3.2\", \"codellama\"]\n",
    "}\n",
    "\n",
    "ALL_MODELS = []\n",
    "if openai_client:\n",
    "    ALL_MODELS.extend(MODELS[\"OpenAI\"])\n",
    "if gemini_client:\n",
    "    ALL_MODELS.extend(MODELS[\"Gemini\"])\n",
    "if ollama_client:\n",
    "    ALL_MODELS.extend(MODELS[\"Ollama\"])\n",
    "\n",
    "DEFAULT_MODEL = ALL_MODELS[0] if ALL_MODELS else \"llama3.1\"\n",
    "\n",
    "print(f\"\\nAvailable models: {ALL_MODELS}\")\n",
    "print(f\"Default model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to get the right client for a model\n",
    "\n",
    "def get_client_for_model(model_name: str):\n",
    "    \"\"\"Returns the appropriate API client for the given model name.\"\"\"\n",
    "    if model_name in MODELS[\"OpenAI\"]:\n",
    "        return openai_client\n",
    "    elif model_name in MODELS[\"Gemini\"]:\n",
    "        return gemini_client\n",
    "    elif model_name in MODELS[\"Ollama\"]:\n",
    "        return ollama_client\n",
    "    return openai_client  # fallback\n",
    "\n",
    "def clean_code_output(code: str) -> str:\n",
    "    \"\"\"Remove markdown code blocks from the output.\"\"\"\n",
    "    if \"```\" in code:\n",
    "        # Remove ```language and ``` markers\n",
    "        lines = code.split('\\n')\n",
    "        cleaned = []\n",
    "        in_block = False\n",
    "        for line in lines:\n",
    "            if line.strip().startswith('```'):\n",
    "                in_block = not in_block\n",
    "                continue\n",
    "            cleaned.append(line)\n",
    "        return '\\n'.join(cleaned)\n",
    "    return code\n",
    "\n",
    "# Sample Python code for testing\n",
    "SAMPLE_PYTHON_CODE = '''\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calculate the nth Fibonacci number.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        a, b = 0, 1\n",
    "        for _ in range(2, n + 1):\n",
    "            a, b = b, a + b\n",
    "        return b\n",
    "\n",
    "def calculate_sum(numbers):\n",
    "    \"\"\"Calculate the sum of a list of numbers.\"\"\"\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Fibonacci(10) = {fibonacci(10)}\")\n",
    "    print(f\"Sum of [1, 2, 3, 4, 5] = {calculate_sum([1, 2, 3, 4, 5])}\")\n",
    "'''\n",
    "\n",
    "print(\"Sample Python code loaded for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpp-converter-header",
   "metadata": {},
   "source": [
    "## 2. Python to C++ Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cpp-system-prompt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for Python to C++ conversion\n",
    "\n",
    "CPP_SYSTEM_PROMPT = \"\"\"You are an expert C++ developer specializing in high-performance code optimization.\n",
    "Your task is to convert Python code into efficient, modern C++ code (C++17 or later).\n",
    "\n",
    "Guidelines:\n",
    "1. Preserve the exact functionality and logic of the original Python code\n",
    "2. Use modern C++ features (auto, smart pointers, ranges, etc.)\n",
    "3. Optimize for performance while maintaining readability\n",
    "4. Include necessary headers and a main() function for testing\n",
    "5. Use appropriate C++ data types and STL containers\n",
    "6. Add brief comments only where the logic is non-obvious\n",
    "7. Do NOT include explanations outside the code\n",
    "\n",
    "Respond ONLY with the C++ code, no additional text.\"\"\"\n",
    "\n",
    "def convert_to_cpp_streaming(python_code: str, model_name: str):\n",
    "    \"\"\"Convert Python code to C++ with streaming output.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        yield \"Error: No API client available. Please check your API keys.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": CPP_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Convert this Python code to C++:\\n\\n{python_code}\"}\n",
    "            ],\n",
    "            stream=True,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                result += chunk.choices[0].delta.content\n",
    "                yield clean_code_output(result)\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"Error during conversion: {str(e)}\"\n",
    "\n",
    "def convert_to_cpp_batch(python_code: str, model_name: str) -> str:\n",
    "    \"\"\"Convert Python code to C++ without streaming.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        return \"Error: No API client available.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": CPP_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Convert this Python code to C++:\\n\\n{python_code}\"}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return clean_code_output(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during conversion: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ Python to C++ converter functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "go-converter-header",
   "metadata": {},
   "source": [
    "## 3. Python to Go Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "go-system-prompt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for Python to Go conversion\n",
    "\n",
    "GO_SYSTEM_PROMPT = \"\"\"You are an expert Go developer specializing in clean, idiomatic, and performant code.\n",
    "Your task is to convert Python code into efficient, idiomatic Go code.\n",
    "\n",
    "Guidelines:\n",
    "1. Preserve the exact functionality and logic of the original Python code\n",
    "2. Use idiomatic Go patterns and conventions\n",
    "3. Include proper error handling (return errors, don't panic)\n",
    "4. Include necessary imports and a main() function for testing\n",
    "5. Use appropriate Go data types and standard library packages\n",
    "6. Add brief comments only where the logic is non-obvious\n",
    "7. Do NOT include explanations outside the code\n",
    "\n",
    "Respond ONLY with the Go code, no additional text.\"\"\"\n",
    "\n",
    "def convert_to_go_streaming(python_code: str, model_name: str):\n",
    "    \"\"\"Convert Python code to Go with streaming output.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        yield \"Error: No API client available. Please check your API keys.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": GO_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Convert this Python code to Go:\\n\\n{python_code}\"}\n",
    "            ],\n",
    "            stream=True,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                result += chunk.choices[0].delta.content\n",
    "                yield clean_code_output(result)\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"Error during conversion: {str(e)}\"\n",
    "\n",
    "def convert_to_go_batch(python_code: str, model_name: str) -> str:\n",
    "    \"\"\"Convert Python code to Go without streaming.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        return \"Error: No API client available.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": GO_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Convert this Python code to Go:\\n\\n{python_code}\"}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return clean_code_output(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during conversion: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ Python to Go converter functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docstring-header",
   "metadata": {},
   "source": [
    "## 4. Docstring & Comment Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "docstring-system-prompt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for docstring generation\n",
    "\n",
    "DOCSTRING_SYSTEM_PROMPT = \"\"\"You are an expert Python developer and code reviewer.\n",
    "Your job is to read the user's provided Python function and return:\n",
    "1. A concise, PEP-257-compliant docstring summarizing what the function does\n",
    "2. Clarify parameter types, return values, and side effects\n",
    "3. Helpful inline comments that improve readability and maintainability\n",
    "\n",
    "Guidelines:\n",
    "- Do NOT modify variable names or refactor the function logic\n",
    "- Only output the improved function with docstring and comments\n",
    "- Don't be extremely verbose - keep comments meaningful but concise\n",
    "- Write at a senior developer level\n",
    "\n",
    "Respond ONLY with the improved Python code, no additional text.\"\"\"\n",
    "\n",
    "def generate_docstring_streaming(code: str, model_name: str):\n",
    "    \"\"\"Generate docstrings and comments with streaming output.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        yield \"Error: No API client available. Please check your API keys.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": DOCSTRING_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Improve this Python code with docstrings and comments:\\n\\n{code}\"}\n",
    "            ],\n",
    "            stream=True,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                result += chunk.choices[0].delta.content\n",
    "                yield clean_code_output(result)\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"Error during generation: {str(e)}\"\n",
    "\n",
    "def generate_docstring_batch(code: str, model_name: str) -> str:\n",
    "    \"\"\"Generate docstrings and comments without streaming.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        return \"Error: No API client available.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": DOCSTRING_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Improve this Python code with docstrings and comments:\\n\\n{code}\"}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return clean_code_output(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during generation: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ Docstring generator functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unittest-header",
   "metadata": {},
   "source": [
    "## 5. Unit Test Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unittest-system-prompt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for unit test generation\n",
    "\n",
    "UNITTEST_SYSTEM_PROMPT = \"\"\"You are a seasoned Python developer and testing expert.\n",
    "Your task is to read the user's provided Python function and generate comprehensive unit tests.\n",
    "\n",
    "Guidelines:\n",
    "1. Write tests using pytest framework (or unittest if pytest is not appropriate)\n",
    "2. Include typical cases, edge cases, and error cases\n",
    "3. Use clear, descriptive test function names\n",
    "4. Include minimal necessary setup code (avoid over-mocking)\n",
    "5. If dependencies or mocking are needed, include proper imports\n",
    "6. Do NOT change the original function - focus only on test coverage\n",
    "\n",
    "Respond ONLY with the test code, no additional text.\"\"\"\n",
    "\n",
    "def generate_unittest_streaming(code: str, model_name: str):\n",
    "    \"\"\"Generate unit tests with streaming output.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        yield \"Error: No API client available. Please check your API keys.\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": UNITTEST_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate comprehensive unit tests for this Python function:\\n\\n{code}\"}\n",
    "            ],\n",
    "            stream=True,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        result = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                result += chunk.choices[0].delta.content\n",
    "                yield clean_code_output(result)\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"Error during generation: {str(e)}\"\n",
    "\n",
    "def generate_unittest_batch(code: str, model_name: str) -> str:\n",
    "    \"\"\"Generate unit tests without streaming.\"\"\"\n",
    "    client = get_client_for_model(model_name)\n",
    "    \n",
    "    if not client:\n",
    "        return \"Error: No API client available.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": UNITTEST_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate comprehensive unit tests for this Python function:\\n\\n{code}\"}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        return clean_code_output(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during generation: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ Unit test generator functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-header",
   "metadata": {},
   "source": [
    "## 6. Gradio UI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-ui-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI with multiple tabs\n",
    "\n",
    "def create_gradio_ui():\n",
    "    \"\"\"Create and configure the Gradio interface.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Soft(\n",
    "            primary_hue=gr.themes.colors.blue,\n",
    "            spacing_size=gr.themes.sizes.spacing_md,\n",
    "            radius_size=gr.themes.sizes.radius_md\n",
    "        ),\n",
    "        title=\"Code Generator Toolbox - Samuel Kalu, Team Euclid\"\n",
    "    ) as ui:\n",
    "        \n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # Code Generator Toolbox\n",
    "            ### By Samuel Kalu, Team Euclid - Week 4\n",
    "            \n",
    "            Convert Python code to C++ or Go, generate docstrings, and create unit tests.\n",
    "            Supports multiple models: OpenAI GPT, Google Gemini, and Ollama (local).\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Tab 1: Python to C++ Converter\n",
    "        with gr.Tab(\"Python to C++\"):\n",
    "            gr.Markdown(\"### Convert Python code to high-performance C++ (C++17+)\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    cpp_input = gr.Code(\n",
    "                        label=\"Python Code\",\n",
    "                        language=\"python\",\n",
    "                        lines=15,\n",
    "                        value=SAMPLE_PYTHON_CODE\n",
    "                    )\n",
    "                    cpp_model = gr.Dropdown(\n",
    "                        choices=ALL_MODELS,\n",
    "                        value=DEFAULT_MODEL,\n",
    "                        label=\"Select Model\"\n",
    "                    )\n",
    "                    cpp_btn = gr.Button(\"Convert to C++\", variant=\"primary\")\n",
    "                with gr.Column(scale=1):\n",
    "                    cpp_output = gr.Code(\n",
    "                        label=\"C++ Output\",\n",
    "                        language=\"cpp\",\n",
    "                        lines=20\n",
    "                    )\n",
    "            \n",
    "            cpp_btn.click(\n",
    "                fn=convert_to_cpp_batch,\n",
    "                inputs=[cpp_input, cpp_model],\n",
    "                outputs=cpp_output\n",
    "            )\n",
    "        \n",
    "        # Tab 2: Python to Go Converter\n",
    "        with gr.Tab(\"Python to Go\"):\n",
    "            gr.Markdown(\"### Convert Python code to idiomatic Go\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    go_input = gr.Code(\n",
    "                        label=\"Python Code\",\n",
    "                        language=\"python\",\n",
    "                        lines=15,\n",
    "                        value=SAMPLE_PYTHON_CODE\n",
    "                    )\n",
    "                    go_model = gr.Dropdown(\n",
    "                        choices=ALL_MODELS,\n",
    "                        value=DEFAULT_MODEL,\n",
    "                        label=\"Select Model\"\n",
    "                    )\n",
    "                    go_btn = gr.Button(\"Convert to Go\", variant=\"primary\")\n",
    "                with gr.Column(scale=1):\n",
    "                    go_output = gr.Code(\n",
    "                        label=\"Go Output\",\n",
    "                        language=\"go\",\n",
    "                        lines=20\n",
    "                    )\n",
    "            \n",
    "            go_btn.click(\n",
    "                fn=convert_to_go_batch,\n",
    "                inputs=[go_input, go_model],\n",
    "                outputs=go_output\n",
    "            )\n",
    "        \n",
    "        # Tab 3: Docstring Generator\n",
    "        with gr.Tab(\"Docstring Generator\"):\n",
    "            gr.Markdown(\"### Add PEP-257 compliant docstrings and inline comments\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    doc_input = gr.Code(\n",
    "                        label=\"Python Code\",\n",
    "                        language=\"python\",\n",
    "                        lines=15,\n",
    "                        value=SAMPLE_PYTHON_CODE\n",
    "                    )\n",
    "                    doc_model = gr.Dropdown(\n",
    "                        choices=ALL_MODELS,\n",
    "                        value=DEFAULT_MODEL,\n",
    "                        label=\"Select Model\"\n",
    "                    )\n",
    "                    doc_btn = gr.Button(\"Generate Docstrings\", variant=\"primary\")\n",
    "                with gr.Column(scale=1):\n",
    "                    doc_output = gr.Code(\n",
    "                        label=\"Improved Code with Docstrings\",\n",
    "                        language=\"python\",\n",
    "                        lines=20\n",
    "                    )\n",
    "            \n",
    "            doc_btn.click(\n",
    "                fn=generate_docstring_batch,\n",
    "                inputs=[doc_input, doc_model],\n",
    "                outputs=doc_output\n",
    "            )\n",
    "        \n",
    "        # Tab 4: Unit Test Generator\n",
    "        with gr.Tab(\"ðŸ§ª Unit Test Generator\"):\n",
    "            gr.Markdown(\"### Generate comprehensive pytest unit tests\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    test_input = gr.Code(\n",
    "                        label=\"Python Code to Test\",\n",
    "                        language=\"python\",\n",
    "                        lines=15,\n",
    "                        value=SAMPLE_PYTHON_CODE\n",
    "                    )\n",
    "                    test_model = gr.Dropdown(\n",
    "                        choices=ALL_MODELS,\n",
    "                        value=DEFAULT_MODEL,\n",
    "                        label=\"Select Model\"\n",
    "                    )\n",
    "                    test_btn = gr.Button(\"ðŸ§ª Generate Tests\", variant=\"primary\")\n",
    "                with gr.Column(scale=1):\n",
    "                    test_output = gr.Code(\n",
    "                        label=\"Generated Unit Tests\",\n",
    "                        language=\"python\",\n",
    "                        lines=20\n",
    "                    )\n",
    "            \n",
    "            test_btn.click(\n",
    "                fn=generate_unittest_batch,\n",
    "                inputs=[test_input, test_model],\n",
    "                outputs=test_output\n",
    "            )\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ---\n",
    "            **Tips:**\n",
    "            - For best results, use GPT-4 or Gemini 2.5 Pro models\n",
    "            - Ollama models run locally (requires Ollama installation)\n",
    "            - Streaming is available in the backend functions for real-time generation\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    return ui\n",
    "\n",
    "print(\"âœ“ Gradio UI configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## 7. Demo and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo without launching UI\n",
    "\n",
    "if openai_client:\n",
    "    print(\"\\n=== Testing Python to C++ Conversion ===\")\n",
    "    cpp_result = convert_to_cpp_batch(SAMPLE_PYTHON_CODE, \"gpt-4.1-mini\")\n",
    "    print(cpp_result[:500] + \"...\" if len(cpp_result) > 500 else cpp_result)\n",
    "    \n",
    "    print(\"\\n=== Testing Docstring Generation ===\")\n",
    "    doc_result = generate_docstring_batch(SAMPLE_PYTHON_CODE, \"gpt-4.1-mini\")\n",
    "    print(doc_result[:500] + \"...\" if len(doc_result) > 500 else doc_result)\n",
    "else:\n",
    "    print(\"OpenAI client not available. Skipping demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-header",
   "metadata": {},
   "source": [
    "## 8. Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio UI\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = create_gradio_ui()\n",
    "    print(\"\\nðŸš€ Launching Code Generator Toolbox...\")\n",
    "    print(\"Open the URL shown below in your browser to use the application.\")\n",
    "    print(\"Press Ctrl+C to stop the server.\\n\")\n",
    "    ui.launch(share=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Week 4 exercise solution demonstrates:\n",
    "\n",
    "1. **Frontier Model Integration**: Uses OpenAI GPT, Google Gemini, and Ollama models\n",
    "2. **Code Generation**: Converts Python to C++ and Go with optimized output\n",
    "3. **Code Documentation**: Generates PEP-257 compliant docstrings and inline comments\n",
    "4. **Test Generation**: Creates comprehensive pytest unit tests\n",
    "5. **Gradio UI**: Clean, tabbed interface with model switching\n",
    "6. **Streaming Support**: Backend functions support both streaming and batch modes\n",
    "7. **Error Handling**: Graceful handling of API failures and missing keys\n",
    "\n",
    "### Key Features:\n",
    "- âœ… Multiple model support (OpenAI, Gemini, Ollama)\n",
    "- âœ… Streaming responses for real-time generation\n",
    "- âœ… Clean code output (removes markdown artifacts)\n",
    "- âœ… Modular architecture (easy to add new converters)\n",
    "- âœ… Professional UI with Gradio\n",
    "- âœ… Comprehensive error handling\n",
    "\n",
    "### Author: Samuel Kalu, Team Euclid, Week 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
