{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4: Reasoning & Code Benchmark System\n",
        "\n",
        "Benchmark **3 models** on generated difficult questions. **Reasoning** or **Code** mode. Select models via checkboxes; run benchmark to evaluate all selected models; view combined results and rankings. Runs locally and on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q gradio python-dotenv openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, re\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB, api_key = True, userdata.get(\"OPENROUTER_API_KEY\")\n",
        "except Exception:\n",
        "    load_dotenv(override=True)\n",
        "    IN_COLAB, api_key = False, os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openrouter = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\") if api_key else None\n",
        "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
        "QUESTION_GEN_MODEL = \"openai/gpt-4o\"\n",
        "if IN_COLAB:\n",
        "    MODEL_CHOICES = [\n",
        "        (\"gpt-4o-mini\", \"openai/gpt-4o-mini\", \"openrouter\"),\n",
        "        (\"Gemma 2 9B\", \"google/gemma-2-9b-it:free\", \"openrouter\"),\n",
        "        (\"Llama 3.1 8B\", \"meta-llama/llama-3.1-8b-instruct:free\", \"openrouter\"),\n",
        "    ]\n",
        "else:\n",
        "    MODEL_CHOICES = [\n",
        "        (\"gpt-4o-mini (OpenRouter)\", \"gpt-4o-mini\", \"openrouter\"),\n",
        "        (\"llama3.2 (Ollama)\", \"llama3.2\", \"ollama\"),\n",
        "        (\"Gemma 2 9B (OpenRouter)\", \"google/gemma-2-9b-it:free\", \"openrouter\"),\n",
        "    ]\n",
        "def get_client(b):\n",
        "    return openrouter if b == \"openrouter\" else ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BENCHMARK_MODES, NUM_QUESTIONS, benchmark_store = [\"reasoning\", \"code\"], 3, []\n",
        "Q_SYSTEM = \"Output only one question. No examples, no preamble, no explanation. Just the question, ending with ?\"\n",
        "\n",
        "def _extract_one_question(raw: str) -> str:\n",
        "    raw = raw.strip()\n",
        "    if not raw or len(raw) < 10:\n",
        "        return \"\"\n",
        "    idx = raw.find(\"?\")\n",
        "    if idx != -1:\n",
        "        return raw[: idx + 1].strip()\n",
        "    return raw[:400].strip()\n",
        "\n",
        "def _gen_one_question(mode: str) -> str:\n",
        "    if not openrouter:\n",
        "        return \"\"\n",
        "    if mode == \"reasoning\":\n",
        "        user = \"One difficult logical or mathematical reasoning question only. No code.\"\n",
        "    else:\n",
        "        user = \"One difficult programming or algorithm question only. No general logic puzzles.\"\n",
        "    try:\n",
        "        r = openrouter.chat.completions.create(\n",
        "            model=QUESTION_GEN_MODEL,\n",
        "            messages=[{\"role\": \"system\", \"content\": Q_SYSTEM}, {\"role\": \"user\", \"content\": user}],\n",
        "            max_tokens=256,\n",
        "        )\n",
        "        raw = (r.choices[0].message.content or \"\").strip()\n",
        "        return _extract_one_question(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def generate_synthetic_questions(mode, n=NUM_QUESTIONS):\n",
        "    qs = []\n",
        "    for _ in range(n * 2):\n",
        "        if len(qs) >= n:\n",
        "            break\n",
        "        q = _gen_one_question(mode)\n",
        "        if q and q not in qs:\n",
        "            qs.append(q)\n",
        "    return qs if qs else [f\"No question for {mode}. Set OPENROUTER_API_KEY and try again.\"]\n",
        "def get_mid_back(lbl):\n",
        "    d = {l: (m, b) for l, m, b in MODEL_CHOICES}\n",
        "    return d.get(lbl, MODEL_CHOICES[0][1:])\n",
        "def run_model(client, mid, qs, mode):\n",
        "    sys = \"Answer concisely. Reasoning: clear steps. Code: working code + brief explanation.\"\n",
        "    t0 = time.perf_counter()\n",
        "    ans = []\n",
        "    for q in qs:\n",
        "        try:\n",
        "            r = client.chat.completions.create(model=mid, messages=[{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\", \"content\": q}], max_tokens=1024)\n",
        "            ans.append((r.choices[0].message.content or \"\").strip())\n",
        "        except Exception as e:\n",
        "            ans.append(f\"[Error: {e}]\")\n",
        "    return ans, time.perf_counter() - t0\n",
        "def score_answers(qs, ans, mode, client, mid):\n",
        "    rub = \"Score correctness/clarity 1-10. Reply with one number.\"\n",
        "    sc = []\n",
        "    for q, a in zip(qs, ans):\n",
        "        try:\n",
        "            r = client.chat.completions.create(model=mid, messages=[{\"role\": \"system\", \"content\": rub}, {\"role\": \"user\", \"content\": f\"Q: {q}\\nA: {a}\"}], max_tokens=10)\n",
        "            raw = (r.choices[0].message.content or \"5\").strip()\n",
        "            sc.append(max(1, min(10, float(\"\".join(c for c in raw if c.isdigit() or c == \".\") or \"5\"))))\n",
        "        except Exception:\n",
        "            sc.append(5.0)\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_benchmark_single(mode, model_label, questions):\n",
        "    if not questions or questions[0].startswith(\"No question\"):\n",
        "        return None, None\n",
        "    mid, back = get_mid_back(model_label)\n",
        "    client = get_client(back)\n",
        "    if not client:\n",
        "        return f\"[{model_label}] No client (set OPENROUTER_API_KEY or Ollama).\", None\n",
        "    ans, elapsed = run_model(client, mid, questions, mode)\n",
        "    scores = score_answers(questions, ans, mode, client, mid)\n",
        "    avg = sum(scores) / len(scores) if scores else 0\n",
        "    acc = sum(1 for s in scores if s >= 7) / len(scores) * 100 if scores else 0\n",
        "    m = {\"avg_score\": round(avg, 2), \"accuracy_7\": round(acc, 1), \"time_sec\": round(elapsed, 2), \"n\": len(questions)}\n",
        "    benchmark_store.append({\"mode\": mode, \"model_name\": model_label, \"questions\": questions, \"answers\": ans, \"scores\": scores, \"time_sec\": elapsed, **m})\n",
        "    md = \"\\n\\n---\\n\\n\".join(f\"**Q{i+1}:** {q[:80]}...\\n**A:** {a[:200]}...\\n*Score: {s}*\" for i, (q, a, s) in enumerate(zip(questions, ans, scores)))\n",
        "    return md, m\n",
        "\n",
        "def run_benchmark_all(mode, model_labels, questions):\n",
        "    if not questions or questions[0].startswith(\"No question\"):\n",
        "        return \"Generate questions first.\", \"No questions\"\n",
        "    if not model_labels:\n",
        "        return \"Select at least one model (checkboxes).\", \"No models selected\"\n",
        "    parts = []\n",
        "    status_parts = []\n",
        "    for lbl in model_labels:\n",
        "        md, m = run_benchmark_single(mode, lbl, questions)\n",
        "        if md is None and m is None:\n",
        "            continue\n",
        "        if m is None:\n",
        "            parts.append(f\"### {lbl}\\n{md}\")\n",
        "            status_parts.append(f\"{lbl}: skipped\")\n",
        "            continue\n",
        "        parts.append(f\"### {lbl}\\n**Avg score:** {m['avg_score']} | **Accuracy (≥7):** {m['accuracy_7']}% | **Time:** {m['time_sec']}s\\n\\n{md}\")\n",
        "        status_parts.append(f\"{lbl}: {m['avg_score']}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts), \"Done. \" + \", \".join(status_parts)\n",
        "def get_table():\n",
        "    if not benchmark_store:\n",
        "        return pd.DataFrame(columns=[\"Rank\", \"Model\", \"Mode\", \"Avg Score\", \"Accuracy (≥7)\", \"Time (s)\", \"N\"])\n",
        "    df = pd.DataFrame([{\"Model\": r[\"model_name\"], \"Mode\": r[\"mode\"], \"Avg Score\": r[\"avg_score\"], \"Accuracy (≥7)\": f\"{r['accuracy_7']}%\", \"Time (s)\": r[\"time_sec\"], \"N\": r[\"n\"]} for r in benchmark_store])\n",
        "    df = df.sort_values(\"Avg Score\", ascending=False).reset_index(drop=True)\n",
        "    df.insert(0, \"Rank\", range(1, len(df) + 1))\n",
        "    return df\n",
        "def get_chart_df():\n",
        "    return pd.DataFrame([{\"model\": r[\"model_name\"], \"avg_score\": r[\"avg_score\"]} for r in benchmark_store]) if benchmark_store else pd.DataFrame({\"model\": [], \"avg_score\": []})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ui_gen(mode):\n",
        "    return \"\\n\\n\".join(f\"**{i}.** \" + q for i, q in enumerate(generate_synthetic_questions(mode, NUM_QUESTIONS), 1))\n",
        "\n",
        "def initial_reasoning_questions():\n",
        "    q = ui_gen(\"reasoning\")\n",
        "    return q, q\n",
        "def parse_q(text):\n",
        "    if not (text and text.strip()):\n",
        "        return []\n",
        "    text = re.sub(r\"\\*\\*\\d+\\.\\*\\*\\s*\", \"\", text)\n",
        "    blocks = [b.strip() for b in text.split(\"\\n\\n\") if b.strip()]\n",
        "    return (blocks if blocks else [ln.strip() for ln in text.split(\"\\n\") if ln.strip()])[:NUM_QUESTIONS]\n",
        "def ui_run(mode, selected_models, text):\n",
        "    qs = parse_q(text)\n",
        "    if not qs:\n",
        "        return \"Paste/generate questions first.\", \"\"\n",
        "    md, status = run_benchmark_all(mode, selected_models or [], qs)\n",
        "    return md, status\n",
        "def ui_refresh():\n",
        "    return get_table(), get_chart_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Reasoning & Code Benchmark\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Reasoning & Code Benchmark (contest: 3 models)\")\n",
        "    gr.Markdown(\"Generate questions → select models (checkboxes) → run benchmark for all selected → view answers and Performance.\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"1. Contest\"):\n",
        "            mode_c = gr.Radio(choices=BENCHMARK_MODES, value=\"reasoning\", label=\"Mode\")\n",
        "            q_out = gr.Markdown(label=\"Questions\")\n",
        "            gr.Button(\"Generate questions\").click(fn=ui_gen, inputs=[mode_c], outputs=[q_out])\n",
        "        with gr.TabItem(\"2. Run & Answers\"):\n",
        "            mode_r = gr.Radio(choices=BENCHMARK_MODES, value=\"reasoning\", label=\"Mode\")\n",
        "            model_labels = [l for l, _, _ in MODEL_CHOICES]\n",
        "            model_checkboxes = gr.CheckboxGroup(choices=model_labels, value=model_labels, label=\"Models to benchmark (select 1–3)\")\n",
        "            q_in = gr.Textbox(placeholder=\"Paste questions from Contest (or use auto-loaded reasoning questions)\", lines=10, label=\"Questions\")\n",
        "            btn_run = gr.Button(\"Run benchmark (all selected models)\")\n",
        "            ans_out, status_out = gr.Markdown(label=\"Answers (all models)\"), gr.Textbox(label=\"Status\", interactive=False)\n",
        "            btn_run.click(fn=ui_run, inputs=[mode_r, model_checkboxes, q_in], outputs=[ans_out, status_out])\n",
        "        with gr.TabItem(\"3. Performance\"):\n",
        "            gr.Markdown(\"Ranking & metrics for **all models** from every benchmark run.\")\n",
        "            perf_t = gr.Dataframe(label=\"Ranking\", interactive=False)\n",
        "            perf_c = gr.BarPlot(get_chart_df(), x=\"model\", y=\"avg_score\", title=\"Avg score by model\", vertical=False)\n",
        "            gr.Button(\"Refresh\").click(fn=ui_refresh, inputs=[], outputs=[perf_t, perf_c])\n",
        "    demo.load(fn=initial_reasoning_questions, inputs=[], outputs=[q_out, q_in])\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
