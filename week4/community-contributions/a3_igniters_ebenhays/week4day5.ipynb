{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a38e8be3",
      "metadata": {},
      "source": [
        "## Week 4 Day 5: LLM Coding Challenge Arena\n",
        "\n",
        "**Workflow:**\n",
        "1. Submit a coding problem\n",
        "2. Two selected LLMs each generate a solution\n",
        "3. Each LLM generates unit tests for its own solution\n",
        "4. Solutions are executed, scored on pass %, runtime, and Pylint quality\n",
        "5. A winner banner is displayed based on composite score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d7b418",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from helper_functions import (\n",
        "    generate_code,\n",
        "    generate_unit_tests,\n",
        "    parse_tests,\n",
        "    evaluate_solution,\n",
        "    display_metrics,\n",
        "    display_winner,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd30b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS = [\n",
        "    {\"name\": \"GPT-4.1 Mini\", \"slug\": \"openai/gpt-4.1-mini\"},\n",
        "    {\"name\": \"Claude 3.5 Sonnet\", \"slug\": \"anthropic/claude-3.5-sonnet\"},\n",
        "    {\"name\": \"GPT-4o Mini\", \"slug\": \"openai/gpt-4o-mini\"},\n",
        "    {\"name\": \"Gemini 2.5 Pro\", \"slug\": \"google/gemini-2.5-pro\"},\n",
        "    {\"name\": \"GPT-oss-20b\", \"slug\": \"openai/gpt-oss-20b\"},\n",
        "    {\"name\": \"Qwen3.5 Plus 2026-02-15\", \"slug\": \"qwen/qwen3.5-plus-02-15\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe576442",
      "metadata": {},
      "outputs": [],
      "source": [
        "def blank_state():\n",
        "    return [\n",
        "        \"Ready ‚Äî enter a problem and select two LLMs to compete.\",\n",
        "        gr.update(visible=False),\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        gr.update(visible=False),\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e02af9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_challenge(problem: str, model1_name: str, model2_name: str):\n",
        "    if not problem.strip():\n",
        "        out = blank_state()\n",
        "        out[0] = \"‚ö†Ô∏è Please enter a coding problem first.\"\n",
        "        yield out\n",
        "        return\n",
        "\n",
        "    model1_slug = next(m[\"slug\"] for m in MODELS if m[\"name\"] == model1_name)\n",
        "    model2_slug = next(m[\"slug\"] for m in MODELS if m[\"name\"] == model2_name)\n",
        "    results = []\n",
        "\n",
        "    def mid(msg, *, r1=False, c1=\"\", t1=\"\", m1=\"\", r2=False, c2=\"\", t2=\"\", m2=\"\"):\n",
        "        \"\"\"Intermediate yield ‚Äî keeps banner empty until the final frame.\"\"\"\n",
        "        return [msg, gr.update(visible=r1), c1, t1, m1, gr.update(visible=r2), c2, t2, m2, \"\"]\n",
        "\n",
        "    yield mid(f\"‚è≥ {model1_name} is generating solution...\")\n",
        "    code1 = generate_code(model1_slug, problem)\n",
        "\n",
        "    yield mid(f\"‚è≥ {model1_name} is generating unit tests...\")\n",
        "    tests1, test_display1 = parse_tests(\n",
        "        generate_unit_tests(problem, code1, model1_slug, focus=\"edge\")\n",
        "    )\n",
        "\n",
        "    if not tests1:\n",
        "        yield mid(f\"‚ùå {model1_name} failed to generate valid tests.\",\n",
        "                   r1=True, c1=code1, t1=test_display1, m1=\"Failed to generate valid tests\")\n",
        "        return\n",
        "\n",
        "    yield mid(f\"‚è≥ Evaluating {model1_name} solution...\")\n",
        "    pass_pct1, runtime_ms1, pylint1, error1 = evaluate_solution(code1, tests1)\n",
        "    result1 = {\n",
        "        \"model_name\": model1_name, \"slug\": model1_slug,\n",
        "        \"code\": code1, \"tests\": test_display1,\n",
        "        \"pass_pct\": pass_pct1, \"runtime_ms\": runtime_ms1,\n",
        "        \"runtime_ms_str\": f\"{runtime_ms1:.1f}\" if runtime_ms1 != float(\"inf\") else \"N/A\",\n",
        "        \"pylint\": pylint1, \"error\": error1,\n",
        "    }\n",
        "    results.append(result1)\n",
        "    metrics1 = display_metrics(result1)\n",
        "\n",
        "    yield mid(f\"‚è≥ {model2_name} is generating solution...\",\n",
        "               r1=True, c1=code1, t1=test_display1, m1=metrics1)\n",
        "    code2 = generate_code(model2_slug, problem)\n",
        "\n",
        "    yield mid(f\"‚è≥ {model2_name} is generating unit tests...\",\n",
        "               r1=True, c1=code1, t1=test_display1, m1=metrics1)\n",
        "    tests2, test_display2 = parse_tests(\n",
        "        generate_unit_tests(problem, code2, model2_slug, focus=\"typical\")\n",
        "    )\n",
        "\n",
        "    if not tests2:\n",
        "        yield mid(f\"‚ùå {model2_name} failed to generate valid tests.\",\n",
        "                   r1=True, c1=code1, t1=test_display1, m1=metrics1,\n",
        "                   r2=True, c2=code2, t2=test_display2, m2=\"Failed to generate valid tests\")\n",
        "        return\n",
        "\n",
        "    yield mid(f\"‚è≥ Evaluating {model2_name} solution...\",\n",
        "               r1=True, c1=code1, t1=test_display1, m1=metrics1)\n",
        "    pass_pct2, runtime_ms2, pylint2, error2 = evaluate_solution(code2, tests2)\n",
        "    result2 = {\n",
        "        \"model_name\": model2_name, \"slug\": model2_slug,\n",
        "        \"code\": code2, \"tests\": test_display2,\n",
        "        \"pass_pct\": pass_pct2, \"runtime_ms\": runtime_ms2,\n",
        "        \"runtime_ms_str\": f\"{runtime_ms2:.1f}\" if runtime_ms2 != float(\"inf\") else \"N/A\",\n",
        "        \"pylint\": pylint2, \"error\": error2,\n",
        "    }\n",
        "    results.append(result2)\n",
        "    metrics2 = display_metrics(result2)\n",
        "\n",
        "    n_pass = sum(1 for r in results if r[\"pass_pct\"] == 100)\n",
        "    yield [\n",
        "        f\"‚úÖ Challenge complete! {n_pass}/2 solutions passed all tests.\",\n",
        "        gr.update(visible=True), code1, test_display1, metrics1,\n",
        "        gr.update(visible=True), code2, test_display2, metrics2,\n",
        "        display_winner(results),\n",
        "    ]\n",
        "\n",
        "\n",
        "def clear_challenge():\n",
        "    return blank_state() + [\"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d18bfe2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio UI\n",
        "CSS = open(\"styles.css\").read()\n",
        "with gr.Blocks(title=\"LLM Coding Challenge Arena\", css=CSS) as demo:\n",
        "\n",
        "    gr.HTML(\"\"\"\n",
        "    <div id=\"arena-header\">\n",
        "      <h1>üèÜ LLM Coding Challenge Arena</h1>\n",
        "      <p style=\"color:#64748b\">Two LLMs compete ¬∑ Each generates solution + unit tests</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            problem_in = gr.Textbox(\n",
        "                label=\"Coding Problem\",\n",
        "                placeholder=\"e.g. Write a function to return the nth Fibonacci number\",\n",
        "                lines=3,\n",
        "            )\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"Given an integer x, return true if x is a palindrome, and false otherwise.\"],\n",
        "                    [\"Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\\n\\nNotice that the solution set must not contain duplicate triplets.\"],\n",
        "                    [\"You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\\n\\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself\"],\n",
        "                ],\n",
        "                inputs=problem_in,\n",
        "                label=\"Examples (Click to load)\"\n",
        "            )\n",
        "        with gr.Column(scale=2):\n",
        "            model1_sel = gr.Dropdown(\n",
        "                choices=[m[\"name\"] for m in MODELS], value=MODELS[0][\"name\"], label=\"LLM Competitor 1\"\n",
        "            )\n",
        "            model2_sel = gr.Dropdown(\n",
        "                choices=[m[\"name\"] for m in MODELS], value=MODELS[1][\"name\"], label=\"LLM Competitor 2\"\n",
        "            )\n",
        "\n",
        "    with gr.Row():\n",
        "        gen_btn   = gr.Button(\"üöÄ Start Challenge\", variant=\"primary\",   size=\"lg\")\n",
        "        clear_btn = gr.Button(\"üóëÔ∏è Clear\",           variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "    status_out        = gr.Markdown(\"Ready ‚Äî enter a problem and select two LLMs to compete.\")\n",
        "    winner_banner_out = gr.HTML(\"\")\n",
        "\n",
        "    gr.Markdown(\"---\\n## Competitor Solutions\")\n",
        "\n",
        "    model1_accordion = gr.Accordion(\"ü§ñ Competitor 1\", open=True, visible=False)\n",
        "    with model1_accordion:\n",
        "        model1_code_out    = gr.Code(language=\"python\", label=\"Solution\")\n",
        "        model1_test_out    = gr.Code(language=\"python\", label=\"Unit Tests  „Äîfocus: edge cases & boundaries„Äï\")\n",
        "        model1_metrics_out = gr.Markdown()\n",
        "\n",
        "    model2_accordion = gr.Accordion(\"ü§ñ Competitor 2\", open=True, visible=False)\n",
        "    with model2_accordion:\n",
        "        model2_code_out    = gr.Code(language=\"python\", label=\"Solution\")\n",
        "        model2_test_out    = gr.Code(language=\"python\", label=\"Unit Tests  „Äîfocus: typical use cases & breadth„Äï\")\n",
        "        model2_metrics_out = gr.Markdown()\n",
        "\n",
        "    challenge_outputs = [\n",
        "        status_out,\n",
        "        model1_accordion, model1_code_out, model1_test_out, model1_metrics_out,\n",
        "        model2_accordion, model2_code_out, model2_test_out, model2_metrics_out,\n",
        "        winner_banner_out,\n",
        "    ]\n",
        "\n",
        "    gen_btn.click(fn=run_challenge, inputs=[problem_in, model1_sel, model2_sel], outputs=challenge_outputs)\n",
        "    clear_btn.click(fn=clear_challenge, inputs=[], outputs=challenge_outputs + [problem_in])\n",
        "\n",
        "    demo.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
