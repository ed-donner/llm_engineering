{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02ecc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import importlib\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import ctypes\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5879048",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Register models\n",
    "MODELS = {\n",
    "    \"Kimi K2 Thinking\": {\"id\": \"moonshotai/Kimi-K2-Thinking\", \"temperature\": 1.0},\n",
    "    \"GLM-4.6\":          {\"id\": \"zai-org/GLM-4.6\",             \"temperature\": 0.7},\n",
    "    \"Qwen3 235B\":       {\"id\": \"Qwen/Qwen3-235B-A22B\",        \"temperature\": 0.7},\n",
    "    \"DeepSeek-R1\":      {\"id\": \"deepseek-ai/DeepSeek-R1\",     \"temperature\": 0.6},\n",
    "    \"DeepSeek-V3.2\":    {\"id\": \"deepseek-ai/DeepSeek-V3.2\",   \"temperature\": 0.7},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2b5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client() -> OpenAI:\n",
    "    return OpenAI(\n",
    "        base_url=\"https://router.huggingface.co/v1\",\n",
    "        api_key=os.environ[\"HF_TOKEN\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c12dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_fences(code: str) -> str:\n",
    "    \"\"\"Remove markdown code fences from LLM output.\"\"\"\n",
    "    lines = code.strip().splitlines()\n",
    "    return \"\\n\".join(l for l in lines if not l.strip().startswith(\"```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "912c5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symbol(lib_path: str, function_name: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use `nm` to find the real exported symbol name in a compiled library.\n",
    "    Handles macOS leading-underscore mangling and minor name variations.\n",
    "    \"\"\"\n",
    "    r = subprocess.run([\"nm\", \"-gU\", lib_path], capture_output=True, text=True)\n",
    "    for line in r.stdout.splitlines():\n",
    "        # Only look at text (T) symbols ‚Äî exported functions\n",
    "        if \" T \" not in line and \" t \" not in line:\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if not parts:\n",
    "            continue\n",
    "        raw = parts[-1]\n",
    "        # Strip leading underscore added by macOS linker\n",
    "        clean = raw.lstrip(\"_\")\n",
    "        if clean == function_name:\n",
    "            return clean\n",
    "        # Fuzzy: symbol contains the function name\n",
    "        if function_name in clean:\n",
    "            return clean\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b2e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_code(client: OpenAI, model_id: str, temperature: float,\n",
    "                   target: str, python_code: str, function_name: str) -> str:\n",
    "    base = f\"The function MUST be named exactly `{function_name}`.\"\n",
    "    prompts = {\n",
    "        \"cpp\": f\"\"\"Convert this Python function to a standalone, high-performance C++ function.\n",
    "Requirements:\n",
    "- Include all necessary headers\n",
    "- Expose via extern \"C\" so it is callable from Python via ctypes\n",
    "- Use modern C++17\n",
    "- {base}\n",
    "- Optimize for speed: prefer stack allocation, avoid heap where possible\n",
    "- Return ONLY valid C++ source code, no explanation, no markdown\n",
    "\n",
    "Python code:\n",
    "{python_code}\"\"\",\n",
    "\n",
    "        \"rust\": f\"\"\"Convert this Python function to high-performance Rust.\n",
    "Requirements:\n",
    "- Write a single lib.rs file for a cdylib crate\n",
    "- Annotate the function with #[no_mangle] and pub extern \"C\"\n",
    "- {base}\n",
    "- Use unsafe only if strictly necessary\n",
    "- Optimize for speed\n",
    "- Return ONLY valid Rust source code, no explanation, no markdown\n",
    "\n",
    "Python code:\n",
    "{python_code}\"\"\",\n",
    "\n",
    "        \"cython\": f\"\"\"Convert this Python function to optimized Cython (.pyx file).\n",
    "Requirements:\n",
    "- Use static typing with cdef / cpdef throughout\n",
    "- cimport C libraries where beneficial\n",
    "- {base}\n",
    "- Optimize for speed\n",
    "- Return ONLY valid Cython source code, no explanation, no markdown\n",
    "\n",
    "Python code:\n",
    "{python_code}\"\"\",\n",
    "    }\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompts[target]}],\n",
    "    )\n",
    "    return strip_fences(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "451b8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark helpers \n",
    "\n",
    "def benchmark_python(python_code: str, function_name: str, test_args: tuple, n: int = 1000) -> tuple:\n",
    "    namespace: dict = {}\n",
    "    exec(python_code, namespace)  # noqa: S102\n",
    "    fn = namespace[function_name]\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n):\n",
    "        fn(*test_args)\n",
    "    return (time.perf_counter() - start) / n, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26619ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_cpp(code: str, function_name: str, test_args: tuple, n: int = 1000) -> tuple:\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        src = Path(tmp) / \"perf.cpp\"\n",
    "        lib = Path(tmp) / \"perf.so\"\n",
    "        src.write_text(code)\n",
    "\n",
    "        r = subprocess.run(\n",
    "            [\"g++\", \"-O3\", \"-march=native\", \"-shared\", \"-fPIC\",\n",
    "             \"-std=c++17\", \"-o\", str(lib), str(src)],\n",
    "            capture_output=True, text=True,\n",
    "        )\n",
    "        if r.returncode != 0:\n",
    "            return None, f\"Compile error:\\n{r.stderr[:400]}\"\n",
    "\n",
    "        symbol = find_symbol(str(lib), function_name)\n",
    "        if not symbol:\n",
    "            return None, f\"Symbol '{function_name}' not found. Exported symbols:\\n\" + \\\n",
    "                subprocess.run([\"nm\", \"-gU\", str(lib)], capture_output=True, text=True).stdout[:300]\n",
    "\n",
    "        h = ctypes.CDLL(str(lib))\n",
    "        fn = getattr(h, symbol)\n",
    "        fn.restype = ctypes.c_double\n",
    "        fn.argtypes = [ctypes.c_long] * len(test_args)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n):\n",
    "            fn(*test_args)\n",
    "        return (time.perf_counter() - start) / n, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63fab5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_rust(code: str, function_name: str, test_args: tuple, n: int = 1000) -> tuple:\n",
    "    cargo_path = shutil.which(\"cargo\") or os.path.expanduser(\"~/.cargo/bin/cargo\")\n",
    "    if not os.path.exists(cargo_path):\n",
    "        return None, \"cargo not found ‚Äî run: curl https://sh.rustup.rs -sSf | sh\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        tmp = Path(tmp)\n",
    "        (tmp / \"src\").mkdir()\n",
    "        (tmp / \"src\" / \"lib.rs\").write_text(code)\n",
    "        (tmp / \"Cargo.toml\").write_text(\n",
    "            '[package]\\nname = \"perf_ext\"\\nversion = \"0.1.0\"\\nedition = \"2021\"\\n\\n'\n",
    "            '[lib]\\ncrate-type = [\"cdylib\"]\\n'\n",
    "        )\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env[\"PATH\"] = os.path.expanduser(\"~/.cargo/bin\") + \":\" + env.get(\"PATH\", \"\")\n",
    "\n",
    "        r = subprocess.run(\n",
    "            [cargo_path, \"build\", \"--release\"],\n",
    "            capture_output=True, text=True, cwd=tmp, env=env,\n",
    "        )\n",
    "        if r.returncode != 0:\n",
    "            return None, f\"Compile error:\\n{r.stderr[:400]}\"\n",
    "\n",
    "        # macOS ‚Üí .dylib, Linux ‚Üí .so\n",
    "        libs = (list((tmp / \"target\" / \"release\").glob(\"*.dylib\")) or\n",
    "                list((tmp / \"target\" / \"release\").glob(\"*.so\")))\n",
    "        if not libs:\n",
    "            return None, \"No compiled library found in target/release\"\n",
    "\n",
    "        symbol = find_symbol(str(libs[0]), function_name)\n",
    "        if not symbol:\n",
    "            return None, f\"Symbol '{function_name}' not found in Rust lib\"\n",
    "\n",
    "        h = ctypes.CDLL(str(libs[0]))\n",
    "        fn = getattr(h, symbol)\n",
    "        fn.restype = ctypes.c_double\n",
    "        fn.argtypes = [ctypes.c_long] * len(test_args)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n):\n",
    "            fn(*test_args)\n",
    "        return (time.perf_counter() - start) / n, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e4964a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_cython(code: str, function_name: str, test_args: tuple, n: int = 1000) -> tuple:\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        tmp = Path(tmp)\n",
    "        pyx = tmp / f\"{function_name}.pyx\"\n",
    "        pyx.write_text(code)\n",
    "        (tmp / \"setup.py\").write_text(\n",
    "            \"from setuptools import setup\\n\"\n",
    "            \"from Cython.Build import cythonize\\n\"\n",
    "            f'setup(ext_modules=cythonize(\"{function_name}.pyx\", '\n",
    "            'compiler_directives={\"language_level\": 3}))\\n'\n",
    "        )\n",
    "\n",
    "        r = subprocess.run(\n",
    "            [sys.executable, \"setup.py\", \"build_ext\", \"--inplace\"],\n",
    "            capture_output=True, text=True, cwd=tmp,\n",
    "        )\n",
    "        if r.returncode != 0:\n",
    "            return None, f\"Compile error:\\n{r.stderr[:400]}\"\n",
    "\n",
    "        sys.path.insert(0, str(tmp))\n",
    "        try:\n",
    "            # Invalidate any cached module from a previous run\n",
    "            if function_name in sys.modules:\n",
    "                del sys.modules[function_name]\n",
    "            mod = importlib.import_module(function_name)\n",
    "            fn = getattr(mod, function_name)\n",
    "            start = time.perf_counter()\n",
    "            for _ in range(n):\n",
    "                fn(*test_args)\n",
    "            return (time.perf_counter() - start) / n, None\n",
    "        except Exception as exc:\n",
    "            return None, str(exc)\n",
    "        finally:\n",
    "            sys.path.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f7ec3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Gradio implementation\n",
    "\n",
    "def run_benchmark(python_code: str, function_name: str, test_args_str: str, model_label: str):\n",
    "    try:\n",
    "        test_args = eval(f\"({test_args_str},)\")  # noqa: S307\n",
    "    except Exception as exc:\n",
    "        yield f\"‚ùå Could not parse test args: {exc}\", None\n",
    "        return\n",
    "\n",
    "    model_cfg = MODELS[model_label]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    temperature = model_cfg[\"temperature\"]\n",
    "    client = get_client()\n",
    "\n",
    "    log = f\"ü§ñ Model: **{model_label}** (`{model_id}`)\\n\\n\"\n",
    "    yield log + \"üîÑ Translating to C++, Rust, Cython...\\n\", None\n",
    "\n",
    "    # Translations \n",
    "    translations: dict[str, str] = {}\n",
    "    for target in (\"cpp\", \"rust\", \"cython\"):\n",
    "        try:\n",
    "            translations[target] = translate_code(\n",
    "                client, model_id, temperature, target, python_code, function_name\n",
    "            )\n",
    "            log += f\"  ‚úÖ {target.upper()} translation done\\n\"\n",
    "        except Exception as exc:\n",
    "            log += f\"  ‚ùå {target.upper()} translation failed: {exc}\\n\"\n",
    "            translations[target] = \"\"\n",
    "        yield log, None\n",
    "\n",
    "    # Benchmarks \n",
    "    log += \"\\n‚è±Ô∏è Running benchmarks...\\n\"\n",
    "    yield log, None\n",
    "\n",
    "    results: dict[str, tuple] = {}\n",
    "\n",
    "    results[\"python\"] = benchmark_python(python_code, function_name, test_args)\n",
    "    log += f\"  ‚úÖ PYTHON: {results['python'][0]*1e6:.2f} ¬µs\\n\"\n",
    "    yield log, None\n",
    "\n",
    "    for target, bench_fn in [\n",
    "        (\"cpp\",    benchmark_cpp),\n",
    "        (\"rust\",   benchmark_rust),\n",
    "        (\"cython\", benchmark_cython),\n",
    "    ]:\n",
    "        if not translations.get(target):\n",
    "            results[target] = (None, \"Translation failed\")\n",
    "        else:\n",
    "            results[target] = bench_fn(translations[target], function_name, test_args)\n",
    "\n",
    "        t, err = results[target]\n",
    "        status = f\"{t*1e6:.2f} ¬µs\" if t else f\"FAILED ‚Äî {err}\"\n",
    "        icon = \"‚úÖ\" if t else \"‚ùå\"\n",
    "        log += f\"  {icon} {target.upper()}: {status}\\n\"\n",
    "        yield log, None\n",
    "\n",
    "    # Results table \n",
    "    baseline = results[\"python\"][0]\n",
    "    valid = {k: v[0] for k, v in results.items() if v[0] is not None}\n",
    "    winner = min(valid, key=valid.get)\n",
    "    winner_speedup = baseline / valid[winner]\n",
    "\n",
    "    table = []\n",
    "    for target, (t, _err) in results.items():\n",
    "        if t is None:\n",
    "            table.append([target.upper(), \"FAILED\", \"‚Äî\", \"‚ùå\"])\n",
    "        else:\n",
    "            speedup = baseline / t\n",
    "            trophy = \"üèÜ\" if target == winner and target != \"python\" else \"\"\n",
    "            table.append([target.upper(), f\"{t*1e6:.2f}\", f\"{speedup:.2f}x\", trophy])\n",
    "\n",
    "    summary = log\n",
    "    summary += f\"\\n---\\nüèÜ **Winner: {winner.upper()}** ‚Äî {winner_speedup:.1f}x faster than Python\\n\"\n",
    "    if winner in translations:\n",
    "        summary += f\"\\n**{winner.upper()} code:**\\n```\\n{translations[winner]}\\n```\"\n",
    "\n",
    "    yield summary, table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37f8490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI \n",
    "\n",
    "with gr.Blocks(title=\"‚ö° Multi-Target Compiler Benchmark\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"# ‚ö° Python ‚Üí C++ / Rust / Cython Benchmark\\n\"\n",
    "        \"Translate a Python function with an LLM, compile all three targets, and benchmark them.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=list(MODELS.keys()),\n",
    "                value=\"Kimi K2 Thinking\",\n",
    "                label=\"ü§ñ Translation Model\",\n",
    "            )\n",
    "            code_input = gr.Code(\n",
    "                label=\"Python Function\",\n",
    "                language=\"python\",\n",
    "                value=(\n",
    "                    \"def dot_product(n):\\n\"\n",
    "                    \"    a = list(range(n))\\n\"\n",
    "                    \"    b = list(range(n))\\n\"\n",
    "                    \"    return sum(x * y for x, y in zip(a, b))\"\n",
    "                ),\n",
    "            )\n",
    "            fn_name = gr.Textbox(label=\"Function Name\", value=\"dot_product\")\n",
    "            test_args = gr.Textbox(\n",
    "                label=\"Test Args (comma-separated)\",\n",
    "                value=\"1000\",\n",
    "                placeholder=\"e.g. 1000   or   500, 'hello'\",\n",
    "            )\n",
    "            run_btn = gr.Button(\"üöÄ Run Benchmark\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            log_output = gr.Markdown(label=\"Progress & Output\")\n",
    "            table_output = gr.Dataframe(\n",
    "                headers=[\"Target\", \"Time (¬µs)\", \"Speedup\", \"\"],\n",
    "                label=\"Results\",\n",
    "                interactive=False,\n",
    "            )\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=run_benchmark,\n",
    "        inputs=[code_input, fn_name, test_args, model_selector],\n",
    "        outputs=[log_output, table_output],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62502d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
