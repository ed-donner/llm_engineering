{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Google API Key exists and begins AI\n",
            "Groq API Key exists and begins gs\n",
            "OpenRouter API Key exists and begins sk\n",
            "ollama API Key exists and begins ad\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
        "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "    \n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set (and this is optional)\")\n",
        "\n",
        "if groq_api_key:\n",
        "    print(f\"Groq API Key exists and begins {groq_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Groq API Key not set (and this is optional)\")\n",
        "    \n",
        "if openrouter_api_key:\n",
        "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"OpenRouter API Key not set (and this is optional)\")\n",
        "\n",
        "if ollama_api_key:\n",
        "    print(f\"ollama API Key exists and begins {ollama_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"ollama API Key not set (and this is optional)\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai = OpenAI()\n",
        "\n",
        "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "groq_url = \"https://api.groq.com/openai/v1\"\n",
        "ollama_url = \"http://localhost:11434/v1\"\n",
        "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "\n",
        "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
        "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
        "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
        "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\"gpt-4.1-mini\", \"gemini-2.5-flash\", \"deepseek-v3.1:671b-cloud\", \"groq/compound\", \"openrouter/free\" , \"google/gemma-3-27b-it:free\" ]\n",
        "\n",
        "clients = {\"gpt-4.1-mini\": openai, \"gemini-2.5-flash\": gemini, \"deepseek-v3.1:671b-cloud\": ollama, \"groq/compound\": groq, \"openrouter/free\": openrouter, \"openrouter/free\": openrouter}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'installed': True,\n",
              " 'rustc': {'path': 'C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\rustc.EXE',\n",
              "  'version': 'rustc 1.88.0 (6b00bc388 2025-06-23)',\n",
              "  'host_triple': 'x86_64-pc-windows-msvc',\n",
              "  'release': '1.88.0',\n",
              "  'commit_hash': '6b00bc3880198600130e1cf62b8f8a93494488cc'},\n",
              " 'cargo': {'path': 'C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\cargo.EXE',\n",
              "  'version': 'cargo 1.88.0 (873a06493 2025-05-10)'},\n",
              " 'rustup': {'path': 'C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\rustup.EXE',\n",
              "  'version': 'rustup 1.28.2 (e4f3ad6f8 2025-04-28)',\n",
              "  'active_toolchain': 'stable-x86_64-pc-windows-msvc (default)',\n",
              "  'default_toolchain': '',\n",
              "  'toolchains': ['stable-x86_64-pc-windows-msvc (active, default)'],\n",
              "  'targets_installed': ['x86_64-pc-windows-msvc']},\n",
              " 'rust_analyzer': {'path': 'C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\rust-analyzer.EXE'},\n",
              " 'env': {'CARGO_HOME': 'C:\\\\Users\\\\vasanth\\\\.cargo',\n",
              "  'RUSTUP_HOME': 'C:\\\\Users\\\\vasanth\\\\.rustup',\n",
              "  'RUSTFLAGS': '',\n",
              "  'CARGO_BUILD_TARGET': ''},\n",
              " 'execution_examples': ['\"C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\cargo.EXE\" build',\n",
              "  '\"C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\cargo.EXE\" run',\n",
              "  '\"C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\cargo.EXE\" test',\n",
              "  '\"C:\\\\Users\\\\vasanth\\\\.cargo\\\\bin\\\\rustc.EXE\" hello.rs -o hello']}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from system_info import retrieve_system_info, rust_toolchain_info\n",
        "\n",
        "system_info = retrieve_system_info()\n",
        "rust_info = rust_toolchain_info()\n",
        "rust_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = f\"\"\"\n",
        "Below is detailed system information for my machine.\n",
        "\n",
        "My goal is to compile and run a **single Rust source file (`main.rs`)** with the **fastest possible runtime performance** on this platform. Compile time is not important.\n",
        "\n",
        "Tasks:\n",
        "1. Determine whether a Rust toolchain is already installed and sufficient.\n",
        "2. If not installed, explain whether I need to install one and provide the **simplest step-by-step installation instructions**.\n",
        "3. If Rust is already set up, provide the **exact shell commands** needed to:\n",
        "   - Compile `main.rs` with **maximum runtime optimization**\n",
        "   - Execute the resulting binary\n",
        "\n",
        "Then, show how to run those commands from Python using `subprocess.run`, filling in **exact values** for:\n",
        "- `compile_command`\n",
        "- `run_command`\n",
        "\n",
        "Constraints:\n",
        "- Assume this is a single-file Rust program (no Cargo project).\n",
        "- Prefer `rustc` directly over Cargo unless Cargo is strictly required.\n",
        "- Optimize for **fastest possible runtime performance** for this platform (e.g., `-O`, `-C target-cpu=native`, LTO, etc.).\n",
        "- The output should be only the required commands, in **Markdown code blocks**, with no unnecessary commentary.\n",
        "\n",
        "System information:\n",
        "{system_info}\n",
        "\n",
        "Rust toolchain information:\n",
        "{rust_info}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```bash\n",
              "\"C:\\Users\\vasanth\\.cargo\\bin\\rustc.EXE\" main.rs -C opt-level=3 -C target-cpu=native -C lto -o main.exe\n",
              ".\\main.exe\n",
              "```\n",
              "\n",
              "```python\n",
              "import subprocess\n",
              "\n",
              "compile_command = [\n",
              "    r\"C:\\Users\\vasanth\\.cargo\\bin\\rustc.EXE\",\n",
              "    \"main.rs\",\n",
              "    \"-C\", \"opt-level=3\",\n",
              "    \"-C\", \"target-cpu=native\",\n",
              "    \"-C\", \"lto\",\n",
              "    \"-o\", \"main.exe\"\n",
              "]\n",
              "\n",
              "run_command = [r\".\\main.exe\"]\n",
              "\n",
              "subprocess.run(compile_command, check=True)\n",
              "subprocess.run(run_command, check=True)\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = openai.chat.completions.create(model=models[0], messages=[{\"role\": \"user\", \"content\": message}])\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import io\n",
        "import sys\n",
        "\n",
        "compile_command = [\n",
        "    r\"C:\\Users\\vasanth\\.cargo\\bin\\rustc.EXE\",\n",
        "    \"main.rs\",\n",
        "    \"-C\", \"opt-level=3\",\n",
        "    \"-C\", \"target-cpu=native\",\n",
        "    \"-C\", \"lto\",\n",
        "    \"-o\", \"main.exe\"\n",
        "]\n",
        "run_command = [r\".\\main.exe\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "language = \"Rust\" # or \"C++\"\n",
        "extension = \"rs\" if language == \"Rust\" else \"cpp\"\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "Your task is to convert Python code into high performance {language} code.\n",
        "Respond only with {language} code. Do not provide any explanation other than occasional comments.\n",
        "The {language} response needs to produce an identical output in the fastest possible time.\n",
        "\"\"\"\n",
        "\n",
        "def user_prompt_for(python):\n",
        "    return f\"\"\"\n",
        "Port this Python code to {language} with the fastest possible implementation that produces identical output in the least time.\n",
        "The system information is:\n",
        "{system_info}\n",
        "Your response will be written to a file called main.{language} and then compiled and executed; the compilation command is:\n",
        "{compile_command}\n",
        "Respond only with {language} code.\n",
        "Python code to port:\n",
        "\n",
        "```python\n",
        "{python}\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_output(code):\n",
        "    with open(f\"main.{extension}\", \"w\") as f:\n",
        "        f.write(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def port(model, python):\n",
        "    client = clients[model]\n",
        "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
        "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace('```cpp','').replace('```rust','').replace('```','')\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_python(code):\n",
        "    globals_dict = {\"__builtins__\": __builtins__}\n",
        "\n",
        "    buffer = io.StringIO()\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer\n",
        "\n",
        "    try:\n",
        "        exec(code, globals_dict)\n",
        "        output = buffer.getvalue()\n",
        "    except Exception as e:\n",
        "        output = f\"Error: {e}\"\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "python_hard = \"\"\"# Be careful to support large numbers\n",
        "\n",
        "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
        "    value = seed\n",
        "    while True:\n",
        "        value = (a * value + c) % m\n",
        "        yield value\n",
        "        \n",
        "def max_subarray_sum(n, seed, min_val, max_val):\n",
        "    lcg_gen = lcg(seed)\n",
        "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
        "    max_sum = float('-inf')\n",
        "    for i in range(n):\n",
        "        current_sum = 0\n",
        "        for j in range(i, n):\n",
        "            current_sum += random_numbers[j]\n",
        "            if current_sum > max_sum:\n",
        "                max_sum = current_sum\n",
        "    return max_sum\n",
        "\n",
        "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
        "    total_sum = 0\n",
        "    lcg_gen = lcg(initial_seed)\n",
        "    for _ in range(20):\n",
        "        seed = next(lcg_gen)\n",
        "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
        "    return total_sum\n",
        "\n",
        "# Parameters\n",
        "n = 10000         # Number of random numbers\n",
        "initial_seed = 42 # Initial seed for the LCG\n",
        "min_val = -10     # Minimum value of random numbers\n",
        "max_val = 10      # Maximum value of random numbers\n",
        "\n",
        "# Timing the function\n",
        "import time\n",
        "start_time = time.time()\n",
        "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
        "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the commands from GPT 5\n",
        "\n",
        "def compile_and_run(code):\n",
        "    write_output(code)\n",
        "    try:\n",
        "        subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
        "        run_result = subprocess.run(run_command, check=True, text=True, capture_output=True)\n",
        "        return run_result.stdout\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"An error occurred:\\n{e.stderr}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1496340570.py\", line 4, in port\n",
            "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1192, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: reasoning_effort', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
            "    raise exc\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
            "    stream = self._connect(request)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 982, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1496340570.py\", line 4, in port\n",
            "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1192, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1014, in request\n",
            "    raise APIConnectionError(request=request) from err\n",
            "openai.APIConnectionError: Connection error.\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\3180071184.py\", line 4, in compile_and_run\n",
            "    write_output(code)\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1146502019.py\", line 3, in write_output\n",
            "    f.write(code)\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
            "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 585: character maps to <undefined>\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\3180071184.py\", line 4, in compile_and_run\n",
            "    write_output(code)\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1146502019.py\", line 3, in write_output\n",
            "    f.write(code)\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
            "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2011' in position 585: character maps to <undefined>\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1496340570.py\", line 2, in port\n",
            "    client = clients[model]\n",
            "             ~~~~~~~^^^^^^^\n",
            "KeyError: 'google/gemma-3-27b-it:free'\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1496340570.py\", line 4, in port\n",
            "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1192, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: reasoning_effort', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
            "    raise exc\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
            "    stream = self._connect(request)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 982, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 61, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\vasanth\\AppData\\Local\\Temp\\ipykernel_10576\\1496340570.py\", line 4, in port\n",
            "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1192, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\LLM-Engineering-New\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1014, in request\n",
            "    raise APIConnectionError(request=request) from err\n",
            "openai.APIConnectionError: Connection error.\n"
          ]
        }
      ],
      "source": [
        "from styles import CSS\n",
        "\n",
        "with gr.Blocks(title=f\"Port from Python to {language}\") as ui:\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=6):\n",
        "            python = gr.Code(\n",
        "                label=\"Python (original)\",\n",
        "                value=python_hard,\n",
        "                language=\"python\",\n",
        "                lines=26\n",
        "            )\n",
        "        with gr.Column(scale=6):\n",
        "            cpp = gr.Code(\n",
        "                label=f\"{language} (generated)\",\n",
        "                value=\"\",\n",
        "                language=\"cpp\",\n",
        "                lines=26\n",
        "            )\n",
        "\n",
        "    with gr.Row(elem_classes=[\"controls\"]):\n",
        "        python_run = gr.Button(\"Run Python\", elem_classes=[\"run-btn\", \"py\"])\n",
        "        model = gr.Dropdown(models, value=models[0], show_label=False)\n",
        "        convert = gr.Button(f\"Port to {language}\", elem_classes=[\"convert-btn\"])\n",
        "        cpp_run = gr.Button(f\"Run {language}\", elem_classes=[\"run-btn\", \"cpp\"])\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=6):\n",
        "            python_out = gr.TextArea(label=\"Python result\", lines=8, elem_classes=[\"py-out\"])\n",
        "        with gr.Column(scale=6):\n",
        "            cpp_out = gr.TextArea(label=f\"{language} result\", lines=8, elem_classes=[\"cpp-out\"])\n",
        "\n",
        "    convert.click(fn=port, inputs=[model, python], outputs=[cpp])\n",
        "    python_run.click(fn=run_python, inputs=[python], outputs=[python_out])\n",
        "    cpp_run.click(fn=compile_and_run, inputs=[cpp], outputs=[cpp_out])\n",
        "\n",
        "ui.launch(inbrowser=True, css=CSS, theme=gr.themes.Monochrome())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gemini :\n",
        "    Total Maximum Subarray Sum (20 runs): 10980\n",
        "    Execution Time: 1.026621 seconds"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
