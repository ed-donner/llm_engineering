{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc507f2",
   "metadata": {},
   "source": [
    "# ü§ñ AI Code Generation Assistant\n",
    "\n",
    "## Objective\n",
    "Generate code snippets, unit tests, documentation, and refactor suggestions using multiple LLMs.\n",
    "\n",
    "## Features\n",
    "- Streamed generation for all models (OpenRouter + Ollama)\n",
    "- File upload support: `.py`, `.txt`, `.ipynb`\n",
    "- Single-model selector UI with provider toggle\n",
    "- Fully modular ‚Äî each concern is its own cell and function\n",
    "- Interactive Gradio UI\n",
    "\n",
    "\n",
    "Author: Naheem Quadri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell1",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e148b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell2",
   "metadata": {},
   "source": [
    "---\n",
    "## Client Setup\n",
    "Reads `OPENROUTER_API_KEY` and optionally `OLLAMA_BASE_URL` from  `.env` file\n",
    "and returns ready-to-use OpenAI-compatible clients for both providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8adc39d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenRouter client ready.\n",
      " Ollama client pointed at: http://localhost:11434/v1\n"
     ]
    }
   ],
   "source": [
    "def setup_clients():\n",
    "    \"\"\"Initialise and return (openrouter_client, ollama_client).\n",
    "\n",
    "    Reads credentials from environment / .env file.\n",
    "    Raises RuntimeError if the OpenRouter API key is missing.\n",
    "    \"\"\"\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENROUTER_API_KEY not found. \"\n",
    "            \"Add it to your .env file: OPENROUTER_API_KEY=sk-...\"\n",
    "        )\n",
    "\n",
    "    openrouter_client = OpenAI(\n",
    "        base_url='https://openrouter.ai/api/v1',\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1')\n",
    "    ollama_client = OpenAI(\n",
    "        base_url=ollama_url,\n",
    "        api_key='ollama'   # required by the library; ignored by Ollama\n",
    "    )\n",
    "\n",
    "    print(f\" OpenRouter client ready.\")\n",
    "    print(f\" Ollama client pointed at: {ollama_url}\")\n",
    "    return openrouter_client, ollama_client\n",
    "\n",
    "\n",
    "openrouter, ollama = setup_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell3",
   "metadata": {},
   "source": [
    "---\n",
    "##File Reading\n",
    "Handles `.py`, `.txt`, and `.ipynb` uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6ee5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notebook_cells(content: str) -> str:\n",
    "    \"\"\"Parse a .ipynb JSON string and return all cell sources as plain text.\n",
    "\n",
    "    Each cell is prefixed with its type ([CODE CELL] / [MARKDOWN CELL])\n",
    "    so the model understands the context. Empty cells are skipped.\n",
    "\n",
    "    Args:\n",
    "        content: Raw .ipynb file content as a string.\n",
    "\n",
    "    Returns:\n",
    "        All non-empty cell sources joined by double newlines.\n",
    "    \"\"\"\n",
    "    nb = json.loads(content)\n",
    "    extracted = []\n",
    "    for cell in nb.get('cells', []):\n",
    "        src = ''.join(cell.get('source', [])).strip()\n",
    "        if src:\n",
    "            cell_type = cell.get('cell_type', 'code').upper()\n",
    "            extracted.append(f\"# [{cell_type} CELL]\\n{src}\")\n",
    "    return '\\n\\n'.join(extracted)\n",
    "\n",
    "\n",
    "def read_uploaded_file(file_obj) -> str:\n",
    "    \"\"\"Read an uploaded file and return its contents as a string.\n",
    "\n",
    "    Supported formats:\n",
    "      - .py  / .txt  ‚Äî returned as-is\n",
    "      - .ipynb       ‚Äî cells extracted and concatenated via extract_notebook_cells()\n",
    "\n",
    "    Args:\n",
    "        file_obj: Gradio file object (has a .name path attribute), or None.\n",
    "\n",
    "    Returns:\n",
    "        File contents as a plain string, or \"\" if no file was provided.\n",
    "    \"\"\"\n",
    "    if file_obj is None:\n",
    "        return \"\"\n",
    "\n",
    "    path = file_obj if isinstance(file_obj, str) else file_obj.name\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    if path.endswith('.ipynb'):\n",
    "        return extract_notebook_cells(content)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell4",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt Builder\n",
    "Constructs the structured prompt sent to the model.\n",
    "Always requests a JSON response with keys: `code`, `tests`, `docs`, `refactor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b0525f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(task_description: str, uploaded_code: str) -> str:\n",
    "    \"\"\"Build the full user prompt from a task description and optional code.\"\"\"\n",
    "    code_section = (\n",
    "        f\"Input code / context:\\n```\\n{uploaded_code}\\n```\"\n",
    "        if uploaded_code.strip()\n",
    "        else \"No input code provided.\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"Task:\n",
    "{task_description}\n",
    "\n",
    "{code_section}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell5",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Helpers\n",
    "One generator function per provider.\n",
    "Both yield the accumulated response text after each incoming token chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f73abf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert AI coding assistant.\n",
    "\n",
    "When given a coding task, respond using exactly this Markdown structure ‚Äî no deviations:\n",
    "\n",
    "## üíª Generated Code\n",
    "```python\n",
    "# your code here\n",
    "```\n",
    "\n",
    "## üß™ Unit Tests\n",
    "```python\n",
    "# your pytest tests here\n",
    "```\n",
    "\n",
    "## üìñ Documentation\n",
    "Write clear docstrings, inline comments, and a short usage example here.\n",
    "\n",
    "## ‚ôªÔ∏è Refactor Suggestions\n",
    "Write any refactoring or optimisation suggestions here. If none, write: *No refactor suggestions.*\n",
    "\n",
    "Rules:\n",
    "- Always include all four sections, in order\n",
    "- Use the exact section headers shown above\n",
    "- Do not add any text before the first section or after the last\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def stream_openrouter(model: str, prompt: str, temperature: float = 0.0):\n",
    "    \"\"\"Stream a chat completion from OpenRouter.\n",
    "\n",
    "    Args:\n",
    "        model:       OpenRouter model ID (e.g. 'openai/gpt-4o-mini').\n",
    "        prompt:      The user prompt to send.\n",
    "        temperature: Sampling temperature (0 = deterministic).\n",
    "\n",
    "    Yields:\n",
    "        Accumulated response text after each token chunk.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    stream = openrouter.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    collected = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        collected += delta\n",
    "        yield collected\n",
    "\n",
    "\n",
    "def stream_ollama(model: str, prompt: str, temperature: float = 0.5):\n",
    "    \"\"\"Stream a chat completion from a local Ollama instance.\n",
    "\n",
    "    Args:\n",
    "        model:       Ollama model tag (e.g. 'mistral:7b').\n",
    "        prompt:      The user prompt to send.\n",
    "        temperature: Sampling temperature.\n",
    "\n",
    "    Yields:\n",
    "        Accumulated response text after each token chunk.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    collected = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content or \"\"\n",
    "        collected += delta\n",
    "        yield collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell6",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Formatting\n",
    "Parses the raw JSON response and renders each section as clean, readable Markdown.\n",
    "Falls back gracefully if the model returns malformed JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "format_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(raw: str, model_name: str, source_label: str) -> str:\n",
    "    \"\"\"Prepend a model header to the raw Markdown response.\n",
    "\n",
    "    Args:\n",
    "        raw:          Raw Markdown text returned by the model.\n",
    "        model_name:   Display name of the model used.\n",
    "        source_label: Provider label e.g. '‚òÅÔ∏è OpenRouter' or 'üñ•Ô∏è Ollama'.\n",
    "\n",
    "    Returns:\n",
    "        Complete Markdown string ready for gr.Markdown.\n",
    "    \"\"\"\n",
    "    if not raw.strip():\n",
    "        return (\n",
    "            f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n\"\n",
    "            f\"> ‚ö†Ô∏è Model returned an empty response.\\n\\n\"\n",
    "            f\"Possible causes:\\n\"\n",
    "            f\"- Model ID `{model_name}` is incorrect or unavailable\\n\"\n",
    "            f\"- The request was rejected (content policy / token limit)\\n\"\n",
    "            f\"- Ollama: model may not be pulled ‚Äî run `ollama pull {model_name}`\"\n",
    "        )\n",
    "\n",
    "    header = f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n\"\n",
    "    return header + raw.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell7",
   "metadata": {},
   "source": [
    "---\n",
    "## Orchestration\n",
    "`generate_code` is the single entry point called by Gradio.\n",
    "It wires together all the module functions in sequence and streams output progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1eafc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(\n",
    "    model_source: str,\n",
    "    model_name: str,\n",
    "    task_description: str,\n",
    "    pasted_code: str,\n",
    "    temperature: float\n",
    "):\n",
    "    \"\"\"Orchestrate the full code-generation pipeline for a single model.\"\"\"\n",
    "\n",
    "   \n",
    "    if not task_description.strip():\n",
    "        yield \"> ‚ö†Ô∏è Please enter a task description before generating.\"\n",
    "        return\n",
    "\n",
    "    uploaded_code = pasted_code or \"\"\n",
    "   \n",
    "    prompt = build_prompt(task_description, uploaded_code)\n",
    "\n",
    "    source_label = \"‚òÅÔ∏è OpenRouter\" if model_source == \"OpenRouter\" else \"üñ•Ô∏è Ollama\"\n",
    "    stream_fn    = stream_openrouter if model_source == \"OpenRouter\" else stream_ollama\n",
    "\n",
    "    yield f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n‚è≥ *Connecting to model...*\"\n",
    "\n",
    "    try:\n",
    "        raw = \"\"\n",
    "        chunk_count = 0\n",
    "        for raw in stream_fn(model_name, prompt, temperature):\n",
    "            chunk_count += 1\n",
    "            yield (\n",
    "                f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n\"\n",
    "                f\"‚è≥ *Generating... ({len(raw)} chars)*\\n\\n```json\\n{raw}\\n```\"\n",
    "            )\n",
    "\n",
    "        if not raw.strip():\n",
    "            yield (\n",
    "                f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n\"\n",
    "                f\"> ‚ö†Ô∏è **Model returned an empty response.**\\n\\n\"\n",
    "                f\"Possible causes:\\n\"\n",
    "                f\"- Model ID `{model_name}` is incorrect or unavailable\\n\"\n",
    "                f\"- The request was rejected (content policy / token limit)\\n\"\n",
    "                f\"- Ollama: model may not be pulled ‚Äî run `ollama pull {model_name}`\\n\\n\"\n",
    "                f\"Try a different model or check your OpenRouter credits.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        yield (\n",
    "            f\"## ü§ñ {model_name}\\n> {source_label}\\n\\n---\\n\\n\"\n",
    "            f\"> ‚ùå **Stream error:** `{e}`\\n\\n\"\n",
    "            f\"Check that the model ID is correct and your API key is valid.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    \n",
    "    yield format_output(raw, model_name, source_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell8",
   "metadata": {},
   "source": [
    "---\n",
    "## UI Helpers & Model Lists\n",
    "Defines the available models and the Gradio callback that swaps the\n",
    "dropdown list when the user changes provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ui_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model lists\n",
    "\n",
    "OPENROUTER_MODELS = [\n",
    "    'openai/gpt-oss-120b',\n",
    "    'x-ai/grok-4',\n",
    "    'qwen/qwen3.5-27b',\n",
    "    'anthropic/claude-3.5-haiku',\n",
    "]\n",
    "\n",
    "OLLAMA_MODELS = [\n",
    "    'mistral:7b',\n",
    "    'llama3.2:3b',\n",
    "    'phi4-mini:latest',\n",
    "    'gemma3:270m',\n",
    "]\n",
    "\n",
    "\n",
    "def get_model_list(source: str) -> list:\n",
    "    \"\"\"Return the correct model list for the chosen provider.\n",
    "\n",
    "    Args:\n",
    "        source: 'OpenRouter' or 'Ollama'.\n",
    "\n",
    "    Returns:\n",
    "        List of model ID strings.\n",
    "    \"\"\"\n",
    "    return OPENROUTER_MODELS if source == \"OpenRouter\" else OLLAMA_MODELS\n",
    "\n",
    "\n",
    "def update_model_dropdown(source: str):\n",
    "    \"\"\"Gradio callback: swap the model dropdown when the provider radio changes.\n",
    "\n",
    "    Args:\n",
    "        source: Value from the model_source Radio widget.\n",
    "\n",
    "    Returns:\n",
    "        gr.Dropdown update object with new choices and default value.\n",
    "    \"\"\"\n",
    "    choices = get_model_list(source)\n",
    "    return gr.Dropdown(choices=choices, value=choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cell9",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradio UI & Launch\n",
    "Assembles all components into the final interface and starts the Gradio server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "af2483a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(title='AI Code Generation Assistant', theme=gr.themes.Soft()) as demo:\n",
    "\n",
    "   \n",
    "    gr.Markdown(\"# ü§ñ AI Code Generation Assistant\")\n",
    "    gr.Markdown(\n",
    "        \"Select a provider and model, describe your task, then click **Generate**.\\n\\n\"\n",
    "        \"Optionally upload a `.py`, `.txt`, or `.ipynb` file as context.\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"### ‚öôÔ∏è Model Selection\")\n",
    "        with gr.Row():\n",
    "            model_source = gr.Radio(\n",
    "                choices=[\"OpenRouter\", \"Ollama\"],\n",
    "                value=\"OpenRouter\",\n",
    "                label=\"üîå Provider\",\n",
    "                info=\"OpenRouter = cloud APIs  |  Ollama = local models\"\n",
    "            )\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=OPENROUTER_MODELS,\n",
    "                value=OPENROUTER_MODELS[0],\n",
    "                label=\"üß† Model\",\n",
    "                info=\"Updates automatically when you switch provider\"\n",
    "            )\n",
    "\n",
    "    \n",
    "    model_source.change(\n",
    "        fn=update_model_dropdown,\n",
    "        inputs=model_source,\n",
    "        outputs=model_dropdown\n",
    "    )\n",
    "\n",
    "   \n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"### üìù Task\")\n",
    "        with gr.Row():\n",
    "            task_input = gr.Textbox(\n",
    "                label=\"Task Description\",\n",
    "                placeholder='e.g. \"Write a Python function to normalise a CSV column\"',\n",
    "                lines=4,\n",
    "                scale=3\n",
    "            )\n",
    "            code_input = gr.Code(\n",
    "            label=\"üìÇ Paste Code (optional ‚Äî .py / .txt / .ipynb content)\",\n",
    "            language=\"python\",\n",
    "            lines=10,\n",
    "            scale=1\n",
    "        )\n",
    "\n",
    "   \n",
    "    with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
    "        temperature = gr.Slider(\n",
    "            minimum=0.0,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.2,\n",
    "            label=\"üé® Temperature\",\n",
    "            info=\"Lower = more deterministic  |  Higher = more creative\"\n",
    "        )\n",
    "\n",
    "   \n",
    "    generate_btn = gr.Button(\"üöÄ Generate Code\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "\n",
    "    gr.Markdown(\"### üìÑ Output\")\n",
    "    output_panel = gr.Markdown(\n",
    "        value=\"*Output will appear here after generation.*\"\n",
    "    )\n",
    "\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=generate_code,\n",
    "        inputs=[model_source, model_dropdown, task_input, code_input, temperature],\n",
    "        outputs=[output_panel]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "demo.launch(ssr_mode=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
