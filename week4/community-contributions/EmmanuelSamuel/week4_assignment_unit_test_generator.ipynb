{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Week 4 Assignment: Unit Test Code Generator\n",
    "\n",
    "This tool generates comprehensive unit tests for Python code covering:\n",
    "- **Positive cases**: Normal, expected behavior\n",
    "- **Negative cases**: Error handling and invalid inputs\n",
    "- **Edge cases**: Boundary conditions and special values\n",
    "- **Stress testing**: Performance with large inputs\n",
    "\n",
    "Use frontier models to generate high-quality pytest test cases that can be run directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "import contextlib\n",
    "import unittest\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# pytest is required to run generated tests\n",
    "try:\n",
    "    import pytest\n",
    "    print(f\"pytest found: {pytest.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"WARNING: pytest not installed. Generated tests will fail to run.\")\n",
    "    print(\"From the project root (llm_engineering) run: uv sync\")\n",
    "    print(\"Then restart the kernel and ensure this notebook uses that Python (e.g. select the '.venv' kernel).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59863df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)\n",
    "openrouter = OpenAI(api_key=openrouter_api_key, base_url=openrouter_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gpt-5\",\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    \"grok-4\",\n",
    "    \"gemini-2.5-pro\",\n",
    "    \"qwen2.5-coder\",\n",
    "    \"deepseek-coder-v2\",\n",
    "    \"gpt-oss:20b\",\n",
    "    \"qwen/qwen3-coder-30b-a3b-instruct\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "]\n",
    "\n",
    "clients = {\n",
    "    \"gpt-5\": openai,\n",
    "    \"claude-sonnet-4-5-20250929\": anthropic,\n",
    "    \"grok-4\": grok,\n",
    "    \"gemini-2.5-pro\": gemini,\n",
    "    \"openai/gpt-oss-120b\": groq,\n",
    "    \"qwen2.5-coder\": ollama,\n",
    "    \"deepseek-coder-v2\": ollama,\n",
    "    \"gpt-oss:20b\": ollama,\n",
    "    \"qwen/qwen3-coder-30b-a3b-instruct\": openrouter,\n",
    "}\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "# models = [\"gpt-5-nano\", \"claude-haiku-4-5\", \"gemini-2.5-flash-lite\"]\n",
    "# clients = {\"gpt-5-nano\": openai, \"claude-haiku-4-5\": anthropic, \"gemini-2.5-flash-lite\": gemini}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0a437",
   "metadata": {},
   "source": [
    "## Unit Test Generation Prompts\n",
    "\n",
    "These prompts instruct the model to generate comprehensive tests covering all test categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert Python testing engineer specializing in comprehensive test coverage.\n",
    "Your task is to generate extensive, runnable unit tests for the given Python code.\n",
    "\n",
    "MANDATORY REQUIREMENTS:\n",
    "\n",
    "1. **Test Coverage Categories** - You MUST include ALL four types:\n",
    "   a) POSITIVE CASES: Normal, expected behavior with valid inputs\n",
    "   b) NEGATIVE CASES: Error handling, exceptions, invalid inputs\n",
    "   c) EDGE CASES: Boundary conditions, empty inputs, None values, special characters\n",
    "   d) STRESS TESTING: Large inputs, performance under load, memory efficiency\n",
    "\n",
    "2. **Technical Requirements**:\n",
    "   - Use pytest framework with assert statements\n",
    "   - Use pytest.raises() for exception testing\n",
    "   - Use pytest.mark.parametrize for multiple test cases\n",
    "   - Add fixtures for complex setup if needed\n",
    "   - Mock external dependencies (requests, databases, file I/O) using unittest.mock\n",
    "\n",
    "3. **Code Structure**:\n",
    "   - Include the original code first (copy it exactly)\n",
    "   - Then add all test code below\n",
    "   - Group tests by category with clear comments\n",
    "   - Each test function should have a descriptive docstring\n",
    "\n",
    "4. **Output Format**:\n",
    "   - Respond ONLY with Python code\n",
    "   - NO markdown code fences\n",
    "   - NO explanations outside of Python comments\n",
    "   - All code must be executable as a single block\n",
    "\n",
    "5. **Test Organization**:\n",
    "   - Start with: # ============ POSITIVE TEST CASES ============\n",
    "   - Then: # ============ NEGATIVE TEST CASES ============\n",
    "   - Then: # ============ EDGE CASES ============\n",
    "   - Finally: # ============ STRESS TESTS ============\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python_code: str) -> str:\n",
    "    return f\"\"\"Generate comprehensive pytest unit tests for this Python code.\n",
    "\n",
    "You MUST include tests for ALL four categories:\n",
    "1. Positive cases (normal expected behavior)\n",
    "2. Negative cases (error handling and exceptions)\n",
    "3. Edge cases (boundary conditions, empty values, None)\n",
    "4. Stress tests (large inputs, performance)\n",
    "\n",
    "Include the original code in your response first, then all test code.\n",
    "Respond only with Python code that can be executed directly.\n",
    "\n",
    "Python code to test:\n",
    "\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python_code: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python_code)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(reply: str) -> str:\n",
    "    \"\"\"Strip markdown code fences from model output.\"\"\"\n",
    "    reply = reply.strip()\n",
    "    # Remove markdown code fences\n",
    "    reply = re.sub(r'^```python\\s*', '', reply, flags=re.MULTILINE)\n",
    "    reply = re.sub(r'^```\\s*', '', reply, flags=re.MULTILINE)\n",
    "    reply = re.sub(r'```\\s*$', '', reply, flags=re.MULTILINE)\n",
    "    return reply.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unit_tests(model: str, python_code: str) -> str:\n",
    "    \"\"\"Call the chosen model to generate comprehensive unit tests.\"\"\"\n",
    "    if not python_code or not python_code.strip():\n",
    "        return \"# Please paste some Python code first.\"\n",
    "    \n",
    "    client = clients.get(model)\n",
    "    if not client:\n",
    "        return f\"# Error: Unknown model: {model}\"\n",
    "    \n",
    "    try:\n",
    "        kwargs = {\"model\": model, \"messages\": messages_for(python_code)}\n",
    "        \n",
    "        # Add reasoning_effort for OpenAI models (except those via other providers)\n",
    "        if \"gpt\" in model and \"groq\" not in model and \"openrouter\" not in model:\n",
    "            kwargs[\"reasoning_effort\"] = \"high\"\n",
    "        \n",
    "        response = client.chat.completions.create(**kwargs)\n",
    "        reply = response.choices[0].message.content or \"\"\n",
    "        return extract_code(reply)\n",
    "    except Exception as e:\n",
    "        return f\"# Error calling model: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92c12",
   "metadata": {},
   "source": [
    "## Running the Generated Tests\n",
    "\n",
    "Functions to execute the generated test code using pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_ansi(text: str) -> str:\n",
    "    \"\"\"Remove ANSI escape codes for clean output in Gradio.\"\"\"\n",
    "    return re.sub(r\"\\x1b\\[[0-9;]*m\", \"\", text)\n",
    "\n",
    "def _clean_pytest_output(text: str, temp_path: str = \"\") -> str:\n",
    "    \"\"\"Clean up pytest output: strip ANSI and simplify temp file paths.\"\"\"\n",
    "    text = _strip_ansi(text)\n",
    "    if temp_path:\n",
    "        base = os.path.basename(temp_path)\n",
    "        text = text.replace(temp_path, \"generated_tests.py\")\n",
    "        # Replace full relative path\n",
    "        text = re.sub(r\"(\\.\\./)+[^\\s]*\" + re.escape(base), \"generated_tests.py\", text)\n",
    "        # Remove leftover ../ before generated_tests.py\n",
    "        text = re.sub(r\"(\\.\\./)*(\\.\\.)?generated_tests\\.py\", \"generated_tests.py\", text)\n",
    "    return text\n",
    "\n",
    "def _output_to_html(plain_text: str) -> str:\n",
    "    \"\"\"Convert pytest output to HTML with green for PASSED, red for FAILED.\"\"\"\n",
    "    import html\n",
    "    lines = plain_text.splitlines()\n",
    "    out = []\n",
    "    for line in lines:\n",
    "        escaped = html.escape(line)\n",
    "        # Highlight PASSED in green\n",
    "        if \" PASSED\" in line or line.strip().endswith(\"PASSED\"):\n",
    "            escaped = escaped.replace(\"PASSED\", '<span style=\"color:#0a0;font-weight:600\">PASSED</span>')\n",
    "        # Highlight FAILED in red\n",
    "        if \" FAILED\" in line or line.strip().endswith(\"FAILED\"):\n",
    "            escaped = escaped.replace(\"FAILED\", '<span style=\"color:#c00;font-weight:600\">FAILED</span>')\n",
    "        # Summary lines\n",
    "        if \"passed\" in line.lower() and \"=\" in line:\n",
    "            escaped = f'<div style=\"margin-top:0.75em;font-weight:600;color:#0a0;\">{escaped}</div>'\n",
    "        elif \"failed\" in line.lower() and \"=\" in line:\n",
    "            escaped = f'<div style=\"margin-top:0.75em;font-weight:600;color:#c00;\">{escaped}</div>'\n",
    "        out.append(escaped)\n",
    "    return \"<pre style='margin:0;font-family:monospace;font-size:0.9em;line-height:1.4;'>\" + \"\\n\".join(out) + \"</pre>\"\n",
    "\n",
    "def run_unit_tests(code: str) -> str:\n",
    "    \"\"\"Execute the combined code (source + tests) using pytest.\"\"\"\n",
    "    if not code or not code.strip():\n",
    "        return \"No code to run.\"\n",
    "    \n",
    "    buffer = io.StringIO()\n",
    "    temp_path = \"\"\n",
    "    \n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buffer), contextlib.redirect_stderr(buffer):\n",
    "            # Ensure pytest is available\n",
    "            try:\n",
    "                import pytest\n",
    "            except ImportError:\n",
    "                buffer.write(\n",
    "                    \"pytest is not installed in this Python environment.\\n\\n\"\n",
    "                    \"Fix: From the project root folder (llm_engineering) run:\\n\"\n",
    "                    \"  uv sync\\n\\n\"\n",
    "                    \"Then restart the kernel and select the kernel that uses this project's \"\n",
    "                    \"environment (click kernel name top-right ‚Üí Select Another Kernel ‚Üí \"\n",
    "                    \"choose the one showing '.venv' or 'llm_engineering').\\n\"\n",
    "                )\n",
    "                return buffer.getvalue()\n",
    "            \n",
    "            # Execute the code to define functions and tests\n",
    "            ns = {}\n",
    "            try:\n",
    "                exec(code, ns)\n",
    "            except ModuleNotFoundError as e:\n",
    "                if \"pytest\" in str(e):\n",
    "                    buffer.write(\n",
    "                        \"pytest not found in this environment.\\n\\n\"\n",
    "                        \"Use the project's Python: from llm_engineering folder run 'uv sync', \"\n",
    "                        \"then restart kernel and select the .venv kernel.\\n\"\n",
    "                    )\n",
    "                    return buffer.getvalue()\n",
    "                raise\n",
    "            \n",
    "            # Check for unittest.TestCase subclasses\n",
    "            test_cases = [\n",
    "                obj for obj in ns.values()\n",
    "                if isinstance(obj, type) and issubclass(obj, unittest.TestCase)\n",
    "            ]\n",
    "            \n",
    "            if test_cases:\n",
    "                # Run unittest-style tests\n",
    "                suite = unittest.TestSuite()\n",
    "                for case in test_cases:\n",
    "                    suite.addTests(unittest.defaultTestLoader.loadTestsFromTestCase(case))\n",
    "                runner = unittest.TextTestRunner(stream=buffer, verbosity=2)\n",
    "                runner.run(suite)\n",
    "            else:\n",
    "                # Run pytest-style tests: write to temp file\n",
    "                with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n",
    "                    f.write(code)\n",
    "                    temp_path = f.name\n",
    "                try:\n",
    "                    pytest.main([temp_path, \"-v\", \"--tb=short\", \"-x\"])\n",
    "                finally:\n",
    "                    os.unlink(temp_path)\n",
    "    except Exception as e:\n",
    "        buffer.write(f\"Error executing tests: {e}\")\n",
    "    \n",
    "    out = buffer.getvalue()\n",
    "    return _clean_pytest_output(out, temp_path)\n",
    "\n",
    "def run_unit_tests_html(code: str) -> str:\n",
    "    \"\"\"Run tests and return HTML-formatted output with color highlighting.\"\"\"\n",
    "    plain = run_unit_tests(code)\n",
    "    return _output_to_html(plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734a634",
   "metadata": {},
   "source": [
    "## Example Code to Test\n",
    "\n",
    "Here are some sample functions you can use to test the unit test generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code_simple = '''def add(a, b):\n",
    "    \"\"\"Add two numbers and return the result.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a, b):\n",
    "    \"\"\"Divide a by b. Raises ValueError if b is zero.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "'''\n",
    "\n",
    "example_code_medium = '''def factorial(n):\n",
    "    \"\"\"Calculate factorial of n. Raises ValueError for negative numbers.\"\"\"\n",
    "    if not isinstance(n, int):\n",
    "        raise TypeError(\"Input must be an integer\")\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Factorial not defined for negative numbers\")\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    result = 1\n",
    "    for i in range(2, n + 1):\n",
    "        result *= i\n",
    "    return result\n",
    "\n",
    "def is_palindrome(text):\n",
    "    \"\"\"Check if a string is a palindrome (case-insensitive).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"Input must be a string\")\n",
    "    cleaned = ''.join(c.lower() for c in text if c.isalnum())\n",
    "    return cleaned == cleaned[::-1]\n",
    "'''\n",
    "\n",
    "example_code_complex = '''class BankAccount:\n",
    "    \"\"\"A simple bank account with deposit, withdraw, and balance operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_balance=0):\n",
    "        if initial_balance < 0:\n",
    "            raise ValueError(\"Initial balance cannot be negative\")\n",
    "        self._balance = initial_balance\n",
    "    \n",
    "    def deposit(self, amount):\n",
    "        \"\"\"Deposit money into the account.\"\"\"\n",
    "        if amount <= 0:\n",
    "            raise ValueError(\"Deposit amount must be positive\")\n",
    "        self._balance += amount\n",
    "        return self._balance\n",
    "    \n",
    "    def withdraw(self, amount):\n",
    "        \"\"\"Withdraw money from the account.\"\"\"\n",
    "        if amount <= 0:\n",
    "            raise ValueError(\"Withdrawal amount must be positive\")\n",
    "        if amount > self._balance:\n",
    "            raise ValueError(\"Insufficient funds\")\n",
    "        self._balance -= amount\n",
    "        return self._balance\n",
    "    \n",
    "    def get_balance(self):\n",
    "        \"\"\"Get current account balance.\"\"\"\n",
    "        return self._balance\n",
    "\n",
    "def find_max_subarray_sum(arr):\n",
    "    \"\"\"Find maximum sum of contiguous subarray (Kadane's algorithm).\"\"\"\n",
    "    if not arr:\n",
    "        return 0\n",
    "    max_sum = current_sum = arr[0]\n",
    "    for num in arr[1:]:\n",
    "        current_sum = max(num, current_sum + num)\n",
    "        max_sum = max(max_sum, current_sum)\n",
    "    return max_sum\n",
    "'''\n",
    "\n",
    "print(\"Example codes loaded. Use these in the Gradio interface below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "source": [
    "## Test the Generator (Without Gradio)\n",
    "\n",
    "You can test the generator directly here before launching the Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-direct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple example\n",
    "model = models[0]  # Use first available model\n",
    "generated_tests =  (model, example_code_simple)\n",
    "print(\"Generated Tests:\")\n",
    "print(\"=\"*80)\n",
    "print(generated_tests)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "run-tests-direct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the generated tests\n",
    "if generated_tests and not generated_tests.startswith(\"#\"):\n",
    "    print(\"\\nRunning Tests:\")\n",
    "    print(\"=\"*80)\n",
    "    test_output = run_unit_tests(generated_tests)\n",
    "    print(test_output)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-section",
   "metadata": {},
   "source": [
    "## Gradio Interface\n",
    "\n",
    "Interactive web interface for generating and running unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_generate(model: str, python_code: str) -> str:\n",
    "    \"\"\"Wrapper for Gradio: generate tests.\"\"\"\n",
    "    return generate_unit_tests(model, python_code)\n",
    "\n",
    "def gradio_run_tests(test_code: str) -> str:\n",
    "    \"\"\"Wrapper for Gradio: run tests and return HTML output.\"\"\"\n",
    "    return run_unit_tests_html(test_code)\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Unit Test Generator\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Unit Test Code Generator\n",
    "    \n",
    "    Generate comprehensive unit tests for your Python code covering:\n",
    "    - **Positive Cases**: Normal, expected behavior\n",
    "    - **Negative Cases**: Error handling and invalid inputs  \n",
    "    - **Edge Cases**: Boundary conditions and special values\n",
    "    - **Stress Tests**: Performance with large inputs\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=models,\n",
    "                value=models[0],\n",
    "                label=\"Select Model\",\n",
    "                info=\"Choose which AI model to generate tests\"\n",
    "            )\n",
    "            \n",
    "            input_code = gr.Code(\n",
    "                label=\"Python Code to Test\",\n",
    "                language=\"python\",\n",
    "                lines=20,\n",
    "                placeholder=\"Paste your Python code here...\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                example_btn1 = gr.Button(\"Load Simple Example\", size=\"sm\")\n",
    "                example_btn2 = gr.Button(\"Load Medium Example\", size=\"sm\")\n",
    "                example_btn3 = gr.Button(\"Load Complex Example\", size=\"sm\")\n",
    "            \n",
    "            generate_btn = gr.Button(\"üî® Generate Unit Tests\", variant=\"primary\", size=\"lg\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_tests = gr.Code(\n",
    "                label=\"Generated Unit Tests\",\n",
    "                language=\"python\",\n",
    "                lines=20\n",
    "            )\n",
    "            \n",
    "            run_btn = gr.Button(\"‚ñ∂Ô∏è Run Tests\", variant=\"secondary\", size=\"lg\")\n",
    "            \n",
    "            test_results = gr.HTML(\n",
    "                label=\"Test Results\",\n",
    "                value=\"<p style='color:#666;'>Test output will appear here...</p>\"\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ### Instructions:\n",
    "    1. Choose a model from the dropdown\n",
    "    2. Paste your Python code or load an example\n",
    "    3. Click \"Generate Unit Tests\" to create comprehensive tests\n",
    "    4. Review the generated tests\n",
    "    5. Click \"Run Tests\" to execute them and see results\n",
    "    \n",
    "    The generated tests will include positive, negative, edge, and stress test cases.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Event handlers\n",
    "    example_btn1.click(fn=lambda: example_code_simple, outputs=input_code)\n",
    "    example_btn2.click(fn=lambda: example_code_medium, outputs=input_code)\n",
    "    example_btn3.click(fn=lambda: example_code_complex, outputs=input_code)\n",
    "    \n",
    "    generate_btn.click(\n",
    "        fn=gradio_generate,\n",
    "        inputs=[model_dropdown, input_code],\n",
    "        outputs=output_tests\n",
    "    )\n",
    "    \n",
    "    run_btn.click(\n",
    "        fn=gradio_run_tests,\n",
    "        inputs=output_tests,\n",
    "        outputs=test_results\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
